{"id":"VMK7D8tQvwsLtnxKHzNdF9c4Yhtuqm","title":"Hacker News: Best","displayTitle":"HN","url":"https://hnrss.org/best","feedLink":"https://news.ycombinator.com/best","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":30,"items":[{"title":"Updated practice for review articles and position papers in ArXiv CS category","url":"https://blog.arxiv.org/2025/10/31/attention-authors-updated-practice-for-review-articles-and-position-papers-in-arxiv-cs-category/","date":1762009085,"author":"dw64","guid":195,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45782136"},{"title":"Do you know that there is an HTML tables API?","url":"https://christianheilmann.com/2025/10/08/abandonware-of-the-web-do-you-know-that-there-is-an-html-tables-api/","date":1762001901,"author":"begoon","guid":192,"unread":true,"content":"<p>When people turn data into  tables using JavaScript, they either use the  methods (createElement() and the likes), but most of the time just append a huge string and use innerHTML, which always is a security concern. However, did you know that  tables also have an <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/HTMLTableElement\" title=\"\">old, forgotten </a> ? Using this one, you can loop over tables, create bodies, rows, cells, heads, footers, captions an summaries (yes,  tables have all of those) and access the table cells. Without having to re-render the whole table on each change. Check out the <a href=\"https://codepen.io/codepo8/pen/RNrVPzq?editors=1111\" title=\"\">Codepen</a> to see how you can create a table from a nested array:</p><div><table><tbody><tr><td><pre>let table \nlet b  document.\nlet t  document.\nb.t\ntable.rowri\n  let r  t.ri\n  row.li\n    let c  r.i\n    c. l</pre></td></tr></tbody></table><p>let table = [\n  ['one','two','three'],\n  ['four','five','six']\n];\nlet b = document.body;\nlet t = document.createElement('table');\nb.appendChild(t);\ntable.forEach((row,ri) =&gt; {\n  let r = t.insertRow(ri);\n  row.forEach((l,i) =&gt; {\n    let c = r.insertCell(i);\n    c.innerText = l;  \n  })\n});</p></div><p>You can then access each table cell with an index (with t being a reference to the table):</p><div><table><tbody><tr><td><pre>console.t..</pre></td></tr></tbody></table><p>console.log(t.rows[1].cells[1]);\n// =&gt; &lt;td&gt;five&lt;/td&gt;</p></div><p>You can also delete and create cells and rows, if you want to add a row to the end of the table with a cell, all you need to do is:</p><div><table><tbody><tr><td><pre>t.\nt..\nt...</pre></td></tr></tbody></table><p>t.insertRow(-1);\nt.rows[2].insertCell(0);\nt.rows[2].cells[0].innerText = 'foo';</p></div><p>There are a few things here that are odd – adding a -1 to add a row at the end for example – and there seems to be no way to create a TH element instead of a TD. All table cells are just cells.</p><p>However, seeing how much of a pain it is to create tables, it would be fun to re-visit this  and add more functionality to it. We did add a lot of things to  forms, like formData and the change event, so why not add events and other features to tables. That way they’d finally get the status as data structures and not a hack to layout content on the web.</p>","contentLength":1911,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45781293"},{"title":"You can't refuse to be scanned by ICE's facial recognition app, DHS document say","url":"https://www.404media.co/you-cant-refuse-to-be-scanned-by-ices-facial-recognition-app-dhs-document-says/","date":1761987534,"author":"nh43215rgb","guid":191,"unread":true,"content":"<div>Photos captured by Mobile Fortify will be stored for 15 years, regardless of immigration or citizenship status, the document says.</div>","contentLength":130,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45780228"},{"title":"Hard Rust requirements from May onward","url":"https://lists.debian.org/debian-devel/2025/10/msg00285.html","date":1761982300,"author":"rkta","guid":190,"unread":true,"content":"<pre>Hi all,\n\nI plan to introduce hard Rust dependencies and Rust code into\nAPT, no earlier than May 2026. This extends at first to the\nRust compiler and standard library, and the Sequoia ecosystem.\n\nIn particular, our code to parse .deb, .ar, .tar, and the\nHTTP signature verification code would strongly benefit\nfrom memory safe languages and a stronger approach to\nunit testing.\n\nIf you maintain a port without a working Rust toolchain,\nplease ensure it has one within the next 6 months, or\nsunset the port.\n\nIt's important for the project as whole to be able to\nmove forward and rely on modern tools and technologies\nand not be held back by trying to shoehorn modern software\non retro computing devices.\n\nThank you for your understanding.\n-- \ndebian developer - deb.li/jak | jak-linux.org - free software dev\nubuntu core developer                              i speak de, en\n</pre>","contentLength":874,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45779860"},{"title":"The profitable startup","url":"https://linear.app/now/the-profitable-startup","date":1761967084,"author":"doppp","guid":217,"unread":true,"content":"<p>For years, startups have been taught to prioritize growth over everything else. Profitability was seen as unambitious or even wrong – something to worry about when you hit scale. Why focus on profits when money and valuations were easy to come by?</p><p>But that thinking was always flawed.</p><p>Profitability isn't unambitious; it's controlling your own destiny. It means you don't have to rely on investors for survival. It means you can focus on your unaltered vision and mission. And it means you as a founder decide the pace of growth. And once you experience it, it's hard to imagine doing things any other way.</p><p><a href=\"https://paulgraham.com/ramenprofitable.html\">Paul Graham famously wrote about \"ramen profitability\"</a> – the point where a founding team could survive without external funding. He argued this made startups more attractive to investors, showing they could get customers to pay, were serious about building valuable products, and were disciplined with expenses.</p><p>Graham wrote his essay in 2009. I’d argue that we now live in a world where it’s not just easier to get ramen profitable, but traditionally profitable – while also growing fast.</p><p>At Linear we didn't set out to be profitable but kind of stumbled into it. We believed that to win this market we really needed to build a superior tool. The best way we knew how to do that was to keep the team small and focused. And when we launched after a year in private beta, almost all of our 100 beta users converted to paid customers. To our surprise, we realized it wouldn't take that long to become profitable if we kept the costs in check. Twelve months after launch, we hit profitability, and we've stayed profitable ever since.</p><p>I don't know why hiring massive teams ever became the norm. In my own experience, small teams always delivered better quality, and faster. Maybe it's fear of missing out if you don't grow the team fast. Maybe it's investors whispering that your team is \"understaffed compared to benchmarks.\" Being understaffed compared to benchmarks almost always should be a source of pride, not a problem. People should be surprised how small your team is, not how big it is.</p><p>What holds you back is rarely team size – it's the clarity of your focus, skill and ability to execute. Larger teams mean slower progress, more management overhead, more meetings, more opinions, and usually dilution of vision and standards. Yet growing the team has somehow become a symbol of success.</p><p>At Linear, we hired our first employee after six months and roughly doubled the team each year. With each hire, we make sure they truly elevate the team. We don't set out to hire ten engineers – we hire the next  engineer. This intentional approach has allowed us to maintain both quality and culture.</p><p>The most underrated thing about profitability is how much peace of mind it gives you. Once you're profitable, you stop worrying about survival and focus on what really matters: building something great. Building the way you want. Instead of optimizing for the next fundraising round, you optimize for value creation.</p><p>While profitability might not come quickly for every startup, I believe it's achievable sooner than most think. If you're creating a new market, or truly require massive scale like a social network, or significant upfront investment like a hardware company, it might take longer. But if you're in a category where there isn't hard upfront investment, and you get some level of product-market fit with customers willing to pay, you can probably be profitable. You can decide to become profitable. And usually, it's a decision about how much and how fast you hire.</p><p>Revenue per employee is one of the clearest ways to see you’re hiring appropriately. While some of the best public companies benchmark at $1-2M per employee, for startups it's not unreasonable to target the range of $500k-$1M per employee.</p><p><strong>Understand Your Risk Profile</strong></p><p>Are you building something highly speculative where you're not sure if there's a market for it, or are you building something that already has a market but with a different take on it? In the former case profitability takes longer, but in the latter it could happen right away. Most software today, especially in the B2B space, is about building a modern version of something existing.</p><p><strong>Hire Intentionally and Slower</strong></p><p>For most software startups, ten people before product-market fit should be your ceiling, not your target. After PMF, every hire should address a specific, pressing need – not just fill out an org chart. At Linear, our deliberately slow headcount growth forced us to be selective, which meant making better hires. It also protected our culture, since rapid hiring often dilutes the very things that made your startup special in the first place. When you hire less, you naturally hire better.</p><p>Being profitable doesn't mean you have to be anti-investors. It means you have that choice, and investors are quite interested in profitable companies that also grow fast. You can raise more, less, or nothing. You can wait for the right timing, the right partner, or fund. For most ambitious startups, it can still be a good idea to raise something even if you could get by bootstrapping. Investors can still be helpful, and the additional cash balance can help you to make larger investments, or acquisitions.</p><p>The point is that you can be and are allowed to be profitable as a startup. It's not a bad thing, it's not an oxymoron or as hard as people make it out to be. The secret is that a lot of successful companies actually were quite profitable early on, they just didn't talk about it. When you're profitable, you make decisions based on what's best for your customers and your product, not what's best for impressing investors.</p><p>I didn't set out to build a profitable startup. But once I got there, I realized I wouldn't want to build a company any other way.</p>","contentLength":5825,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45778984"},{"title":"Show HN: Strange Attractors","url":"https://blog.shashanktomar.com/posts/strange-attractors","date":1761953039,"author":"shashanktomar","guid":161,"unread":true,"content":"<p>A few months back, while playing around with <a target=\"_blank\" rel=\"noopener\" href=\"https://threejs.org/\">Three.js</a>, I came across something that completely derailed my plans. Strange attractors - fancy math that creates beautiful patterns. At first I thought I'd just render one and move on, but then soon I realized that this is too much fun. When complexity emerges from three simple equations, when you see something chaotic emerge into beautiful, it's hard not to waste some time. I've spent countless hours, maybe more than I'd care to admit, watching these patterns form. I realized there's something deeply satisfying about seeing order emerge from randomness. Let me show you what kept me hooked.</p><h2><a aria-hidden=\"true\" tabindex=\"-1\" href=\"https://blog.shashanktomar.com/posts/strange-attractors#the-basics-dynamical-systems-and-chaos-theory\"></a>The Basics: Dynamical Systems and Chaos Theory</h2><p>Dynamical Systems are a mathematical way to understand how things . Imagine you have a system, which\ncould be anything from the movement of planets to the growth of a population. In this system, there are rules that\ndetermine how it evolves from one moment to the next. These rules tell you what will happen next based on what is\nhappening now. Some examples are, a pendulum, the weather patterns, a flock of birds, the spread of a virus in a\npopulation (we are all too familiar with this one), and stock market.</p><p>There are two primary things to understand about this system:</p><ul><li>: This is like a big collection of all the possible states the system can be in. Each state is like a\nsnapshot of the system at a specific time. This is also called the  or the .</li><li>: These are the rules that takes one state of the system and moves it to the next state. It can be\nrepresented as a function that transforms the system from now to later.</li></ul><p>For instance, when studying population growth, a phase-space (world-state) might consist of the current population size\nand the rate of growth or decline at a specific time. The dynamics would then be derived from models of population\ndynamics, which, considering factors like birth rates, death rates, and carrying capacity of the environment, dictate\nthe changes in population size over time.</p><p>Another way of saying this is that the dynamical systems describe how things change over time, in a space of\npossibilities, governed by a set of rules. Numerous fields such as biology, physics, economics, and applied mathematics,\nstudy systems like these, focusing on the specific rules that dictate their evolution. These rules are grounded in\nrelevant theories, such as Newtonian mechanics, fluid dynamics, and mathematics of economics, among others.</p><p>There are different ways of classifying dynamical systems, and one of the most interesting is the classification into\nchaotic and non-chaotic systems. The change over time in non-chaotic systems is more deterministic as compared to\nchaotic systems which exhibit randomness and unpredictability.</p><p> is the sub branch of dynamical systems that studies chaotic systems and challenges the traditional\ndeterministic views of causality. Most of the natural systems we observe are chaotic in nature, like the weather, a drop\nof ink dissolving in water, social and economic behaviours etc. In contrast, systems like the movement of planets,\npendulums, and simple harmonic oscillators are extremely predictable and non-chaotic.</p><p>Chaos Theory deals with systems that exhibit irregular and unpredictable behavior over time, even though they follow\ndeterministic rules. Having a set of rules that govern the system, and yet exhibit randomness and unpredictability,\nmight seem a bit contradictory, but it is because the rules do not always represent the whole system. In fact, most of\nthe time, these rules are an approximation of the system and that is what leads to the unpredictability. In complex\nsystems, we do not have enough information to come up with a perfect set of rules. And by using incomplete information\nto make predictions, we introduce uncertainty, which amplifies over time, leading to the chaotic behaviour.</p><p>Chaotic systems generally have many non-linear interacting components, which we partially understand (or can partially\nobserve) and which are very sensitive to small changes. A small change in the initial conditions can lead to a\ncompletely different outcome, a phenomenon known as the . In this post, we will try to see the\nbutterfly effect in action but before that, let's talk about .</p><p>To understand Strange Attractors, let's first understand what an attractor is. As discussed earlier, dynamical systems\nare all about . During this change, the system moves through different possible states (remember the\nphase space jargon?). An attractor is a set of states towards which a system tends to settle over time, or you can say,\ntowards which it is . It's like a magnet that pulls the system towards it.</p><p>For example, think of a pendulum. When you release it, it swings back and forth, but eventually, it comes to rest at the\nbottom. The bottom is the attractor in this case. It's the state towards which the pendulum is attracted.</p><p>This happens due to the system's inherent dynamics, which govern how states in the phase space change. Here are some of\nthe reasons why different states get attracted towards attractors:</p><ul><li>: Attractors are stable states of the system, meaning that once the system reaches them, it tends to stay\nthere. This stability arises from the system's dynamics, which push it towards the attractor and keep it there.</li><li>: Many dynamical systems have dissipative forces, which cause the system to lose energy over time. This\nloss of energy leads the system to settle into a lower-energy state, which often corresponds to an attractor. This is\nwhat happens in the case of the pendulum.</li><li>: In some regions of the phase space, the system's dynamics cause trajectories to converge. This\ncontraction effect means that nearby states will tend to come closer together over time, eventually being drawn\ntowards the attractor.</li></ul><p>Some attractors have complex governing equations that can create unpredictable trajectories or behaviours. These\nnonlinear interactions can result in multiple stable states or periodic orbits, towards which the system evolves. These\ncomplex attractors are categorised as . They are called \"strange\" due to their unique\ncharacteristics.</p><ol><li>: Strange attractors often have a fractal-like structure, meaning they display intricate\npatterns that repeat at different scales. This complexity sets them apart from simpler, regular attractors.</li><li><strong>Sensitive Dependence on Initial Conditions</strong>: Systems with strange attractors are highly sensitive to their initial\nconditions. Small changes in the starting point can lead to vastly different long-term behaviors, a phenomenon known\nas the \"butterfly effect\".</li><li><strong>Unpredictable Trajectories</strong>: The trajectories on a strange attractor never repeat themselves, exhibiting\nnon-periodic motion. The system's behavior appears random and unpredictable, even though it is governed by\ndeterministic rules.</li><li><strong>Emergent Order from Chaos</strong>: Despite their chaotic nature, strange attractors exhibit a form of underlying order.\nPatterns and structures emerge from the seemingly random behavior, revealing the complex dynamics at play.</li></ol><p>You can observe most of these characteristics in the visualisation. The one which is most fascinating to observe is the\nbutterfly effect.</p><blockquote><p>A butterfly can flutter its wings over a flower in China and cause a hurricane in the Caribbean.</p></blockquote><p>One of the defining features of strange attractors is their sensitivity to initial conditions. This means that small\nchanges in the starting state of the system can lead to vastly different long-term behaviors, a phenomenon known as the\n. In chaotic systems, tiny variations in the initial conditions can amplify over time, leading to\ndrastically different outcomes.</p><p>In our visualisation, let's observe this behavior on Thomas Attractor. It is governed by the following equations:</p><p>A small change in the parameter  can lead to vastly different particle trajectories and the overall shape of the\nattractor. Change this value in the control panel and observe the butterfly effect in action.</p><p>There is another way of observing the butterfly effect in this visualisation. Change the  from  to\n in the control panel and observe how the particles move differently in the two cases. The particles\neventually get attracted to the same states but have different trajectories.</p><p>This visualization required rendering a large number of particles using Three.js. To achieve this efficiently, we used a\ntechnique called . This method handles iterative updates of particle systems directly on the GPU,\nminimizing data transfers between the CPU and GPU. It utilizes two frame buffer objects (FBOs) that alternate roles: One\nstores the current state of particles and render them on the screen, while the other calculates the next state.</p><ol><li><p><strong>Setting Up Frame Buffer Objects (FBOs):</strong> We start by creating two FBOs,  and , to hold the current and\nnext state of particles. These buffers store data such as particle positions in RGBA channels, making efficient use\nof GPU resources.</p></li><li><p><strong>Shader Programs for Particle Dynamics:</strong> The shader programs execute on the GPU and apply attractor dynamics to\neach particle. Following is the attractor function which update the particle positions based on the attractor equation.</p></li><li><p><strong>Rendering and Buffer Swapping:</strong> In each frame, the shader computes the new positions based on the attractor's\nequations and stores them in the inactive buffer. After updating, the roles of the FBOs are swapped: The previously\ninactive buffer becomes active, and vice versa.</p></li></ol><p>This combination of efficient shader calculations and the ping-pong technique allows us to render the particle system.</p><p>If you have any comments, please leave them on <strong><a target=\"_blank\" rel=\"noopener\" href=\"https://github.com/shashanktomar/blog-discussion/discussions/1\">this GitHub discussions topic</a></strong>. Sooner or later, I will integrate it with the blog. The  discussion can be found <a href=\"https://news.ycombinator.com/item?id=45777810\">here</a>.</p>","contentLength":9689,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45777810"},{"title":"S.A.R.C.A.S.M: Slightly Annoying Rubik's Cube Automatic Solving Machine","url":"https://github.com/vindar/SARCASM","date":1761951798,"author":"chris_overseas","guid":189,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45777682"},{"title":"Addiction Markets","url":"https://www.thebignewsletter.com/p/addiction-markets-abolish-corporate","date":1761932575,"author":"toomuchtodo","guid":188,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45774640"},{"title":"Use DuckDB-WASM to query TB of data in browser","url":"https://lil.law.harvard.edu/blog/2025/10/24/rethinking-data-discovery-for-libraries-and-digital-humanities/","date":1761932235,"author":"mlissner","guid":216,"unread":true,"content":"<p>Libraries, digital humanities projects, and cultural heritage organizations have long had to perform a balancing act when sharing their collections online, negotiating between access and affordability. Providing robust features for data discovery, such as browsing, filtering, and search, has traditionally required dedicated computing infrastructure such as servers and databases. Ongoing server hosting, regular security and software updates, and consistent operational oversight are expensive and require skilled staff. Over years or decades, budget changes and staff turnover often strand these projects in an unmaintained or nonfunctioning state.</p><p>The alternative, static file hosting, requires minimal maintenance and reduces expenses dramatically. For example, storing gigabytes of data on Amazon S3 may cost $1/month or less. However, static hosting often diminishes the capacity for rich data discovery. Without a dynamic computing layer between the user’s web browser and the source files, data access may be restricted to brittle pre-rendered browsing hierarchies or search functionality that is impeded by client memory limits. Under such barriers, the collection’s discoverability suffers.</p><p>For years, online collection discovery has been stuck between a rock and a hard place: accept the complexity and expense required for a good user experience, or opt for simplicity and leave users to contend with the blunt limitations of a static discovery layer.</p><h2>Why We Explored a New Approach</h2><p>When LIL began thinking about how to provide discovery for the <a href=\"https://source.coop/harvard-lil/gov-data/\">Data.gov Archive</a>, we decided that building a lightweight and easily maintained access point from the beginning would be worth our team’s effort. We wanted to provide low-effort discovery with minimal impact on our resources. We also wanted to ensure that whatever path we chose would encourage, rather than impede, long-term access.</p><p>This approach builds on our recent experience when the Caselaw Access Project (CAP) hit <a href=\"https://lil.law.harvard.edu/blog/2024/03/26/transitions-for-the-caselaw-access-project/\">a transition moment</a>. At that time, we elected to switch <a href=\"http://case.law\">case.law</a> to a static site and to partner with others dedicated to open legal data to provide more feature-rich access.</p><p>CAP includes some 11 TB of data; the Data.gov Archive represents nearly 18 TB, with the catalog metadata alone accounting for about 1 GB. Manually browsing the archive data in its repository, even for a user who knows what she’s looking for, is laborious and time-consuming. Thus we faced a challenge. Could we enable dynamic, scalable discovery of the Data.gov Archive while enjoying the frugality, simplicity, and maintainability of static hosting?</p><h2>Our Experiment: Rich Discovery, No Server Required</h2><p>Recent advancements in client-side data analysis led us to try something new. Tools like <a href=\"https://duckdb.org/docs/api/wasm/\">DuckDB-Wasm</a>, <a href=\"https://phiresky.github.io/blog/2021/hosting-sqlite-databases-on-github-pages/\">sql.js-httpvfs</a>, and <a href=\"https://protomaps.com\">Protomaps</a>, powered by standards such as WebAssembly, web workers, and HTTP range requests, allow users to efficiently query large remote datasets in the browser. Rather than downloading a 2 GB data file into memory, these tools can incrementally retrieve only the relevant parts of the file and process query results locally.</p><p>We developed Data.gov Archive Search on the same model. Here’s how it works:</p><ul><li> We store Data.gov Archive catalog metadata as sorted, compressed Parquet files on Source.coop, taking advantage of performant static file hosting.</li><li> Our client-side web application loads DuckDB-Wasm, a fully functional database engine running inside the user’s browser.</li><li> When a user navigates to a resource or submits a search, our DuckDB-Wasm client executes a targeted retrieval of the data needed to fulfill the request. No dedicated server is required; queries run entirely in the browser.</li></ul><p>This experiment has not been without obstacles. Getting good performance out of this model demands careful data engineering, and the large DuckDB-Wasm binary imposes a considerable latency cost. As of this writing, we’re continuing to explore speedy alternatives like <a href=\"https://github.com/hyparam/hyparquet\">hyparquet</a> and <a href=\"https://idl.uw.edu/arquero\">Arquero</a> to further improve performance.</p><p>Still, we’re pleased with the result: an inexpensive, low-maintenance static discovery platform that allows users to browse, search, and filter Data.gov Archive records entirely in the browser.</p><h2>Why This Matters for Libraries, Digital Humanities Projects, and Beyond</h2><p>This new pattern offers a compelling model for libraries, academic archives, and DH projects of all sizes:</p><ul><li> By shifting from an expensive server to lower cost static storage, projects can sustainably offer their users access to data.</li><li><strong>Reduced technical overhead:</strong> With no dedicated backend server, security risks are reduced, no patching or upgrades are needed, and crashing servers are not a concern.</li><li> Projects can be set up with care, but without demanding constant attention. Organizations can be more confident that their archive and discovery interfaces remain usable and accessible, even as staffing or funding changes over time.</li></ul><p>Knowing that we are not the only group interested in approaching access in this way, we’re sharing our generalized learnings. We see a few ways forward for others in the knowledge and information world:</p><ul><li> If your organization has large, relatively static datasets, consider experimenting with a browser-based search tool using static hosting.</li><li> Template applications, workflows, and lessons learned can help this new pattern gain adoption and maturity across the community.</li></ul><p>This project is still evolving, and we invite others—particularly those in libraries and digital cultural heritage—to explore these possibilities with us. We’re committed to open sharing as we refine our tools, and we welcome collaboration or feedback at <a href=\"mailto:lil@law.harvard.edu\">lil@law.harvard.edu</a>.</p>","contentLength":5646,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45774571"},{"title":"Just use a button","url":"https://gomakethings.com/just-use-a-button/","date":1761929962,"author":"moebrowne","guid":215,"unread":true,"content":"<p>One of the weirdest “debates” I seem to perpetually have with framework-enthusiastic developers is whether or not a  is “just as good” as a .</p><p> it’s not. Let’s dig in.</p><p>Among the React crowd, and also among people who seem to enjoy HTMX, I see a lot this…</p><div><pre><code data-lang=\"html\">\n\tOpen Modal\n</code></pre></div><div><pre><code data-lang=\"js\"></code></pre></div><ol><li>This element does not announce itself as an interactive element to screen reader users.</li><li>You can’t focus on a  with a keyboard.</li><li>The event only fires on , not when the  or  keys are pressed (again, keyboard users).</li></ol><p>I’ve had arguments with <em>a very prominent React thought leader whose name starts with R</em> who insisted that using a  was “more accessible” than using a , and that Twitter made the right decision in using this pattern in their app.</p><p>It’s wrong. It’s all wrong.</p><p>Many HTML elements have  that tell assistive tech like screen readers what they do.</p><p>The  element is one of them. It has an implicit  of , which tells screen reader users it can be interacted with and will trigger some type of behavior in the app.</p><p>The HTML  attribute can be used to add or modify the role of an element. And so, folks like React Ry–thought-leader-guy will say stuff like (I’m paraphrasing)…</p><blockquote><p>That attribute exists for a reason. You can add  to a  to give it the correct semantics.</p></blockquote><p>OK, that addresses one issue.</p><p>That role doesn’t affect focusability (or lack thereof) or keyboard behavior. Visually impaired users and people who navigate with a keyboard still can’t use it.</p><p>“No worries!” they say. “We can fix that, too!”</p><p>You can make the element focusable with the  attribute.</p><div><pre><code data-lang=\"html\">\n\tOpen Modal\n</code></pre></div><p>You , though! Seriously, just don’t fuck with focus order.</p><p>It’s way too easy to go down this path and then fuck it up and have folks jumping all over the page instead of navigating through in the normal and expected order.</p><p>And again, still no keyboard interactivity.</p><p>But don’t fear! You can add that, too. You just need to listen for all  events, and then filter them out by  so that you only run your code if the  or  keys were pressed (the latter means checking for a literal space: ).</p><p>That can’t run on the element, either. You’ve got to attach that even to the  and figure out which element has focus.</p><div><pre><code data-lang=\"js\"></code></pre></div><p>So um… ok, I guess it is technically a fix, but…</p><h2>You’ve just recreated all of the functionality a  gives you for free</h2><p>Seriously, WTF would you do that?!?</p><p>All of these hoops to write this HTML…</p><div><pre><code data-lang=\"html\">\n\tOpen Modal\n</code></pre></div><p>When you could write this HTML instead…</p><div><pre><code data-lang=\"html\">\n\tOpen Modal\n</code></pre></div><ol><li>Has the correct  implicitly.</li><li>Is automatically focusable.</li><li>Fires a  event in response to  and  presses when it has focus.</li></ol><p>Look, I’m a lazy developer.</p><p>And I suspect, if you’re someone who loves tools like React, you probably are, too. It’s cool, I get it! The best code is the code you didn’t write and all that.</p><p>Use the correct element for the job, and avoid writing a bunch of extra code!</p>","contentLength":2834,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45774182"},{"title":"Futurelock: A subtle risk in async Rust","url":"https://rfd.shared.oxide.computer/rfd/0609","date":1761929366,"author":"bcantrill","guid":187,"unread":true,"content":"<div data-lineno=\"424\"><p>Bounded channels are not really the issue here.  Even in omicron#9259, the capacity=1 channel was basically behaving as documented and as one would expect.  It woke up a sender when capacity was available, and the other senders were blocked to maintain the documented FIFO property.  However, some of the patterns that we use with bounded channels are problematic on their own and, if changed, could prevent the channel from getting caught up in a futurelock.</p></div><div data-lineno=\"426\"><p>In Omicron, we commonly use bounded channels with .  The bound is intended to cap memory usage and provide backpressure, but using the blocking  creates a second  queue: the wait queue for the channel.  Instead, we could consider using a larger capacity channel plus  and propagate failure from .</p></div><div data-lineno=\"428\"><p>As an example, when we use the actor pattern, we typically observe that there’s only one actor and potentially many clients, so there’s not much point in buffering messages  the channel.  So we use  and let clients block in .  But we could instead have  and have clients use  and propagate failure if they’re unable to send the message.  The value  here is pretty arbitrary.  You want it to be large enough to account for an expected amount of client concurrency, but not larger.  If the value is too small, you’ll wind up with spurious failures when the client could have just waited a bit longer.  If the value is too large, you can wind up queueing so much work that the actor is always behind (and clients are potentially even timing out at a higher level).  One might observe:</p></div><div data-lineno=\"430\"><div data-lineno=\"1\"><p>Channel limits, channel limits: always wrong!</p></div><div data-lineno=\"3\"><p>Some too short and some too long!</p></div></div><div data-lineno=\"434\"><p>But as with timeouts, it’s often possible to find values that work in practice.</p></div><div data-lineno=\"436\"><p>Using  is  a mitigation because this still results in the sender blocking.  It needs to be polled after the timeout expires in order to give up.  But with futurelock, it will never be polled.</p></div>","contentLength":1894,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45774086"},{"title":"Another European agency shifts off US Tech as digital sovereignty gains steam","url":"https://www.zdnet.com/article/another-european-agency-ditches-big-tech-as-digital-sovereignty-movement-gains-steam/","date":1761928762,"author":"CrankyBear","guid":214,"unread":true,"content":"<ul><li>Austria's Ministry of Economy has migrated to a Nextcloud platform.</li><li>It's the latest move in a European trend to shift away from Big Tech.</li><li>European governments and agencies want to control sensitive data.</li></ul><p>This shift away from proprietary, foreign-owned cloud services, such as <a href=\"https://www.zdnet.com/article/how-you-can-get-microsoft-365-formerly-office-for-free-3-easy-ways/\">Microsoft 365</a>, to an <a href=\"https://www.zdnet.com/article/what-is-open-source-and-how-does-it-benefit-you/\">open-source</a>, European-based cloud service aligns with a growing trend among European governments and agencies. They want control over sensitive data and to declare their independence from US-based tech providers.&nbsp;</p><p>European companies are encouraging this trend. Many of them have joined forces in the newly created non-profit foundation, the EuroStack Initiative. This foundation's goal is \" to organize action, not just talk, around the pillars of the initiative: <a href=\"https://www.linkedin.com/posts/euro-stack_foundation-press-release-activity-7389635091049840641-J3gZ?utm_source=share&amp;utm_medium=member_desktop&amp;rcm=ACoAAAAAKH4BBvA-ZwpVFbaZDTqwLgneEpGsrHQ\" target=\"_blank\" rel=\"noopener nofollow\">Buy European, Sell European, Fund European</a>.\"&nbsp;</p><p>What's the motive behind these moves away from proprietary tech? Well, in Austria's case, Florian Zinnagl, CISO of the Ministry of Economy, Energy, and Tourism (BMWET), explained, \"We carry responsibility for a large amount of sensitive data -- from employees, companies, and citizens. As a public institution, we take this responsibility very seriously. That's why we view it critically to rely on cloud solutions from non-European corporations for processing this information.\"</p><p>All of these organizations aim to keep data storage and processing within national or European borders to enhance security, comply with privacy laws such as the <a href=\"https://www.zdnet.com/article/gdpr-an-executive-guide-to-what-you-need-to-know/\">EU's General Data Protection Regulation (GDPR)</a>, and mitigate risks from potential commercial and foreign government surveillance.&nbsp;</p><p>Open-source software is seen as combining the virtues of faster development and better security, while providing companies and governments with more control, as general manager Thierry Carrez of the <a href=\"https://openinfra.org/\" target=\"_blank\" rel=\"noopener nofollow\">OpenInfra Foundation</a> recently suggested: \"Open infrastructure allows nations and organizations to maintain control over their applications, their data, and their destiny while benefiting from global collaboration.\" &nbsp;</p><p>While the US may not like it, with NextCloud's help, BMWET completed its migration in just four months. Although BMWET had already begun adopting Microsoft 365 and <a href=\"https://www.zdnet.com/article/microsoft-copilot-is-taking-over-teams-heres-how-ai-will-shape-your-daily-workflow/\">Teams</a>&nbsp;before the project's start, the shift was still considered a success. That's because instead of reversing its path, the ministry implemented a hybrid architecture: Nextcloud handles internal collaboration and secure data management, while Teams remains available for external meetings.</p><p>The project emphasized integration with existing workflows, including seamless integration with Outlook email and calendar via Sendent's Outlook app. This approach minimized disruption and ensured user acceptance. However, not all migrations progress so well.&nbsp;</p><p>For example, in Austria, the <a href=\"https://www.derstandard.de/story/3000000293383/prozentzeichen-statt-bundesadler-wie-die-justiz-ihre-it-umstellt\" target=\"_blank\" rel=\"noopener nofollow\">Ministry of Justice decided to replace Office with LibreOffice</a>. Yet the transition has run into trouble. It appears that the move of 20,000 desktops, which was prompted by a desire to reduce spending on Microsoft licenses, has been, as one person reported, an \"unprofessional, rushed operation.\" Some offices are still on Office, others on LibreOffice, and they're running into incompatible document format problems and misfires in e-mail systems.&nbsp;</p><p>The moral of the story is that any switch from one software suite to another requires careful handling by the IT department and helpdesk staff. Otherwise, you end up with unhappy users.</p><p>That said, BMWET's bold shift to Nextcloud appears to have gone well. This initiative demonstrates that adopting sovereign cloud solutions can be practical, user-friendly, and rapid in the public sector. However, as Austria's Justice Ministry experience has shown, simply shifting to an open-source approach without careful planning can get in the way of getting work done.&nbsp;</p>","contentLength":3746,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45773974"},{"title":"AI scrapers request commented scripts","url":"https://cryptography.dog/blog/AI-scrapers-request-commented-scripts/","date":1761925459,"author":"ColinWright","guid":213,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45773347"},{"title":"Ask HN: Who uses open LLMs and coding assistants locally? Share setup and laptop","url":"https://news.ycombinator.com/item?id=45771870","date":1761917995,"author":"threeturn","guid":212,"unread":true,"content":"Dear Hackers,\nI’m interested in your real-world workflows for using open-source LLMs and open-source coding assistants on your laptop (not just cloud/enterprise SaaS). Specifically:<p>Which model(s) are you running (e.g., Ollama, LM Studio, or others) and which open-source coding assistant/integration (for example, a VS Code plugin) you’re using?</p><p>What laptop hardware do you have (CPU, GPU/NPU, memory, whether discrete GPU or integrated, OS) and how it performs for your workflow?</p><p>What kinds of tasks you use it for (code completion, refactoring, debugging, code review) and how reliable it is (what works well / where it falls short).</p><p>I'm conducting my own investigation, which I will be happy to share as well when over.</p>","contentLength":723,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45771870"},{"title":"Attention lapses due to sleep deprivation due to flushing fluid from brain","url":"https://news.mit.edu/2025/your-brain-without-sleep-1029","date":1761916463,"author":"gmays","guid":211,"unread":true,"content":"<p>Nearly everyone has experienced it: After a night of poor sleep, you don’t feel as alert as you should. Your brain might seem foggy, and your mind drifts off when you should be paying attention.</p><p>A new study from MIT reveals what happens inside the brain as these momentary failures of attention occur. The scientists found that during these lapses, a wave of cerebrospinal fluid (CSF) flows out of the brain — a process that typically occurs during sleep and helps to wash away waste products that have built up during the day. This flushing is believed to be necessary for maintaining a healthy, normally functioning brain.</p><p>When a person is sleep-deprived, it appears that their body attempts to catch up on this cleansing process by initiating pulses of CSF flow. However, this comes at a cost of dramatically impaired attention.</p><p>“If you don’t sleep, the CSF waves start to intrude into wakefulness where normally you wouldn’t see them. However, they come with an attentional tradeoff, where attention fails during the moments that you have this wave of fluid flow,” says Laura Lewis, the Athinoula A. Martinos Associate Professor of Electrical Engineering and Computer Science, a member of MIT’s Institute for Medical Engineering and Science and the Research Laboratory of Electronics, and an associate member of the Picower Institute for Learning and Memory.</p><p>Lewis is the senior author of the study, which <a href=\"https://www.nature.com/articles/s41593-025-02098-8\" target=\"_blank\">appears today</a> in . MIT visiting graduate student Zinong Yang&nbsp;is the lead author of the paper.</p><p>Although sleep is a critical biological process, it’s not known exactly why it is so important. It appears to be essential for maintaining alertness, and it has been well-documented that sleep deprivation leads to impairments of attention and other cognitive functions.</p><p>During sleep, the cerebrospinal fluid that cushions the brain helps to remove waste that has built up during the day. In a 2019&nbsp;<a href=\"https://www.science.org/doi/10.1126/science.aax5440\" target=\"_blank\">study</a>, Lewis and colleagues showed that CSF flow during sleep follows a rhythmic pattern in and out of the brain, and that these flows are linked to changes in brain waves during sleep.</p><p>That finding led Lewis to wonder what might happen to CSF flow after sleep deprivation. To explore that question, she and her colleagues recruited 26 volunteers who were tested twice — once following a night of sleep deprivation in the lab, and once when they were well-rested.</p><p>In the morning, the researchers monitored several different measures of brain and body function as the participants performed a task that is commonly used to evaluate the effects of sleep deprivation.</p><p>During the task, each participant wore an electroencephalogram (EEG) cap that could record brain waves while they were also in a functional magnetic resonance imaging (fMRI) scanner. The researchers used a modified version of fMRI that allowed them to measure not only blood oxygenation in the brain, but also the flow of CSF in and out of the brain. They also measured each subject’s heart rate, breathing rate, and pupil diameter.</p><p>The participants performed two attentional tasks while in the fMRI scanner, one visual and one auditory. For the visual task, they had to look at a screen that had a fixed cross. At random intervals, the cross would turn into a square, and the participants were told to press a button whenever they saw this happen. For the auditory task, they would hear a beep instead of seeing a visual transformation.</p><p>Sleep-deprived participants performed much worse than well-rested participants on these tasks, as expected. Their response times were slower, and for some of the stimuli, the participants never registered the change at all.</p><p>During these momentary lapses of attention, the researchers identified several physiological changes that occurred at the same time. Most significantly, they found a flux of CSF out of the brain just as those lapses occurred. After each lapse, CSF flowed back into the brain.</p><p>“The results are suggesting that at the moment that attention fails, this fluid is actually being expelled outward away from the brain. And when attention recovers, it’s drawn back in,” Lewis says.</p><p>The researchers hypothesize that when the brain is sleep-deprived, it begins to compensate for the loss of the cleansing that normally occurs during sleep, even though these pulses of CSF flow&nbsp;come with the cost of attention loss.</p><p>“One way to think about those events is because your brain is so in need of sleep, it tries its best to enter into a sleep-like state to restore some cognitive functions,” Yang says. “Your brain’s fluid system is trying to restore function by pushing the brain to iterate between high-attention and high-flow states.”</p><p>The researchers also found several other physiological events linked to attentional lapses, including decreases in breathing and heart rate, along with constriction of the pupils. They found that pupil constriction began about 12 seconds before CSF flowed out of the brain, and pupils dilated again after the attentional lapse.</p><p>“What’s interesting is it seems like this isn’t just a phenomenon in the brain, it’s also a body-wide event. It suggests that there’s a tight coordination of these systems, where when your attention fails, you might feel it perceptually and psychologically, but it’s also reflecting an event that’s happening throughout the brain and body,” Lewis says.</p><p>This close linkage between disparate events may indicate that there is a single circuit that controls both attention and bodily functions such as fluid flow, heart rate, and arousal, according to the researchers.</p><p>“These results suggest to us that there’s a unified circuit that’s governing both what we think of as very high-level functions of the brain — our attention, our ability to perceive and respond to the world — and then also really basic fundamental physiological processes like fluid dynamics of the brain, brain-wide blood flow, and blood vessel constriction,” Lewis says.</p><p>In this study, the researchers did not explore what circuit might be controlling this switching, but one good candidate, they say, is the noradrenergic system. Recent research has shown that this system, which regulates many cognitive and bodily functions through the neurotransmitter norepinephrine, oscillates during normal sleep.</p><p>The research was funded by the National Institutes of Health, a National Defense Science and Engineering Graduate Research Fellowship, a NAWA Fellowship, a McKnight Scholar Award, a Sloan Fellowship, a Pew Biomedical Scholar Award, a One Mind Rising Star Award, and the Simons Collaboration on Plasticity in the Aging Brain.</p>","contentLength":6622,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45771636"},{"title":"How OpenAI uses complex and circular deals to fuel its multibillion-dollar rise","url":"https://www.nytimes.com/interactive/2025/10/31/technology/openai-fundraising-deals.html","date":1761915826,"author":"reaperducer","guid":210,"unread":true,"content":"<p>Sam Altman, the chief executive of OpenAI, says that technological revolutions are driven by more than just technology. They are also driven, he argues, by new ways of paying for them.</p><p>“There is always a lot of focus on technological innovation. What really drives a lot of progress is when people also figure out how to innovate on the financial model,” he recently said at the site of a data center that OpenAI is building in Abilene, Texas.</p><p>Over the last several years, Mr. Altman’s company has found unusual and creative ways of paying for the computing power needed to fuel its ambitions.</p><p>Many of the deals OpenAI has struck — with chipmakers, cloud computing companies and others — are strangely circular. OpenAI receives billions from tech companies before sending those billions back to the same companies to pay for computing power and other services.</p><p>Industry experts and financial analysts have welcomed the start-up’s creativity. But these unorthodox arrangements have also fueled concerns that OpenAI is helping to inflate a potential financial bubble as it builds what is still a highly speculative technology.</p><p>Here are unusual financial agreements helping to drive the ambitions of OpenAI, the poster child of the artificial intelligence revolution.</p><figure></figure><p>From 2019 through 2023, Microsoft was OpenAI’s primary investor. The tech giant pumped more than  into the start-up. Then OpenAI funneled most of those billions back into Microsoft, buying  needed to fuel the development of new A.I. technologies.</p><p>(The New York Times has <a href=\"https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.html\">sued</a> OpenAI and Microsoft, claiming copyright infringement of news content related to A.I. systems. The two companies have denied the suit’s claims.)</p><figure></figure><p>By the summer of last year, OpenAI could not get all the computing power it wanted from Microsoft. So it started signing cloud computing contracts with other companies, including Oracle and little-known start-ups with names like CoreWeave.</p><p>Across three different deals signed this year, OpenAI agreed to pay CoreWeave, a company that builds A.I. data centers, more than  for computing power. As part of these agreements, OpenAI received  in CoreWeave stock, which could ultimately help pay for this computing power.</p><figure></figure><p>OpenAI also struggled to get the additional investment dollars it wanted from Microsoft. So, it turned to other investors. Earlier this year, the Japanese conglomerate SoftBank led a  investment in OpenAI.</p><p>At the same time, OpenAI has been working with various companies to build its own computing data centers, rather than rely on cloud computing deals. This also includes SoftBank, which is known for highly speculative technological bets that don’t always pay off. The company is raising  to help OpenAI build data centers in Texas and Ohio.</p><figure></figure><p>Similarly, Oracle, a software and cloud computing giant, has agreed to spend  building new data centers for OpenAI in Texas, New Mexico, Michigan and Wisconsin. OpenAI will then pay Oracle roughly the same amount to use these  over the next several years.</p><figure></figure><p>The United Arab Emirates was part of an OpenAI’s fund-raising round in October 2024. Now, G42, a firm with close ties to the Emirati government, is building a roughly  for OpenAI in the Emirates.</p><figure></figure><p>Last month, Nvidia announced that it intended to invest  in OpenAI over the next several years. This could help OpenAI pay for its new data centers. As OpenAI buys or leases specialized chips from Nvidia, Nvidia will pump billions back into OpenAI.</p><figure></figure><p>Two weeks later, OpenAI signed an agreement with AMD that allows OpenAI to buy up to  in the chipmaker at a penny per share. That translates to roughly a 10 percent stake in the company. This stock could supply OpenAI with additional capital as it works to build new data centers.</p><p>OpenAI pulls in billions of dollars in revenue each year from customers who pay for ChatGPT, computer programming tools and other technologies. But it still loses more money than it makes, according to a person familiar with the company’s finances.</p><p>If the company can use its new data centers to significantly improve A.I. technologies and expand its revenue over the next several years, it can become a viable business, as Mr. Altman believes it will. If technology progress stalls, OpenAI – and its many partners – could lose enormous amounts of money. Smaller companies like CoreWeave, which are taking on enormous amounts of debt to build new data centers, could go bankrupt.</p><p>In some cases, companies are hedging their bets. Nvidia and AMD, for instance, have the option of reducing the cash and stock they send to OpenAI if the A.I. market does not expand as quickly as expected. But others would be left with enormous debt, which could send ripples across the larger economy.</p>","contentLength":4718,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45771538"},{"title":"The cryptography behind electronic passports","url":"https://blog.trailofbits.com/2025/10/31/the-cryptography-behind-electronic-passports/","date":1761910421,"author":"tatersolid","guid":209,"unread":true,"content":"<p>Did you know that most modern passports are actually embedded devices containing an entire filesystem, access controls, and support for several cryptographic protocols? Such passports display a small symbol indicating an electronic machine-readable travel document (eMRTD), which digitally stores the same personal data printed in traditional passport booklets in its embedded filesystem. Beyond allowing travelers in some countries to skip a chat at border control, these documents use cryptography to prevent unauthorized reading, eavesdropping, forgery, and copying.</p><p>This blog post describes how electronic passports work, the threats within their threat model, and how they protect against those threats using cryptography. It also discusses the implications of using electronic passports for novel applications, such as zero-knowledge identity proofs. Like many widely used electronic devices with long lifetimes, electronic passports and the systems interacting with them support insecure, legacy protocols that put passport holders at risk for both standard and novel use cases.</p><h2>Electronic passport basics</h2><p>A passport serves as official identity documentation, primarily for international travel. The International Civil Aviation Organization (ICAO) defines the standards for electronic passports, which (as suggested by the “Chip Inside” symbol) contain a contactless integrated circuit (IC) storing digital information. Essentially, the chip contains a filesystem with some access control to protect unauthorized reading of data. The full technical details of electronic passports are specified in <a href=\"https://www.icao.int/publications/doc-series/doc-9303\">ICAO Doc 9303</a>; this blog post will mostly focus on part 10, which specifies the logical data structure (LDS), and part 11, which specifies the security mechanisms.</p><p>The filesystem architecture is straightforward, comprising three file types: master files (MFs) serving as the root directory; dedicated files (DFs) functioning as subdirectories or applications; and elementary files (EFs) containing actual binary data. As shown in the above figure, some files are mandatory, whereas others are optional. This blog post will focus on the eMRTD application. The other applications are part of LDS 2.0, which would allow the digital storage of travel records (digital stamps!), electronic visas, and additional biometrics (so you can just update your picture instead of getting a whole new passport!).</p><h3>How the eMRTD application works</h3><p>The following figure shows the types of files the eMRTD contains:</p><p>There are generic files containing common or security-related data; all other files are so-called data groups (DGs), which primarily contain personal information (most of which is also printed on your passport) and some additional security data that will become important later. All electronic passports must contain DGs 1 and 2, whereas the rest is optional.</p><p>Comparing the contents of DG1 and DG2 to the main passport page shows that most of the written data is stored in DG1 and the photo is stored in DG2. Additionally, there are two lines of characters at the bottom of the page called the machine readable zone (MRZ), which contains another copy of the DG1 data with some check digits, as shown in the following picture.</p><h2>Digging into the threat model</h2><p>Electronic passports operate under a straightforward threat model that categorizes attackers based on physical access: those who hold a passport versus those who don’t. If you are near a passport but you do not hold it in your possession, you should not be able to do any of the following:</p><ul><li>Read any personal information from that passport</li><li>Eavesdrop on communication that the passport has with legitimate terminals</li><li>Figure out whether it is a specific passport so you can trace its movements</li></ul><p>Even if you do hold one or more passports, you should not be able to do the following:</p><ul><li>Forge a new passport with inauthentic data</li><li>Make a digital copy of the passport</li><li>Read the fingerprint (DG3) or iris (DG4) information</li></ul><p>Electronic passports use short-range RFID for communication (ISO 14443). You can communicate with a passport within a distance of 10–15 centimeters, but eavesdropping is possible at distances of several meters. Because electronic passports are embedded devices, they need to be able to withstand attacks where the attacker has physical access to the device, such as elaborate side-channel and fault injection attacks. As a result, they are often certified (e.g., under Common Criteria).</p><p>We focus here on the threats against the electronic components of the passport. Passports have many physical countermeasures, such as visual effects that become visible under certain types of light. Even if someone can break the electronic security that prevents copying passports, they would still have to defeat these physical measures to make a full copy of the passport. That said, some systems (such as online systems) only interact digitally with the passport, so they do not perform any physical checks at all.</p><p>The earliest electronic passports lacked most cryptographic mechanisms. Malaysia issued the first electronic passport in 1998, which predates the first ICAO eMRTD specifications from 2003. Belgium subsequently issued the first ICAO-compliant eMRTD in 2004, which in turn predates the first cryptographic mechanism for confidentiality specified in 2005.</p><p>While we could focus solely on the most advanced cryptographic implementations, electronic passports remain in circulation for extended periods (typically 5–10 years), meaning legacy systems continue operating alongside modern solutions. This means that there are typically many old passports floating around that do not support the latest and greatest access control mechanisms. Similarly, not all inspection systems/terminals support all of the protocols, which means passports potentially need to support multiple protocols. All protocols discussed in the following are described in more detail in ICAO Doc 9303 Part 11.</p><p>Legacy protection mechanisms for electronic passports provide better security than what they were replacing (nothing), even though they have key shortcomings regarding confidentiality and (to a lesser extent) copying.</p><h4>Legacy confidentiality protections: How basic access control fails</h4><p>In order to prevent eavesdropping, you need to set up a secure channel. Typically, this is done by deriving a shared symmetric key, either from some shared knowledge, or through a key exchange. However, the passport cannot have its own static public key and send it over the communication channel, because this would enable tracing of specific passports.</p><p>Additionally, it should only be possible to set up this secure channel if you have the passport in your possession. So, what sets holders apart from others? Holders can read the physical passport page that contains the MRZ!</p><p>This brings us to the original solution to set up a secure channel with electronic passports: basic access control (BAC). When you place your passport with the photo page face down into an inspection system at the airport, it scans the page and reads the MRZ. Now, both sides derive encryption and message authentication code (MAC) keys from parts of the MRZ data using SHA-1 as a KDF. Then, they exchange freshly generated challenges and encrypt-then-MAC these challenges together with some fresh keying material to prove that both sides know the key. Finally, they derive session keys from the keying material and use them to set up the secure channel.</p><p>However, BAC fails to achieve any of its security objectives. The static MRZ is just some personal data and does not have very high entropy, which makes it guessable. Even worse, if you capture one valid exchange between passport and terminal, you can brute-force the MRZ offline by computing a bunch of unhardened hashes. Moreover, passive listeners who know the MRZ can decrypt all communications with the passport. Finally, the fact that the passport has to check both the MAC and the challenge has opened up the potential for <a href=\"https://www.ifca.ai/pub/fc10/17_43.pdf\">oracle attacks that allow tracing</a> by replaying valid terminal responses.</p><h4>Forgery prevention: Got it right the first time</h4><p>Preventing forgery is relatively simple. The passport contains a file called the Document Security Object (EF.SOD), which contains a list of hashes of all the Data Groups, and a signature over all these hashes. This signature comes from a key pair that has a certificate chain back to the Country Signing Certificate Authority (CSCA). The private key associated with the CSCA certificate is one of the most valuable assets in this system, because anyone in possession of this private key can issue legitimate passports containing arbitrary data.</p><p>The process of reading the passport, comparing all contents to the SOD, and verifying the signature and certificate chain is called passive authentication (PA). This will prove that the data in the passport was signed by the issuing country. However, it does nothing to prevent the copying of existing passports: anyone who can read a passport can copy its data into a new chip and it will pass PA. While this mechanism is listed among the legacy ones, it meets all of its objectives and is therefore still used without changes.</p><h4>Legacy copying protections: They work, but some issues remain</h4><p>Preventing copying requires having something in the passport that cannot be read or extracted, like the private key of a key pair. But how does a terminal know that a key pair belongs to a genuine passport? Since countries are already signing the contents of the passport for PA, they can just put the public key in one of the data groups (DG15), and use the private key to sign challenges that the terminal sends. This is called active authentication (AA). After performing both PA and AA, the terminal knows that the data in the passport (including the AA public key) was signed by the government and that the passport contains the corresponding private key.</p><p>This solution has two issues: the AA signature is not tied to the secure channel, so you can relay a signature and pretend that the passport is somewhere it’s not. Additionally, the passport signs an arbitrary challenge without knowing the semantics of this message, which is generally considered a dangerous practice in cryptography.</p><p>Extended Access Control (EAC) fixes some of the issues related to BAC and AA. It comprises chip authentication (CA), which is a better AA, and terminal authentication (TA), which authenticates the terminal to the passport in order to protect access to the sensitive information stored in DG3 (fingerprint) and DG4 (iris). Finally, password authenticated connection establishment (PACE, described below) replaces BAC altogether, eliminating its weaknesses.</p><h4>Chip Authentication: Upgrading the secure channel</h4><p>CA is very similar to AA in the sense that it requires countries to simply store a public key in one of the DGs (DG14), which is then authenticated using PA. However, instead of signing a challenge, the passport uses the key pair to perform a static-ephemeral Diffie-Hellman key exchange with the terminal, and uses the resulting keys to upgrade the secure channel from BAC. This means that passive listeners that know the MRZ cannot eavesdrop after doing CA, because they were not part of the key exchange.</p><h4>Terminal Authentication: Protecting sensitive data in DG3 and DG4</h4><p>Similar to the CSCA for signing things, each country has a Country Verification Certificate Authority (CVCA), which creates a root certificate for a PKI that authorizes terminals to read DG3 and DG4 in the passports of that country. Terminals provide a certificate chain for their public key and sign a challenge provided by the passport using their private key. The CVCA can authorize document verifiers (DVs) to read one or both of DG3 and DG4, which is encoded in the certificate. The DV then issues certificates to individual terminals. Without such a certificate, it is not possible to access the sensitive data in DG3 and DG4.</p><h4>Password Authenticated Connection Establishment: Fixing the basic problems</h4><p>The main idea behind PACE is that the MRZ, much like a password, does not have sufficient entropy to protect the data it contains. Therefore, it should not be used directly to derive keys, because this would enable offline brute-force attacks. PACE can work with various mappings, but we describe only the simplest one in the following, which is the generic mapping. Likewise, PACE can work with other passwords besides the MRZ (such as a PIN), but this blog post focuses on the MRZ.</p><p>First, both sides use the MRZ data (the password) to derive a password key. Next, the passport encrypts a nonce using the password key and sends it to the terminal, which can decrypt it if it knows the password. The terminal and passport also perform an ephemeral Diffie-Hellman key exchange. Now, both terminal and passport derive a new generator of the elliptic curve by applying the nonce as an additive tweak to the (EC)DH shared secret. Using this new generator, the terminal and passport perform another (EC)DH to get a second shared secret. Finally, they use this second shared secret to derive session keys, which are used to authenticate the (EC)DH public keys that they used earlier on in the protocol, and to set up the secure channel. Figure 6 shows a simplified protocol diagram.</p><p>Anyone who does not know the password cannot follow the protocol to the end, which will become apparent in the final step when they need to authenticate the data with the session keys. Before authenticating the terminal, the passport does not share any data that enables brute-forcing the password key. Non-participants who do know the password cannot derive the session keys because they do not know the ECDH private keys.</p><h2>Gaps in the threat model: Why you shouldn’t give your passport to just anyone</h2><p>When considering potential solutions to maintaining passports’ confidentiality and authenticity, it’s important to account for what the inspection system  with your passport, and not just the fancy cryptography the passport supports. If an inspection system performs only BAC/PACE and PA, anyone who has seen your passport could make an electronic copy and pretend to be you when interacting with this system. This is true even if your passport supports AA or CA.</p><p>Another important factor is tracing: the specifications aim to ensure that someone who does not know a passport’s PACE password (MRZ data in most cases) cannot trace that passport’s movements by interacting with it or eavesdropping on communications it has with legitimate terminals. They attempt to achieve this by ensuring that passports always provide random identifiers (e.g., as part of Type A or Type B ISO 14443 contactless communication protocols) and that the contents of publicly accessible files (e.g., those containing information necessary for performing PACE) are the same for every citizen of a particular country.</p><p>However, all of these protections go out of the window when the attacker knows the password. If you are entering another country and border control scans your passport, they can provide your passport contents to others, enabling them to track the movements of your passport. If you <a href=\"https://cert-agid.gov.it/news/in-vendita-documenti-di-identita-trafugati-da-hotel-italiani/\">visit a hotel in Italy and they store a scan of your passport and get hacked</a>, anyone with access to this information can track your passport. This method can be a bit onerous, as it requires contacting various nearby contactless communication devices and trying to authenticate to them as if they were your passport. However, some may still choose to include it in their threat models.</p><p>Some countries state in their issued passports that the holder should give it to someone else only if there is a statutory need. At Italian hotels, for example, it is sufficient to provide a prepared copy of the passport’s photo page with most data redacted (such as your photo, signature, and any personal identification numbers). In practice, not many people do this.</p><p>Even without the passport, the threat model says nothing about tracking particular groups of people. Countries typically buy large quantities of the same electronic passports, which comprise a combination of an IC and the embedded software implementing the passport specifications. This means that people from the same country likely have the same model of passport, with a unique fingerprint comprising characteristics like communication time, execution time, supported protocols (ISO 14443 Type A vs Type B), etc. Furthermore, each country may use different parameters for PACE (supported curves or mappings, etc.), which may aid an attacker in fingerprinting different types of passports, as these parameters are stored in publicly readable files.</p><h2>Security and privacy implications of zero-knowledge identity proofs</h2><p>An emerging approach in both academic research and industry applications involves using zero-knowledge (ZK) proofs with identity documents, enabling verification of specific identity attributes without revealing complete document contents. This is a nice idea in theory, because this will allow proper use of passports where there is no statutory need to hand over your passport. However, there are security implications.</p><p>First of all, passports cannot generate ZK proofs by themselves, so this necessarily involves exposing your passport to a prover. Letting anyone or anything read your passport means that you downgrade your threat model with respect to that entity. So when you provide your passport to an app or website for the purposes of creating a ZK proof, you need to consider what they will do with the information in your passport. Will it be processed locally on your device, or will it be sent to a server? If the data leaves your device, will it be encrypted and only handled inside a trusted execution environment (TEE)? If so, has this whole stack been audited, including against malicious TEE operators?</p><p>Second, if the ZK proving service relies on PA for its proofs, then anyone who has ever seen your passport can pretend to be you on this service. Full security requires AA or CA. As long as there exists any service that relies only on PA, anyone whose passport data is exposed is vulnerable to impersonation. Even if the ZK proving service does not incorporate AA or CA in their proofs, they should still perform one of these procedures with the passport to ensure that only legitimate passports sign up for this service.</p><p>Finally, the system needs to consider what happens when people share their ZK proof with others. The nice thing about a passport is that you cannot easily make copies (if AA or CA is used), but if I can allow others to use my ZK proof, then the value of the identification decreases.</p><p>It is important that such systems are audited for security, both from the point of view of the user and the service provider. If you’re implementing ZK proofs of identity documents, <a href=\"https://www.trailofbits.com/contact/\">contact us</a> to evaluate your design and implementation.</p>","contentLength":18906,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45770875"},{"title":"My Impressions of the MacBook Pro M4","url":"https://michael.stapelberg.ch/posts/2025-10-31-macbook-pro-m4-impressions/","date":1761905620,"author":"secure","guid":208,"unread":true,"content":"<p>I have been using a MacBook Pro M4 as my portable computer for the last half a\nyear and wanted to share a few short impressions. As always, I am not a\nprofessional laptop reviewer, so in this article you won’t find benchmarks, just\nsubjective thoughts!</p><p>Back in 2021, I wrote about the <a href=\"https://michael.stapelberg.ch/posts/2021-11-28-macbook-air-m1/\">MacBook Air\nM1</a>, which was the first computer I used that\ncontained Apple’s own ARM-based CPU. Having a silent laptop with long battery\nlife was a game-changer, so I wanted to keep those properties.</p><p>When the US government announced tariffs, I figured I would replace my 4-year\nold MacBook Air M1 with a more recent model that should last a few more\nyears. Ultimately, Apple’s prices remained stable, so, in retrospect, I could\nhave stayed with the M1 for a few more years. Oh well.</p><h2>The nano-textured display</h2><p>I went to the Apple Store to compare the different options in\nperson. Specifically, I was curious about the display and whether the increased\nweight and form factor of the MacBook Pro (compared to a MacBook Air) would be\nacceptable. Another downside of the Pro model is that it comes with a fan, and I\nreally like absolutely quiet computers. Online, I read from other MacBook Pro\nowners that the fan mostly stays off.</p><p>In general, I would have preferred to go with a MacBook Air because it has\nenough compute power for my needs and I like the case better (no ventilation\nslots), but unfortunately only the MacBook Pro line has the better displays.</p><p>Why aren’t all displays nano-textured? The employee at the Apple Store presented\nthe trade-off as follows: The nano texture display is great at reducing\nreflections, at the expense of also making the picture slightly less vibrant.</p><p>I could immediately see the difference when placing two laptops side by side:\nThe bright Apple Store lights showed up very prominently on the normal display\n(left), and were almost not visible at all on the nano texture display (right):</p><a href=\"https://michael.stapelberg.ch/posts/2025-10-31-macbook-pro-m4-impressions/2025-10-30-macbooks-displays.jpg\"><img srcset=\"https://michael.stapelberg.ch/posts/2025-10-31-macbook-pro-m4-impressions/2025-10-30-macbooks-displays_hu_8c87fe476c5344b6.jpg 2x,https://michael.stapelberg.ch/posts/2025-10-31-macbook-pro-m4-impressions/2025-10-30-macbooks-displays_hu_243730d5507ecc98.jpg 3x\" src=\"https://michael.stapelberg.ch/posts/2025-10-31-macbook-pro-m4-impressions/2025-10-30-macbooks-displays_hu_4cb7e6e8fae8437.jpg\" alt=\"MacBook Air (left) vs. MacBook Pro (right)\" title=\"MacBook Air (left) vs. MacBook Pro (right)\" width=\"600\" height=\"409\" loading=\"lazy\"></a><p>Personally, I did not perceive a big difference in “vibrancy”, so my choice was\nclear: I’ll pick the MacBook Pro over the MacBook Air (despite the weight) for\nthe nano texture display!</p><p>After using the laptop in a number of situations, I am very happy with this\nchoice. In normal scenarios, I notice no reflections at all (where my previous\nlaptop did show reflections!). This includes using the laptop on a train (next\nto the window), or using the laptop outside in daylight.</p><p>(When I chose the new laptop, Apple’s M4 chips were current. By now, they have\nreleased the first devices with M5 chips.)</p><p>I decided to go with the MacBook Pro with M4 chip instead of the M4  chip\nbecause I don’t need the extra compute, and the M4 needs less cooling — the M4\nPro apparently runs hotter. This increases the chance of the fan staying off.</p><p>Here are the specs I ended up with:</p><ul><li>14\" Liquid Retina XDR Display with nano texture</li><li>Apple M4 Chip (10 core CPU, 10 core GPU)</li><li>32 GB RAM (this is the maximum!), 2 TB SSD (enough for this computer)</li></ul><p>One thing I noticed is that the MacBook Pro M4 sometimes gets warm, even when it\nis connected to power, but is suspended to RAM (and has been fully charged for\nhours). I’m not sure why.</p><p>Luckily, the fan indeed stays silent. I think I might have heard it spin up once\nin half a year or so?</p><p>The battery life is amazing! The previous MacBook Air M1 had amazing all-day\nbattery life already, and this MacBook Pro M4 lasts even longer. For example,\nwatching videos on a train ride (with VLC) for 3 hours consumed only 10% of\nbattery life. I generally never even carry the charger.</p><p>Because of that, Apple’s re-introduction of MagSafe, a magnetic power connector\n(so you don’t damage the laptop when you trip over it), is nice-to-have but\ndoesn’t really make much of a difference anymore. In fact, it might be better to\npack a USB-C cable when traveling, as that makes you more flexible in how you\nuse the charger.</p><p>I was curious whether the 120 Hz display would make a difference in practice. I\nmostly notice the increased refresh rate when there are animations, but not,\nfor example, when scrolling.</p><p>One surprising discovery (but obvious in retrospect) is that even non-animations\ncan become faster. For example, when running a Go web server on , I\nnoticed that navigating between pages by clicking links felt faster on the 120\nHz display!</p><p>The following illustration shows why that is, using a page load that takes 6ms\nof processing time. There are three cases (the illustration shows an average\ncase and the worst case):</p><ol><li>Best case: Page load finishes  the next frame is displayed: no delay.</li><li>Worst case: Page load finishes  a frame is displayed: one frame of delay.</li><li>Most page loads are somewhere in between. We’ll have 0.x to 1.0 frames of delay</li></ol><a href=\"https://michael.stapelberg.ch/posts/2025-10-31-macbook-pro-m4-impressions/2025-10-31-delay-60-vs-120.svg\"><img src=\"https://michael.stapelberg.ch/posts/2025-10-31-macbook-pro-m4-impressions/2025-10-31-delay-60-vs-120.svg\" alt=\"delay\" title=\"delay\" loading=\"lazy\"></a><p>As you can see, the waiting time becomes shorter when going from 60 Hz (one\nframe every 16.6ms) to 120 Hz (one frame every 8.3ms). So if you’re working with\na system that has &lt;8ms response times, you might observe actions completing (up\nto) twice as fast!</p><p>I don’t notice going back to 60 Hz displays on computers. However, on phones,\nwhere a lot more animations are a key part of the user experience, I think 120\nHz displays are more interesting.</p><p>My ideal MacBook would probably be a MacBook Air, but with the nano-texture display! :)</p><p>I still don’t like macOS and would prefer to run Linux on this laptop. But\n<a href=\"https://asahilinux.org/\">Asahi Linux</a> still needs some work before it’s usable\nfor me (I need external display output, and M4 support). This doesn’t bother me\ntoo much, though, as I don’t use this computer for serious work.</p>","contentLength":5491,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45770304"},{"title":"Reasoning models reason well, until they don't","url":"https://arxiv.org/abs/2510.22371","date":1761902621,"author":"optimalsolver","guid":207,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45769971"},{"title":"AMD could enter ARM market with Sound Wave APU built on TSMC 3nm process","url":"https://www.guru3d.com/story/amd-enters-arm-market-with-sound-wave-apu-built-on-tsmc-3nm-process/","date":1761880068,"author":"walterbell","guid":206,"unread":true,"content":"AMD is expanding its processor portfolio beyond the x86 architecture with its first ARM-based APU, internally known as “Sound Wave.” The chip’s existence was uncovered through customs import records, confirming several details about its design and purpose. Built with a BGA-1074 package measuring 32 mm × 27 mm, the processor fits within standard mobile SoC dimensions, making it suitable for thin and light computing platforms. It employs a 0.8 mm pitch and FF5 interface, replacing the FF3 socket previously used in Valve’s Steam handheld devices, further hinting at a new generation of compact AMD-powered hardware.\n                                    <p>According to leaks from industry insiders such as @Moore’s Law Is Dead and @KeplerL2, “Sound Wave” is manufactured on  and aims for a  range, positioning it directly against Qualcomm’s Snapdragon X Elite. The chip is expected to power future  products scheduled for release in 2026.&nbsp;<strong>four RDNA 3.5 compute units</strong><strong>machine learning acceleration</strong></p><p>Memory support is another highlight: the chip integrates a <strong>128-bit LPDDR5X-9600 controller</strong> and will reportedly include , aligning with current trends in unified memory designs used in ARM SoCs. Additionally, the APU carries AMD’s <strong>fourth-generation AI engine</strong>, enabling on-device inference tasks and enhanced efficiency for workloads such as speech recognition, image analysis, and real-time translation.</p><p>While AMD experimented with ARM over a decade ago through the abandoned “Project Skybridge,” this new effort represents a more mature and strategic approach. With industry interest in efficient, ARM-based computing accelerating, “Sound Wave” could help AMD diversify its portfolio while leveraging its strengths in graphics and AI acceleration. If reports are accurate, the processor will enter production in late 2025, with commercial devices expected the following year.</p>","contentLength":1896,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45767916"},{"title":"John Carmack on mutable variables","url":"https://twitter.com/id_aa_carmack/status/1983593511703474196","date":1761878076,"author":"azhenley","guid":205,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45767725"},{"title":"Ground stop at JFK due to staffing","url":"https://www.fly.faa.gov/adv/adv_otherdis?advn=13&adv_date=10312025&facId=JFK&title=ATCSCC%20ADVZY%20013%20JFK/ZNY%2010/31/2025%20CDM%20GROUND%20STOP&titleDate=10/31/2025","date":1761875319,"author":"akersten","guid":204,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45767505"},{"title":"ICE and the Smartphone Panopticon","url":"https://www.newyorker.com/culture/infinite-scroll/ice-and-the-smartphone-panopticon","date":1761873236,"author":"fortran77","guid":203,"unread":true,"content":"<p>Last week, as  raids ramped up in New York, city residents set about resisting in the ways they had available: confronting agents directly on sidewalks, haranguing them as they processed down blocks, and recording them on phone cameras held aloft. Relentless <a href=\"https://www.newyorker.com/magazine/2025/08/11/ices-spectacle-of-intimidation\">documentation</a> has proved something of an effective tool against President Donald Trump’s empowerment of ; agents have taken to wearing masks in fear of exposure, and the proliferation of imagery showing armed police and mobilized National Guard troops in otherwise calm cities has underlined the cruel absurdity of their activities. Activist memes have been minted on social media: a woman on New York’s Canal Street, dressed in a polka-dotted office-casual dress, flipping  agents off; a man in Washington, D.C., throwing a Subway sandwich at a federal agent in August. The recent “No Kings” marches were filled with protesters in inflatable frog costumes, inspired by a similarly outfitted man who got pepper-sprayed protesting outside the U.S. Immigration and Customs Enforcement Building in Portland, Oregon. Some might write the memes off as resistance porn, but digital content is at least serving as a lively defense mechanism in the absence of functional politics.</p><p>At the same time, social media has served as a reinvigorated source of transparency in recent weeks, harking back to the days when Twitter became an organizing tool during the Arab Spring, in the early twenty-tens, or when Facebook and Instagram helped fuel the Black Lives Matter marches of 2020. The grassroots optimism of that earlier social-media era is long gone, though, replaced by a sense of posting as a last resort. After Trump authorized the deployment of the National Guard <a href=\"https://www.newyorker.com/news/the-lede/the-conflict-on-the-streets-of-chicago\">in Chicago</a> earlier this month, the governor of Illinois, J.&nbsp;B. Pritzker, told residents to “record and narrate what you see—put it on social media.” But, if the anti- opposition is taking advantage of the internet,  and the Trump Administration are, too. Right-wing creators have been using the same channels to identify and publicize targets for raids. According to reporting in Semafor, the Trump-friendly YouTuber Nick Shirley’s videos of African migrant vendors on Canal Street seemed to help drive recent  sweeps of the area.  itself is also working to monitor social media. The investigative outlet  found documents revealing that the agency has enlisted an A.I.-driven surveillance product called Zignal Labs that creates “curated detection feeds” to aid in criminal investigations. According to reporting in ,  also has plans to build out a team of dozens of analysts to monitor social media and identify targets. Recent videos, identified by 404 Media and other publications, have purportedly shown  agents using technology developed by the data-analytics firm Palantir, founded by Peter Thiel and others, to scan social-media accounts, government records, and biometrics data of those they detain. Social media has become a political panopticon in which your posts are a conduit for your politics, and what you post can increasingly be used against you.</p><p>Meanwhile, a new wave of digital tools has emerged to help surveil the surveillants. The apps ICEBlock, Red Dot, and DEICER all allow users to pinpoint where  agents are active, forming an online version of a whisper network to alert potential targets. Eyes Up provides a way for users to record and upload footage of abusive law-enforcement activity, building an archive of potential evidence. Its creator is a software developer named Mark (who uses only his first name to separate the project from his professional work); he was inspired to create Eyes Up earlier this year, when he began seeing clips of  abductions and harassment circulating on social media and worried about their shelf life. As he put it to me, “They could disappear at any given moment, whether the platforms decide to moderate, whether the individual deletes their account or the post.”</p><p>Ultimately, the app itself was also vulnerable to sudden disappearance. After launching, on September 1st, Eyes Up accumulated thousands of downloads and thousands of minutes of uploaded footage. Then, on October 3rd, Mark received a notice that Apple was removing the app from its store on the grounds that it may “harm a targeted individual or group.” Eyes Up is not alone. ICEBlock and Red Dot have been blocked from both Apple and Google’s app stores, the two largest marketplaces; DEICER, like Eyes Up, was removed by Apple. Pressure on the tech platforms seemed to come from the Trump Administration; after a deadly shooting at an  field office in Dallas in late September, the Attorney General, Pam Bondi, said in a statement to Fox News Digital that ICEBlock “put ICE agents at risk just for doing their jobs.” Mark is contesting Apple’s decision about Eyes Up through its official channels, and the creator of ICEBlock, Joshua Aaron, has argued that his app should be treated no differently than services, such as Google’s Waze, that allow users to warn one another of highway speed traps. But for now they must try to make do with a limited reach.</p>","contentLength":5138,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45767325"},{"title":"Kimi Linear: An Expressive, Efficient Attention Architecture","url":"https://github.com/MoonshotAI/Kimi-Linear","date":1761869256,"author":"blackcat201","guid":202,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45766937"},{"title":"Leaker reveals which Pixels are vulnerable to Cellebrite phone hacking","url":"https://arstechnica.com/gadgets/2025/10/leaker-reveals-which-pixels-are-vulnerable-to-cellebrite-phone-hacking/","date":1761865930,"author":"akyuu","guid":185,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45766501"},{"title":"A change of address led to our Wise accounts being shut down","url":"https://shaun.nz/why-were-never-using-wise-again-a-cautionary-tale-from-a-business-burned/","date":1761864110,"author":"jemmyw","guid":201,"unread":true,"content":"<p>For years, one of my businesses has been a regular user of  (formerly TransferWise). Wise is a financial service that lets you send and receive money across currencies, often at a better rate and lower fee than traditional banks. Sounds great, right?</p><p>This is our story – a sobering, frustrating, and frankly  experience that ended with our business and personal accounts being shut down, without any meaningful reason, support, or recourse.</p><p>And all we did? We .</p><h2><strong>🏢 A Routine Change Turned Nightmare</strong></h2><p>Like many businesses, we recently moved into a new office. Alongside the usual updates to suppliers and records, we updated our  with Wise. Not long after, we received an email requesting us to  the new address.</p><p>Fair enough – we had no problem with that.</p><p>Wise provided a dropdown list of acceptable documents: a lease agreement, rates notice, tax document, utilities bill, or telecommunications bill. Due to our company structure, most of those documents are in the name of our parent company or show our PO Box (which NZ Post requires, since they won’t deliver to our street address). But we had a  that ticked every box:</p><ul><li>Correct physical street address ✅</li><li>Even detailed our fibre connection at the new premises ✅</li></ul><p>So we uploaded it – and assumed that would be the end of it.</p><h2><strong>📞 The Call That Made No Sense</strong></h2><p>Days later, we received an email: our document was . </p><p>No clear reason. So, I called Wise and explained the situation to the customer service representative.</p><p>Her response left me stunned.</p><blockquote><p>“The document was rejected because it was a , not a .”</p></blockquote><p>I paused, trying to process this. I politely explained that in <strong>New Zealand, a “tax invoice” is a legal form of a bill</strong> – even down to the name “tax invoice” being a legal requirement by IRD, and that’s how telecommunications companies issue invoices here. But she refused to accept it.</p><blockquote><p>“It needs to say  at the top,” she insisted.</p></blockquote><blockquote><p>“A tax invoice isn’t acceptable.”</p></blockquote><p>This is simply , and completely out of touch with New Zealand’s business documentation standards. The rep wouldn’t budge.</p><h2><strong>🧠 The “Solution” That Was Beyond Belief</strong></h2><p>Still trying to find a solution, I asked: <em>what do you recommend I do then?</em></p><blockquote><p>“You should find a local shared workspace, lease a desk under your company name, change your registered office to that address, and use that lease document to verify your address with us.”</p></blockquote><p>Yes, you read that right.</p><p>Wise’s advice was to <strong>artificially lease a desk we didn’t need, change our registered address, and use that document</strong> – just to verify an address we actually operate from.</p><p>I asked to speak to a manager. That request was . She told me, flatly:</p><blockquote><p>“I  providing you with the correct information.”</p></blockquote><p>A bit more back and forth… then the call .</p><h2><strong>📞 A Glimmer of Hope – Then The Hammer Falls</strong></h2><p>Later that day, I received a call back from Wise – not from a manager (because apparently, <strong>Wise doesn’t have managers</strong>), but from a more “senior” representative.</p><p>This rep was  and agreed the document should have been acceptable. She escalated the issue, resubmitted the document herself, and said she’d personally follow up if it was rejected again.</p><h2><strong>🚫 “We’ve Restricted Your Account”</strong></h2><p>I woke to an email with a stunning subject line:</p><blockquote><p><strong>“We’ve restricted your account”</strong></p></blockquote><p>Just like that, our  was locked. No warning. No reason. No discussion.</p><p>We could no longer send or receive money, use our Wise cards, or even contact support. The email stated:</p><blockquote><p>“Due to our current risk policies, your account will be closed in a few months. You will not be able to use support channels.”</p></blockquote><p>Even worse? My  was locked too. The same personal account which did have its address fully verified, by a rates invoice for my personal address.</p><h2><strong>🧾 An “Appeal” That Wasn’t an Appeal</strong></h2><p>The email offered an option to . Naturally, I did.</p><p>The appeal process asked for our <strong>articles of incorporation</strong> and . No problem.</p><p>Then it asked us to provide our preferred currency, and <strong>bank account details to refund the balances</strong>.</p><p>Wait… I thought this was an appeal? A chance to discuss and resolve the issue?</p><p>That was the end. There was <strong>no opportunity to explain anything</strong>, no communication, no questions asked. The decision was made, and we were , permanently.</p><p>To summarise the absurdity of this:</p><ul><li>We moved office, and updated our address with Wise</li><li>We provided a legal, NZ-compliant  showing our entity and address</li><li>It was rejected because it was labelled a “Tax Invoice”</li><li>A rep told us to lease a coworking desk elsewhere just to get a different document</li><li>A senior rep agreed we were right, and escalated it</li><li>Then <strong>our accounts were shut down – with no explanation or recourse</strong></li></ul><p>Even trying to call support now gets an automated message: “Because your account is restricted, we cannot connect you.”</p><h2><strong>⚠️ Our Final Word: Be Very, Very Careful</strong></h2><p>We had used Wise for . Regular monthly supplier payments. International stock orders. Five-figure transactions. Never a problem – until this. A minor change triggered a totally flawed process that , with no transparency or logical path to resolution.</p><p>We’re not alone – a quick search shows <strong>many others facing similar horror stories</strong> with Wise.</p><p>So this is my word of warning:</p><blockquote><p>💡 <strong>Don’t put all your eggs in the Wise basket.</strong></p></blockquote><p>If you’re a business, don’t rely on them as your sole means of transferring funds. For us, it’s back to traditional banks – slower, yes, but at least <strong>they have humans you can talk to, and actual escalation paths</strong>.</p><h2>🧾 <strong>28th October update on our Wise debacle – it gets worse.</strong></h2><p>Following the so-called “appeal” (which gave us no option to provide any information), we received the unsurprising outcome: Wise has decided to  as we had <strong>breached their acceptable use policy</strong>. 🤨</p><p>What was surprising, however, was the  they gave after I queried what was breached in Wise’s Acceptable Use Policy:</p><p>I was told my  was being closed for allegedly breaching their Acceptable Use Policy — specifically, section 1.4.e, which states <em>“you may not use your personal Wise account to receive business payments.”</em></p><p>I’ve  used my personal account for business transactions — in fact, over 99% of transfers were to overseas family members. When I asked for clarification or examples, I got none. Just a vague statement and the very strange line:</p><blockquote><p><em>“Just because we can’t offer you our services going forward doesn’t mean that we think your business activities are illegal or illegitimate — it just means that we don’t support those types of activities.”</em></p></blockquote><p>What activities?! Again, </p><p>To make matters worse — our business account’s refund transfer . Why? Because it requires documentation — the  Wise previously rejected for address verification, claiming a <em>telecommunications tax invoice isn’t a bill</em>.</p><p>After a few days, the transfer was then cancelled as of course, Wise was unable to “verify” us.</p><p>So now our , their support ticket is marked “final response,” and our attempts to get clarity have gone nowhere. We’ve escalated the issue to <strong>Financial Services Complaints Ltd</strong>, Wise’s dispute resolution provider in New Zealand.</p><p>Funds stuck. No clear reason. No accountability. Wise still gets a 0/10 from us.</p><p>This isn’t just poor service — it’s unacceptable.</p><p><strong>Think twice before trusting Wise with your money.</strong></p>","contentLength":7250,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45766253"},{"title":"Phone numbers for use in TV shows, films and creative works","url":"https://www.acma.gov.au/phone-numbers-use-tv-shows-films-and-creative-works","date":1761860951,"author":"nomilk","guid":200,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45765787"},{"title":"Denmark reportedly withdraws Chat Control proposal following controversy","url":"https://therecord.media/demark-reportedly-withdraws-chat-control-proposal","date":1761860142,"author":"layer8","guid":199,"unread":true,"content":"<p> Denmark’s justice minister on Thursday said he will no longer push for an EU law requiring the mandatory scanning of electronic messages, including on end-to-end encrypted platforms. </p><p> Earlier in its European Council presidency, Denmark had brought back a draft law which would have required the scanning, sparking an intense backlash. Known as Chat Control, the measure was intended to crack down on the trafficking of child sex abuse materials (CSAM). </p><p> After days of silence, the German government on October 8 announced it would <a href=\"https://therecord.media/chat-control-eu-germany-will-not-support-law\" target=\"_blank\" rel=\"noopener noreferrer\">not support</a> the proposal, tanking the Danish effort. </p><p> Danish Justice Minister Peter Hummelgaard told reporters on Thursday that his office will support voluntary CSAM detections. </p><p> \"This will mean that the search warrant will not be part of the EU presidency's new compromise proposal, and that it will continue to be voluntary for the tech giants to search for child sexual abuse material,\" Hummelgaard said, <a href=\"https://politiken.dk/viden/art10605607/Hummelgaard-opgiver-kontroversielt-forslag-om-overv%C3%A5gning\" target=\"_blank\" rel=\"noopener noreferrer\">according to</a> local news reports. </p><p> The current model allowing for voluntary scanning expires in April, Hummelgaard said. </p><p> \"Right now we are in a situation where we risk completely losing a central tool in the fight against sexual abuse of children,” he said. \"That's why we have to act no matter what. We owe it to all the children who are subjected to monstrous abuse.\"&nbsp; </p><p> Meredith Whittaker, the president of the Signal Foundation, lobbied hard against the original measure, saying the organization would leave the European market if the provision was adopted. </p><p> “What they propose is in effect a mass surveillance free-for-all, opening up everyone’s intimate and confidential communications, whether government officials, military, investigative journalists, or activists,” she said at the time. </p>","contentLength":1748,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45765664"},{"title":"Minecraft HDL, an HDL for Redstone","url":"https://github.com/itsfrank/MinecraftHDL","date":1761850742,"author":"sleepingreset","guid":198,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45763877"},{"title":"How We Found 7 TiB of Memory Just Sitting Around","url":"https://render.com/blog/how-we-found-7-tib-of-memory-just-sitting-around","date":1761848705,"author":"anurag","guid":197,"unread":true,"content":"<figure><div><blockquote><div>Debugging infrastructure at scale is rarely about one big aha moment. It’s often the result of many small questions, small changes, and small wins stacked up until something clicks.</div></blockquote></div></figure><p>Plenty of teams run Kubernetes clusters bigger than ours. <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/bchess/k8s-1m%20\"></a>, more pods, more ingresses, you name it. In most dimensions, someone out there has us beat.</p><p>There's one dimension where I suspect we might be near the very top: namespaces. I say that because we keep running into odd behavior in any process that has to keep track of them. In particular, anything that listwatches them ends up using a surprising amount of memory and puts real pressure on the apiserver. This has become one of those scaling quirks you only really notice once you hit a certain threshold. As this memory overhead adds up, efficiency decreases: each byte we have to use for management is a byte we can't put towards user services.</p><p>The problem gets significantly worse when a daemonset needs to listwatch namespaces or network policies (netpols, which we define per namespace). Since daemonsets run a pod on every node, each of those pods independently performs a listwatch on the same resources. As a result, memory usage increases with the number of nodes.</p><p>Even worse, these listwatch calls can put significant load on the apiserver. If many daemonset pods restart at once, such as during a rollout, they can overwhelm the server with requests and cause real disruption.</p><p>A few months ago, if you looked at our nodes, the largest memory consumers were often daemonsets. In particular, Calico and Vector which handle configuring networking and log collection respectively.</p><p>We had already done some work to reduce Calico’s memory usage, <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/projectcalico/calico/pull/9514\"></a> with the project’s maintainers to make it scale more efficiently. That optimization effort was a big win for us, and it gave us useful insight into how memory behaves when namespaces scale up.</p><p>To support that work, we set up a staging cluster with several hundred thousand namespaces. We knew that per-namespace network policies (netpols) were the scaling factor that stressed Calico, so we reproduced those conditions to validate our changes.</p><p>While running those tests, we noticed something strange. Vector, another daemonset, also started consuming large amounts of memory.</p><p>The pattern looked familiar, and we knew we had another problem to dig into. Vector obviously wasn’t looking at netpols but after poking around a bit we found it was listwatching namespaces from every node in order to allow referencing namespace labels per-pod in the <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://vector.dev/docs/reference/configuration/sources/kubernetes_logs/\"></a>.</p><p>That gave us an idea: what if Vector didn’t need to use namespaces at all? Was that even possible?</p><p>As it turns out, yes, they were in use in our configuration, but only to check whether a pod belonged to a user namespace.</p><p>Conveniently, we realized we could hackily describe that condition in another way, and the memory savings were absolutely worth it.</p><p>At that point, we were feeling a bit too lucky. We reached out to the Vector maintainers to ask whether disabling this behavior would actually work, and whether they would be open to accepting a contribution if we made it happen.</p><p>From there, all that was left was to try it. The code change was straightforward. We added a new config option and threaded it through the relevant parts of the codebase.</p><p>After a few hours of flailing at rustc, a Docker image finally built and we were ready to test the theory. The container ran cleanly with no errors in the logs, which seemed promising.</p><p>But then we hit a snag. Nothing was being emitted. No logs at all. I couldn’t figure out why.</p><p>Thankfully, our pal Claude came to the rescue:</p> I rebuilt it (which took like 73 hours because Rust), generated a new image, updating staging, and watched nervously. This time, logs were flowing like normal and…<p>The change saved 50 percent of memory. A huge win. We were ready to wrap it up and ship to production.</p><p>But then Hieu, one of our teammates, asked a very good question.</p><p>He was right, something didn’t add up.</p><p>A few hours later, after repeatedly running my head into a wall, I still hadn’t found anything. There was still a full gibibyte of memory unaccounted for. My whole theory about how this worked was starting to fall apart.</p><p>I even dropped into the channel to see if anyone had Valgrind experience:</p><p><em> anybody got a background in valgrind? seems pretty straightforward to get working so far but it won’t end up interfacing with pyroscope. we’ll have to exec in and gdb manually.</em></p><p>In a last-ditch effort to profile it again, I finally saw the answer. It had been staring me in the face the whole time.</p><p>We actually had  kubernetes_logs sources on user nodes. I had only set the flag on one of them. Once I applied it to both, memory usage dropped to the level we had seen in staging before the extra namespaces were added.</p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://vector.dev/docs/reference/configuration/sources/kubernetes_logs/#insert_namespace_fields\"></a><p>Around the same time, our colleague Mark happened to be on-call. He did his usual magic — pulled everything together, tested the rollout in staging, and got it shipped to production.</p><p>I’ll let the results speak for themselves. </p> Our largest cluster saw a 1 TiB memory drop, with savings across our other clusters adding up to a total of just over 7 TiB.<p>Debugging infrastructure at scale is rarely about one big “aha” moment. It’s often the result of many small questions, small changes, and small wins stacked up until something clicks.</p><p>In this case, it started with a memory chart that didn’t look quite right, a teammate asking the right question at the right time, and a bit of persistence. When applied to our whole infrastructure, that simple fix freed up , reduced risk during rollouts, and made the system easier to reason about.</p><p>Huge thanks to Hieu for pushing the investigation forward, Mark for shipping it smoothly, and the Vector maintainers for being responsive and open to the change.</p><p>If you’re running daemonsets at scale and seeing unexplained memory pressure, it might be worth asking:</p><p>Do you really need those namespace labels?</p>","contentLength":5952,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=45763359"}],"tags":["dev","hn"]}