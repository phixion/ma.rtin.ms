{"id":"EfcLDDAkyqguXw9Vbtcae7fRhxCsY1chPUNLpwbK9oHS42b4dGEMeGvA2hWHB2j3LFSAo7qhibLNgPBcA5djbGp95Jk5T","title":"top scoring links : programming","displayTitle":"Reddit - Programming","url":"https://www.reddit.com/r/programming/top/.rss?sort=top&t=day&limit=6","feedLink":"https://www.reddit.com/r/programming/top/?sort=top&t=day&limit=6","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":6,"items":[{"title":"Local WebSocket: Building Real-Time Apps That Work Without the Cloud","url":"https://medium.com/@dr.e.rashidi/local-websocket-building-real-time-apps-that-work-without-the-cloud-a0f46ae14dd7","date":1771191449,"author":"/u/_Flame_Of_Udun_","guid":340,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1r5quh6/local_websocket_building_realtime_apps_that_work/"},{"title":"How Michael Abrash doubled Quake framerate","url":"https://fabiensanglard.net/quake_asm_optimizations/index.html","date":1771183571,"author":"/u/NXGZ","guid":344,"unread":true,"content":"<div>How Michael Abrash doubled Quake framerate</div><p>With the 1999 release of the Quake source code, came a <a href=\"https://github.com/id-Software/Quake/blob/master/readme.txt\">readme.txt</a> written by John Carmack. There is a particular sentence in that text that piqued my curiosity.</p><pre>Masm is also required to build the assembly language files.  It is possible to\nchange a #define and build with only C code, but the software rendering versions\nlose almost .\n</pre><p>Quake would be 50% faster thanks to its hand-crafted assembly? Let's find out if that is true, how it works, and what are the most important optimizations.</p><div>Establishing stock fps on my machine</div><p>Before doing anything with the source I needed to establish what was the framerate of the released version of  on my Pentium MMX 233MHz.</p><pre>\nC:\\winquake&gt; winquake.exe -wavonly +d_subdiv16 0 +timedemo demo1\n\n</pre><p>I disabled  because it has no C implementation (that will make C vs ASM comparison impossible). This makes the engine fallback to D_DrawSpans8 instead of D_DrawSpans16 (perspective sampling every 8 pixels instead of 16).  is the fastest audio backend (also known as <a href=\"https://fabiensanglard.net/winquake\">\"fastvid\" option in wq.bat)</a>.</p><a href=\"https://fabiensanglard.net/quake_asm_optimizations/42.3.png\"><img loading=\"lazy\" src=\"https://fabiensanglard.net/quake_asm_optimizations/42.3.png\" width=\"1600\" height=\"1200\"></a><p>Stock winquake completed  at 42.3fps.</p><p>Following the steps in <a href=\"https://fabiensanglard.net/compile_like_1997\">Let's compile like it's 1997!</a>, I built  in release mode  the ASM optimizations. I really hoped VC++6 compiler did not get significant improvement<a name=\"back_1\" href=\"https://fabiensanglard.net/quake_asm_optimizations/index.html#footnote_1\"></a> over VC++4 (the version id software used to ship winquake in 1997).</p><pre>\nC:\\winquake&gt; WinQuake_ASM.exe -wavonly +d_subdiv16 0 +timedemo demo1\n\n</pre><a href=\"https://fabiensanglard.net/quake_asm_optimizations/42.2.png\"><img loading=\"lazy\" src=\"https://fabiensanglard.net/quake_asm_optimizations/42.2.png\" width=\"1600\" height=\"1200\"></a><p>I was relieved to see <a href=\"https://fabiensanglard.net/quake_asm_optimizations/WinQuake_fab_ASM.exe\">WinQuake_ASM.exe</a> run at nearly the same framerate, 42.2 fps. I was on a good track.</p><p>As John Carmack mentioned, building without ASM only requires setting  to  in .</p><a href=\"https://fabiensanglard.net/quake_asm_optimizations/1.png\"><img loading=\"lazy\" src=\"https://fabiensanglard.net/quake_asm_optimizations/1.png\" width=\"1024\" height=\"768\"></a><p>That broke the linker because a VC6 project meant running on an Intel CPU at the time.</p><a href=\"https://fabiensanglard.net/quake_asm_optimizations/2.png\"><img loading=\"lazy\" src=\"https://fabiensanglard.net/quake_asm_optimizations/2.png\" width=\"1024\" height=\"768\"></a><p>All I had to do was to add <a href=\"https://fabiensanglard.net/quake_asm_optimizations/nonintel.c\">nointel.c</a> to the project and I had a working executable.</p><a href=\"https://fabiensanglard.net/quake_asm_optimizations/nointel.c.png\"><img loading=\"lazy\" src=\"https://fabiensanglard.net/quake_asm_optimizations/nointel.c.png\" width=\"1024\" height=\"768\"></a><div>Quake without ASM optimizations</div><pre>\nC:\\winquake&gt; WinQuake_No_ASM.exe -wavonly +d_subdiv16 0 +timedemo demo1\n\n</pre><a href=\"https://fabiensanglard.net/quake_asm_optimizations/22.7.png\"><img loading=\"lazy\" src=\"https://fabiensanglard.net/quake_asm_optimizations/22.7.png\" width=\"1600\" height=\"1200\"></a><p>Son of a BLiT! The game indeed runs at  instead of ! As John Carmack warned, Quake framerate is halved without Michael Abrash's optimizations!.</p><p>There is a lot of assembly in Quake. In total, grep found 63 functions spread across 21 files.</p><pre>$ find . -name \"*.s\" | wc -l\n21</pre><pre>$ find . -name \"*.s\" -exec grep -H \".globl C(\" {} \\;\n./server/worlda.s:.globl C(SV_HullPointContents)\n./server/math.s:.globl C(BoxOnPlaneSide)\n./client/d_copy.s:.globl C(VGA_UpdatePlanarScreen)\n./client/d_copy.s:.globl C(VGA_UpdateLinearScreen)\n./client/d_draw.s:.globl C(D_DrawSpans8)\n./client/d_draw.s:.globl C(D_DrawZSpans)\n./client/surf16.s:.globl C(R_Surf16Start)\n./client/surf16.s:.globl C(R_DrawSurfaceBlock16)\n./client/surf16.s:.globl C(R_Surf16End)\n./client/surf16.s:.globl C(R_Surf16Patch)\n./client/d_scana.s:.globl C(D_DrawTurbulent8Span)\n./client/r_drawa.s:.globl C(R_ClipEdge)\n./client/d_parta.s:.globl C(D_DrawParticle)\n./client/d_polysa.s:.globl C(D_PolysetCalcGradients)\n./client/d_polysa.s:.globl C(D_PolysetRecursiveTriangle)\n./client/d_polysa.s:.globl C(D_PolysetAff8Start)\n./client/d_polysa.s:.globl C(D_PolysetDrawSpans8)\n./client/d_polysa.s:.globl C(D_PolysetAff8End)\n./client/d_polysa.s:.globl C(D_Aff8Patch)\n./client/d_polysa.s:.globl C(D_PolysetDraw)\n./client/d_polysa.s:.globl C(D_PolysetScanLeftEdge)\n./client/d_polysa.s:.globl C(D_PolysetDrawFinalVerts)\n./client/d_polysa.s:.globl C(D_DrawNonSubdiv)\n./client/sys_wina.s:.globl C(MaskExceptions)\n./client/sys_wina.s:.globl C(unmaskexceptions)\n./client/sys_wina.s:.globl C(Sys_LowFPPrecision)\n./client/sys_wina.s:.globl C(Sys_HighFPPrecision)\n./client/sys_wina.s:.globl C(Sys_PushFPCW_SetHigh)\n./client/sys_wina.s:.globl C(Sys_PopFPCW)\n./client/sys_wina.s:.globl C(Sys_SetFPCW)\n./client/math.s:.globl C(Invert24To16)\n./client/math.s:.globl C(TransformVector)\n./client/math.s:.globl C(BoxOnPlaneSide)\n./client/d_draw16.s:.globl C(D_DrawSpans16)\n./client/r_aclipa.s:.globl C(R_Alias_clip_bottom)\n./client/r_aclipa.s:.globl C(R_Alias_clip_top)\n./client/r_aclipa.s:.globl C(R_Alias_clip_right)\n./client/r_aclipa.s:.globl C(R_Alias_clip_left)\n./client/snd_mixa.s:.globl C(SND_PaintChannelFrom8)\n./client/snd_mixa.s:.globl C(Snd_WriteLinearBlastStereo16)\n./client/r_aliasa.s:.globl C(R_AliasTransformAndProjectFinalVerts)\n./client/d_spr8.s:.globl C(D_SpriteDrawSpans)\n./client/r_edgea.s:.globl C(R_EdgeCodeStart)\n./client/r_edgea.s:.globl C(R_InsertNewEdges)\n./client/r_edgea.s:.globl C(R_RemoveEdges)\n./client/r_edgea.s:.globl C(R_StepActiveU)\n./client/r_edgea.s:.globl C(R_GenerateSpans)\n./client/r_edgea.s:.globl C(R_EdgeCodeEnd)\n./client/r_edgea.s:.globl C(R_SurfacePatch)\n./client/surf8.s:.globl C(R_Surf8Start)\n./client/surf8.s:.globl C(R_DrawSurfaceBlock8_mip0)\n./client/surf8.s:.globl C(R_DrawSurfaceBlock8_mip1)\n./client/surf8.s:.globl C(R_DrawSurfaceBlock8_mip2)\n./client/surf8.s:.globl C(R_DrawSurfaceBlock8_mip3)\n./client/surf8.s:.globl C(R_Surf8End)\n./client/surf8.s:.globl C(R_Surf8Patch)\n./client/sys_dosa.s:.globl C(MaskExceptions)\n./client/sys_dosa.s:.globl C(unmaskexceptions)\n./client/sys_dosa.s:.globl C(Sys_LowFPPrecision)\n./client/sys_dosa.s:.globl C(Sys_HighFPPrecision)\n./client/sys_dosa.s:.globl C(Sys_PushFPCW_SetHigh)\n./client/sys_dosa.s:.globl C(Sys_PopFPCW)\n./client/sys_dosa.s:.globl C(Sys_SetFPCW)\n</pre><p>As a comparison, DOOM has only two  files and three functions to speed up the engine.</p><p>A lot of these functions can be discarded from this study. Some do things that cannot be done in C like setting the floating-point unit precision or setting up the High-precision counter (). Some are not used (). Some are duplicated (one for server, one for client). Some optimizations use self-modifying code requiring markers so the  region can be updated from  to  and patched ().</p><pre>$ find . -name \"*.s\" -exec grep -H \".globl C(\" {} \\;\n./server/worlda.s:.globl C(SV_HullPointContents)\n           // Duplicate from ./client/math.s\n // DOS\n // DOS\n./client/d_draw.s:.globl C(D_DrawSpans8)\n./client/d_draw.s:.globl C(D_DrawZSpans)\n   // Experimental 16-bit rendering\n\n./client/d_scana.s:.globl C(D_DrawTurbulent8Span)\n./client/r_drawa.s:.globl C(R_ClipEdge)\n./client/d_parta.s:.globl C(D_DrawParticle)\n./client/d_polysa.s:.globl C(D_PolysetCalcGradients)\n./client/d_polysa.s:.globl C(D_PolysetRecursiveTriangle)\n\n./client/d_polysa.s:.globl C(D_PolysetDrawSpans8)\n\n./client/d_polysa.s:.globl C(D_PolysetDraw)\n./client/d_polysa.s:.globl C(D_PolysetScanLeftEdge)\n./client/d_polysa.s:.globl C(D_PolysetDrawFinalVerts)\n./client/d_polysa.s:.globl C(D_DrawNonSubdiv)\n\n./client/math.s:.globl C(TransformVector)\n./client/math.s:.globl C(BoxOnPlaneSide)\n./client/d_draw16.s:.globl C(D_DrawSpans16)\n./client/r_aclipa.s:.globl C(R_Alias_clip_bottom)\n./client/r_aclipa.s:.globl C(R_Alias_clip_top)\n./client/r_aclipa.s:.globl C(R_Alias_clip_right)\n./client/r_aclipa.s:.globl C(R_Alias_clip_left)\n./client/snd_mixa.s:.globl C(SND_PaintChannelFrom8)\n./client/snd_mixa.s:.globl C(Snd_WriteLinearBlastStereo16)\n./client/r_aliasa.s:.globl C(R_AliasTransformAndProjectFinalVerts)\n./client/d_spr8.s:.globl C(D_SpriteDrawSpans)\n\n./client/r_edgea.s:.globl C(R_InsertNewEdges)\n./client/r_edgea.s:.globl C(R_RemoveEdges)\n./client/r_edgea.s:.globl C(R_StepActiveU)\n./client/r_edgea.s:.globl C(R_GenerateSpans)\n\n./client/surf8.s:.globl C(R_DrawSurfaceBlock8_mip0)\n./client/surf8.s:.globl C(R_DrawSurfaceBlock8_mip1)\n./client/surf8.s:.globl C(R_DrawSurfaceBlock8_mip2)\n./client/surf8.s:.globl C(R_DrawSurfaceBlock8_mip3)\n</pre><p>This still leaves 32 methods pertaining to math, sound, render, and draw. The distinction between R_ and D_ is not obvious. The R_ code is in charge of *what* to draw. The D_ code is in charge of *how* to draw it.</p><pre>\n./client/d_spr8.s:.globl C(D_SpriteDrawSpans)            // Draw sprite facing camera\n./client/d_draw.s:.globl C(D_DrawSpans8)                 // World draw  8 pixels persp\n./client/d_draw.s:.globl C(D_DrawZSpans)                 // World write to Z-Buffer\n./client/d_draw16.s:.globl C(D_DrawSpans16)              // World draw 16 pixels persp\n./client/d_scana.s:.globl C(D_DrawTurbulent8Span)\n./client/d_parta.s:.globl C(D_DrawParticle)\n./client/d_polysa.s:.globl C(D_PolysetCalcGradients)     // All the polysets are for\n./client/d_polysa.s:.globl C(D_PolysetRecursiveTriangle) // alias models rendering. \n./client/d_polysa.s:.globl C(D_PolysetDrawSpans8)        \n./client/d_polysa.s:.globl C(D_PolysetDraw)\n./client/d_polysa.s:.globl C(D_PolysetScanLeftEdge)\n./client/d_polysa.s:.globl C(D_PolysetDrawFinalVerts)\n./client/d_polysa.s:.globl C(D_DrawNonSubdiv)            // Also model drawing\n\n./client/math.s:.globl C(TransformVector)\n./client/math.s:.globl C(BoxOnPlaneSide)\n./server/worlda.s:.globl C(SV_HullPointContents)\n\n./client/snd_mixa.s:.globl C(SND_PaintChannelFrom8)\n./client/snd_mixa.s:.globl C(Snd_WriteLinearBlastStereo16)\n\n./client/r_drawa.s:.globl C(R_ClipEdge)\n./client/r_aclipa.s:.globl C(R_Alias_clip_bottom)\n./client/r_aclipa.s:.globl C(R_Alias_clip_top)\n./client/r_aclipa.s:.globl C(R_Alias_clip_right)\n./client/r_aclipa.s:.globl C(R_Alias_clip_left)\n./client/r_aliasa.s:.globl C(R_AliasTransformAndProjectFinalVerts)\n./client/r_edgea.s:.globl C(R_InsertNewEdges)\n./client/r_edgea.s:.globl C(R_RemoveEdges)\n./client/r_edgea.s:.globl C(R_StepActiveU)\n./client/r_edgea.s:.globl C(R_GenerateSpans)\n./client/surf8.s:.globl C(R_DrawSurfaceBlock8_mip0)       // Surface caching generation\n./client/surf8.s:.globl C(R_DrawSurfaceBlock8_mip1)       // Surface caching generation\n./client/surf8.s:.globl C(R_DrawSurfaceBlock8_mip2)       // Surface caching generation\n./client/surf8.s:.globl C(R_DrawSurfaceBlock8_mip3)       // Surface caching generation\n</pre><p>The next thing to do, before going deeper was to quantify how much each function contributes to improving the framerate from 22.7fps to 42.2fps. To find out, I modified the engine to enable one ASM function at a time and ran the same timedemo over and over again.</p><table><tbody><tr><th>Frames per Second (fps) gain</th></tr><tr></tr></tbody></table><p>Without surprise, the most important optimizations are in the low-level drawing routines with  to render the walls,  to combine texture and lightmap into a surface, and  to draw the models. The rest barely registered on my (rather crude) benchmark.</p><a href=\"https://fabiensanglard.net/quake_asm_optimizations/chart.svg\"><img loading=\"lazy\" src=\"https://fabiensanglard.net/quake_asm_optimizations/chart.svg\" width=\"759\" height=\"309\"></a><p>The Polyset* functions are intertwined in such a way they cannot be individually switched to C/ASM. They have to be all C or all ASM.</p><p>The ASM optimizations I found often involve loop unrolling, self-modifying code, avoiding mis-predictions, leveraging the Pentium FPU pipeline to hide latency, and creating \"overlap\" where both Pentium U/V pipelines and the FPU pipeline are executing instructions in parallel.</p><p>Here are a few detailed functions. For those willing to go ever deeper in that rabbit hole, I suggest reading <i>Optimizations for Intel's\n32-Bit Processors (Feb 94)</i><a name=\"back_2\" href=\"https://fabiensanglard.net/quake_asm_optimizations/index.html#footnote_2\"></a> which covers Pentium extensively. Be warned it is more powerful than 20g of melatonin.</p><p>The function  is a good introduction to the P5 FPU. It is a simple matrix-vector multiplication., used extensively to project everything in screen space, from world polygons, model/alias polygons, and sprites.</p><pre>typedef float vec_t;\ntypedef vec_t vec3_t[3];\n\nvec3_t  vpn, vright, vup;  \n\n#define DotProduct(x,y) (x[0]*y[0]+x[1]*y[1]+x[2]*y[2])\n\nvoid TransformVector (vec3_t in, vec3_t out) {\n  out[0] = DotProduct(in,vright);\n  out[1] = DotProduct(in,vup);\n  out[2] = DotProduct(in,vpn);    \n}\n</pre><p>Let's look at the assembly. I kept mabrash's asm in AT&amp;T notation<a name=\"back_3\" href=\"https://fabiensanglard.net/quake_asm_optimizations/index.html#footnote_3\"></a> on the left. On the right is what VC6 generated, in Intel notation, decompiled by Ninja.</p><table><tbody><tr><td><pre>\n\n.globl C(TransformVector)\n\nmovl  in(%esp),%eax\nmovl  out(%esp),%edx\n\nflds  (%eax)    \nfmuls C(vright) \nflds  (%eax)    \nfmuls C(vup)    \nflds  (%eax)    \nfmuls C(vpn)    \n\nflds  4(%eax)   \nfmuls C(vright)+4 \nflds  4(%eax)   \nfmuls C(vup)+4  \nflds  4(%eax)   \nfmuls C(vpn)+4  \nfxch  %st(2)    \n\nfaddp %st(0),%st(5) \nfaddp %st(0),%st(3) \nfaddp %st(0),%st(1) \n\nflds  8(%eax)   \nfmuls C(vright)+8 \nflds  8(%eax)   \nfmuls C(vup)+8    \nflds  8(%eax)   \nfmuls C(vpn)+8    \nfxch  %st(2)    \n\nfaddp %st(0),%st(5) \nfaddp %st(0),%st(3) \nfaddp %st(0),%st(1) \n\nfstps 8(%edx)   \nfstps 4(%edx)   \nfstps (%edx)     \n\nret\n</pre></td><td><pre>\n\nfloat* TransformVector(float* a1, float* a2)\n\nmov     eax, dword [esp+0x4 {a1}]\nmov     ecx, dword [esp+0x8 {a2}]\n\nfld     st0, dword [0x2970]  \nfmul    st0, dword [eax]\nfld     st0, dword [0x2978]  \nfmul    st0, dword [eax+0x8] \nfaddp   st1, st0\nfld     st0, dword [0x2974]  \nfmul    st0, dword [eax+0x4]\nfaddp   st1, st0\nfstp    dword [ecx], st0\n\nfld     st0, dword [0x2974]  \nfmul    st0, dword [eax]\nfld     st0, dword [0x297c]  \nfmul    st0, dword [eax+0x8]\nfaddp   st1, st0\nfld     st0, dword [0x2978]  \nfmul    st0, dword [eax+0x4]\nfaddp   st1, st0\nfstp    dword [ecx+0x4], st0\n\nfld     st0, dword [0x296c]  \nfmul    st0, dword [eax]\nfld     st0, dword [0x2974]  \nfmul    st0, dword [eax+0x8]\nfaddp   st1, st0\nfld     st0, dword [0x2970]  \nfmul    st0, dword [eax+0x4]\nfaddp   st1, st0\nfstp    dword [ecx+0x8], st0\n\n\n\n\n\n\nretn     {__return_addr}</pre></td></tr></tbody></table><p> The FPU is used like a 487 FPU. Namely an un-pipelined stack where operands are picked up from the top of the stack and results are pushed back on the top of the stack (if you know how a JVM works, that is the same principle). Instructions are found in the same order as on the code, one dot-product after another. And each dot product is *, *, +, *, +. The whole sequence looks as follows.</p><pre>*, *, +, *, +, store\n*, *, +, *, +, store\n*, *, +, *, +, store</pre><p>This approach incurs stalls. A  takes three cycles<a name=\"back_4\" href=\"https://fabiensanglard.net/quake_asm_optimizations/index.html#footnote_4\"></a> to return a result. This means that each  stalls for two cycles while waiting for  result to be available.</p><p> That is a radically different approach. It enqueue as many independent (result not depending on previous operation) instructions as possible in the pipeline. On a 487 that would be a problem because the operands would have to be re-organized with costly  (4 cycles!) to swap their location on the stack.</p><p> But  is free (0 cycle) on Pentium. This instruction allows developers to use nearly all the registers () in the FPU stack. It turns the cumbersome legacy FPU stack into a convenient register array.</p><p>This allows to calculate three dot products in parallel, with three partial sums on the x87 stack at all times. And the computation looks as follows.</p><pre>* * * * * *\n+ + +\n* * *\n+ + +\nstore, store, store\n</pre><p>By the time it does the additions, the results of the multiplication are already available. This hides  latency and lets the P5 avoid stales completely. </p><p> Another optimization in Abrash's version, are the stores () located at the end instead of being mixed with other operations like in the VC6 output. Storing a value (fstp) immediately after calculating results in a 1-cycle stall because the write-back stage of the pipeline cannot be bypassed<a name=\"back_5\" href=\"https://fabiensanglard.net/quake_asm_optimizations/index.html#footnote_5\"></a>. Having the stores at the end ensures that the last faddp has enough cycles to complete before the fstp tries to move that data into memory.</p><p>This function is not actually used in Quake. It is likely one of these optimizations that Michael Abrash wrote and had to be abandoned because John Carmack rewrote the engine completely.</p><blockquote>\nMichael Abrash focused on the x86 assembly optimizations. There were some times where he had spent a lot of effort on a low level routine, then I changed the architecture and he had to start over, which I felt a little bad about, even though it was net-positive.<p>He did use a NeXT for some things (he managed the code merges between us), but he had to do his assembly timings on DOS.\n</p><div>- Conversation with John Carmack</div></blockquote><pre>fixed16_t Invert24To16(fixed16_t val) {\n  if (val &lt; 256)\n    return (0xFFFFFFFF);\n\n  return (fixed16_t)\n      (((double)0x10000 * (double)0x1000000 / (double)val) + 0.5);\n}\n</pre><p>What is cool to see is that no stone were left unturned. Here the main goal of the rewrite is to avoid a call to Microsoft costly CRT  function.</p><table><tbody><tr><td><pre>.globl C(Invert24To16)\n\n  movl  val(%esp),%ecx\n  movl  $0x100,%edx // 0x10000000000 dividend\n  cmpl  %edx,%ecx\n  jle   LOutOfRange\n\n  subl  %eax,%eax\n  divl  %ecx\n\n  ret\n\nLOutOfRange:\n  movl  $0xFFFFFFFF,%eax\n  ret</pre></td><td><pre>int32_t _Invert24To16(int32_t arg1)\n\ncmp     dword [esp+0x4 {arg1}], 0x100\njge     0xf04\n\nor      eax, 0xffffffff  {0xffffffff}\nretn     {__return_addr}\n\nfild    st0, dword [esp+0x4 {arg1}]\nfdivr   st0, qword [__real@4270000]\nfadd    st0, qword [__real@3fe0000]\njmp     __ftol\n\n\n\n</pre></td></tr></tbody></table><p>By the time the engine reaches R_DrawSurfaceBlock8, it has determined which part of a wall is visible. Now the enderer needs to \"bake\" the lightmap into the texture. The result is called a \"Surface\" (that is later handed to the rawer which rasterizes to the framebuffer). Michael Abrash describes this part extensively in <a href=\"https://www.phatcode.net/res/224/files/html/ch68/68-01.html\">Chapter 68: Quake’s Lighting Model</a> so I won't elaborate more on it.</p><p>There are four R_DrawSurfaceBlock8_mip functions. One for each level of mipmap. Here is a clickable image modified engine to show where each level triggers.</p><img src=\"https://fabiensanglard.net/quake_asm_optimizations/mipmap2.png\" onclick=\"flip(this); \"><p>The C version of all four functions is <a href=\"https://github.com/id-Software/Quake/blob/bf4ac424ce754894ac8f1dae6a3981954bc9852d/WinQuake/r_surf.c#L343\">here</a>. The ASM versions are <a href=\"https://github.com/id-Software/Quake/blob/bf4ac424ce754894ac8f1dae6a3981954bc9852d/WinQuake/surf8.s#L47\">here</a>. And the VC6 output for  is <a href=\"https://fabiensanglard.net/quake_asm_optimizations/R_DrawSurfaceBlock8_mip0.txt\">here</a>.</p><p>The most obvious optimization is the self-modifying code. Several memory locations are <a href=\"https://github.com/id-Software/Quake/blob/bf4ac424ce754894ac8f1dae6a3981954bc9852d/WinQuake/surf8.s#L121\">hard-coded to 0x12345678</a> and patched in <a href=\"https://github.com/id-Software/Quake/blob/bf4ac424ce754894ac8f1dae6a3981954bc9852d/WinQuake/surf8.s#L766\">R_Surf8Patch</a> just before R_DrawSurfaceBlock8 is called. The patching bakes the colormap base into the instruction stream which avoids using a register to keep the base. Moreover, this avoids an extra ADD to lookup the colormap.</p><p>The inner \"b\" loop is fully unrolled. This further saves a register by avoiding a loop counter. And one misprediction is avoided on the last iteration (the P5 always picks the backward jmp destination in order to excel at loops).</p><p>Given the importance of this function, I understand better now why Michael Abrash mentioned it in his book.</p><blockquote>\nAs it turns out, the raw speed of surface-based lighting is pretty good. Although an extra step is required to build the surface, moving lighting and tiling into a separate loop from texture mapping allows each of the two loops to be optimized very effectively, with almost all variables kept in registers.<p>The surface-building inner loop is particularly efficient, because it consists of nothing more than interpolating intensity, combining it with a texel and using the result to look up a lit texel color, and storing the results with a dword write every four texels.</p><p>In assembly language, we got this code down to 2.25 cycles per lit texel in Quake. </p><div>- Michael Abrash, Chapter 68: Quake’s Lighting Model</div></blockquote><p>Quake uses an Active Edge Table to render polygons as horizontal spans (I wrote about that <a href=\"https://fabiensanglard.net/quake2/quake2_software_renderer.php\">15 years ago</a> if you want to see it in action). The <a href=\"https://github.com/id-Software/Quake/blob/bf4ac424ce754894ac8f1dae6a3981954bc9852d/WinQuake/d_scan.c#L257\">C version</a> is a pretty big function which spans over 218 lines of code. VC6 <a href=\"https://fabiensanglard.net/quake_asm_optimizations/D_DrawSpans8.txt\">generated</a> 256 lines of ASM. And the <a href=\"https://github.com/id-Software/Quake/blob/bf4ac424ce754894ac8f1dae6a3981954bc9852d/WinQuake/d_draw.s#L91\">hand-optimized version</a> is a 650 lines juggernaut.</p><p>D_DrawSpans8 receives a list of spans (a portion of a surface) to be rasterized to the framebuffer. The goal is to be perspective correct every 8 pixels (vs D_DrawSpans16 which does it every 16 pixels) and interpolate the rest.</p><p>The biggest challenge of this function is that interpolating Z in screenspace does not work. In order to be perspective-correct, the interpolation must be done on 1/z. A division is the worst thing you can ask from the P5 FPU since it can take up to 39 cycles on a P5.</p><p>The main optimization here is a huge \"overlap\" where an  is issued for the next 8-pixel span at the very beginning of the current span. While the FPU is doing that division for 30+ cycles, the CPU's integer U and V pipelines draw the current 8 pixels. Many comments mention how the divide is \"in-flight\".  A funny comment from Michael Abrash assesses of the extensive care he put to do other things in the integer pipelines while fdiv is running in the floating-point pipeline.</p><pre> fdiv  %st(1),%st(0) </pre><p>To avoid a mis-prediction on the last part of a span (which may feature less than 8 pixels, a Jump table is used. The code calculates the number of pixels to draw in the span, looks up a memory address in a table, and jumps directly to a label like Entry3_8. Zero mis-prediction possible here.</p><p>There are other tiny optimizations but given how white hot this function is, everything counts. This is the case of clamp. In the C version, it performs two tests, one for \"too high\" and another one for \"below zero\" which is two branches that can result in mis-predictions.  By using  (Jump if Above), an unsigned comparison on signed integers, both high and low conditions are tested at once (if the value is negative, it turns into a very big integer that is above \"too high\"). This is super neat.</p><p> Throughout the ASM code of Quake, there are several mentions where Michael was looking for \"overlap\". This seems to indicate an obsession to find places where the FPU and the integer pipeline could process instructions in parallel.</p><pre> \n  // TODO: any overlap from rearranging?\n\n</pre><p>Like it was the case for R_DrawSurfaceBlock8_mip, Michael Abrash brought up D_DrawSpans in his <i>Graphic Programming Black Book</i> which underlines further how paramount this optimization was at the time.</p><blockquote>\nThe texture-mapping inner loop, which overlaps an FDIV for floating-point perspective correction with integer pixel drawing in 16-pixel bursts, has been squeezed down to 7.5 cycles per pixel on a Pentium, so the combined inner loop times for building and drawing a surface is roughly in the neighborhood of 10 cycles per pixel which is fast enough to do 40 frames/second at 640×400 on a Pentium/100.<div>- Michael Abrash, Chapter 68: Quake’s Lighting Model</div></blockquote><p>If you want to dig deeper, <a href=\"https://fabiensanglard.net/quake_asm_optimizations/objs.zip\">here</a> are the objs resulting from a compilation of Quake with assembly optimizations disabled. The disassembly can easily be extracted with Binary Ninja.</p>","contentLength":21350,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1r5ni65/how_michael_abrash_doubled_quake_framerate/"},{"title":"How to Choose Between Hindley-Milner and Bidirectional Typing","url":"https://thunderseethe.dev/posts/how-to-choose-between-hm-and-bidir/","date":1771178837,"author":"/u/thunderseethe","guid":343,"unread":true,"content":"<p>This question is common enough you’ve probably heard it posed countless times:\n“Should my new programming language use a Hindley-Milner (HM) type system or a Bidirectional (Bidir) one?”\nWhat’s that?\nI need to understand friends don’t just bring up type inference in casual conversation?</p><p>OK, ouch, fair enough.\nBut…whatever.\nThis is my blog.\nWe’re doing it anyway!\nI don’t know what you expected when you clicked on a programming languages blog.</p><p>Picking a type system is a real barrier for would be language developers.\nEyes full of trepidation as they navigate the labyrinth of nuanced choice that goes into everything a programming language asks of them.\nWhich type system to choose is just another quandary in the quagmire as they trudge towards a working prototype.</p><p>Its understandable they’d want to make a quick decision and return to marching.\nBut this is the wrong question to ask.\nThe question presumes that HM and Bidir are two ends of a spectrum.\nOn one end you have HM with type variables and unification and all that jazz.\nOn the other end you have bidirectional typing where annotations decide your types and little inference is involved.\nThis spectrum, however, is a false dichotomy.</p><p>What folks should actually be asking is “Does my language need generics?”.\nThis question frames the problem around what your language needs, rather than an arbitrary choice between two algorithms of abstract tradeoffs.\nPerhaps more importantly, it determines if you’ll need unification or not.</p><p>Generics, generally, require a type system that supports unification.\nUnification is the process of assigning and solving type variables.\nIf you’ve ever seen Rust infer a type like , that’s unification chugging along.</p><div><div>We don’t have time today. But if you’re interested in how unification works, I have a <a href=\"https://thunderseethe.dev/posts/unification\">tutorial about it\n</a>.</div></div><p>When facing down designing a type system, knowing if you need unification or not decides a lot for you.\nUnification sits center stage in Hindley-Milner.\nWhen you pick HM you pick unification.</p><p>The story is more interesting for bidirectional typing.\nIf you look to the literature, you’ll find plenty of example of bidirectional typing without a unification in sight.\nBy introducing annotations at key locations, you can type check sophisticated programs with no type variables.\nA key insight of bidirectional type is how much you can do without unification.\nAnd don’t get me wrong; it is cool how much it can do.</p><p>But this leads to the incorrect perception that bidir  or  use unification.\nThe opposite couldn’t be more true.\nBidirectional typing supports all the same features as HM typing, and more, forming more of a superset relationship.\nUnification slots into bidirectional typing like a vim user slots into home row.</p><p>This is because bidirectional typechecking is a superset of HM.\nImagine we have some AST (in Rust):</p><div><pre tabindex=\"0\"><code data-lang=\"rs\"></code></pre></div><p>And we have a Type we’d like to assign to our AST:</p><div><pre tabindex=\"0\"><code data-lang=\"rs\"></code></pre></div><p>With an HM system, we provide an  function:</p><div><pre tabindex=\"0\"><code data-lang=\"rs\"></code></pre></div><p>Wow, just like that we have a real actual HM type system.</p><div><div>Please ignore all the details we’re brushing over.</div></div><p>We can imagine we’re doing all sorts of unification in .\nIf we want to make that system bidirectional, it’s just a matter of adding a  function:</p><div><pre tabindex=\"0\"><code data-lang=\"rs\"></code></pre></div><p>Technically,  doesn’t even have to do anything.\nA perfectly valid implementation of check would be:</p><div><pre tabindex=\"0\"><code data-lang=\"rs\"></code></pre></div><p>We infer a type for our AST and check that it’s equal to the expected type.\nThat’s all it takes to be bidirectional.\nHowever, equality is pretty stringent here.\nThe first time we check a type like  against a type like  our entire type inference grinds to a halt.</p><p>Instead, let’s slot unification into the same position.\nRather than requiring strict equality, we can loosen  to require that our types unify:</p><div><pre tabindex=\"0\"><code data-lang=\"rs\"></code></pre></div><p>With that modest adjustment, we’re bidirectional and we’re unifying.\nNow, of course, once we’ve done that we’re free to make better use of our  whenever we like.\nLet’s say we happen to know our AST has functions:</p><div><pre tabindex=\"0\"><code data-lang=\"rs\"></code></pre></div><p>And we’re good language developers, so of course that means Type gets a function case as well:</p><div><pre tabindex=\"0\"><code data-lang=\"rs\"></code></pre></div><p>We return to our check case and notice that, rather than inferring functions, we can take a little shortcut:</p><div><pre tabindex=\"0\"><code data-lang=\"rs\"></code></pre></div><p>But the point is we don’t have to.\nIf you are going to choose a Hindley-Milner type system, you might as well add four lines of code and make it a bidirectional system.\nIts free real estate.</p><p>Okay so you’re sold on bidirectional typing.\nI can see it in your eyes.\nLet’s return to our underlying question “should I support generics or not?”.\nUnification is a daunting task.\nWhen does it make sense and when does it not?</p><p>Unification is great when you don’t want to have to spell out the type of every variable in your program.\nIt’s even wormed it’s way into older language likes Java and C++ because it’s so handy to not have to spell out types.\nEven Go, a diehard in the anti-generics camp, finally capitulated and added generics.\nAnyone aiming to make a general purpose programming language should consider generics a must.\nBut, that’s not every language’s goal.</p><p>A lot of people embark on making a programming language as a learning exercise.\nIn those cases unification can present a bundle of extra complexity that doesn’t really teach you anything about what you want to learn.\nIf you’re interested in learning about type systems, unification is a must.\nBut if you just need some types so you can emit code later, that is a great time to look at bidirectional type systems that don’t use unification and require type annotations.</p><p>Or perhaps your language isn’t general purpose and you’re after a Domain Specific Language perfectly suited to your niche.\nDSLs don’t have to cover all of computation and can eschew generics to reduce surface area and concepts in the language, depending on use case.\nWith the warning that successful DSLs grow up to become general purpose programming languages, looking at you awk, and then you really hurt for the lack of features.</p><p>Regardless of where your aims, the real question you should be asking yourself is “Do I want generics or not?”.\nWhatever your answer, bidirectional typing has got you covered.</p>","contentLength":6124,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1r5lg6n/how_to_choose_between_hindleymilner_and/"},{"title":"Ring programming language version 1.26 is released!","url":"https://ring-lang.github.io/doc1.26/whatisnew26.html","date":1771160142,"author":"/u/mrpro1a1","guid":342,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1r5dyof/ring_programming_language_version_126_is_released/"},{"title":"Package Management Namespaces","url":"https://nesbitt.io/2026/02/14/package-management-namespaces.html","date":1771145831,"author":"/u/max123246","guid":341,"unread":true,"content":"<p>Every package needs a name. The rules for how those names work is one of the most consequential decisions a package manager makes, and one of the hardest to change later. I <a href=\"https://nesbitt.io/2025/12/29/categorizing-package-registries.html\">categorized the approaches</a> previously and touched on the <a href=\"https://nesbitt.io/2025/12/05/package-manager-tradeoffs.html\">tradeoffs</a> briefly.</p><p>RubyGems, PyPI, crates.io, Hex, Hackage, CRAN, and LuaRocks all use flat namespaces: one global pool of names, first-come-first-served. You pick a name, and if nobody has it, it’s yours.</p><p>This gives you , , . The names are short, memorable, and greppable, with no punctuation to remember and no organization to look up.</p><p>At scale, though, good names run out. Someone registers  on day one and never publishes a real package. Or they publish something, abandon it, and the name sits there forever, pointing at a library last updated in 2013. PyPI has over 600,000 projects. Many of the short, obvious names were claimed years ago by packages with single-digit downloads.</p><p>Name scarcity creates pressure, and you end up with  because  was taken,  because  was the old version, or  because the original  package was abandoned and PyPI doesn’t recycle names. New developers have to learn not just what to install but which of several similar-sounding packages is the right one.</p><p>Flat namespaces also make <a href=\"https://nesbitt.io/2025/12/17/typosquatting-in-package-managers.html\">typosquatting</a> straightforward. Someone registers  next to  and waits. The attack works because there’s nothing between the user’s keystrokes and the registry lookup, no organization to verify and no hierarchy to navigate, just a string match against a flat list.</p><p>Some registries add normalization rules to limit this. PyPI treats hyphens, underscores, and dots as equivalent, so  and  resolve to the same thing. crates.io does similar normalization. RubyGems doesn’t, which is why both  and  can coexist as unrelated packages.</p><p>npm added scopes in 2014. Instead of just , you could publish . Packagist has always used  format: , . JSR, Ansible Galaxy, Puppet Forge, and others follow similar patterns.</p><p>Scopes split the package name into two parts: who published it, and what they called it. Different organizations can use the same package name without collision, so  and  coexist without confusion.</p><p>npm’s implementation is interesting because scopes are optional. You can still publish unscoped packages to the flat namespace. So npm actually has two systems running in parallel: a flat namespace for legacy packages and a scoped namespace for newer ones.</p><p>Most of the ecosystem’s most-used packages (, , ) predate scopes and sit in the flat namespace. Scopes are most common for organizational packages (everything under , for example) and type definitions (). And because so much of the ecosystem depends on unscoped names, npm can never require scopes without breaking the world.</p><p>Packagist required scopes from the start. Every Composer package is , no exceptions. This avoided the split-namespace problem npm has, but it means you need to know the vendor name. Is it  or ? You have to look it up. And vendor names themselves are first-come-first-served, just pushing the squatting problem up one level. The stakes are higher, though, because squatting a vendor name locks out an entire family of package names rather than just one. Someone could register the  vendor on Packagist before Google gets there, and that blocks every  package at once.</p><p>Scopes also require governance. Who decides that  belongs to the Babel team? npm ties scopes to user accounts and organizations, which means you need account management, ownership transfer procedures, and dispute resolution. When a maintainer leaves a project, their scoped packages might need to move. This is solvable but adds operational overhead that flat registries avoid.</p><p>Maven Central uses reverse-domain naming: <code>org.apache.commons:commons-lang3</code>, . The group ID is supposed to correspond to a domain you control.</p><p>The reverse-domain approach ties naming authority to DNS. If you own , you can publish under . This defers governance to the existing DNS system rather than requiring the registry to manage name ownership. Maven Central enforces this by requiring you to prove domain ownership, or for projects without their own domain, to use  as a fallback.</p><p>That fallback is interesting because it quietly undermines the premise: the whole point of reverse-domain naming is that you prove ownership of infrastructure you control, but  just defers to GitHub’s namespace. It’s URL-based naming wearing a reverse-domain costume.</p><p>Organizations with stable domains get clean namespaces out of this. Apache, Google, and Spring all have clear homes. The trade-off is verbose identifiers. <code>org.springframework.boot:spring-boot-starter-web</code> is a lot of characters. IDE autocompletion papers over this in Java, but the verbosity is real when reading build files or discussing dependencies.</p><p>Domain ownership is also less stable than it looks. Companies get acquired and change domains. Open source projects move between hosting organizations. A package published under  in 2005 might need to live under  after the acquisition, except it can’t, because changing the group ID would break every project that depends on the old one. So old names persist as historical artifacts.</p><p>The hierarchy also doesn’t prevent all squatting. Someone could register a domain specifically to claim a Maven namespace. More concerning is domain resurrection: when a domain expires after its owner has already registered a Maven group ID, anyone can buy that domain and potentially claim the namespace. Maven Central <a href=\"https://central.sonatype.org/register/namespace/\">verifies domain ownership</a> when you first register a group ID, requiring a DNS TXT record, but that verification is a point-in-time check.</p><p>In January 2024, security firm Oversecured published <a href=\"https://blog.oversecured.com/Introducing-MavenGate-a-supply-chain-attack-method-for-Java-and-Android-applications/\">MavenGate</a>, an analysis of 33,938 domains associated with Maven group IDs. They found that 6,170 of them, roughly 18%, had expired or were available for purchase. The affected group IDs included widely-used libraries like , , and . A new owner of any of those domains could publish new versions under the existing group ID. Existing artifacts on Maven Central are immutable so old versions wouldn’t change, but build files that pull the latest version would pick up the attacker’s release.</p><p>Sonatype responded by disabling accounts tied to expired domains and tightening their verification process, but they haven’t announced ongoing domain monitoring. PyPI, facing the same problem with account email domains, <a href=\"https://blog.pypi.org/posts/2025-08-18-preventing-domain-resurrections/\">built automated daily checks</a> in 2025 and found around 1,800 accounts to unverify.</p><p>Clojars shows what happens when a registry in the Maven ecosystem takes a different approach. Clojure libraries are distributed as Maven artifacts, but Clojars originally let you use any group ID without verification. You could publish under  or  with no domain proof. This was simpler for the Clojure community, where most libraries are small and maintained by individuals, but it meant Clojars had a much more relaxed namespace than Maven Central.</p><p>Since build tools can pull from both registries, the gap created a dependency confusion risk: an attacker could register an unverified group on Clojars that shadows a legitimate Maven Central library. In 2021, after dependency confusion attacks became widely understood, Clojars <a href=\"https://github.com/clojars/clojars-web/wiki/Verified-Group-Names\">started requiring verified group names</a> for new projects, adopting the same reverse-domain convention as Maven Central. Existing projects with unverified groups were grandfathered in, so the old flat names still exist alongside the new hierarchical ones.</p><p>Go modules use import paths that are URLs: , . There’s no registration step. The URL points to a repository, and the module system fetches code from there (or from the Go module proxy, which caches it).</p><p>This model sidesteps the registry as naming authority entirely. You publish code to a repository and the URL is the identifier, with no approval step required. Name collisions don’t arise because URLs are globally unique by construction, and owning the repo means owning the name.</p><p>Names become tied to hosting infrastructure, though. When  is the package identity, a GitHub org rename breaks every downstream consumer. Go addressed this with the module proxy, which caches modules so they survive repo disappearance, but the name still reflects the original location even if the code has moved. Import paths like  that redirect to  create confusion about which is canonical. And your package identity depends on a third party either way: GitHub controls the  namespace, so if they ban your account or the organization renames, your package identity changes. You’ve traded one governance dependency for another, a hosting platform instead of a registry.</p><p>“No registration step” has its own consequences. Without a registry to mediate names, there’s no obvious place to check for existing packages, no search, no download counts, no centralized vulnerability database. Go built most of these features separately with pkg.go.dev and the module proxy. The URL-based naming stayed, but the surrounding infrastructure converged toward what registries provide anyway, just assembled differently.</p><p>Deno launched with raw URL imports and eventually built <a href=\"https://jsr.io\">JSR</a>, a scoped registry with semver resolution, because URL imports created <a href=\"https://deno.com/blog/http-imports\">problems they couldn’t solve</a> at the URL layer: duplicated dependencies when the same package was imported from slightly different URLs, version management scattered across every import statement, and reliability issues when hosts went offline. You can start without a registry, but the things registries do (search, versioning, deduplication, availability) keep needing to be solved, and solving them piecemeal tends to reconverge on something registry-shaped.</p><p>Apple hired Max Howell to build SwiftPM in 2015. He’d created Homebrew and used both CocoaPods and Carthage heavily, so he arrived with strong opinions about how a language package manager should work. As he told <a href=\"https://changelog.com/podcast/232\">The Changelog</a>: “I’d been involved with CocoaPods and Carthage and used them heavily, and obviously made Homebrew, so I had lots of opinions about how a package manager should be.” He was drawn to decentralization, something he wished Homebrew had from the start.</p><p>Carthage had already demonstrated the approach in the Apple ecosystem, launching in 2014 as a deliberate reaction against CocoaPods’ centralized registry, using bare Git URLs with no registry at all. SwiftPM followed the same path, using Git repository URLs as package identifiers with no central registry.</p><p>Go made the same choice but then spent years building infrastructure around it: a module proxy that caches source in immutable storage so deleted repos still resolve, a checksum database () that uses a transparency log to guarantee every user gets identical content for a given version, and pkg.go.dev for search and discovery.</p><p>SwiftPM doesn’t have any of this yet. Every  clones directly from the Git host. If a repo disappears, resolution fails with no fallback. SwiftPM records a fingerprint per package version the first time it downloads it, but that fingerprint lives on your machine only. There’s no global database to verify that what you downloaded matches what everyone else got, no way to detect a targeted attack serving different content to different users.</p><p>A <a href=\"https://checkmarx.com/blog/chainjacking-the-new-supply-chain-attack/\">2022 Checkmarx study</a> found thousands of packages across Go and Swift vulnerable to repo-jacking, where an attacker registers an abandoned GitHub username and recreates a repo that existing packages still point to. Go’s proxy mitigates this because cached modules don’t re-fetch from the source, but SwiftPM has no such layer.</p><p>The pieces to fix this are partly in place. Apple defined a <a href=\"https://github.com/swiftlang/swift-evolution/blob/main/proposals/0292-package-registry-service.md\">registry protocol</a> (SE-0292, shipped in Swift 5.7) and built client support for it in SwiftPM, including package signing. The client tooling is ready, the protocol is specified, and the ecosystem is still small enough that introducing a namespace layer wouldn’t require the kind of painful migration that npm or PyPI face. The <a href=\"https://swiftpackageindex.com\">Swift Package Index</a>, community-run and Apple-sponsored, already tracks around 12,000 packages. What’s missing is the public registry service itself and the integrity infrastructure around it, and the window for adding these before the ecosystem’s size makes it much harder is not open forever.</p><p>As I wrote about in <a href=\"https://nesbitt.io/2026/01/23/package-management-is-a-wicked-problem.html\">Package Management is a Wicked Problem</a>, once PyPI accepted namespace-less package names, that was permanent. If PyPI added mandatory namespaces tomorrow, every existing , every tutorial, every CI script would need updating. The new system would have to support both namespaced and un-namespaced packages indefinitely. You haven’t replaced the flat namespace, you’ve just added a layer on top of it.</p><p>npm’s experience shows what this looks like in practice. Scoped packages have been available since 2014, but most of the ecosystem still uses flat names. The existence of scopes didn’t make  become  because too much already depends on the existing name. Scopes ended up being used primarily for new packages and organizational groups rather than as a migration path for the existing namespace.</p><p>NuGet went through a partial migration. It added package ID prefix reservation in 2017, letting Microsoft reserve the  prefix. But this is a bolt-on: the underlying namespace is still flat, and the prefixes are just a verified badge on the registry UI. It helps users identify official packages but doesn’t change the naming model.</p><p>PyPI is threading this needle right now with <a href=\"https://peps.python.org/pep-0752/\">PEP 752</a>, which proposes letting organizations reserve package name prefixes. Google could reserve , Apache could reserve <code>apache-airflow-providers-</code>, and future uploads matching those prefixes would require authorization from the namespace owner. Like NuGet’s approach, it requires no installer changes and leaves existing packages unaffected. It only applies going forward, though, and the thousands of existing packages with no organizational prefix remain as they are.</p><p>Cargo and crates.io are attempting something more ambitious. The Rust community has been discussing namespaces since at least 2014, and after several earlier proposals that leaned toward npm-style user or org scopes, they settled on <a href=\"https://rust-lang.github.io/rfcs/3243-packages-as-optional-namespaces.html\">RFC 3243</a> (“Packages as Optional Namespaces”), authored by Manish Goregaokar, who had been working on the problem since at least 2018 when the first “packages as namespaces” pre-RFC appeared.</p><p>The approach treats existing crate names as potential namespace roots: if you own the  crate, you can publish , and only owners of  can create crates in that namespace. Ownership flows down automatically. The  separator was chosen after extensive debate because it aligns with Rust’s existing path syntax, so <code>serde::derive::Deserialize</code> reads naturally in Rust source. An earlier proposal used  but that conflicted with Cargo’s feature syntax.</p><p>The design is carefully scoped. Namespaces are optional, so the flat namespace stays and nothing breaks. It’s framed around projects rather than organizations, with the primary use cases being things like  or  rather than org-level grouping. Only single-level nesting is supported for now. And they explicitly chose not to do NuGet-style prefix reservation because in a flat namespace where  already exists, reserving the  prefix would create confusion about whether existing  crates are actually owned by .</p><p>The migration challenges are real even with this careful design. A crate like  already exists in the flat namespace, and transitioning it to  means a new name that every downstream consumer would need to update. The RFC suggests maintaining re-export crates during transition, but there’s no alias mechanism yet. Some projects face an even harder version of this problem: the  project manages a family of  crates, but someone else owns the  crate, so they can’t use it as their namespace root.</p><p>The RFC was accepted and became an official Rust project goal for 2025, led by Ed Page on the Cargo team. As of late 2025, Cargo support is partially implemented but compiler support is still in progress, requiring coordination across the lang, compiler, and crates.io teams. It’s the most carefully designed attempt at retrofitting namespaces onto a flat registry that I’m aware of, and the fact that it’s taking years of design and implementation work for a well-resourced community with strong governance shows how hard this problem is once a flat namespace is established.</p><p>If you’re starting a registry today, you don’t have to require namespaces from day one, but you could reserve the separator character and the ownership semantics so that namespaces can be added later without conflicting with existing names. The reason crates.io can use  is that no existing crate name contains it. If they’d allowed colons in crate names from the start, this whole approach would have been foreclosed. Keeping your options open costs almost nothing at launch and can save years of design work later.</p>","contentLength":16879,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1r59xjq/package_management_namespaces/"},{"title":"The Next Two Years of Software Engineering","url":"https://addyosmani.com/blog/next-two-years/","date":1771142337,"author":"/u/fagnerbrack","guid":345,"unread":true,"content":"<p>The software industry sits at a strange inflection point. AI coding has evolved from autocomplete on steroids to agents that can autonomously execute development tasks. The economic boom that fueled tech’s hiring spree has given way to an efficiency mandate: companies now often favor profitability over growth, experienced hires over fresh graduates, and smaller teams armed with better tools.</p><p>Meanwhile, a new generation of developers is entering the workforce with a different calculus: pragmatic about career stability, skeptical of hustle culture, and raised on AI assistance from day one.</p><p>What happens next is genuinely uncertain. Below are five critical questions that may shape software engineering through 2026, with two contrasting scenarios for each. These aren’t really predictions, but lenses for preparation. The goal is a clear roadmap for handling what comes next, grounded in current data and tempered by the healthy skepticism this community is known for.</p><h2>1. The Junior developer question</h2><p><strong>The bottom line: Junior developer hiring could collapse as AI automates entry-level tasks, or rebound as software spreads into every industry. Both futures require different survival strategies.</strong></p><p>The flip scenario: AI unlocks massive demand for developers across every industry, not just tech. Healthcare, agriculture, manufacturing, and finance all start embedding software and automation. Rather than replacing developers, AI becomes a force multiplier that spreads development work into domains that never employed coders. We’d see more entry-level roles, just different ones: “AI-native” developers who quickly build automations and integrations for specific niches.</p><p>The long-term risk of the pessimistic scenario is often overlooked: today’s juniors are tomorrow’s senior engineers and tech leaders. Cut off the talent pipeline entirely and you create a leadership vacuum in 5-10 years. <a href=\"https://www.finalroundai.com/blog/ai-is-making-it-harder-for-junior-developers-to-get-hired\">Industry veterans call this the “slow decay”</a>: an ecosystem that stops training its replacements.</p><p> Make yourself AI-proficient and versatile. Demonstrate that one junior plus AI can match a small team’s output. Use AI coding agents (Cursor/Antigravity/Claude Code/Gemini CLI) to build bigger features, but understand and explain every line if not most. Focus on skills AI can’t easily replace: communication, problem decomposition, domain knowledge. Look at adjacent roles (QA, DevRel, data analytics) as entry points. Build a portfolio, especially projects integrating AI APIs. Consider apprenticeships, internships, contracting, or open source. Don’t be “just another new grad who needs training”; be an immediately useful engineer who learns quickly.</p><p> Fewer juniors means more grunt work landing on your plate. Lean on automation for routine tasks, but don’t do everything yourself. Set up CI/CD, linters, and AI-assisted testing to catch basic issues. Mentor unofficially through open source or coaching colleagues in other departments. <a href=\"https://www.finalroundai.com/blog/ai-is-making-it-harder-for-junior-developers-to-get-hired\">Be frank with management about the risks of all-senior teams</a>. If junior demand rebounds, be ready to onboard effectively and delegate in ways that use AI. Your value is in multiplying the whole team’s output, not just your own code.</p><p><strong>The bottom line: Core programming skills could atrophy as AI writes most code, or become more critical than ever as human developers focus on oversight. The coming years determine whether we trade understanding for speed.</strong></p><p><a href=\"https://stackoverflow.blog/2025/09/10/ai-vs-gen-z/\">84% of developers now use AI assistance regularly</a>. For many, the first instinct when facing a bug or new feature isn’t to write code from scratch, but to compose a prompt and stitch together AI-generated pieces. Entry-level coders are skipping the “hard way”: they might never build a binary search tree from scratch or debug a memory leak on their own.</p><p>The skillset is shifting from implementing algorithms to knowing how to ask the AI the right questions and verify its output. <a href=\"https://www.cio.com/article/4062024/demand-for-junior-developers-softens-as-ai-takes-over.html\">The first rung of the ladder now demands prompting and validating AI</a> rather than demonstrating raw coding ability. Some senior engineers worry this produces a generation who can’t code well independently, a kind of deskilling. AI-generated code introduces subtle bugs and security vulnerabilities that less-experienced developers might miss.</p><p>The counter-scenario: as AI handles the routine 80%, humans focus on the hardest 20%. Architecture, tricky integrations, creative design, edge cases: the problems machines alone can’t solve. Rather than making deep knowledge obsolete, AI’s ubiquity makes human expertise more important than ever. This is the “high-leverage engineer” who uses AI as a force multiplier but must deeply understand the system to wield it effectively.</p><p>If everyone has AI coding agent access, what distinguishes great developers is knowing when the AI is wrong or suboptimal. <a href=\"https://www.cio.com/article/4062024/demand-for-junior-developers-softens-as-ai-takes-over.html\">As one senior engineer put it</a>: “The best software engineers won’t be the fastest coders, but those who know when to distrust AI.”</p><p>Developer discourse in 2025 was split. Some admitted they hardly ever write code “by hand” and think coding interviews should evolve. Others argued that skipping fundamentals leads to more firefighting when AI’s output breaks. <a href=\"https://www.cio.com/article/4062024/demand-for-junior-developers-softens-as-ai-takes-over.html\">The industry is starting to expect engineers to bring both</a>: AI speed and foundational wisdom for quality.</p><p> Use AI as a learning tool, not a crutch. When AI coding agents (Cursor/Antigravity/Claude Code/Gemini CLI) suggest code review why it works, identify weaknesses. Occasionally disable your AI helper and write key algorithms from scratch. Prioritize CS fundamentals: data structures, algorithms, complexity, memory management. Implement projects twice, once with AI, once without, and compare. Learn prompt engineering and tool mastery. Train yourself in rigorous testing: write unit tests, read stack traces without immediately asking AI, get comfortable with debuggers. Deepen complementary skills AI can’t replicate: system design, user experience intuition, concurrency reasoning. Show you can both crank out solutions with AI and tackle thorny issues when it fails.</p><p> Position yourself as the guardian of quality and complexity. Sharpen your core expertise: architecture, security, scaling, domain knowledge. Practice modeling systems with AI components and think through failure modes. Stay current on vulnerabilities in AI-generated code. Embrace your role as mentor and reviewer: define where AI use is acceptable and where manual review is mandatory (payment or safety code). Lean into creative and strategic work; let the junior+AI combo handle routine API hookups while you decide which APIs to build. Invest in soft skills and cross-domain knowledge. Stay current on new tools and best practices. Double down on what makes a human developer indispensable: sound judgment, system-level thinking, and mentorship.</p><p><strong>The bottom line: The developer role could shrink into limited auditing (overseeing AI-generated code) or expand into a pivotal orchestrator position designing and governing AI-driven systems. Either way, adding value means more than just coding.</strong></p><p>The extremes here are stark. In one vision, developers see their creative responsibilities diminished. Rather than building software, they mostly audit and babysit AI outputs. AI systems (or “citizen developers” using no-code platforms) handle production; human developers review auto-generated code, check for errors, bias, or security issues, and approve deployments. Maker becomes checker. The joy of code creation replaced by the anxiety of risk management.</p><p>There are reports of engineers spending more time evaluating AI-generated pull requests and managing automated pipelines, less time crafting code from scratch. Programming feels less like creative problem-solving and more like compliance. As one engineer lamented: “I don’t want to end up as a code janitor, cleaning up what the AI throws over the wall.”</p><p>The alternative future is far more interesting: developers evolve into high-level orchestrators, combining technical, strategic, and ethical responsibilities. AI “workers” mean human developers take on an architect or general contractor role, designing the overall system, deciding which tasks go to which AI or software component, weaving solutions from many moving parts.</p><p><a href=\"https://www.cio.com/article/4062024/demand-for-junior-developers-softens-as-ai-takes-over.html\">A CEO of a low-code platform articulated this vision</a>: in an “agentic” development environment, engineers become “composers,” orchestrating ensembles of AI agents and software services. They won’t write every note themselves, but they define the melody: architecture, interfaces, how agents interact. This role is interdisciplinary and creative: part software engineer, part system architect, part product strategist.</p><p>The optimistic take: as AI handles rote work, developer roles shift toward higher-value activities by necessity. Jobs may become more interesting. Someone has to decide what the AI should build, verify the product makes sense, and continuously improve it.</p><p>Which way it goes may depend on how organizations choose to integrate AI. Companies that see AI as labor replacement might trim dev teams and ask remaining engineers to keep automations running. Companies that see AI as a way to amplify their teams might keep headcounts similar but have each engineer deliver more ambitious projects.</p><p> Seek opportunities beyond just writing code. Volunteer for test case writing, CI pipeline setup, or application monitoring: skills aligned with an auditor/custodian role. Keep your creative coding alive through personal projects so you don’t lose the joy of building. Develop a systems mindset: learn how components communicate, what makes APIs well-designed. Read engineering blogs and case studies of system designs. Familiarize yourself with AI and automation tools beyond code generation: orchestration frameworks, AI APIs. Improve communication skills, written and verbal. Write documentation as if explaining to someone else. Ask senior colleagues not just “Does my code work?” but “Did I consider the right things?” Prepare to be verifier, designer, and communicator, not just coder.</p><p> Lean into leadership and architectural responsibilities. Shape the standards and frameworks that AI and junior team members follow. Define code quality checklists and ethical AI usage policies. Stay current on compliance and security topics for AI-produced software. Focus on system design and integration expertise; volunteer to map data flows across services and identify failure points. Get comfortable with orchestration platforms (Kubernetes, Airflow, serverless frameworks, agent orchestration tools). Double down on your role as technical mentor: more code reviews, design discussions, technical guidelines. Hone your ability to quickly assess someone else’s (or something’s) code and give high-level feedback. Develop product and business sense; understand why features get built and what customers care about. Shadow a product manager or join customer feedback sessions. Protect your creative passion through prototypes, hackathons, or emerging tech research. Evolve from coder to conductor.</p><h2>4. The Specialist vs. Generalist question</h2><p><strong>The bottom line: Narrow specialists risk finding their niche automated or obsolete. The fast-changing, AI-infused landscape rewards T-shaped engineers: broad adaptability with one or two deep skills.</strong></p><p>Given how quickly models, tools and frameworks rise and fall, betting your career on a single technology stack is risky. A guru in a legacy framework might suddenly find themselves in less demand when a new AI tool handles that tech with minimal human intervention. Developers who specialize narrowly in “a single stack, framework or product area” might wake up to find that area declining or redundant.</p><p>Think of COBOL developers, Flash developers, or mobile game engine specialists who didn’t pivot when the industry moved. What’s different now is the pace of change. AI automation can make certain programming tasks trivial, undercutting roles that revolved around those tasks. A specialist who only knows one thing (fine-tuning SQL queries, slicing Photoshop designs into HTML) could find AI handling 90% of that work.</p><p>Hiring managers chase the newest niche. A few years ago everyone wanted cloud infrastructure specialists; now there’s a surge in AI/ML engineers. Those who specialized narrowly in yesterday’s technology feel stalled as that niche loses luster.</p><p>AI tools actually augment generalists more, making it easier for one person to handle multiple components. A back-end engineer can rely on AI help to create a reasonable UI; a front-end specialist can have AI generate server boilerplate. An AI-rich environment lets people operate more broadly. Meanwhile, deep specialists might find their niche partly automated with no easy way to branch out.</p><p> Establish a broad foundation early. Even if hired for a specific role, peek outside that silo. If you’re doing mobile, learn backend basics; if you’re doing front-end, try writing a simple server. Learn the deployment process and tools like Docker or GitHub Actions. Identify one or two areas that genuinely excite you and go deeper: this becomes your vertical expertise. Brand yourself as a hybrid: “full-stack developer with cloud security focus” or “frontend developer with UX expertise.” Use AI tools to learn new domains quickly; when you’re a novice in backend, have ChatGPT generate starter API code and study it. Build the habit of continuous re-skilling. Participate in hackathons or cross-functional projects to force yourself into generalist mode. Tell your manager you want exposure to different parts of the project. Adaptability is a superpower early in your career.</p><p> Map your skill graph: what are you expert in, what related domains have you only touched superficially? Pick one or two adjacent domains and commit to becoming conversant. If you’re a back-end database specialist, get comfortable with a modern front-end framework or learn ML pipeline basics. Do a small project in your weak area with AI assistance. Integrate your deep expertise with new contexts; if you specialize in web app performance, explore how those skills apply to ML inference optimization. Advocate for or design your role to be more cross-functional. Volunteer to be the “integration champion” for projects touching multiple areas. Mentor others to spread skills around while picking up something from them in return. Update your resume to reflect versatility. Use your experience to identify patterns and transferable knowledge. Become the T-shaped role model: deep in your specialty (giving authority and confidence) but actively stretching horizontally.</p><h2>5. The Education question</h2><p><strong>The bottom line: Will a CS degree remain the gold standard, or will faster learning paths (bootcamps, online platforms, employer training) overtake it? Universities may struggle to keep up with an industry that changes every few months.</strong></p><p>A four-year computer science degree has long been the primary ticket into software roles. But that tradition is being questioned.</p><p>One future: universities remain important but struggle to stay relevant. Degrees stay the default credential, but programs lag behind rapidly evolving needs, hampered by slow curriculum update cycles and bureaucratic approval processes. Students and employers feel academia is disconnected from industry, teaching theory or outdated practice that doesn’t translate to job skills.</p><p>Recent grads report never learning about cloud computing, modern DevOps, or AI tooling during their degree. If universities demand high time and financial investment while delivering low-relevance education, they risk being seen as expensive gatekeepers. But many companies still require a bachelor’s degree out of inertia, so the burden shifts to students to fill the gap with bootcamps, online courses, and self-taught projects.</p><p>Bootcamps have matured. They produce grads who get hired at top companies alongside CS grads. These programs are shorter (12-week intensive) and focus on practical skills: current frameworks, cloud services, teamwork. The hiring currency is shifting toward live portfolios, micro-credentials, and verified skills. A strong GitHub portfolio or recognized certification can bypass degree requirements.</p><p>Employer-driven education is emerging: companies creating their own training pipelines or partnering with bootcamps. Some big tech companies have started internal “universities” for non-traditional candidates. AI itself offers new ways to learn: AI tutors, interactive coding sandboxes, personalized instruction outside university settings.</p><p>A modular ecosystem of learning is far more accessible than an expensive four-year degree. A kid in a country without strong CS universities can take the same Coursera courses and build the same portfolio as someone in Silicon Valley.</p><p><em>Aspiring/junior developers:</em> If in a traditional CS program, don’t rely on it exclusively. Augment coursework with real-world projects: build a web app, contribute to open source. Seek internships or co-ops. If your curriculum misses hot topics, learn them through online platforms. Earn industry-recognized certifications (GCP, AWS, Azure) to signal practical knowledge. If self-teaching or in a bootcamp, focus on a compelling portfolio: at least one substantial project with good documentation. Be active in the developer community: contribute to open source, write technical posts. Network through LinkedIn, meetups, dev events. Get an experienced developer to vouch for you. Keep learning continuously; the half-life of technical skills is short. Use AI as your personal tutor. Prove your skills in concrete ways: portfolio, certification, and ability to talk intelligently about your work will open doors.</p><p><em>Senior developers and leaders:</em> Your credential alone won’t carry you forever. Invest in continuous education: online courses, workshops, conferences, certifications. Validate your skills in new ways; be prepared for interviews that assess current competency through real problems. Maintain side projects with new tech. Reassess job requirements: do you really need a new hire to have a CS degree, or do you need certain skills and learning ability? Push for skills-first hiring to widen your talent pool. Support internal training programs or apprenticeship-style roles. Champion mentorship circles for junior devs without formal backgrounds. Engage with academia and alternatives: advisory boards, guest lectures, feedback on curriculum gaps. Reflect this in your own career growth: real-world achievements and continuous learning matter more than additional degrees.</p><p>These scenarios aren’t mutually exclusive. Reality will draw elements from all of them. Some companies will reduce junior hiring while others expand it in new domains. AI will automate routine coding while raising standards for the code humans touch. Developers might spend mornings reviewing AI outputs and afternoons crafting high-level architecture.</p><p>The consistent thread: change is the only constant. By keeping a finger on technology trends (and skepticism around them), you avoid being caught off-guard by hype or doom. By updating skills, diversifying abilities, and focusing on uniquely human aspects (creativity, critical thinking, collaboration) you remain in the loop.</p><p>Whether the future brings a coding renaissance or a world where code writes itself, there will always be demand for engineers who think holistically, learn continuously, and drive technology toward solving real problems.</p><p>The best way to predict the future is to actively engineer it.</p>","contentLength":19570,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1r58zqv/the_next_two_years_of_software_engineering/"}],"tags":["dev","reddit"]}