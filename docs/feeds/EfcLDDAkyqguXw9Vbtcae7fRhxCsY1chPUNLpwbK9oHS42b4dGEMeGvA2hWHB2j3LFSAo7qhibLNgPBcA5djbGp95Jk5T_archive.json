{"id":"EfcLDDAkyqguXw9Vbtcae7fRhxCsY1chPUNLpwbK9oHS42b4dGEMeGvA2hWHB2j3LFSAo7qhibLNgPBcA5djbGp95Jk5T","title":"top scoring links : programming","displayTitle":"Reddit - Programming","url":"https://www.reddit.com/r/programming/top/.rss?sort=top&t=day&limit=6","feedLink":"https://www.reddit.com/r/programming/top/?sort=top&t=day&limit=6","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":6,"items":[{"title":"HTTP/3 is everywhere but nowhere","url":"https://httptoolkit.com/blog/http3-quic-open-source-support-nowhere/","date":1741796821,"author":"/u/pimterry","guid":350,"unread":true,"content":"<p data-text=\"true\" font-size=\"m\" color=\"darkGrey\">We've developed a totally new version of HTTP, and we're on track to migrate more than 1/3 of web traffic to it already! This is astonishing progress.</p><p data-text=\"true\" font-size=\"m\" color=\"darkGrey\">At the same time, neither QUIC nor HTTP/3 are included in the standard libraries of any major languages including Node.js, Go, Rust, Python or Ruby. Curl recently <a href=\"https://curl.se/docs/http3.html\" target=\"_blank\" rel=\"noopener noreferrer\">gained support</a> but it's experimental and disabled in most distributions. There are a rare few external libraries for some languages, but all are experimental and/or independent of other core networking APIs. Despite mobile networking being a key use case for HTTP/3, Android's most popular HTTP library <a href=\"https://github.com/square/okhttp/blob/59cbf64f6ba98e2c8f95bf9db41dc47ad2232f94/okhttp/src/commonJvmAndroid/kotlin/okhttp3/Protocol.kt#L86-L94\" target=\"_blank\" rel=\"noopener noreferrer\">has no support</a>. Popular servers like Nginx have only <a href=\"https://nginx.org/en/docs/quic.html\" target=\"_blank\" rel=\"noopener noreferrer\">experimental support</a>, disabled by default, Apache has no support or published plan for support, and Ingress-Nginx (arguably the most popular Kubernetes reverse proxy) has <a href=\"https://github.com/kubernetes/ingress-nginx/issues/4760\" target=\"_blank\" rel=\"noopener noreferrer\">dropped all plans for HTTP/3 support</a> punting everything to a totally new (as yet unreleased) successor project instead.</p><p data-text=\"true\" font-size=\"m\" color=\"darkGrey\">Really it's hard to point to any popular open-source tools that fully support HTTP/3: rollout has barely even started.</p><p data-text=\"true\" font-size=\"m\" color=\"darkGrey\">This seems contradictory. What's going on?</p><h2 data-heading=\"true\" font-size=\"m\" color=\"lightGrey\">Why do we need more than HTTP/1.1?</h2><p data-text=\"true\" font-size=\"m\" color=\"darkGrey\">Let's step back briefly. Why does this matter? Who cares about whether HTTP/3 is being rolled out successfully or not? If browser traffic and the big CDNs support HTTP/3, do we even need it in other client or server implementations?</p><p data-text=\"true\" font-size=\"m\" color=\"darkGrey\">You could make much the same argument for HTTP/3: this is useful for the high-latency many-requests traffic of web browsers &amp; CDNs, but irrelevant elsewhere.</p><p data-text=\"true\" font-size=\"m\" color=\"darkGrey\">Even just considering HTTP/1.1 vs HTTP/2 though, the reality of multiplexing benefits is more complicated:</p><ul><li>Latency of responses isn't just network RTT: a slow server response because of server processing will block your TCP connection just as hard as network latency.</li><li>Your load balancer is often  co-located with your backend, e.g. if you serve everything through a geographically distributed CDN, which serves most content from its cache, but falls back to a separate application server backend for dynamic content &amp; cache misses.</li><li>Long-lived TCP connections die. Networking can fail in a million ways, even within data centers, and 'keep-alive' is a desperate plea at best. Even HTTP itself will force this: there's cases like a response body failing half-way through that are unrecoverable in HTTP/1.1 without killing the connection entirely.</li><li>Any spikes in traffic mean you'll end up with the wrong number of TCP connections one way or the other: either you need an enormous unused pool available at all times, or you'll need to open a flood of new connections as traffic spikes come in, and so deal with TCP slow start, RTT &amp; extra TLS handshakes as you do so.</li><li>There's a lot of traffic that's not websites  not just within in datacenters. Mobile apps definitely do have network latency issues, API servers absolutely do have slow endpoints that can block up open connections, and IoT is a world built almost exclusively of unreliable networks and performance challenges. All of these cases get value from HTTP/2 &amp; HTTP/3.</li></ul><p data-text=\"true\" font-size=\"m\" color=\"darkGrey\">Moving beyond multiplexing, there's plenty of other HTTP/2 benefits that apply beyond just load balancers &amp; browsers:</p><ul><li>HTTP header compression (<a href=\"https://blog.cloudflare.com/hpack-the-silent-killer-feature-of-http-2/\" target=\"_blank\" rel=\"noopener noreferrer\">HPACK</a> and <a href=\"https://datatracker.ietf.org/doc/rfc9204/\" target=\"_blank\" rel=\"noopener noreferrer\">QPACK</a> in HTTP/2 &amp; HTTP/3 respectively) makes for a significant reduction in traffic in many cases,  on long-lived connections such as within internal infrastructure. This is useful on the web, but can be an even bigger boost on mobile &amp; IoT scenarios where networks are limited and unrealiable.</li><li>Bidirectional HTTP request &amp; response streaming (only possible in HTTP/2 &amp; HTTP/3) enables entirely different communication patterns. Most notably used in gRPC (which  HTTP/2 for most usage) this means the client and server can exchange continuous data within the same 'request' at the same time, acting similarly to a websocket but within existing HTTP semantics.</li><li>Both protocols support advanced prioritization support, allowing clients to indicate priority of requests to servers, to more efficiently allocate processing &amp; receive the data they need most urgently. This is valuable for clients, but also between the load balancer and server: a cache miss for a waiting client has a very different priority to an optional background cache revalidation.</li></ul><p data-text=\"true\" font-size=\"m\" color=\"darkGrey\">All that is just for HTTP/2. HTTP/3 improves on this yet further, with:</p><ul><li>Significantly increased resilience to unreliable networks. By moving away from TCP's strict packet ordering, HTTP/3 makes each stream within the connection fully independent, so a missed packet on one stream doesn't slow down another stream.</li><li>Zero round-trip connection initialization. TLS1.3 allows zero round-trip connection setup when resuming a connection to a known server, so you don't need to wait for the TLS handshake before sending application data. For HTTP/1 &amp; HTTP/2 though, you still need a TCP handshake first. With QUIC, you can do <a href=\"https://blog.cloudflare.com/even-faster-connection-establishment-with-quic-0-rtt-resumption/\" target=\"_blank\" rel=\"noopener noreferrer\">0RTT TLS handshakes</a>, meaning you can connect to a server and send an HTTP request immediately, without waiting for a single packet in response, so there's no unnecessary RTT delay whatsoever.</li><li>Reductions in traffic size, connection count, and network round trips that can result in reduced battery use for clients and reduced processing, latency &amp; bandwidth for servers.</li><li>Support for <a href=\"https://pulse.internetsociety.org/blog/how-quic-helps-you-seamlessly-connect-to-different-networks\" target=\"_blank\" rel=\"noopener noreferrer\">connection migration</a> allowing a client to continue the same connection even as its IP address changes, and in theory even supporting multi-path connections using multiple addresses (e.g. a single connection to a server using both WiFi &amp; cellular at the same time for extra bandwidth/reliability) in future.</li><li>Improved network congestion handling by moving away from TCP: QUIC can use <a href=\"https://research.google/pubs/bbr-congestion-based-congestion-control-2/\" target=\"_blank\" rel=\"noopener noreferrer\">Bottleneck Bandwidth and RTT</a> (BBR) for improved congestion control via active detection of network conditions, includes timestamps in each packet to help measure RTT, has improved detection of and recovery from packet loss, has better support for explicit congestion notifications (ECN) to actively manage congestion before packet loss, and may gain forward-error correction (FEC) support <a href=\"https://datatracker.ietf.org/doc/draft-michel-quic-fec/\" target=\"_blank\" rel=\"noopener noreferrer\">in future</a> too.</li><li>Support for <a href=\"https://github.com/w3c/webtransport/blob/main/explainer.md\" target=\"_blank\" rel=\"noopener noreferrer\">WebTransport</a>, a new protocol providing bidirectional full-duplex connections (similar to WebSockets) but supporting multiplexed streams to avoid head-of-line blocking, fixing various legacy WebSocket limitations (like incompatibility with CORS), and allowing streams to be unreliable and unordered - effectively providing UDP-style guarantees and lower-latency within web-compatible stream connections.</li></ul><p data-text=\"true\" font-size=\"m\" color=\"darkGrey\">In addition to the theory, there's some concrete measureable benefits being reported already. RequestMetric ran some <a href=\"https://requestmetrics.com/web-performance/http3-is-fast/\" target=\"_blank\" rel=\"noopener noreferrer\">detailed benchmarks</a> showing some astonishing performance improvements for example:</p><p data-text=\"true\" font-size=\"m\" color=\"darkGrey\">And Fastly shared the major improvements in time-to-first-byte they're seeing in the real world:</p><p data-text=\"true\" font-size=\"m\" color=\"darkGrey\">This all very clearly Good Stuff.</p><p data-text=\"true\" font-size=\"m\" color=\"darkGrey\">Now that the technology is standardized, widely supported in browsers &amp; CDNs and thoroughly battle-tested, I think it's clear that  developers should be able to get these benefits built into their languages, servers &amp; frameworks.</p><p data-text=\"true\" font-size=\"m\" color=\"darkGrey\">That's not what's happened though: despite its benefits and frequent use in network traffic, most developers can't easily start using HTTP/3 end-to-end today. In this, HTTP/3 has thrown a long-growing divide on the internet into sharp relief. Nowadays, there's two very different kinds of web traffic:</p><ul><li>Major browsers plus some very-specific mobile app traffic, where a small set of finely tuned &amp; automatic-updating clients talk to a small set of very big servers, with as much traffic as possible handled by either an enormous CDN (Cloudflare, Akamai, Fastly, CloudFront) and/or significant in-house infra (Google, Meta, Amazon, Microsoft).</li><li>Everybody else: backend API clients &amp; servers, every other mobile app, every smaller CDN, websites without a CDN, desktop apps, IoT, bots &amp; indexers &amp; scrapers, niche web browsers, self-hosted homelabs, CLI tools &amp; scripts, students learning about network protocols, you name it.</li></ul><p data-text=\"true\" font-size=\"m\" color=\"darkGrey\">Let's simplify a bit, and describe these two cases as 'hyperscale web' and 'long-tail web'. Both groups are building on the same basic standards, but they have very different focuses and needs, and increasingly different tools &amp; platforms. This has been true for quite a while, but the reality of HTTP/3 makes it painfully clear.</p><p data-text=\"true\" font-size=\"m\" color=\"darkGrey\">There's a few notable differences in these groups:</p><ul><li>The long-tail world is fragmented into different implementations, almost by definition. Most of the biggest implementations are open-source organizations with relatively little direct funding or full-time engineering power available, and much work is done by volunteers with no mandated central direction or clear focus.</li><li>The hyperscale world is controlled by a relatively small number of key stakeholders on both client &amp; server side (you can count the relevant companies without taking your socks off). This lets them agree standards to fit their needs quickly &amp; effectively - literally putting a representative of every implementation in the same room.</li><li>The hyperscale ecosystem has far more concentrated cash &amp; motivations. It's a small number of players comprising some of the most valuable companies in the world, with business models that tie milliseconds of web performance directly to their bottom line.</li><li>The long-tail is completely dependent on open-source implementations and shared code. If you want to build a new mobile app tomorrow, you obviously should not start by building an HTTP parser.</li><li>The hyperscale ecosystem isn't worried about access to open-source implementations at all. They have sufficient engineering resources and complicated use cases that building their own custom implementation of a protocol from scratch can make perfect sense. Google.com is not going to be served by an Apache module with default settings, and Instagram is not sending requests with the same HTTP library as a Hello-World app.</li><li>The combination of hyperscale's evergreen clients plus money &amp; motivation plus tight links between implementers and the business using the tools, means they can move fast to quickly build, ship &amp; iterate new approaches.</li><li>Long-tail implementations are only updated relatively rarely (how many outdated Apache servers are there on the web?) and the maintainers are a tiny subset of the users, who care significantly about stability and avoiding breaking changes. Long-tail tool maintainers  just move fast and break things.</li></ul><p data-text=\"true\" font-size=\"m\" color=\"darkGrey\">You can see the picture I'm painting. These two groups exist on the same web, but in very different ways.</p><p data-text=\"true\" font-size=\"m\" color=\"darkGrey\">Some of this might sound like the hyperscale gang are the nefarious baddies. That is not what I mean (fine, yes, there's an interesting conversation there more broadly, but talking strictly about network protocols here). Regarding HTTP/3 specifically, this is some  engineering work that is solving real problems on the web, through some astonishingly tidy cooperation on open standards across different massive organizations. That's great!</p><p data-text=\"true\" font-size=\"m\" color=\"darkGrey\">There are many  people using services built by these companies, and their obsession with web performance is improving the quality of those services for large numbers of real people every day. This is very cool.</p><p data-text=\"true\" font-size=\"m\" color=\"darkGrey\">However, this would be much cooler if it was accessible to every other server, client &amp; developer too. Most notably, this means the next generation of web technology is being defined &amp; built by one minority group, and the larger majority have effectively no way to access this technology right now (despite years of standardization and development) other than paying money to the CDNs of that first minority group to help. Not cool.</p><p data-text=\"true\" font-size=\"m\" color=\"darkGrey\">I think the hyperscaler/long-tail divide is the fundamental cause here, but that's created quite a few more concrete issues downstream, the most notable of which is OpenSSL's approach to QUIC.</p><ul><li>BoringSSL shipped a usable API for QUIC implementations back in 2018.</li><li>OpenSSL did not, so various forks like QuicTLS appeared, providing OpenSSL plus BoringSSL's QUIC API.</li><li>An ecosystem of QUIC &amp; HTTP/3 implementations (most notably Quiche, msh3/msquic, and nghttp3/ngtcp2) were built on top of BoringSSL and these forks over the many subsequent years.</li><li>OpenSSL has since slowly implemented an incompatible approach that this ecosystem can't directly use, with client support released in OpenSSL 3.2 (2023), and server support landing imminently in OpenSSL 3.5 (2025).</li></ul><p data-text=\"true\" font-size=\"m\" color=\"darkGrey\">Some would argue this is a major mistake by OpenSSL, while I think OpenSSL would argue that BoringSSL's design is flawed and/or unsuitable for OpenSSL, and it was worth taking the time to do it right.</p><p data-text=\"true\" font-size=\"m\" color=\"darkGrey\">Regardless of who's actually 'right', this has created a significant schism in the entire ecosystem. Curl has a good overview of the state of play  OpenSSL:</p><p data-text=\"true\" font-size=\"m\" color=\"darkGrey\">OpenSSL's approach doesn't work easily in the TLS section for any of the existing QUIC &amp; HTTP/3 implementations. In effect, they've started another column, but with no compatible implementations currently available in the HTTP/3 &amp; QUIC spots.</p><p data-text=\"true\" font-size=\"m\" color=\"darkGrey\">This is a notable issue because for most major projects it would be an enormous &amp; problematic undertaking to drop support for OpenSSL, which effectively means they still cannot ship built-in QUIC support today. Node.js recently <a href=\"https://github.com/nodejs/node/issues/57379\" target=\"_blank\" rel=\"noopener noreferrer\">briefly discussed</a> even dropping OpenSSL entirely because of this, in favour of BoringSSL or similar, but it's clear that it's not practical: it would be an enormous breaking change, no alternative offers the same levels of long-term support guarantees, and Node and other languages are often shipped in enviroments like Linux distributions where it uses the system's shared OpenSSL implementation, so this would create big headaches downstream too.</p><p data-text=\"true\" font-size=\"m\" color=\"darkGrey\">This is one example of the difference in fundamental pressures of the two tiers of organizations on the web here: open-source tools can't break things like this, and the libraries available to the long-tail are fragmented and uncoordinated. Meanwhile hyperscalers can make decisions quickly and near-unilaterally to set up implementations that work for their environments today, allowing them to get the benefits of new technologies without worrying too much about the state of the open-source common ecosystem for everybody else.</p><p data-text=\"true\" font-size=\"m\" color=\"darkGrey\">I hope this is makes it clear there's a big problem here: underlying organizational differences are turning into a fundamental split in technologies on the Internet. There's an argument that despite the benefits, the long-tail web doesn't  HTTP/3, so they can just ignore it, or use a CDN with built-in support if they really care, and there is no real obligation as such for the hyperscalers to provide convenient implementations to the rest of us just because they want to use some neat new tech between themselves.</p><p data-text=\"true\" font-size=\"m\" color=\"darkGrey\">The problem here though is that there are real concrete benefits to these technologies. QUIC is a significant improvement on alternatives, especially on slow or unreliable mobile internet (e.g. everywhere outside the well-connected offices of the developed world) and when roaming between connections. There are technologies built on top of it, like WebTransport which provides additional significant new features &amp; performance to replace WebSockets. There will be more features that depend on HTTP/3 in future (see how gRPC built on HTTP/2 for example). Again: the technology here is great! But it's a challenge if those benefits are not evenly distributed, and only accessible to a small set of organizations and their customers.</p><p data-text=\"true\" font-size=\"m\" color=\"darkGrey\">Continuing down this road has some plausible serious consequences:</p><ul><li>In the short term, the long-tail web gains a concerete disadvantage vs the hyperscale web, as HTTP/3 and QUIC makes hyperscale sites faster &amp; more reliable (especially on slow &amp; mobile internet).</li><li>Other web tools and components (React et al) used by developers either working for hyperscale organizations or building on top of their tools &amp; infra will increase in complexity to match, taking HTTP/3's benefits for granted and moving forwards on that basis, making them less and less relevant to other use cases.</li><li>If we're not careful, the split between the long-tail &amp; hyperscale cases will widen. New features &amp; tools for each use case will emerge, and won't be implemented by the other, and tooling will increasingly stratify.</li><li>If hyperscale-only tech is widespread but implementations are not, it becomes increasingly difficult to build tools to integrate with these. Building a Postman-like client for WebTransport is a whole lot harder if you're implementing the protocol from scratch instead of just a UI.</li><li>You'll start to see lack of HTTP/3 support used as a signal to trigger captchas &amp; CDN blocks, like as TLS fingerprinting is already today. HTTP/3 support could very quickly &amp; easily become a way to detect many non-browser clients, cutting long-tail clients off from the modern web entirely.</li><li>As all this escalates and self-reinforces, it becomes less &amp; less sensible for the hyperscale side to worry about the long-tail's needs at all, and the ecosystem could stratify completely</li></ul><p data-text=\"true\" font-size=\"m\" color=\"darkGrey\">All of that is a way away, and quite hypothetical! I suspect  of this will happen to some degree, but there's a wide spectrum of possibility. It's notable though that this doesn't just apply to HTTP/3: the centralization and coordination of a few CDN &amp; web clients like this could easily play out similarly in many other kinds of technological improvements too.</p><p data-text=\"true\" font-size=\"m\" color=\"darkGrey\">For HTTP/3 at least, I'm hopeful that there will be a happy resolution here to improve on this split in time, although I don't know if it will come soon enough to avoid notable consequences. Many of the external libraries and experimental implementations of QUIC &amp; HTTP/3 will mature with time, and I think eventually (I really really hope) the OpenSSL QUIC API schism will get resolved to open the door to QUIC support in the  OpenSSL-based environments, either with adapters to support both approaches or via a new HTTP/3 &amp; QUIC stack that supports OpenSSL model directly. If you're interested in working on either, and there's anything I can do to help directly or to help fund that work, please <a href=\"https://httptoolkit.com/contact/\" target=\"_blank\" rel=\"noopener noreferrer\">get in touch</a>.</p><p data-text=\"true\" font-size=\"m\" color=\"darkGrey\">None of that will happen today though, so unfortunately if you want to use HTTP/3 end-to-end in your application, you may in for a hard time for a while yet. Watch this space.</p><p data-text=\"true\" font-size=\"m\" color=\"darkGrey\"><em>Want to debug HTTP/1 and HTTP/2 in the meantime? Test out  now. Open-source one-click HTTP(S) interception &amp; debugging for web, Android, terminals, Docker &amp; more.</em></p>","contentLength":18303,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1j9o1tx/http3_is_everywhere_but_nowhere/"},{"title":"Microservices: The Architectural Cult That’s Bankrupting Your Sanity (and Your Startup)","url":"https://medium.com/mr-plan-publication/microservices-the-architectural-cult-thats-bankrupting-your-sanity-and-your-startup-877e33453785?sk=0d5e112b5ed7b53ea0633f83a9b2c57a","date":1741779619,"author":"/u/TerryC_IndieGameDev","guid":349,"unread":true,"content":"<p>You’re debugging a payment failure at 2 AM when Slack explodes:<em>“Why is Kubernetes pod #4281 mining Bitcoin?!”</em></p><p>As you chug cold coffee, a horrifying realization hits: <strong>Your “modern” architecture has become a distributed game of Jenga.</strong> And like every overconfident engineer who came before you, you’re one shaky service mesh away from total collapse.</p><p>Welcome to the microservices lie.</p><p>Remember when software had ? When you could trace a bug from frontend click to database query without needing a PhD in distributed tracing?</p><p>The early cloud pioneers sold us a beautiful dream: <em>“Break your monolith into microservices! Scale components independently! Innovate faster!”</em> What they didn’t mention? </p>","contentLength":705,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1j9i2du/microservices_the_architectural_cult_thats/"},{"title":"What′s new in Java 24","url":"https://pvs-studio.com/en/blog/posts/java/1233/","date":1741771073,"author":"/u/Xaneris47","guid":353,"unread":true,"content":"<ul></ul><p>On March 18, a new Java version is set to arrive! Let's take a peek at new features, including the long-awaited final implementation of Stream Gatherers!</p><div><p>The order of the JEPs (JDK Enhancement Proposal) presented here is based on our assessment of their \"interestingness\" rather than their official numbering.</p></div><h2>JEP 485: Stream Gatherers</h2><p>As you know, Stream API operations are divided into  operations that generate a new  and  operations that create a result or have a side effect. However, terminal operations have , which allows us to create custom operations via the  implementation. The set of intermediate ones has only , , , , , , and . That's the case—until Java 24, which introduces .</p><p>The key points of the new feature are as follows:</p><p>1. New  method added to .</p><p>2. New <code>java.util.stream.Gatherer</code> interface, which consists of four methods:</p><ul><li> creates an initial intermediate state using ;</li><li> handles elements, optionally uses the intermediate state, and sends results further down the stream. It relies on the new  functional interface.</li><li> merges states using ;</li><li> performs on the intermediate state and sends the result further down the stream after all elements have been processed. It uses .</li></ul><p>3. New <code>java.util.stream.Gatherers</code> class, which provides several standard implementations of :</p><ul><li> is similar to the  operation;</li><li> performs incremental accumulation;</li><li> is a standard implementation of Fixed Window;</li><li> is a standard implementation of Sliding Window.</li></ul><div><div><p>The Sliding Window technique creates a window of the N size over the input data and then shifts that window. A simple example with a number array and a window size of 3 illustrates this concept:</p><p>The result of this operation is  containing all the subarrays highlighted by the blue frame.</p><p>With Gatherers, we can easily output all subarrays as follows:</p><pre><code>public static void main(String[] args) {\n    var list = List.of(\n        \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \n        \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\"\n    );\n    int k = 3;\n\n    list.stream()\n        .gather(Gatherers.windowSliding(k))\n        .forEach(sublist -&gt; System.out.printf(\"%s \", sublist));\n    System.out.println();\n}</code></pre><p>[1, 2, 3] [2, 3, 4] [3, 4, 5] [4, 5, 6] [5, 6, 7] [6, 7, 8] [7, 8, 9] [8, 9, 10] [9, 10, 11] [10, 11, 12] [11, 12, 13] [12, 13, 14]</p><p>Fixed Window has a similar implementation—except that the shift is equal to the window size.</p><p>The usage remains the same:</p><pre><code>public static void main(String[] args) {\n    var list = List.of(\n        \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\",\n        \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\"\n    );\n    int k = 3;\n\n    list.stream()\n        .gather(Gatherers.windowFixed(k))\n        .forEach(sublist -&gt; System.out.printf(\"%s \", sublist));\n    System.out.println();\n}</code></pre><p>[1, 2, 3] [4, 5, 6] [7, 8, 9] [10, 11, 12] [13, 14]</p></div></div><p>In addition to creating custom classes that implement , Java provides static factory methods:</p><ul><li><code>Gatherer.ofSequential(integrator)</code></li></ul><p>Both methods have variations with different additional arguments in the form of functional interfaces—, , , and —which were mentioned earlier.</p><p>A custom API to work with class files was introduced in JDK 22. Now, with JDK 24, this API has been finalized!</p><p>The rapid Java development in recent years has led to frequent and regular bytecode updates with which standard tools like , , and others interact. They use libraries like ASM for this interaction. To support new bytecode versions, tools should wait for library updates—yet libraries, in turn, wait for the final implementation of new JDK versions. This dependency chain slows down the development and adoption of new class file features.</p><p>While this API may not be directly useful for most developers, it's essential for various frameworks and libraries—including Spring and Hibernate—that work with bytecode and use ASM. The problem is that older ASM versions are incompatible with newer JDK releases. If we need to update the JDK version in a project, ASM must be updated as well—so, we need to update everything that depends on it... well, almost everything. Yet we just wanted to upgrade the JDK version.</p><p>Let's explore a new API. After experimenting with it, we've put together a simple example that reads static constant primitive fields (a basic understanding of class-file structures is required):</p><pre><code>public class ClassFileExample {\n\n  public static void main(String[] args) throws IOException {\n    var classFile = ClassFile.of().parse(Path.of(\"./Main.class\"));\n\n    for (var field : classFile.fields()) {\n      var flags = field.flags();\n      if (flags.has(AccessFlag.STATIC) &amp;&amp; flags.has(AccessFlag.FINAL)) {\n        System.out.printf(\"static final field %s = \", field.fieldName());\n        var value = field.attributes().stream()\n          .filter(ConstantValueAttribute.class::isInstance)\n          .map(ConstantValueAttribute.class::cast)\n          .findFirst()\n          .map(constant -&gt; constant.constant().constantValue().toString())\n          .orElse(\"null\");\n        System.out.printf(\"%s%n\", value);\n      }\n    }\n  }\n}</code></pre><p>This can be surprisingly helpful because reflection leads to class initialization. One day, we might delve into the consequences of such  initialization.</p><p>Developers familiar with ASM may notice that the authors chose not to use the  pattern because of Java's new features, particularly pattern matching.</p><h2>JEP 483: Ahead-of-Time Class Loading &amp; Linking</h2><p>This new feature aims to streamline application loading time. To achieve this, Java now enables caching of loaded classes. The process of generating and using this cache consists of three steps:</p><p>1. <strong>Generating the AOT configuration</strong>. Run the application with the  flag and specify the output file path via <code>-XX:AOTConfiguration=PATH</code>:</p><p><code>java -XX:AOTMode=record -XX:AOTConfiguration=app.aotconf -jar app.jar</code></p><p>2. <strong>Generating the cache with configuration</strong>. Change the  mode to  and specify the cache output path using the  flag:</p><p><code>java -XX:AOTMode=create -XX:AOTConfiguration=app.aotconf -XX:AOTCache=app.aot -jar app.jar</code></p><p>3. <strong>Running the application using the cache</strong>. Use only the  flag:</p><p><code>java -XX:AOTCache=app.aot -jar app.jar</code></p><p>According to JEP, the loading time for a simple program using the  decreased from 0.031 seconds to 0.018 seconds (a 42% difference). The loading time for a Spring-based project (Spring PetClinic) dropped from 4.486 seconds to 2.604 seconds.</p><p>I also looked at the simple Quarkus application from the recently published book  (<a href=\"https://github.com/xstefank/quarkus-in-action/tree/main\" target=\"_blank\" rel=\"nofollow\">GitHub</a>). The loading time decreased from 3.480 to 2.328 seconds (a 39.67% decrease).</p><h2>JEP 491: Synchronize Virtual Threads without Pinning</h2><p>This JEP resolves the issue of platform thread blocking when using virtual threads in  blocks. To understand the impact of this change, let's first take a look at <a href=\"https://openjdk.org/projects/loom/\" target=\"_blank\" rel=\"nofollow\">Project Loom</a> and, more specifically, virtual threads.</p><p>When virtual threads were introduced in <a href=\"https://openjdk.org/jeps/444\" target=\"_blank\" rel=\"nofollow\">JEP 444</a>, two scenarios were specified in which they wouldn't release the platform thread they were using when blocked:</p><ul><li>Blocking occurs in a  block;</li><li>Blocking occurs in native methods—whether they're JNI or Foreign Functions.</li></ul><p>Now, the first case is invalid. Developers are now free to choose between using the  keyword and the <code>java.util.concurrent.locks</code> package, allowing them to focus solely on the specific requirements of their task.</p><h2>JEP 490: ZGC: Remove the Non-Generational Mode</h2><p>Z Garbage Collector (ZGC) used to support two modes:  and . Since Generational ZGC is the preferred option in most cases, developers have decided to streamline further ZGC support by disabling one of the modes, Non-Generational. The  flag is now deprecated, and the warning message will be displayed if it's used:</p><h2>JEP 498: Warn upon Use of Memory-Access Methods in sun.misc.Unsafe</h2><p>If memory-related methods from  are called, the warning will be issued. These changes align with the transition toward modern alternatives such as the  and the <code>Foreign Function &amp; Memory API</code>. Additionally, they pass Java closer to removing memory-related methods from , which have already been marked as . This update also encourages library developers to migrate to the new APIs.</p><h2>JEP 472: Prepare to Restrict the Use of JNI</h2><p>Using  (JNI) and <code>Foreign Function &amp; Memory</code> (FFM) now issues a warning:</p><p>This is the first step in restricting the use of JNI and FFM. In the future, an exception should be thrown for the code. However, this doesn't mean these features will be removed—which would be ironic, since FFM was released in Java 22. This step is needed for the policy of integrity by default. It just means that developers, who enable native access, should explicitly state that they consider unsafe features of the JDK.</p><h2>JEP 493: Linking Run-Time Images without JMOD</h2><p>The <code>‑‑enable-linkable-runtime</code> flag, introduced for JDK builds, allows  to create images without relying on  files from the JDK. This optimization reduces the final image size by 25%.</p><div><div><p>JMOD files have been around since Project Jigsaw (Java 9) and are used in the optional linking phase when using  to create a space-optimized JRE.</p><p>Unlike JAR files, JMODs can store not only  files and resources but also native libraries, licenses, and executables, which are then included in the final JRE. However, JAR files may be sufficient for developing standard applications, there is no need to use JMOD. Additionally, JMOD documentation is severely lacking. </p></div></div><p>Although this change doesn't directly impact developers, it's particularly relevant for containers or when creating minimal runtime images. However, this optimization isn't enabled by default—it's up to individual JDK providers to decide whether to implement it.</p><p>For example, Eclipse Temurin has already <a href=\"https://github.com/adoptium/temurin-build/issues/4035\" target=\"_blank\" rel=\"nofollow\">started</a> using this flag, and GraalVM has <a href=\"https://github.com/graalvm/mandrel/issues/808\" target=\"_blank\" rel=\"nofollow\">added</a> support for such builds as well.</p><h2>JEP 486: Permanently Disable the Security Manager</h2><p>Preparations for disabling <code>java.lang.SecurityManager</code> started back in Java 17, when it was marked  due to the rare usage of this class at high maintenance costs. Now let's go to the changes.</p><p>The  flag (in any form) is no longer supported and causes an error, except for <code>-Djava.security.manager=disallow</code>:</p><p>Calling <code>System::setSecurityManager</code> throws an <code>UnsupportedOperationException</code> exception.</p><p>System properties related to  are now ignored, and the <code>conf/security/java.policy</code> file has been removed.</p><p>Other changes are documentation-related, for example, references to  and  have been removed.</p><p>It should be noted that the classes and methods aren't removed but degraded to \"empty\"—they either return , , pass through the caller's request, or throw a  or <code>UnsupportedOperationException</code>.</p><h2>JEP 479: Remove the Windows 32-bit x86 Port</h2><p>Support for Windows 32-bit x86 is finally being discontinued. This facilitates the build and test infrastructure, freeing up resources that are no longer needed to maintain the platform.</p><p>One of the reasons for removing this port is the lack of support for , which fall back to classic . Additionally, support for the latest 32-bit version of Windows 10 will end in October 2025.</p><h2>JEP 501: Deprecate the 32-bit x86 Port for Removal</h2><p>The fate of the other 32-bit platforms is clear: they'll be removed, but not in this release.</p><p>So, Linux remains the last 32-bit supported platform. Building the 32-bit version now requires adding the <code>‑‑enable-deprecated-ports=yes</code> flag:</p><pre><code>bash ./configure –enable-deprecated-ports=yes</code></pre><p>However, the complete removal of this port is expected as early as Java 25.</p><h2>JEP 496: Quantum-Resistant Module-Lattice-Based Key Encapsulation Mechanism</h2><p>This and the following JEP focus on post-quantum cryptography.</p><p>Post-quantum cryptography refers to the creation of cryptographic algorithms that will be effective even after the advent of quantum computers. </p><p>According to the FIPS 203 standard, the  implementation for , ,  APIs, namely , , and  has been introduced. Now we can generate the key pairs as follows:</p><pre><code>KeyPairGenerator generator = KeyPairGenerator.getInstance(\"ML-KEM-1024\");\nKeyPair keyPair = generator.generateKeyPair();</code></pre><h2>JEP 497: Quantum-Resistant Module-Lattice-Based Digital Signature Algorithm</h2><p>As a follow-up to the previous JEP, according to the FIPS 204 standard, the implementation for , ,  APIs, namely , , and  has been added. Similar to the previous point, let's look at an example of obtaining an appropriate signature:</p><pre><code>Signature signature = Signature.getInstance(\"ML-DSA\");</code></pre><h2>JEP 475: Late Barrier Expansion for G1</h2><p>A final JEP that doesn't directly impact Java developers involves changes to the Garbage-First (G1) garbage collector, shifting the implementation of its barriers to a later C2 JIT compilation stage. This adjustment aims to simplify barrier logic for future developers while also reducing C2 compilation time.</p><p>Beyond this list of new features, some changes remain in the Preview or Experimental status.</p><p>Hopefully, one of the upcoming releases will enable us to see these innovations in action.</p><p>You can explore the full list of the JEP links <a href=\"https://openjdk.org/projects/jdk/24/\" target=\"_blank\" rel=\"nofollow\">here</a>.</p><p>Java continues to evolve at a brisk pace, and the introduction of the  reduces the number of dependencies, accelerating updates across the platform even further. That wraps up the Java 24 release for now—so once again, we turn our attention to the long-awaited <a href=\"https://openjdk.org/projects/valhalla/\" target=\"_blank\" rel=\"nofollow\">Project Valhalla</a>.</p>","contentLength":13019,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1j9fzru/whats_new_in_java_24/"},{"title":"Why Go for TypeScript compiler?","url":"https://github.com/microsoft/typescript-go/discussions/411","date":1741755589,"author":"/u/RobertVandenberg","guid":354,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1j9bm1e/why_go_for_typescript_compiler/"},{"title":"Survey Surfaces High DevOps Burnout Rates Despite AI Advances - DevOps.com","url":"https://devops.com/survey-surfaces-high-devops-burnout-rates-despite-ai-advances/","date":1741736466,"author":"/u/Inner-Chemistry8971","guid":352,"unread":true,"content":"<p>A <a href=\"https://jellyfish.co/newsroom/jellyfish-releases-2024-state-of-engineering-management-report/\" target=\"_blank\" rel=\"noopener\">survey</a> of 604 software developers and engineering professionals finds that while 61% work for organizations that are employing artificial intelligence (AI) to build software to some degree nearly two-thirds (65%) still experience burnout. In fact, after maintaining a high-performing team (47%), burnout (41%) is the second most often challenge, survey respondents cited.</p><p>Conducted by Kickstand Research on behalf of Jellyfish, a provider of a platform for managing software engineering teams, the survey finds that among organizations that have adopted AI, a full 94% said it positively influenced their team’s productivity, with 81% reporting AI increases the quality of code.</p><p>A total of 84% also said AI frees up time to focus on high-value activities, the survey finds.</p><p>However, the survey makes it clear there is a disconnect between the managers (48%) and developers/engineers who participated. More than three-quarters of executives (76%) believe their team has embraced AI, while only 52% of the rank-and-file respondents agreed.</p><p>Of the non-AI users, 48% said their team had not adopted the technology due to security concerns, followed by just over a third (34%) citing a lack of expertise. Just under a quarter (24%) said budget constraints had prevented them from using AI. Notably, 19% of executives who work for organizations that have not embraced AI view it as a gimmick.</p><p>Jellyfish CEO Andrew Lau said that while there is a lot of hyperbole being thrown around when it comes to AI, software engineering teams should lean into it. AI agents and copilots are bringing inevitable change to software engineering that will change and impact the role of software engineering, he added.</p><h3>AI Models Will Become More Commonplace</h3><p>Long term, there’s no doubt AI will enable software engineering teams to be more productive as additional advances are made, said Lau. For example, AI models trained for specific domains such as software engineering will become more commonplace, he noted. Many managers and businesspeople in the short term, however, are overestimating the impact AI can in the short term have on software development, he added.</p><p>In fact, 43% of the developers and engineers surveyed said feel that leadership at their company is out of the loop regarding the challenges software engineering teams face. Just under a third (31%) said their team lacks sufficient visibility into project status and well over a third (37%) said efficiency, predictability and productivity have all decreased on their team in the past year.</p><p>Overall, more than two-thirds said their engineering organization received a budget increase last year, with 57% of engineering leaders noting the size of their engineering team has increased over the past 12 months. More than half (56%) of all respondents expect the headcount in their department to increase over the next 12 months.</p><p>However, more than a third (34%) of developers/engineers do not feel the potential for advancement in their current role, with just under a third (32%) considering a career change. Nevertheless, 80% of all respondents said the work they do is rewarding.</p><p>Thanks to the rise of AI more software will probably be developed in the next few years than all of the past decade. The challenge now becomes how to manage a volume of code that is about to exponentially increase beyond what was once thought to be ever imaginable.</p>","contentLength":3386,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1j958n9/survey_surfaces_high_devops_burnout_rates_despite/"},{"title":"Graph RAG explained","url":"https://diamantai.substack.com/p/graph-rag-explained?r=336pe4&amp;utm_campaign=post&amp;utm_medium=web&amp;triedRedirect=true","date":1741721046,"author":"/u/Diamant-AI","guid":351,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/programming/comments/1j8z5v0/graph_rag_explained/"}],"tags":["dev","reddit"]}