{"id":"4W3i2hyrJTVfcLq85rWgduVQ3y3BMzrSD4pVUCMqPpFhmXWpqxQ","title":"Hacker News: Front Page","displayTitle":"HN Front","url":"https://hnrss.org/frontpage?points=75","feedLink":"https://news.ycombinator.com/","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":8,"items":[{"title":"Magnus Carlsen Wins the Freestyle (Chess960) World Championship","url":"https://www.fide.com/magnus-carlsen-wins-2026-fide-freestyle-world-championship/","date":1771193830,"author":"prophylaxis","guid":264,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47028227"},{"title":"Show HN: Microgpt is a GPT you can visualize in the browser","url":"https://microgpt.boratto.ca/","date":1771180835,"author":"b44","guid":219,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47026186"},{"title":"Editor's Note: Retraction of article containing fabricated quotations","url":"https://arstechnica.com/staff/2026/02/editors-note-retraction-of-article-containing-fabricated-quotations/","date":1771180194,"author":"bikenaga","guid":262,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47026071"},{"title":"Modern CSS Code Snippets: Stop writing CSS like it's 2015","url":"https://modern-css.com/","date":1771178650,"author":"eustoria","guid":261,"unread":true,"content":"<div>, , ; {  { : ; } }          </div>","contentLength":28,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47025851"},{"title":"Show HN: Knock-Knock.net – Visualizing the bots knocking on my server's door","url":"https://knock-knock.net/","date":1771175185,"author":"djkurlander","guid":218,"unread":true,"content":"<p>Set up an unprotected server on the net, and the bots start swarming!</p><p> This site shows bots attempting (unsuccessfully) to break into an ordinary internet server.</p><p>This constant chatter of bots knocking on the doors of machines on the net has been referred to as <em>\"the background radiation of the Internet\".</em></p><p>Knock-knock.net is a visualization of this bot traffic.\n\t    It shows the bot activity in real-time, and provides historic stats of the bot attacks over time: where they are coming from, the most common usernames and passwords attempted, the worst offending ISPs, and in some cases, why the password or username was chosen.</p><p>Have fun! Send questions or comments to:<a href=\"https://knock-knock.net/#\"></a></p>","contentLength":666,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47025338"},{"title":"Gwtar: A static efficient single-file HTML format","url":"https://gwern.net/gwtar","date":1771169826,"author":"theblazehen","guid":257,"unread":true,"content":"<div><blockquote><p>Archiving HTML files faces a trilemma: it is easy to create an archival format which is any two of static (self-contained ie. all assets included, no special software or server support), a single file (when stored on disk), and efficient (lazy-loads assets only as necessary to display to a user), but no known format allows all 3 simultaneously.</p><p>We introduce a new format,  (<a href=\"https://gwern.net/doc/cs/algorithm/information/compression/2026-01-23-dbohdan-gpt5imagemini-gwtarlogo-guitar.svg\" data-link-icon=\"image\" data-link-icon-type=\"svg\" data-link-icon-color=\"#ffb338\" data-filesize-bytes=\"4219\" data-filesize-percentage=\"1\" data-image-height=\"1024\" data-image-width=\"1024\" data-aspect-ratio=\"1 / 1\" title=\"Bohdan 2026\">logo</a>; pronounced “guitar”,  extension), which achieves all 3 properties simultaneously. A Gwtar is a classic fully-inlined HTML file, which is then processed into a self-extracting concatenated file of an HTML + JavaScript header followed by a tarball of the original HTML and assets. The HTML header’s JS stops web browsers from loading the rest of the file, loads just the original HTML, and then hooks requests and turns them into range requests into the tarball part of the file.</p><p>Thus, a regular web browser loads what seems to be a normal HTML file, and all assets download only when they need to. In this way, a static HTML page can inline anything—such as gigabyte-size media files—but those will not be downloaded until necessary, even while the server sees just a single large HTML file it serves as normal. And because it is self-contained in this way, it is forwards-compatible: no future user or host of a Gwtar file needs to treat it specially, as all functionality required is old standardized web browser/server functionality.</p><p>Gwtar allows us to easily and reliably archive even the largest HTML pages, while still being user-friendly to read.</p><p>Example pages:  (vs —: 286MB download).</p></blockquote></div><section><p>There are 3 major properties we would like of an HTML archive format, beyond the basics of actually capturing a page in the first place: it should not depend in any way on the original web page, because then it is not an archive and will inevitably break; it should be easy to manage and store, so you can scalably create them and store them for the long run; and it should be efficient, which for HTML largely means that readers should be able to download only the parts they need in order to view the current page.</p></section><section><p>No current format achieves all 3. The built-in web browser save-as-HTML format achieves single and efficient, but not static; save-as-HTML-with-directory achieves static partially and efficient, but not single; <a href=\"https://en.wikipedia.org/wiki/MHTML\" data-link-icon=\"wikipedia\" data-link-icon-type=\"svg\" data-url-iframe=\"https://en.m.wikipedia.org/wiki/MHTML#bodyContent\" title=\"MHTML\">MHTML</a>, <a href=\"https://en.wikipedia.org/wiki/Mozilla_Archive_Format\" data-link-icon=\"wikipedia\" data-link-icon-type=\"svg\" data-url-iframe=\"https://en.m.wikipedia.org/wiki/Mozilla_Archive_Format#bodyContent\" title=\"Mozilla Archive Format\">MAFF</a>, <a href=\"https://github.com/gildas-lormeau/SingleFile/\" data-link-icon=\"github\" data-link-icon-type=\"svg\" title=\"'SingleFile', Lormeau 2026\">SingleFile</a>, &amp; <a href=\"https://gildas-lormeau.github.io/Polyglot-HTML-ZIP-PNG/SUMMARY.html\" title=\"How to Create HTML/ZIP/PNG Polyglot Files\">SingleFileZ</a> (a <a href=\"https://en.wikipedia.org/wiki/ZIP\" data-link-icon=\"wikipedia\" data-link-icon-type=\"svg\" data-url-iframe=\"https://en.m.wikipedia.org/wiki/ZIP#bodyContent\" title=\"ZIP\">ZIP</a>-compressed variant) achieve static, single, but not efficiency; <a href=\"https://en.wikipedia.org/wiki/WARC_(file_format)\" data-link-icon=\"wikipedia\" data-link-icon-type=\"svg\" data-url-iframe=\"https://en.m.wikipedia.org/wiki/WARC_(file_format)#bodyContent\" title=\"WARC (file format)\">WARCs</a>/<a href=\"https://specs.webrecorder.net/wacz/1.1.1/\" title=\"Web Archive Collection Zipped (WACZ)\">WACZs</a> achieve static and efficient, but not single (because while the WARC is a single file, it relies on a complex software installation like <a href=\"https://webrecorder.net/\" title=\"Webrecorder: Web Archiving for All\">WebRecorder</a>/<a href=\"https://replayweb.page/\" title=\"ReplayWeb.page\">Replay Webpage</a> to display).</p><p>An ordinary ‘save as page HTML’ browser command doesn’t work because “Web Page, HTML Only” leaves out most of a web page; even “Web Page, Complete” is inadequate because a lot of assets are dynamic and only appear when you interact with the page—especially images. If you want a  HTML archive, one which has no dependency on the original web page or domain, you have to use a tool specifically designed for this. I usually use SingleFile. SingleFile produces a static snapshot of the live web page, while making sure that <a href=\"https://en.wikipedia.org/wiki/Lazy_loading\" data-link-icon=\"wikipedia\" data-link-icon-type=\"svg\" data-url-iframe=\"https://en.m.wikipedia.org/wiki/Lazy_loading#bodyContent\" title=\"Lazy loading\">lazy-loaded</a> images are first loaded, so they are included in the snapshot.</p><p>SingleFile often produces a useful static snapshot. It also achieves another nice property: the snapshot is a , just a simple single  file, which makes life so much easier in terms of organizing and hosting. Want to mirror a web page? SingleFile it, and upload the resulting single file to a convenient directory somewhere, boom—done forever. Being a single file is important on Gwern.net, where I must host so many files, and I run so many lints and checks and automated tools and track metadata etc. and where other people may rehost my archives.</p><p>However, a user of SingleFile quickly runs into a nasty drawback: snapshots can be surprisingly large. In fact, some snapshots on Gwern.net are over half a gigabyte! For example, the homepage for the research project <a href=\"https://gwern.net/doc/www/lllyasviel.github.io/96def0bcd8813bb1389665c487366a2ac61eaf4e.html\" data-url-archive=\"/doc/www/lllyasviel.github.io/96def0bcd8813bb1389665c487366a2ac61eaf4e.html\" data-url-original=\"https://lllyasviel.github.io/pages/paints_undo/\" data-filesize-bytes=\"507833876\" data-filesize-percentage=\"100\" title=\"PaintsUndo: A Base Model of Drawing Behaviors in Digital Paintings\">“PaintsUndo: A Base Model of Drawing Behaviors in Digital Paintings”</a> is 485MB  size optimization, while the raw HTML is 0.6MB. It is common for an ordinary somewhat-fancy Web 2.0 blog post like a <a href=\"https://en.wikipedia.org/wiki/Medium.com\" data-link-icon=\"wikipedia\" data-link-icon-type=\"svg\" data-url-iframe=\"https://en.m.wikipedia.org/wiki/Medium.com#bodyContent\" title=\"Medium.com\">Medium.com</a> post to be &gt;20MB once fully archived. This is because such web pages wind up importing a lot of <a href=\"https://en.wikipedia.org/wiki/Web_Fonts\" data-link-icon=\"wikipedia\" data-link-icon-type=\"svg\" data-url-iframe=\"https://en.m.wikipedia.org/wiki/Web_Fonts#bodyContent\" title=\"Web Fonts\">fonts</a>, JS, widgets and icons etc., all of which assets must be saved to ensure it is fully static; and then there is additional wasted space overhead due to <a href=\"https://en.wikipedia.org/wiki/Binary-to-text_encoding\" data-link-icon=\"wikipedia\" data-link-icon-type=\"svg\" data-url-iframe=\"https://en.m.wikipedia.org/wiki/Binary-to-text_encoding#bodyContent\" title=\"Binary-to-text encoding\">converting</a> assets from their original binary encoding into <a href=\"https://en.wikipedia.org/wiki/Base64\" data-link-icon=\"wikipedia\" data-link-icon-type=\"svg\" data-url-iframe=\"https://en.m.wikipedia.org/wiki/Base64#bodyContent\" title=\"Base64\">Base64</a> text which can be <a href=\"https://en.wikipedia.org/wiki/Data_URI_scheme\" data-link-icon=\"wikipedia\" data-link-icon-type=\"svg\" data-url-iframe=\"https://en.m.wikipedia.org/wiki/Data_URI_scheme#bodyContent\" title=\"Data URI scheme\">interleaved with the original HTML</a>.</p><p>This is especially bad because, unlike the original web page, anyone viewing a snapshot  download the . That 500MB web page is possibly OK because a reader only downloads the images that they are looking at; but the archived version must download everything. A web browser has to download the entire page, after all, to display it properly; and there is no lazy-loading or ability to optionally load ‘other’ files—there are no other files ‘elsewhere’, that was the whole point of using SingleFile!</p><p>Hence, a SingleFile archive is static, and a single file, but it is not : viewing it requires downloading unnecessary assets.</p><p>So, for some archives, we ‘split’ or ‘deconstruct’ the static snapshot back into a normal HTML file and a directory of asset files, using <a href=\"https://gwern.net/static/build/deconstruct_singlefile.php\" data-link-icon=\"code\" data-link-icon-type=\"svg\" data-link-icon-color=\"#787cb4\" data-filesize-bytes=\"19434\" data-filesize-percentage=\"4\"><code>deconstruct_singlefile.php</code></a> (which incidentally makes it easy to re-compress all the images, which produces large savings as many websites are surprisingly bad at basic stuff like PNG/JPG/GIF compression); then we are back to a static, efficient, but not single file, archive.</p><p>This is fine for our <a href=\"https://gwern.net/archiving#preemptive-local-archiving\" data-filesize-bytes=\"88086\" data-filesize-percentage=\"56\" title=\"‘Archiving URLs § Preemptive Local Archiving’, Gwern 2011\">auto-generated local archives</a> because they are stored in their own directory tree which is off-limits to most Gwern.net infrastructure (and off-limits to search engines &amp; agents or off-site hotlinking), and it doesn’t matter too much if they litter tens of thousands of directories and files. It is not fine for HTML archives I would like to host as first-class citizens, and expose to Google, and hope people will rehost someday when Gwern.net inevitably dies.</p><p>So, we could either host a regular SingleFile archive, which is static, single, and inefficient; or a deconstructed archive, which is static, multiple, and efficient, but not all 3 properties.</p><p>This issue came to a head in January 2026 when I was archiving the Internet Archive snapshots of Brian Moriarty’s famous lectures <a href=\"https://gwern.net/doc/philosophy/religion/1999-03-17-brianmoriarty-whoburiedpaul.html\" data-link-icon=\"internet-archive\" data-link-icon-type=\"svg\" data-filesize-bytes=\"6394956\" data-filesize-percentage=\"93\" title=\"'Who Buried Paul?', Moriarty 1999\">“Who Buried Paul?”</a> and <a href=\"https://gwern.net/doc/philosophy/religion/2010-02-brianmoriarty-thesecretofpsalm46.html\" data-link-icon=\"internet-archive\" data-link-icon-type=\"svg\" data-filesize-bytes=\"299579672\" data-filesize-percentage=\"100\" title=\"‘The Secret of Psalm 46’, Moriarty 2010\">“The Secret of Psalm 46”</a>, since I noticed while writing <a href=\"https://gwern.net/video-game-art\" data-filesize-bytes=\"19787\" data-filesize-percentage=\"27\" title=\"‘Video Games as Art’, Gwern 2025\">an essay drawing on them</a> that his whole website had sadly gone down. I admire them and wanted to host them properly so people could easily find my fast reliable mirrors (unlike the slow, hard-to-find, unreliable IA versions), but realized I was running into our long-standing dilemma: they would be efficient in the local archive system after being split, but unfindable; or if findable, inefficiently large and reader-unfriendly. Specifically, the video of “Who Buried Paul?” was not a problem because it had been linked as a separate file, so I simply <a href=\"https://gwern.net/doc/philosophy/religion/1999-03-17-brianmoriarty-whoburiedpaul-videolecture.mp4\" data-link-icon=\"file-video\" data-link-icon-type=\"svg\" data-filesize-bytes=\"133173584\" data-filesize-percentage=\"99\" data-image-height=\"244\" data-image-width=\"322\" data-aspect-ratio=\"161 / 122\">converted it to MP4</a> and edited the link; but “The Secret of Psalm 46” turned out to inline the OGG/MP3 recordings of the lecture and abruptly increased from &lt;1MB to .</p><p>I discussed it with <a href=\"https://wiki.obormot.net/\" title=\"Welcome to OborWiki\">Said Achmiz</a>, and he began developing a fix.</p></section><section><p>To achieve all 3, we need some way to download only part of a file, and selectively download the rest. This lets us have a single static archive of potentially arbitrarily large size, which can safely store every asset which might be required.</p><p>HTTP already easily supports selective downloading via the ancient <a href=\"https://en.wikipedia.org/wiki/Byte_serving\" data-link-icon=\"wikipedia\" data-link-icon-type=\"svg\" data-url-iframe=\"https://en.m.wikipedia.org/wiki/Byte_serving#bodyContent\" title=\"Byte serving\">HTTP Range query feature</a>, which allows one to query for a precise range of bytes inside a URL. This is mostly used to do things like resume downloads, but you can also <a href=\"https://gwern.net/design-graveyard#range-queries\" data-filesize-bytes=\"200493\" data-filesize-percentage=\"73\">do interesting things</a> like run databases in reverse: a web browser client can run a database application locally which reads a database file stored on a server, because Range queries let the client download only the exact parts of the database file it needs at any given moment, as opposed to the entire thing (which might be terabytes in size).</p><p>This is how formats like WARC can render efficiently: host a WARC as a normal file, and then simply range-query the parts displayed at any moment.</p><p>The challenge is the first part: how do we download  the original HTML and subsequently only the displayed assets? If we have a single HTML file and then a separate giant archive file, we could easily just rewrite the HTML using JS to point to the equivalent ranges in the archive file (or do something server-side), but that would achieve only static and efficiency, not single file. If we combine them, like SingleFile, we are back to static and single file, but not efficiency.</p><p>The simplest solution here would be to decide to complicate the server itself and do the equivalent of <code>deconstruct_singlefile.php</code> on the fly. HTML requests, perhaps detecting some magic string in the URL like , is handed to a <a href=\"https://en.wikipedia.org/wiki/Common_Gateway_Interface\" data-link-icon=\"wikipedia\" data-link-icon-type=\"svg\" data-url-iframe=\"https://en.m.wikipedia.org/wiki/Common_Gateway_Interface#bodyContent\" title=\"Common Gateway Interface\">CGI</a> proxy process, which splits the original single HTML file into a normal HTML file with lazy-loaded references. The client browser sees a normal multiple efficient HTML, while everything on server sees a static single inefficient HTML. (A possible example is <a href=\"https://gwern.net/doc/www/github.com/ff0072519026bd8a7f72adcf3f86a25a1932e14d.html\" data-link-icon=\"github\" data-link-icon-type=\"svg\" data-url-archive=\"/doc/www/github.com/ff0072519026bd8a7f72adcf3f86a25a1932e14d.html\" data-url-original=\"https://github.com/oils-for-unix/wwz\" data-filesize-bytes=\"4157805\" data-filesize-percentage=\"79\" title=\"oils-for-unix/wwz: A WSGI program that serves content out of a zip file (.wwz file). Deploy as CGI or FastCGI\">WWZ</a>.)</p><p>While this solves the immediate Gwern.net problem, it does so at the permanent cost of server complexity, and does not do much to help anyone else. (It is unrealistic to expect more than a handful of people to modify their servers this invasively.) I also considered taking the WARC red pill and going full WebRecorder, but quailed.</p><section><p>How can we trick an HTML file into acting like a <a href=\"https://en.wikipedia.org/wiki/Tar_(computing)\" data-link-icon=\"wikipedia\" data-link-icon-type=\"svg\" data-url-iframe=\"https://en.m.wikipedia.org/wiki/Tar_(computing)#bodyContent\" title=\"Tar (computing)\">tarball</a> or ZIP file, with partial random access?</p><p>Our initial approach was to ship an HTML + JS header with an appended archive, where the JS would do HTTP Range queries into the appended binary archive; the challenge, however, was to  the file from downloading past the header. To do this, we considered some approaches ‘outside’ the page, like encoding the archive index into the filename/URL itself (ie. ) and requiring the server to parse  out and slice the archive down to just the header, which then handled the range requests; this minimized how much special handling the server did, while being backwards/forwards-compatible with non-compliant servers (who would ignore the index and simply return the entire file, and be no worse than before). This worked in our prototypes, but required at least some server-side support and also required that the header be fixed-length (because any changes would in length would invalidate the index).</p><p>Eventually, Achmiz realized that you  stop downloading from  an HTML page, using the JS command ! <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Window/stop\" data-link-icon=\"FF\" data-link-icon-type=\"text,sans\" data-link-icon-color=\"#e66000\" title=\"'Window: <code>stop()</code> method—Web APIs', MDN 2026\">MDN</a> (<a href=\"https://caniuse.com/mdn-api_window_stop\" data-link-icon=\"CanI\" data-link-icon-type=\"text,sans,quad\" data-link-icon-color=\"#c75000\" title=\"Window API: <code>stop</code>\">&gt;96% support</a>, <a href=\"https://html.spec.whatwg.org/multipage/nav-history-apis.html#dom-window-stop\" title=\"HTML Standard\">spec</a>):</p><blockquote><p>The  stops further resource loading in the current browsing context, equivalent to the stop button in the browser.</p><p>Because of how scripts are executed, this method cannot interrupt its parent document’s loading, but it will stop its images, new windows, and other still-loading objects.</p></blockquote><p>This is precisely what we need, and the design falls into place.</p></section></section><section><p>A Gwtar is an HTML file with a HTML + JS + JSON header followed by a tarball and <a href=\"https://gwern.net/gwtar#optional-trailing-data\">possibly further assets</a>. (A Gwtar could be seen as  a <a href=\"https://en.wikipedia.org/wiki/Polyglot_(computing)\" data-link-icon=\"wikipedia\" data-link-icon-type=\"svg\" data-url-iframe=\"https://en.m.wikipedia.org/wiki/Polyglot_(computing)#bodyContent\" title=\"Polyglot (computing)\">polyglot file</a> is a file valid as more than one format—in this case, a  file that is also a  archive, and possibly . But strictly speaking, it is not.)</p><section><section><p>The simple approach is to download the binary assets, encode them into Base64 text, and inject them into the HTML DOM. This is inefficient in both compute and RAM because the web browser must immediately reverse this to get a binary to work with. So we actually use the browser optimization of <a href=\"https://developer.mozilla.org/en-US/docs/Web/API/Blob\" data-link-icon=\"FF\" data-link-icon-type=\"text,sans\" data-link-icon-color=\"#e66000\" title=\"Blob—Web APIs\">blobs</a> to just pass the binary asset straight to the browser.</p><p>A tricky bit is that inline JS can depend on “previously loaded” JS files, which may not have actually loaded  because the first attempt failed (of course) and the real Range request is still racing. We currently solve this by just downloading all JS before rendering the HTML, at some cost to responsiveness.</p><p>So, a web browser will load a normal web page; the JS will halt its loading; a new page loads, and all of its requests initially fail but get repeated immediately and work the second time; the entire archive never gets downloaded unless required. All assets are provided, there is a single Gwtar file, it is efficient; it doesn’t require JS for archival integrity, as just the entire archive downloads if the JS is not executed; and it is cross-platform and standards-compliant, requires no server-side support or future users/hosts to do anything whatsoever, and is a transparent, self-documenting file format which can be easily converted back to a ‘normal’ multiple-file HTML (<code> foo.gwtar.html  xf </code>)  a user can just re-archive it normally with tools like SingleFile.</p></section></section><section><p>In the event of JS problems, <a href=\"https://gwern.net/static/build/gwtar_noscript.html\" data-link-icon=\"code\" data-link-icon-type=\"svg\" data-filesize-bytes=\"1512\" data-filesize-percentage=\"0\">a  message</a> explains what the Gwtar format is and why it requires JS, and links to this page for more details.</p><p>It also detects whether range requests are supported or downgraded to requesting the entire file. If the latter, it will start rendering it.</p><p>This is not as slow as it seems because we can benefit from connection level compression like <a href=\"https://en.wikipedia.org/wiki/Gzip\" data-link-icon=\"wikipedia\" data-link-icon-type=\"svg\" data-url-iframe=\"https://en.m.wikipedia.org/wiki/Gzip#bodyContent\" title=\"Gzip\">gzip</a> or <a href=\"https://en.wikipedia.org/wiki/Brotli_compression\" data-link-icon=\"wikipedia\" data-link-icon-type=\"svg\" data-url-iframe=\"https://en.m.wikipedia.org/wiki/Brotli_compression#bodyContent\" title=\"Brotli compression\">Brotli compression</a>. And because our preprocessing linearize the assets in dependency order, we receive the bytes in order of page appearance, and so in this mode, the “above the fold” images and stuff will still load first and quickly. (This in comparison to the usual SingleFile, where you have to receive every single asset before you’re done, and which may be slower.)</p></section><section><section><p>Strangely, the biggest drawback of Gwtar turns out to be  viewing of HTML archives. SingleFileZ encounters the same issue: in the name of security (<a href=\"https://en.wikipedia.org/wiki/Same-origin_policy\" data-link-icon=\"wikipedia\" data-link-icon-type=\"svg\" data-url-iframe=\"https://en.m.wikipedia.org/wiki/Same-origin_policy#bodyContent\" title=\"Same-origin policy\">origin</a>/<a href=\"https://en.wikipedia.org/wiki/CORS\" data-link-icon=\"wikipedia\" data-link-icon-type=\"svg\" data-url-iframe=\"https://en.m.wikipedia.org/wiki/CORS#bodyContent\" title=\"CORS\">CORS</a>/sandboxing), browsers will not execute certain requests in local HTML pages, so it will break, as it is no longer able to request from itself.</p><p>We regard this as unfortunate, but an acceptable tradeoff, as for local browsing, the file can be easily converted back to the non-JS dependent multiple/single-file HTML formats.</p></section><section><p>Range requests are old, standardized, and important for resuming downloads or viewing large media files like video, and every web server should, in theory, support it by default. In practice, there may be glitches, and one should check.</p><p>An example <a href=\"https://en.wikipedia.org/wiki/CURL\" data-link-icon=\"wikipedia\" data-link-icon-type=\"svg\" data-url-iframe=\"https://en.m.wikipedia.org/wiki/CURL#bodyContent\" title=\"CURL\">curl</a> command which should return a HTTP 206 (not 200) request if range requests are correctly working:</p><div><pre><code></code></pre></div><p>Servers  serve Gwtar files as  if possible. This may require some configuration (eg. <a href=\"https://blog.nginx.org/blog/smart-efficient-byte-range-caching-nginx\" title=\"Smart and Efficient Byte-Range Caching with NGINX\">in nginx</a>), but should be straightforward.</p><section><p>However, Cloudflare has an undocumented, hardwired behavior: its proxy (not cache) will strip Range request headers for  responses regardless of cache settings. This does not break Gwtar rendering, of course, but it does break efficiency and defeats the point of Gwtar for Gwern.net</p><p>As a workaround, we serve Gwtars with the MIME type —web browsers like Firefox &amp; Chromium will content-sniff the opening  tag and render correctly, while Cloudflare passes Range requests through for unrecognized types. (This is not ideal, but a more conventional MIME type like  results in web browsers downloading the file without trying to render it at all; and using a MIME type trick is better than alternatives like trying to serve Gwtars as MP4s, using a special-case subdomain just to bypass Cloudflare completely, using complex tools like Service Workers to try to undo the removal, etc.)</p></section></section></section><section><p>Because a Gwtar can store large binary assets without burdening the viewer and is an archive format, it may be useful for reproducible science/statistics: include datasets, such as <a href=\"https://sqlite.org/wasm/doc/trunk/index.md\" data-link-icon=\"txt\" data-link-icon-type=\"svg\" title=\"sqlite3 WebAssembly &amp;amp; JavaScript Documentation Index\">Sqlite3 databases</a>, and do computation on them like visualization or analysis. The question is, how do we ensure that assets get referenced in a way that SingleFile can “see” them and include them inline (to be stored in the final Gwtar as split-out objects), and then addressed and loaded by simple user JS, in a way which still works  Gwtar?</p><p>A potential approach in Gwtar v1 would be to reference all such assets using the <a href=\"https://developer.mozilla.org/en-US/docs/Web/HTML/Reference/Elements/object\" data-link-icon=\"FF\" data-link-icon-type=\"text,sans\" data-link-icon-color=\"#e66000\" title=\"'<object>', element—HTML MDN\"> tag</a>, and then the user JS adds a simple listener hook to the  event, which will fire either when the browser loads the asset normally (multi-file) or when Gwtar completes its range-fetch rewrite, and then kicks off the actual userland work. This does not require any unusual or contorted user JS, appears to be backwards/forwards compatible, and to satisfy all our desiderata.</p><div><pre><code></code></pre></div></section><section><p>The appended tarball can itself be followed by additional arbitrary binary assets, which can be large since they will usually not be downloaded. (While the exact format of each appended file is up to the users, it’s a good idea to wrap them in tarballs if you can.)</p><p>This flexibility is intended primarily for allowing ad hoc metadata extensions like <a href=\"https://en.wikipedia.org/wiki/Cryptographic_signatures\" data-link-icon=\"wikipedia\" data-link-icon-type=\"svg\" data-url-iframe=\"https://en.m.wikipedia.org/wiki/Cryptographic_signatures#bodyContent\" title=\"Cryptographic signatures\">cryptographic signatures</a> or forward error correction (<a href=\"https://en.wikipedia.org/wiki/Error_correction_code\" data-link-icon=\"wikipedia\" data-link-icon-type=\"svg\" data-url-iframe=\"https://en.m.wikipedia.org/wiki/Error_correction_code#bodyContent\" title=\"Error correction code § Forward error correction\">FEC</a>).</p><section><p>The Gwern.net generation script uses this feature to add <a href=\"https://en.wikipedia.org/wiki/Par2\" data-link-icon=\"wikipedia\" data-link-icon-type=\"svg\" data-url-iframe=\"https://en.m.wikipedia.org/wiki/Par2#bodyContent\" title=\"Par2\">par2</a> FEC in an additional tarball. This allows recovery of the original Gwtar if it has been partially corrupted or lost. (It cannot recover loss of the file as a whole, which is why FEC is ideally done over large corpuses, and not individual files, but this is better than nothing, and gives us free integrity checking as well.)</p><p>PAR2 can find its FEC data even in corrupted files by scanning for FEC data (“packets”) it recognizes, while tar ignores appended data; so adding, say, 25% par2 FEC is as simple as running <code> foo.gwtar.html  cf.  foo.gwtar.html.par2 foo.gwtar.html.vol.par2  foo.gwtar.html  foo.gwtar.html.par2</code>, and .</p><p>This yields the original  without any FEC. A repaired Gwtar file can then have fresh FEC added to be just like the old Gwtar + FEC archive, or be integrated in some broader system which achieves long-term protection some other way.</p></section><section><p>A simple form of cryptographic signing would be to use GPG to sign it as a normal, separate, signature file (creates ): <code> foo.gwtar.html</code>.</p><p>And we could also append an ASCII ‘armored’ GPG signature, as it won’t confuse tar, like <code> foo.gwtar.html  foo.gwtar.html</code>. Since GPG won’t munge a file like PAR2 will, an adhoc format would be to wrap it in tar to assist extracting:</p><div><pre><code></code></pre></div><p>or in magic text, like a HTML comment:</p><div><pre><code></code></pre></div></section></section></section><section><p>This documentation and the Gwtar code is licensed under the <a href=\"https://creativecommons.org/public-domain/cc0/\" data-link-icon=\"creative-commons\" data-link-icon-type=\"svg\" title=\"‘CC-0: Creative Commons public domain license’, Commons 2002\">CC-0</a><a href=\"https://en.wikipedia.org/wiki/Public_domain\" data-link-icon=\"wikipedia\" data-link-icon-type=\"svg\" data-url-iframe=\"https://en.m.wikipedia.org/wiki/Public_domain#bodyContent\" title=\"Public domain\">public domain</a> copyright license. We are unaware of any software patents.</p></section><section><p>Gwtar v1 could be improved with:</p><ol><li><p>Checking of hashsums when rendering (possibly async or deferred)</p></li><li><p>More aggressive prefetching of assets</p></li><li><p>Integration into SingleFile (possibly as a “SingleFileZ2” forma?)</p></li><li><p>Testing: corpus of edge-case test files (inline SVG, , CSS  chains, web fonts, data URIs in CSS…)</p></li></ol><p>A Gwtar v2 could add breaking changes like:</p><ol><li><p>format provides more rigorous validation/checking of HTML &amp; assets; require HTML &amp; asset validity, assets all decode successfully, etc.</p></li><li><p>standardize appending formats</p></li><li><p>built-in compression with Brotli/gzip for formats not already compressed</p></li><li><p>One would try to replace MAFF’s capability of creating sets of documents which are convenient to link/archive and can automatically share assets for de-duplication (eg. page selected by a built-in widget, or perhaps by a hash-anchor like <code>archive.gwtar.html#page=foo.html</code>? Can an initial web page open new tabs of all the other web pages in the archive?)</p></li><li><p>Better de-duplication, eg. content-addressed asset names (hash-based) enabling deduplication across multiple gwtars</p></li></ol></section>","contentLength":19307,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47024506"},{"title":"Two different tricks for fast LLM inference","url":"https://www.seangoedecke.com/fast-llm-inference/","date":1771147653,"author":"swah","guid":254,"unread":true,"content":"<p><a href=\"https://platform.claude.com/docs/en/build-with-claude/fast-mode\">Anthropic</a> and <a href=\"https://openai.com/index/introducing-gpt-5-3-codex-spark/\">OpenAI</a> both recently announced “fast mode”: a way to interact with their best coding model at significantly higher speeds.</p><p>These two versions of fast mode are very different. Anthropic’s <a href=\"https://platform.claude.com/docs/en/build-with-claude/fast-mode#how-fast-mode-works\">offers</a> up to 2.5x tokens per second (so around 170, up from Opus 4.6’s 65). OpenAI’s offers more than 1000 tokens per second (up from GPT-5.3-Codex’s 65 tokens per second, so 15x). So OpenAI’s fast mode is six times faster than Anthropic’s.</p><p>However, Anthropic’s big advantage is that they’re serving their actual model. When you use their fast mode, you get real Opus 4.6, while when you use OpenAI’s fast mode you get GPT-5.3-Codex-Spark, not the real GPT-5.3-Codex. Spark is indeed much faster, but is a notably less capable model: good enough for many tasks, but it gets confused and messes up tool calls in ways that vanilla GPT-5.3-Codex would never do.</p><p>Why the differences? The AI labs aren’t advertising the details of how their fast modes work, but I’m pretty confident it’s something like this: <strong>Anthropic’s fast mode is backed by  inference, while OpenAI’s fast mode is backed by special monster Cerebras chips</strong>. Let me unpack that a bit.</p><h3>How Anthropic’s fast mode works</h3><p>The tradeoff at the heart of AI inference economics is , because the main bottleneck is . GPUs are very fast, but moving data onto a GPU is not. Every inference operation requires copying all the tokens of the user’s prompt onto the GPU before inference can start. Batching multiple users up thus increases overall throughput at the cost of making users wait for the batch to be full.</p><p>A good analogy is a bus system. If you had zero batching for passengers - if, whenever someone got on a bus, the bus departed immediately - commutes would be much faster <em>for the people who managed to get on a bus</em>. But obviously overall throughput would be much lower, because people would be waiting at the bus stop for hours until they managed to actually get on one.</p><p>Anthropic’s fast mode offering is basically a bus pass that guarantees that the bus immediately leaves as soon as you get on. It’s six times the cost, because you’re effectively paying for all the other people who could have got on the bus with you, but it’s way faster because you spend  time waiting for the bus to leave.</p><p>edit: I want to thank a reader for emailing me to point out that the “waiting for the bus” cost is really only paid for the first token, so that won’t affect  latency (just latency per turn or tool call). It’s thus better to think of the performance impact of batch size being mainly that smaller batches require fewer flops and thus execute more quickly. In my analogy, maybe it’s “lighter buses drive faster”, or something.</p><p>Obviously I can’t be fully certain this is right. Maybe they have access to some new ultra-fast compute that they’re running this on, or they’re doing some algorithmic trick nobody else has thought of. But I’m pretty sure this is it. Brand new compute or algorithmic tricks would likely require changes to the model (see below for OpenAI’s system), and “six times more expensive for 2.5x faster” is right in the ballpark for the kind of improvement you’d expect when switching to a low-batch-size regime.</p><h3>How OpenAI’s fast mode works</h3><p>OpenAI’s fast mode does not work anything like this. You can tell that simply because they’re introducing a new, worse model for it. There would be absolutely no reason to do that if they were simply tweaking batch sizes. Also, they told us in the announcement <a href=\"https://openai.com/index/introducing-gpt-5-3-codex-spark/\">blog post</a> exactly what’s backing their fast mode: Cerebras.</p><p>OpenAI <a href=\"https://openai.com/index/cerebras-partnership/\">announced</a> their Cerebras partnership a month ago in January. What’s Cerebras? They build “ultra low-latency compute”. What this means in practice is that they build . A H100 chip (fairly close to the frontier of inference chips) is just over a square inch in size. A Cerebras chip is  square inches.</p><p>You can see from pictures that the Cerebras chip has a grid-and-holes pattern all over it. That’s because silicon wafers this big are supposed to be broken into dozens of chips. Instead, Cerebras etches a giant chip over the entire thing.</p><p>The larger the chip, the more internal memory it can have. The idea is to have a chip with SRAM large enough , so inference can happen entirely in-memory. Typically GPU SRAM is measured in the tens of . That means that a lot of inference time is spent streaming portions of the model weights from outside of SRAM into the GPU compute. If you could stream all of that from the (much faster) SRAM, inference would a big speedup: fifteen times faster, as it turns out!</p><p>So how much internal memory does the latest Cerebras chip have? <a href=\"https://arxiv.org/html/2503.11698v1#:~:text=Most%20recently%2C%20the%20Wafer%20Scale,of%2021%20petabytes%20per%20second.\">44GB</a>. This puts OpenAI in kind of an awkward position. 44GB is enough to fit a small model (~20B params at fp16, ~40B params at int8 quantization), but clearly not enough to fit GPT-5.3-Codex. That’s why they’re offering a brand new model, and why the Spark model has a bit of “small model smell” to it: it’s a smaller <a href=\"https://en.wikipedia.org/wiki/Knowledge_distillation\">distil</a> of the much larger GPT-5.3-Codex model.</p><p>edit: I was wrong about this - the Codex model is almost certainly larger than this, and doesn’t need to fit entirely in one chip’s SRAM (if it did, we’d be seeing faster speeds). Thanks to the Hacker News commenters for correcting me. But I think there’s still a good chance that Spark is SRAM-resident (split across a few Cerebras chips) which is what’s driving the speedup.</p><h3>OpenAI’s version is much more technically impressive</h3><p>It’s interesting that the two major labs have two very different approaches to building fast AI inference. If I had to guess at a conspiracy theory, it would go something like this:</p><ul><li>OpenAI partner with Cerebras in mid-January, obviously to work on putting an OpenAI model on a fast Cerebras chip</li><li>Anthropic have no similar play available, but they know OpenAI will announce some kind of blazing-fast inference in February, and they want to have something in the news cycle to compete with that</li><li>Anthropic thus hustles to put together the kind of fast inference they  provide: simply lowering the batch size on their existing inference stack</li><li>Anthropic (probably) waits until a few days before OpenAI are done with their much more complex Cerebras implementation to announce it, so it looks like OpenAI copied them</li></ul><p>Obviously OpenAI’s achievement here is more technically impressive. Getting a model running on Cerebras chips is not trivial, because they’re so weird. Training a 20B or 40B param distil of GPT-5.3-Codex that is still kind-of-good-enough is not trivial. But I commend Anthropic for finding a sneaky way to get ahead of the announcement that will be largely opaque to non-technical people. It reminds me of OpenAI’s mid-2025 sneaky introduction of the Responses API to help them <a href=\"https://www.seangoedecke.com/responses-api\">conceal their reasoning tokens</a>.</p><h3>Is fast AI inference the next big thing?</h3><p>Seeing the two major labs put out this feature might make you think that fast AI inference is the new major goal they’re chasing. I don’t think it is. If my theory above is right, Anthropic don’t care  much about fast inference, they just didn’t want to appear behind OpenAI. And OpenAI are mainly just exploring the capabilities of their new Cerebras partnership. It’s still largely an open question what kind of models can fit on these giant chips, how useful those models will be, and if the economics will make any sense.</p><p>I personally don’t find “fast, less-capable inference” particularly useful. I’ve been playing around with it in Codex and I don’t like it. The usefulness of AI agents is dominated by <em>how few mistakes they make</em>, not by their raw speed. Buying 6x the speed at the cost of 20% more mistakes is a bad bargain, because most of the user’s time is spent handling mistakes instead of waiting for the model.</p><p>However, it’s certainly possible that fast, less-capable inference becomes a core lower-level primitive in AI systems. Claude Code already uses <a href=\"https://github.com/anthropics/claude-code/issues/1098#issuecomment-2884244872\">Haiku</a> for some operations. Maybe OpenAI will end up using Spark in a similar way.</p><p>edit: there are some good comments about this post on <a href=\"https://news.ycombinator.com/item?id=47022329\">Hacker News</a>. First, a good <a href=\"https://news.ycombinator.com/item?id=47022810\">correction</a>: Cerebras offers a ~355B model, GLM-4.7, at 1000 tokens per second already, so I’m wrong about Spark living in a single chip’s SRAM. Presumably they’re sharding Spark across multiple chips, like they’re doing with GLM-4.7.</p><p>Many commenters disagreed with me (and each other) about the performance characteristics of batching. Some <a href=\"https://news.ycombinator.com/item?id=47025656\">said</a> that continuous batching means nobody ever waits for a bus, or that the <a href=\"https://news.ycombinator.com/item?id=47025997\">volume</a> of requests for Anthropic models means batch wait time is negligible. Other users <a href=\"https://news.ycombinator.com/item?id=47023038\">disagreed</a> about whether chip-to-chip communication is a bottleneck at inference time, or whether chaining chips together affects throughput.</p><p>I only have a layman’s understanding of continuous batching, but it seems to me that you still have to wait for a slot to become available (even if you’re not waiting for the entire previous batch to finish), so the batch size throughput/latency tradeoff still applies. Overall, I think the takeaway is that this stuff is really complicated and hard to form a good, simple mental model around. </p>","contentLength":9173,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=47022329"},{"title":"Real-time PathTracing with global illumination in WebGL","url":"https://erichlof.github.io/THREE.js-PathTracing-Renderer/","date":1770921339,"author":"tobr","guid":252,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=46993014"}],"tags":["dev","hn"]}