{"id":"6C2W1azD1rBsV4qJANNfruBrvGgdqLWrPGWtk6Mp1WgfCNcfvBZHFQw","title":"The System Design Newsletter","displayTitle":"Dev - System Design Newsletter","url":"https://newsletter.systemdesign.one/feed","feedLink":"https://newsletter.systemdesign.one/","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":4,"items":[{"title":"How RPC Actually Works","url":"https://newsletter.systemdesign.one/p/how-rpc-works","date":1761735789,"author":"Neo Kim","guid":39,"unread":true,"content":"<p>Download my system design playbook for FREE on newsletter signup:</p><p><em>This post outlines how RPC works.</em></p><p>How do hundreds of internal services at a company like Netflix or Google talk to each other? And that too, at millions of times per minute?</p><p>A standard REST API, while great for public use, often creates too much overhead for this internal, high-speed chatter. Because it typically uses text-based formats like JSON. While easy for humans to read, it’s inefficient for services that need to have millions of conversations per day.</p><p>Instead, these services need to talk in a language that is incredibly compact and fast for computers to process. Plus, they need a pre-agreed set of rules, like a shared blueprint, that guarantees both sides know exactly how to talk to each other without errors.</p><p>And most importantly, it should offer a good developer experience without sacrificing performance.</p><p>Modern applications use many distributed services. Yet this creates a core challenge:</p><p>How to achieve extreme machine-to-machine efficiency? A function call across a network should feel as simple as a local one.</p><p>The answer is Remote Procedure Call). It’s a protocol designed for high-performance internal communication.</p><p>I want to introduce <a href=\"https://x.com/asmah2107\">Ashutosh</a> as a guest author.</p><p>He’s a software engineer at YouTube, ex-Google Search, and Microsoft Azure.</p><p>Connect with him if you want to get deep dives into system design, AI, and software engineering.</p><p> is a conversational AI and career matchmaker that works on behalf of each person. You spend 15-20 minutes on the phone with him, talking about your experience, your ambitions, and your non-negotiables.</p><p>Dex then scans thousands of roles and companies to identify the most interesting and compatible opportunities.</p><p>Once we’ve found a match,  connects you to hiring managers and even helps you prep for interviews.</p><p>Thousands of exceptional engineers have already signed up, and we’re partnered with many of the UK’s leading start-ups, scale-ups, hedge funds, and tech companies.</p><p>Don’t waste another day at a job you hate. Speak with Dex today.</p><h2>What Is a Remote Procedure Call</h2><p>RPC is a protocol that allows a function call to cross from one program’s address space into another. This frees the developer from manually managing the raw network connection points () or translating data into a sendable format ().</p><p>When your code calls a function like , you aren’t concerned with the complex algorithm running inside the math library. You simply trust its public API and know the function’s signature.</p><p>RPC extends this exact concept across the network.</p><p>The client application gets a stub object - a local proxy that mirrors the remote service’s API. But its methods contain only the logic to forward the call over the network. When you call a function on this stub, it handles all the complex network communication behind the scenes to execute the code on the server.</p><p>So, a call like <code>result = paymentService.charge(userId, amount)</code>doesn’t call the payment logic directly. Instead, it calls a local proxy method, which then makes a remote operation. This makes the remote call feel like a simple local library call.</p><p>Before we go further, let’s understand this first:</p><h2>How Remote Procedure Call Works</h2><h4>1. Client Stub (The Local Representative)</h4><p>Your code doesn’t call the remote service directly. Instead, it talks to the  - a local representative whose only job is to act as a stand-in for the remote service.</p><p>It has the same methods you want to use, for example, “. But its real purpose is to kick off the remote communication process.</p><h4>2. Marshaling (Packing the Message)</h4><p>The stub takes the parameters you provided, like ,  and marshals them. </p><p>Marshaling is a fancy word for serializing. It means converting the data from a programming language format to a standardized format, so you can send it over the network.</p><p>Think of it as neatly packing your instructions into a universal shipping container that any server in the world can open. This format is often JSON or a more compact, faster format like <a href=\"https://protobuf.dev/overview/\">Protocol Buffers</a>.</p><p>The client then sends this packed message across the network to the server, using efficient protocols like <a href=\"https://en.wikipedia.org/wiki/HTTP/2\">HTTP/2</a>. This is the “remote” part of the call.</p><p>On the server, a “skeleton” is listening. It’s the counterpart of the client stub.</p><p>Its job is to receive the incoming message and unmarshal (or deserialize) it. Imagine unpacking the shipping container back into a data format the server can understand.</p><p>The skeleton then calls the actual service method with this unpacked data.</p><p>After the server finishes its work, the entire process happens in reverse.</p><p>The skeleton marshals the return value, for example, . Then sends it back across the network, and the client stub unmarshals it.</p><p>After that, the line of code that made the original call receives the result.</p><p>Next, I’ll walk you through different RPC failure scenarios and approaches to handle them.</p><h2>Handling Failures in Remote Procedure Call</h2><p>In a distributed system, the network might be unreliable.</p><p>Connections drop. Services slow down. Failures occur.</p><p>A robust RPC implementation isn’t just about successful calls, but also about gracefully handling failures. Here are some strategies:</p><h4>1. Timeouts (Don’t Wait Forever)</h4><p>A timeout is a simple deadline. </p><p>Whenever the client makes a request, it starts a timer. If it doesn’t receive a response before the timer runs out, it gives up on the call.</p><p>: Without timeouts, your application could get stuck waiting indefinitely for a response from a slow or dead service. This would tie up resources, such as threads or memory, and could eventually freeze your entire application. Timeouts are the first line of defense to protect the client’s health.</p><h4>2. Retries (If at First Call Doesn’t Succeed)</h4><p>Many network errors are transient: a temporary glitch that quickly resolves itself.</p><p>Sometimes, a server might take too long to respond, or a network packet might get lost in transit. In these cases, it makes sense to retry the failed request automatically.</p><p><em>Important Caveat (Idempotency)</em>: Yet retries can be dangerous if the same request causes side effects each time it runs. An operation is idempotent if performing it many times has the same effect as doing it once.</p><ul><li><p>Safe to Retry (Idempotent): Getting a user’s account balance. Doing it again won’t change the system’s state.</p></li><li><p>Dangerous to Retry (Not Idempotent): Charging a user’s credit card. Retrying it might cause duplicate payments.</p></li></ul><p>So developers must design their systems with idempotency safeguards, such as unique transaction IDs, to retry safely.</p><h4>3. Circuit Breakers (Protecting the System)</h4><p>When a service fails or slows down, sending it more requests will only make things worse.</p><p>A circuit breaker is a smart pattern that prevents this from happening.</p><p>If the client notices many failed calls to a service, the circuit breaker “trips” and stops sending requests to that service for a short time. During this period, any new calls fail instantly instead of being sent over the network.</p><p> Circuit breakers prevent cascading failures. They give the failing service time to recover. And keep the client from wasting time and resources on calls that are likely to fail.</p><h4>4. Deadline Propagation (Don’t Start Work You Can’t Finish)</h4><p>A single user request often triggers a chain of calls between several services. For example, Service A calls B, and B calls C.</p><p>If the original request has a total timeout of 500ms, and Service A already uses 300ms. Then there’s no point in Service C starting a task that takes another 400ms because the client would have already stopped waiting.</p><p> fixes this by passing the remaining time limit along with every call. So each service knows the remaining time and can decide whether it can finish its part before the deadline.</p><p>This approach prevents wasted work and keeps the system fast and responsive. Services can “fail fast” instead of doing long operations for a request that’s already too late to matter.</p><p>Like every technology, RPC isn’t perfect; it has some strengths, but also tradeoffs to watch out for. Let’s keep going!</p><h2>RPC: The Good, the Bad, and the Ugly</h2><ul><li><p>Simple programming model: You can call remote functions just like local ones, which makes your code cleaner and simpler.</p></li><li><p>High performance: RPC uses compact, binary formats that computers can process quickly.</p></li><li><p>Strong typing: Because both sides share a strict contract, it often detects mistakes early before they cause problems.</p></li><li><p>Language interoperability: Client and server can use different programming languages and still communicate smoothly.</p></li></ul><ul><li><p>Tight coupling: If the server’s API changes, the client usually needs to update too, which can slow down development.</p></li><li><p>Less discoverable than REST: You can’t easily test or browse RPC APIs without the specific contract files.</p></li><li><p>Requires specialized tooling: You need special tools to generate the code for the client and server.</p></li><li><p>Abstraction hides network realities: Because remote calls look like local ones, developers might forget they’re dealing with the network and forget to handle timeouts or errors properly.</p></li></ul><p>Now let’s look at how RPC actually works in real-world systems and the challenges of running it at scale.</p><p>Using RPC in real systems comes with several practical challenges. Here are four common ones developers need to handle:</p><h4><strong>1. Service Discovery (How Do Services Find Each Other?)</strong></h4><p>In modern systems, servers are always changing; they’re added, removed, or restarted, often getting new IP addresses.</p><p>But you can’t just hardcode a server’s location in your code. So how does a client know where to send its request?</p><p>That’s where  comes in.</p><p>Think of it like a live, self-updating contact list. You don’t need to remember your friend’s exact home address. Instead, you simply look up their name in your contact list to find the latest address.</p><p>A central registry (like <a href=\"https://developer.hashicorp.com/consul\">Consul</a> or <a href=\"https://zookeeper.apache.org/\">Zookeeper</a>) keeps track of all healthy, running services and their locations. So when a client wants to talk to the Payment Service, it asks the registry, “Where can I find it?”.</p><p>The registry then returns a list of healthy servers, and the client picks one to send the RPC call.</p><h4><strong>2. API Evolution (How Do You Update Services Without Breaking Everything?)</strong></h4><p>When a service adds a new feature, it might need to change a function’s parameters. Things can break if clients do not update right away; this is because of tight coupling.</p><p>The solution is .</p><p>Think of a survey form you’ve sent out. You can safely add a new optional question; old forms still work. But if you delete a question or make a new one required, all old forms become invalid.</p><p>RPCframeworks like <a href=\"https://grpc.io/\">gRPC</a> follow strict rules for updating APIs. For example, you can safely add optional fields, but you shouldn’t remove or change existing ones. This allows new versions of a service to run smoothly while older clients keep working.</p><h4><strong>3. Streaming (More Than Just Request and Response)</strong></h4><p>RPC doesn’t limit itself to the “one request, one response” model.</p><p>Modern frameworks like gRPC support , where data flows continuously between client and server.</p><p>A regular RPC call is like sending a text message and getting one reply. A streaming RPC is like a live phone call or video stream; both sides can exchange information in real time.</p><ul><li><p>Server Streaming: Client sends one request and gets back a stream of responses (e.g., subscribing to live updates).</p></li><li><p>Client Streaming: Client sends a stream of messages and gets back one response (e.g., uploading a large file).</p></li><li><p>Bidirectional Streaming: Both client and server can send messages at any time (e.g., a real-time chat).</p></li></ul><h4><strong>4. Error Handling &amp; Status Codes (Communicating What Went Wrong)</strong></h4><p>When something fails, the client needs more detail than just an “it failed” message. So it can take the necessary steps.</p><p>That’s where  come in.</p><p>Just like HTTP status codes (404 Not Found, 403 Forbidden, 500 Internal Server Error), RPC frameworks have their own standardized codes:</p><ul></ul><p>By returning a specific error code, the server tells the client what to do next.</p><ul><li><p>Retry the call if the service was temporarily unavailable.</p></li><li><p>Report a user input error if the argument was invalid.</p></li><li><p>Or stop trying if it's impossible to fix the problem automatically.</p></li></ul><p>Now that we’ve seen RPC in real-world systems, let’s look at how it compares to other communication patterns.</p><h2>Choosing the Right Communication Pattern</h2><ul><li><p>RPC (<a href=\"https://grpc.io/\">gRPC</a>, <a href=\"https://thrift.apache.org/\">Thrift</a>): Best for high-performance internal microservices.</p></li><li><p>REST: Ideal for public APIs and resource-oriented operations.</p></li><li><p><a href=\"https://graphql.org/\">GraphQL</a>: Great for client-specific data fetching requirements.</p></li><li><p>Message Queues: Perfect for asynchronous, decoupled workflows.</p></li></ul><p>RPC hides most of the network details, but it doesn’t make them disappear; you still need to design for failures.</p><p>While gRPC with Protocol Buffers has become the new standard for fast, reliable communication between internal services.</p><p>Remember, a successful RPC setup depends on supporting systems. So service discovery, tracing, and monitoring are necessary to keep everything running smoothly.</p><p>👋 I’d like to thank  for writing this newsletter!</p><p>Plus, don’t forget to connect with him on:</p><p>He offers good deep dives into system design, AI, and software engineering.</p><p>Subscribe to get simplified case studies delivered straight to your inbox:</p><p><strong>Want to advertise in this newsletter? </strong>📰</p><p>Thank you for supporting this newsletter.</p><p>You are now 180,001+ readers strong, very close to 181k. Let’s try to get 181k readers by 31 October. Consider sharing this post with your friends and get rewards.</p><ul><li><p>Block diagrams created with <a href=\"https://app.eraser.io/auth/sign-up?ref=neo\">Eraser</a></p></li></ul>","contentLength":13493,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/6952f430-02f4-4dbb-8054-20b9dc33b9d0_1280x720.png","enclosureMime":"","commentsUrl":null},{"title":"Stacked Diffs - Simply Explained","url":"https://newsletter.systemdesign.one/p/stacked-diffs","date":1761301943,"author":"Neo Kim","guid":38,"unread":true,"content":"<p>Download my system design playbook for FREE on newsletter signup:</p><p><em>I’m partnering with <a href=\"https://graphite.dev/?utm_medium=influencer&amp;utm_campaign=Graphite-Agent-Launch&amp;utm_source=newsletter&amp;utm_content=ad&amp;ref=plug.dev\">Graphite</a> on this post. (I’ve wanted to write about stacked diffs anyway.)</em></p><ul><li><p><em><a href=\"https://newsletter.systemdesign.one/p/stacked-diffs/?action=share\">Share</a> &amp; I'll send you some rewards for the referrals.</em></p></li></ul><h2>What Is a Stacked Diff? (The Simple Answer)</h2><p>Stacked diffs are a type of code review workflow.</p><p>Imagine you’re building a house with LEGO. Instead of assembling the whole thing at once, you build it layer by layer.</p><p>First, you assemble the foundation. Then you add the walls, roof, and so on. Each part builds on the one before it. You can keep adding new LEGO layers while the lower ones are still being checked by someone.</p><p>Stacked diffs work the same way for code. Instead of making one big change all at once, you create a set of small changes, each built on the previous one. The reviewer can check each change on its own while you keep working on the rest.</p><p>This makes your work easier to review, understand, and improve.</p><h2>Let’s Start With the Pull Request Workflow</h2><p><strong>What’s a pull request workflow?</strong> Think of it as asking someone to review your code before merging into the main branch. This increases the code quality.</p><ol><li><p>Commit my code changes to that branch</p></li><li><p>Open a pull request for review</p></li><li><p>After approval, I merge onto the main branch</p></li></ol><p>See how each feature branch becomes a UNIT of code review? That’s pull request workflow!</p><p>Put simply, a pull request is a branch with many commits. And every pull request represents a feature.</p><h2>So Why Do We Need Stacked Diffs?</h2><p>Although the pull request workflow is simple, it slows you down:</p><ul><li><p>You wait for code reviews on big pull requests, which can stall progress.</p></li><li><p>Waste time by switching between branches for unrelated tasks. (Even if changes don’t conflict.)</p></li><li><p>Rebase endlessly on dependent pull requests when changes are being made during review.</p></li></ul><p>Plus, squashing big pull requests can lower the commit history quality with vague commits. Thus making code maintenance difficult.</p><p>Besides engineers take the shortest path possible.</p><p>With a pull request workflow, putting unrelated changes into a big pull request is easier. And this creates a low-quality commit history.</p><p>But with stacked diffs, by default, you make small changes. This makes reviews easier and code quality better.</p><h2>Real-World Example: New Feature</h2><p>With stacked diffs, the UNIT of review isn’t a branch, but a single commit.</p><p>Imagine you’re developing a new feature.</p><p>Instead of one big pull request with 1000 lines of code, you break it down into smaller diffs:</p><ul><li><p>Boilerplate code → 500 lines</p></li><li><p>Business logic → 350 lines</p></li></ul><ol><li><p>Add boilerplate code in the first diff.</p></li><li><p>Request a review for that diff.</p></li><li><p>Add business logic in the next diff.</p></li><li><p>Submit that diff too for review.</p></li><li><p>Then add edge cases as another diff.</p></li><li><p>Submit that diff as well for review.</p></li><li><p>Merge each diff independently as feedback comes in.</p></li></ol><p>Each diff builds on top of the previous one, but they can all be reviewed in parallel. Plus, you can assign different reviewers for each diff. So you don’t have to wait for one to merge before continuing to work on the next.</p><p>Yet each commit must build successfully and pass tests. Otherwise, the codebase becomes unstable.</p><h2>How Do “Stacked Diffs” Work?</h2><p>Think of stacked diffs like building a LEGO house layer by layer.</p><p>Each layer depends on the shape of the one below. So if you change a lower layer, you’ll need to adjust the ones above to fit. And the house is complete only after you assemble all layers in the correct order.</p><p>Stacked diffs work the same way for code.</p><p>Instead of one big pull request, you create a set of small changes that depend on each other. Here’s a simple comparison:</p><ul><li><p>Small pull requests (diff) → LEGO layers</p></li><li><p>Boilerplate code → foundation</p></li></ul><p>So if you edit an earlier diff to address feedback, you’ll need to rebase the latest diffs that depend on it. Otherwise, it’ll reference the old version of that commit and won’t apply cleanly on top of the new version. Put simply, there’ll be conflicts.</p><h2>How Do We Handle These Stacked Diffs?</h2><p>Stacked diffs improve engineers’ productivity. But it can be painful when diffs depend on each other.</p><ul><li><p>Imagine diff1 needs many changes after review.</p></li><li><p>Now, you must also update diff2 and diff3, which depend on diff1, to include those new changes.</p></li><li><p>To do this, you’ve got to use git rebase, which replays your commits so everything aligns correctly.</p></li><li><p>But if the new version of diff1 conflicts with diff2, then you’ll need to pick manually which parts of the code to keep.</p></li></ul><p>And this could turn into a time sink!</p><p>So we need an automation tool for this - . It makes the workflow easier by:</p><ul><li><p>Hiding Git complexity from engineers.</p></li><li><p>Tracking GitHub pull requests related to each diff.</p></li><li><p>Saving the order of diffs in the stack (which depends on what).</p></li><li><p>Automatically rebasing all dependent diffs if something changes on an earlier diff.</p></li><li><p>Inserting a diff between existing diffs. Or changing the order of diffs without manual work.</p></li></ul><p>Think of <a href=\"https://graphite.dev/?utm_medium=influencer&amp;utm_campaign=Graphite-Agent-Launch&amp;utm_source=newsletter&amp;utm_content=ad&amp;ref=plug.dev\">Graphite</a> like Git on steroids to work with stacked diffs. It removes the manual work of rebasing and merging and maximizes engineering productivity.</p><p>But even with Graphite handling stacks and rebases, reviews still take time. You’ve got to explain changes, address feedback, and update pull requests.</p><p>It’s a unified AI reviewer that explains what changed, suggests fixes, and updates your pull requests in real time (without switching between tools).</p><ul><li><p>Apply or generate fixes instantly</p></li><li><p>Merge confidently with full context</p></li><li><p>Ask questions about your code directly in the pull request</p></li></ul><p>Together, <a href=\"https://graphite.dev/?utm_medium=influencer&amp;utm_campaign=Graphite-Agent-Launch&amp;utm_source=newsletter&amp;utm_content=ad&amp;ref=plug.dev\">Graphite</a> and <a href=\"https://graphite.dev/features/agent?utm_source=newsletter&amp;utm_medium=influencer&amp;utm_campaign=Graphite-Agent-Launch&amp;utm_term=systemdesign&amp;utm_content=ad&amp;ref=plug.dev\">Graphite Agent</a> automate both sides of the workflow: the Git and review side. So you can write, review, and merge faster with less context switching.</p><h2>Stacked Diffs: Real Applications You Use Daily</h2><ul><li><p>Instead of making a big code change, break it into smaller diffs.</p></li><li><p>Each diff adds part of the functionality, making it easier to review and test.</p></li><li><p>You can keep working on the next part without waiting for the earlier parts to be merged.</p></li></ul><ul><li><p>Break large refactoring into logical chunks.</p></li><li><p>For example, create a diff to add a new interface. </p></li><li><p>Then use separate diffs to refactor different modules.</p></li><li><p>This keeps each diff focused and easy to test and review.</p></li></ul><ul><li><p>Some updates, such as renaming an API, touch many files.</p></li><li><p>And you can update each component in separate diffs.</p></li><li><p>This reduces the chance of mistakes during reviews.</p></li></ul><ul><li><p>You can put code changes and tests in separate diffs.</p></li><li><p>This keeps diffs smaller and makes reviews easier.</p></li></ul><ul><li><p>Imagine you discover a bug while a feature is still under review.</p></li><li><p>You don’t have to update existing changes; instead, create a new diff on top of the current stack to fix the bug.</p></li><li><p>This approach increases code review velocity and lets you stay unblocked.</p></li></ul><p>Also stacked diffs let you manage big pull requests with AI-GENERATED code by breaking them into smaller changes.</p><h2>Why Not Just Use Pull Request Workflow?</h2><p>It’s sometimes acceptable to open many pull requests and wait for code review.</p><p>Yet it doesn’t work well when pull requests depend on one another. You’re then blocked on the dependent pull request until someone reviews the first pull request.</p><p>Plus, the local repository has to look similar to the remote. It means more context switching and less productivity. So the pull request workflow is inefficient.</p><ul><li><p>Easy to write code. Easy to review. Easy to revert.</p></li><li><p>You can choose how to organize work for flexibility.</p></li><li><p>And protect your coding streaks for higher productivity.</p></li><li><p>Keep the Git history quality high and make debugging easier.</p></li></ul><p>Both workflows are productive. Yet stacked diffs make development faster by adapting to how the engineer actually works. It doesn’t force an inflexible workflow.</p><p>Remember, <em>context switching is the worst productivity killer for engineers</em>. Stacked diffs reduce it, while the pull request workflow amplifies it.</p><h2>Stacked Diffs With Graphite (Simplified)</h2><ol><li><p>Switch to the main branch and pull the latest changes.</p></li><li><p>Use <a href=\"https://graphite.dev/features#cli?utm_source=newsletter&amp;utm_medium=influencer&amp;utm_campaign=Graphite-Agent-Launch&amp;utm_term=systemdesign&amp;utm_content=ad&amp;ref=plug.dev\">Graphite’s CLI</a> to create a new branch for the first part (diff) of the feature.</p></li><li><p>Create new branches on top of the previous branches for each part of the feature.</p></li><li><p>Push each branch to the remote and open a pull request for that diff. (Graphite can even submit the entire stack of branches as separate pull requests with a single command.)</p></li><li><p>Update the diff after review. Then Graphite will AUTOMATICALLY rebase the remaining branches if you changed a lower diff.</p></li><li><p>Merge each diff’s pull request into the main branch once approved.</p></li><li><p>Graphite can even delete the merged feature branches and update your local main branch.</p></li></ol><p>Imagine <a href=\"https://graphite.dev/?utm_medium=influencer&amp;utm_campaign=Graphite-Agent-Launch&amp;utm_source=newsletter&amp;utm_content=ad&amp;ref=plug.dev\">Graphite</a> as a super smart partner who understands what adjustments to make for maximum productivity. And think of <a href=\"https://graphite.dev/features/agent?utm_source=newsletter&amp;utm_medium=influencer&amp;utm_campaign=Graphite-Agent-Launch&amp;utm_term=systemdesign&amp;utm_content=ad&amp;ref=plug.dev\">Graphite Agent</a> as a team member who reviews your code in real time, explaining and fixing as you go.</p><p>Subscribe to get simplified case studies delivered straight to your inbox:</p><p><strong>Want to advertise in this newsletter? </strong>📰</p><p>Thank you for supporting this newsletter.</p><p>You are now 180,001+ readers strong, very close to 181k. Let’s try to get 181k readers by 31 October. Consider sharing this post with your friends and get rewards.</p><ul><li><p>https://graphite.dev/guides</p></li></ul>","contentLength":8973,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/153a6e64-93dc-4a3e-b459-684a5248b175_1280x720.png","enclosureMime":"","commentsUrl":null},{"title":"How Meta Serverless Handles 11.5 Million Function Calls per Second","url":"https://newsletter.systemdesign.one/p/serverless-architecture","date":1760522566,"author":"Neo Kim","guid":37,"unread":true,"content":"<p>Download my system design playbook for FREE on newsletter signup:</p><p><em>This post outlines serverless architecture. You will find references at the bottom of this page if you want to go deeper.</em></p><p><em>I created the block diagrams using <a href=\"https://app.eraser.io/auth/sign-up?ref=neo\">Eraser</a>.</em></p><p>Once upon at Meta, engineers wrote infrastructure code whenever they launched new services.</p><p>And some services did a ton of concurrent processing during peak traffic. (Think of scheduling notifications or thumbnail creation.)</p><p>Yet a pre-allocated infrastructure doesn’t scale dynamically.</p><p>So they decided to set up a function as a service ().</p><p>A FaaS is the serverless way to scale with increasing load. </p><p>It lets engineers focus only on writing code and handles infrastructure deployment automatically. Thus improving the engineer’s productivity.</p><p>Imagine FaaS as a job queue, but managed by someone else.</p><p> is a framework for building real-time, multimodal AI that can see, hear, and respond in milliseconds.</p><p>Built for developers who want to bring true intelligence to live video without being locked into a single model or transport provider.</p><ul><li><p>Open Source: Fork it, read it, improve it.</p></li><li><p>Open Platform: Works with Stream Video or any WebRTC-based SDK.</p></li><li><p>Flexible Providers: Plug in OpenAI Realtime, Gemini Live, or your favorite STT/TTS and vision models.</p></li></ul><p>Fully open, extensible, and ready for your next AI project.</p><p>It’s possible to build a hyperscale FaaS by adding many servers and putting a load balancer in front.</p><p>But this approach creates new problems.</p><p>Restarting a server is better than keeping it idle for low costs.</p><p>Yet a load balancer keeps servers running even if idle.</p><ul><li><p>Idle functions use up memory if kept warm.</p></li><li><p>81% of functions get invoked at most once per minute.</p></li><li><p>And most functions run for less than a minute on average.</p></li></ul><p>So this approach would cause high costs and waste computing resources. While an under-provisioned server would make a slow system.</p><p>They run functions in separate <a href=\"https://tldp.org/LDP/tlk/kernel/processes.html\">Linux processes</a> for data isolation within the worker. An idle Linux process doesn’t consume many resources.</p><ul><li><p>144 servers idle for 10 minutes → 1 server day WASTED.</p></li><li><p>144 servers idle for 10 minutes → 1 extra machine → same THROUGHPUT.</p></li><li><p>At Meta’s scale, they’d need 33,400 extra servers to achieve the same THROUGHPUT.</p></li></ul><p>So it’s necessary to reduce server idle time. Otherwise, they’d lose money on hardware costs.</p><p>Servers crash. Also autoscaling adds new servers with varying traffic.</p><p>The creation of a new virtual machine to handle a request is called a .</p><p>Yet a cold start takes extra time before it can serve the first request because it’s necessary to:</p><ol><li><p>Start the virtual machine.</p></li><li><p>Download the container image and function code.</p></li><li><p>Initialize the container.</p></li><li><p>Initialize the language runtime and function code.</p></li><li><p>Do just-in-time () compilation.</p></li></ol><p>Also if the container gets shut down because a function isn’t used for a period, then it has a cold start problem again. Put simply, an early shutdown of the container causes latency because of re-initialization.</p><p>And this could overload the services that the function is calling. (Thus affecting system availability.)</p><p>Also there might be high variance in load. So it’s necessary to:</p><ul><li><p>Scale the serverless routing layer (<a href=\"https://systemdesign.one/what-is-service-discovery/\">discovery</a>).</p></li><li><p>Process the functions reliably without causing retry storms.</p></li></ul><p>Besides handling errors gracefully during function execution is important.</p><p>Building a serverless platform with high CPU utilization and low latency is hard.</p><p>So smart engineers at Meta used simple techniques to solve it.</p><p>Here’s how they built a hyperscale distributed operating system (XFaaS):</p><p>They run functions on a server called the .</p><p>And categorizes those workers into namespaces based on programming language. This ensures that each worker gets only compatible code and executes faster.</p><p>Also this technique gives extra security through physical isolation.</p><p>While a worker within a namespace can run any function written in that language to reduce cold start. This is called the .</p><p>Yet each server doesn’t run every function, but only a subset of them. This approach keeps the JIT-compiled code cache small and efficient.</p><p>Also they form within a namespace by grouping workers that run the  functions often. This approach reduces memory usage and improves caching through better JIT code reuse.</p><p>Repeated downloads increase latency and waste network bandwidth.</p><p>So they  the function code and container images onto the server’s <a href=\"https://en.wikipedia.org/wiki/Solid-state_drive\">SSDs</a>. This avoids repeated downloads and keeps an always-active runtime.</p><p>Besides they do <strong>cooperative JIT compilation</strong> for faster execution of functions. It means only specific workers compile new function code. Then those workers send the compiled code to ALL workers. Thus preventing redundant compilation overhead.</p><p>Ready for the next technique?</p><h3>2. Backend Service Protection</h3><p>There’s a risk of overloading the backend services that the function is calling during a load spike.</p><p>So they protect those services using -based throttling.</p><ul><li><p>They use a <a href=\"https://en.wikipedia.org/wiki/Additive_increase/multiplicative_decrease\">technique</a> similar to TCP congestion control for throttling.</p></li><li><p>Rate limiter throttles requests upon a backpressure signal from the backend service. For example, they track error, latency, and capacity.</p></li><li><p>And then slowly increases the requests to those backend services.</p></li><li><p>On congestion detection, it throttles requests again. </p></li></ul><p>Yet it throttles only the functions that call backend services for performance.</p><p>Besides it’s necessary to control the number of concurrent functions calling a backend service to prevent overload.</p><p>So they start with fewer concurrent function requests. This gives backend services enough time to warm up their cache and handle the load.</p><p>It’s necessary to prevent a function from using up the resources during peak traffic.</p><p>So they assign a limit to how many times a function can run or how many resources it can use. After a function reaches that limit, it’s throttled.</p><p>Also they distribute function execution across data centers to distribute the load evenly. And delay the low-priority functions until low-traffic hours. Put simply, they run specific functions only when there’s enough free server capacity.</p><p>Besides most services are stateless and replicated for scalability and fault tolerance. While stateful services are partitioned and replicated.</p>","contentLength":6163,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/ebbad081-9993-4288-8cc0-a5546c4899c2_1280x720.png","enclosureMime":"","commentsUrl":null},{"title":"System Design Interview Question: Design Spotify","url":"https://newsletter.systemdesign.one/p/spotify-system-design","date":1759744962,"author":"Neo Kim","guid":36,"unread":true,"content":"<p>Get my system design playbook for FREE on newsletter signup:</p><p><em>This post will help you prepare for the system design interview.</em></p><p>Building a music streaming platform like Spotify is a classic system design problem.</p><p>It includes audio delivery, metadata management, and everything in between. </p><p>Let’s figure out how to design it during a system design interview.</p><p>He’s a senior software engineer specializing in helping mid-level engineers break through their career plateaus and secure senior roles.</p><p>If you want to master the essential system design skills and land senior developer roles, I highly recommend checking out Hayk’s .</p><p>His approach focuses on what top employers actually care about: system design expertise, advanced project experience, and elite-level interview performance.</p><p>MCP servers are becoming critical components in AI architectures, but they are creating a fundamental new risk that traditional security controls weren’t designed to address.</p><p>Left unsecured, they’re a centralized point of failure for data governance.</p><p>This  will show you how to secure MCP servers properly, using externalized, fine-grained authorization. Inside the ebook, you will find:</p><ul><li><p>How MCP servers fit into your broader risk management and compliance framework</p></li><li><p>Why MCP servers break the traditional chain of identity in enterprise systems</p></li><li><p>How role-based access control fails in dynamic AI environments</p></li><li><p>Real incidents from Asana and Supabase that demonstrate these risks</p></li><li><p>The externalized authorization architecture (PEP/PDP) that enables Zero Trust for AI systems</p></li></ul><p>Get the practical blueprint to secure MCP servers before they become your biggest liability.</p><h2>Requirements &amp; Assumptions</h2><p>We’re looking at roughly 500k users listening to about 30 million songs. The core requirements include:</p><ul><li><p>Artists can upload their songs.</p></li><li><p>Users can search and play songs.</p></li><li><p>Users can create and manage playlists.</p></li><li><p>Users can maintain profiles.</p></li><li><p>Basic monitoring and observability (health checks, error tracking, performance metrics).</p></li></ul><p>For audio formats, we use Ogg and AAC files with different bitrates for adaptive streaming. For example,</p><ul><li><p>64kbps for mobile data saving, </p></li><li><p>128kbps for standard quality,</p></li><li><p>320kbps for premium users.</p></li></ul><p>On average, one song file at normal audio quality (standard bitrate) takes about 3MB of storage.</p><p>While the main constraints are fast playback start times, minimal rebuffering, and straightforward operations. We handle rebuffering using adaptive quality switching.</p><p>Let’s crunch some numbers to understand what we’re working with:</p><ul><li><ul><li><p>3MB × 30M songs ≈ 90TB of raw audio data. </p></li><li><p>This doesn’t include replicas across different regions. </p></li><li><p>It also doesn’t include versioning overhead when artists re-upload songs.</p></li><li><p>That’s why we’re looking at 2-3x this amount.</p></li></ul></li><li><ul><li><p>Each song needs a title, artist references, duration, file URLs, and so on. </p></li><li><p>At roughly 100 bytes per song × 30M songs ≈ 3GB. </p></li><li><p>That’s not so much compared to the audio.</p></li></ul></li><li><ul><li><p>User profiles, preferences, and playlist data are ~1KB × 500k users ≈ 0.5GB.</p></li></ul></li><li><ul><li><p>The average listen time is 3.5 minutes at 128-160kbps; that’s roughly 3-4MB per stream.</p></li><li><p>Let’s assume each user streams 10-15 songs daily.</p></li><li><p>This leads to significant egress costs.</p></li></ul></li></ul><p>The key insight is that audio dominates both storage costs and bandwidth. The metadata is just a small part in comparison.</p><h2>Spotify System Design: High Level Architecture</h2><p>The architecture breaks down into these key components:</p><p>The user-facing application handles UI, search, playback controls, and playlist management.</p><ul><li><p>It makes REST API calls to fetch metadata and manages local playback state.</p></li><li><p>Client streams audio directly from blob storage or CDN using signed URLs.</p></li><li><p>And cache recently played songs locally for faster playback on the same song replay.</p></li><li><p>When things go wrong, it uses retry logic for API calls.</p></li></ul><p>The client handles network interruptions gracefully by pausing playback until connectivity returns.</p><p>The load balancer spreads incoming requests across many API servers to prevent server overload.</p><ul><li><p>It could use round-robin or least-connections algorithms.</p></li><li><p>Also it performs health checks every 30 seconds and removes unhealthy instances from rotation.</p></li><li><p>This is essential for managing traffic spikes during album releases.</p></li></ul><p>Besides, it provides high availability against server failures and enables zero-downtime deployments.</p><p>The application servers are stateless; they handle business logic, authentication, and data access.</p><ul><li><p>They validate JWT tokens and query the database for song metadata.</p></li><li><p>They generate signed URLs for audio access and also save user actions for analytics.</p></li><li><p>For reliability, they implement circuit breakers for database connections. Circuit breakers stop requests to failing services, preventing cascading failures.</p></li><li><p>They use connection pooling to manage resources. This means reusing database connections instead of creating new ones for each request.</p></li></ul><p>They also provide fallback responses for non-critical features when dependencies are down.</p><p>Object storage systems like AWS S3 could hold all the audio files. </p><ul><li><p>Files are organized in a hierarchical structure like /artist/album/song.ogg</p></li><li><p>They’re accessed via signed URLs that expire after a few hours for security.</p></li><li><p>Blob storage offers virtually unlimited scalability. It also comes with built-in durability and cost-effectiveness for large files. They replicate the storage across many regions for durability. </p></li></ul><p>However, it has higher latency than local storage and potentially higher egress costs. We could instead use a distributed file system like Hadoop Distributed File System (). But blob storage is more managed and reliable for most use cases.</p><p>Now that we’ve covered the high-level architecture, let’s explore the request workflow:</p><p>Here’s what happens when the user :</p><ol><li><p>User hits play → App sends a GET request </p></li><li><p>API authentication → API server validates the JWT token.</p></li><li><p>Metadata lookup → API server queries SQL database for song details (metadata).</p></li><li><p>URL generation → API server creates a signed URL for blob storage access.</p></li><li><p>Audio streaming → App fetches chunks of audio (range requests) directly from blob storage using HTTP-based adaptive streaming (HLS or DASH), which allows smooth playback, automatic bitrate switching, and broad device compatibility.</p></li><li><p>Analytics → App periodically calls  to track plays and listening time.</p></li></ol><p>Here’s what happens when the :</p><ol><li><p>Artist uploads a song → The app sends it to the server using POST request with multipart form data.</p></li><li><p>File validation → API server checks its format, duration, and file size limits.</p></li><li><p>Blob storage → API server uploads file to <code>/pending/{upload_id}/song.ogg</code></p></li><li><p>Metadata processing → API server triggers a separate background service to read the uploaded file and extract details like duration, bitrate, and also generate preview images.</p></li><li><p>Database insert → API server then creates entries in these tables:</p><ul><li><p>: adds the new song record.</p></li><li><p>: adds the artist details if not already there.</p></li><li><p>: links the artist to the song.</p></li></ul></li><li><p>File promotion → The system moves the song from pending to its final, organized location <code>/artist/{id}/album/{id}/song.ogg</code></p></li><li><p>CDN invalidation → CDN stores cached copies of content (like artist pages, album metadata, song details) closer to users for fast delivery. It clears old cached data so users see the latest songs and artist updates right away.</p></li></ol><p>Our core endpoints would look something like:</p><ul><li><p>Search different content types with pagination</p><ul><li><p><code>GET /search?q={query}&amp;type=song,artist&amp;limit=20&amp;offset=0</code></p></li></ul></li><li><p>Get trending songs, optionally filtered by genre</p><ul><li><p><code>GET /songs/trending?genre={genre}&amp;limit=50</code></p></li></ul></li><li><p>Get all songs by a specific artist</p><ul><li><p><code>GET /artists/{id}/songs?limit=50</code></p></li></ul></li></ul><ul><li><p>Get song metadata and streaming URL</p></li><li><p>Direct streaming endpoint (alternative to signed URLs)</p></li><li><p>Get playlist details with an optional song list</p><ul><li><p><code>GET /playlists/{id}?include_songs=true</code></p></li></ul></li></ul><ul></ul><pre><code>{\n  “name”: “My Favorites”,\n  “is_public”: false\n}</code></pre><ul><li><p>Add songs to the playlist</p><ul><li><p><code>PUT /playlists/{id}/songs</code></p></li></ul></li></ul><pre><code>{\n  “song_ids”: [123, 456, 789],\n  “position”: 5\n}</code></pre><ul><li><p>Remove a song from the playlist</p><ul><li><p><code>DELETE /playlists/{id}/songs/{song_id}</code></p></li></ul></li></ul><ul><li><p>Get the current user’s playlists</p></li><li><p>Get songs liked by the user</p><ul><li><p><code>GET /users/me/liked-songs?limit=50&amp;offset=0</code></p></li></ul></li><li><ul><li><p><code>POST /users/me/follow/{artist_id}</code></p></li></ul></li></ul><p>Most endpoints return JSON with a consistent structure. The JSON includes metadata such as total_count, limit, offset for pagination, and proper HTTP status codes.</p><p>And the app sends the JWT token in the  header for authentication.</p><p>This is where we split responsibilities between two storage types:</p><p>The system uses blob storage to .</p><ul><li><p>The audio files are immutable; they rarely change once uploaded.</p></li><li><p>We’d organize them with a sensible folder structure</p><ul><li><p><code>/artist/{artistId}/album/{albumId}/{songId}.ogg</code></p></li></ul></li></ul><p>This makes it easy to manage uploads and serves as your source of truth for audio content.</p><p>The system uses an SQL database, such as <a href=\"https://www.postgresql.org/\">PostgreSQL</a>, to  with strong consistency. </p><ul><li><p>Users: stores information about each user.</p></li></ul><ul><li><p>Artists: stores information about each artist.</p></li></ul><ul><li><p>Songs: stores information about each song.</p></li></ul><ul><li><p>Playlists: stores information about playlists created by users.</p></li></ul><ul><li><p>PlaylistItems: stores the songs inside a playlist.</p></li></ul><ul><li><p>ArtistSongs: a junction table that stores relationships between songs and artists. For example, there could be many-to-many relationships with a song by many artists or many songs by a single artist.</p></li></ul><p>We chose SQL because we need JOINs for complex queries.</p><ul><li><p>For example, to find all songs by artists that a user follows.</p></li></ul><p>Also, you want referential integrity to prevent orphaned records. It means ensuring every record in one table points to a valid record in another table, so there are no orphan entries with missing links.</p><p>We can optimize common queries by adding database indexes on frequently accessed columns.</p><p>The trigram indexes on song titles and artist names enable fast search capabilities, handling typos and partial matches that users expect.</p><p>And the playlist items index speeds up queries when fetching songs for a specific playlist or finding which playlists contain a specific song.</p><p>Here’s the full user journey when they :</p><ol><li><p>App sends a  request to the API.</p></li><li><p>API queries the SQL database for song metadata.</p></li><li><p>The API returns metadata along with a signed URL pointing directly to blob storage or proxies the audio stream itself.</p></li><li><p>The client makes range requests to stream the audio in chunks.</p></li><li><p>App periodically sends progress information to the API for analytics.</p></li></ol><p>The signed URL approach is better because it takes the load off the API servers. But the proxy approach gives us more control over access patterns.</p><p>When we hit real scale, the numbers get much bigger than in the initial phase:</p><ul><li><p>We have <strong>200 million songs and 50 million users</strong></p></li><li><p>Song metadata: 100 bytes × 200M songs ≈ 20 GB</p></li><li><p>User metadata: 1KB × 50M users ≈ 50 GB</p></li><li><p>Audio storage: 3MB × 200M songs ≈ 600 TB (before replicas and regional copies)</p></li></ul><p>Now, we need to make some major architectural changes:</p><p>Popular songs get cached at edge locations worldwide to reduce latency for users. For example, a song cached in Germany serves much faster to European users than fetching it from the origin storage in the United States.</p><p>We could implement least recently used () eviction for cache management. LRU automatically removes the least accessed content when storage fills up. And the cache would fetch from the origin storage on a cache miss.</p><p>The CDN fetches content from blob storage using signed URLs, which allows it to serve the content securely.</p><p>The major advantage is a better user experience with fast load times and reduced capacity costs. You’re not serving every stream from your origin servers.</p><p>We need to introduce <strong>leader-follower replication</strong> to scale the database. Followers handle read traffic like searches and metadata lookups. While the leader handles writes like new uploads and playlist changes.</p><p>This approach works well because music streaming is heavily read-biased. Users search and browse far more than they upload or change playlists.</p><p>The major advantage is that you can scale read capacity by adding more followers. It also provides automatic failover if the leader goes down.</p><p>However, we might deal with replication lag. Followers could be slightly behind the leader. Also, there’s increased complexity from managing many database instances. We also face potential consistency issues where a user might not immediately see their own writes if routed to a follower.</p><p>When catalog growth or hot partitions become an issue, we’ll need to consider sharding strategies.  splits data across many database instances. For example, we could shard users by geography. US users go on one shard, EU users on another. Or we partition songs by artist ID ranges.</p><p>This gives us horizontal scalability and improves performance by keeping related data together. But the downside is that cross-shard queries become expensive or impossible. We lose some SQL features like foreign keys across shards. Besides, rebalancing data on growth becomes a complex operational challenge.</p><p>For a music platform, we’d start with geographic sharding. Most users primarily listen to artists from their region. However, you’d need a strategy for global artists whose songs are popular everywhere.</p><h4>API Layer Horizontal Scaling</h4><p>Morning commutes and evening listening sessions create predictable traffic spikes. So it’s necessary to set up many stateless API servers behind the load balancer to handle varying loads throughout the day.</p><p>These servers don’t maintain any user session state and use JWT tokens.</p><p>Adding capacity is straightforward.</p><p>We just spin up more instances, and the load balancer automatically includes them. This approach is cost-effective. Scale up during peak hours, then scale down overnight. It provides fault tolerance. If one server crashes, the others continue serving. It also enables rolling deployments without downtime.</p><p>The main tradeoff is that keeping servers truly stateless requires careful design. </p><p>You can’t store anything locally between requests. This means more database calls and potential performance issues.</p><p>Let’s also introduce monitoring to track system health and find performance issues.</p><ul><li><p>Health checks across all services with automatic traffic routing away from unhealthy instances.</p></li><li><p>Retries with exponential backoff for transient failures, especially for audio streaming, where network hiccups are common.</p></li></ul><p>Here are some key metrics to watch:</p><ul><li><p>Time to first byte for audio streams</p></li><li><p>Error rates across all endpoints</p></li><li><p>CDN cache hit ratios (should be 85% + for popular content)</p></li></ul><p>Besides, implement a simple fallback strategy:</p><ul><li><p>If the CDN misses, then fetch from origin storage and populate the cache for next time.</p></li><li><p>If database replicas are down, fall back to the leader with circuit breaker protection.</p></li></ul><p>Start with a simple design and scale incrementally. We don’t over-engineer for problems we don’t have yet, but the architecture naturally accommodates growth when it’s needed.</p><p>👋 I’d like to thank <a href=\"https://linkedin.com/in/hayksimonyan\">Hayk</a> for writing this newsletter!</p><p>It’ll help you master the essential system design skills, not just in theory but in practice, and land senior developer roles.</p><p>Subscribe to get simplified case studies delivered straight to your inbox:</p><p><strong>Want to advertise in this newsletter? </strong>📰</p><p>Thank you for supporting this newsletter.</p><p>You are now 175,001+ readers strong, very close to 176k. Let’s try to get 176k readers by 10 October. Consider sharing this post with your friends and get rewards.</p><ul><li><p>Block diagrams created with <a href=\"https://app.eraser.io/auth/sign-up?ref=neo\">Eraser</a></p></li></ul>","contentLength":15283,"flags":null,"enclosureUrl":"https://substack-post-media.s3.amazonaws.com/public/images/ba330676-2588-4bb1-80bf-59b9b29480dd_1280x720.png","enclosureMime":"","commentsUrl":null}],"tags":["dev"]}