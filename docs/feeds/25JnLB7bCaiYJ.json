{"id":"25JnLB7bCaiYJ","title":"Tech News","displayTitle":"Tech News","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":85,"items":[{"title":"UK competition probe of mobile browsers finds Apple-Google duopoly is ‚Äòanti-innovation‚Äô","url":"https://techcrunch.com/2025/03/12/uk-competition-probe-of-mobile-browsers-finds-apple-google-duopoly-is-anti-innovation/","date":1741802381,"author":"Natasha Lomas","guid":179,"unread":true,"content":"<p>A U.K. competition authority investigation of Apple and Google‚Äôs mobile browsers has concluded that the mobile duopoly‚Äôs policies are ‚Äúholding back innovation‚Äù and could also be limiting economic growth. ‚ÄúMobile browsers are apps which provide the primary gateway for consumers to access the web on their mobile devices, and hence for businesses to reach them [‚Ä¶]</p><p>¬© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":438,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Donald Trump Jr. has been boosting MAGA-related startups","url":"https://techcrunch.com/2025/03/12/donald-trump-jr-has-been-boosting-maga-related-startups/","date":1741801061,"author":"Dominic-Madori Davis","guid":178,"unread":true,"content":"<p>Since Donald Trump Jr. joined VC firm 1789 Capital after his father won the election, he‚Äôs been busy monetizing the Make America Great Again (MAGA) ecosystem. He‚Äôs been making bets in media, pharmaceutical, guns, and crypto while pushing against environmental, social, and governance (ESG) and diversity, equity, and inclusion (DEI), the Financial Times reported. The [‚Ä¶]</p><p>¬© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":440,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FTC Asks To Delay Amazon Prime Deceptive Practices Case, Citing Staffing Shortfalls","url":"https://tech.slashdot.org/story/25/03/12/176223/ftc-asks-to-delay-amazon-prime-deceptive-practices-case-citing-staffing-shortfalls?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1741800600,"author":"msmash","guid":265,"unread":true,"content":"The Federal Trade Commission asked a judge in Seattle to delay the start of its trial accusing Amazon of duping consumers into signing up for its Prime program, citing resource constraints. CNBC: Attorneys for the FTC made the request during a status hearing on Wednesday before Judge John Chun in the U.S. District Court for the Western District of Washington. Chun had set a Sept. 22 start date for the trial. Jonathan Cohen, an attorney for the FTC, asked Chun for a two-month continuance on the case due to staffing and budgetary shortfalls. \n\nThe FTC's request to delay due to staffing constraints comes amid a push by the Trump administration's Department of Government Efficiency to reduce spending. DOGE, which is led by tech baron Elon Musk, has slashed the federal government's workforce by more than 62,000 workers in February alone. \"We have lost employees in the agency, in our division and on our case team,\" Cohen said.","contentLength":934,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OpenStack comes to the Linux Foundation","url":"https://techcrunch.com/2025/03/12/openstack-comes-to-the-linux-foundation/","date":1741799589,"author":"Frederic Lardinois","guid":177,"unread":true,"content":"<p>Back in 2010, Rackspace and NASA launched a project called OpenStack, which was meant to become an open-source option for running an AWS-style cloud inside of private data centers. The two companies then moved OpenStack to the OpenStack Foundation, which has steadfastly shepherded the project through its many ups and downs. Right now, with the [‚Ä¶]</p><p>¬© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":414,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"US Schools Deploy AI Surveillance Amid Security Lapses, Privacy Concerns","url":"https://news.slashdot.org/story/25/03/12/1654217/us-schools-deploy-ai-surveillance-amid-security-lapses-privacy-concerns?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1741798440,"author":"msmash","guid":264,"unread":true,"content":"Schools across the United States are increasingly using artificial intelligence to monitor students' online activities, raising significant privacy concerns after Vancouver Public Schools inadvertently released nearly 3,500 unredacted, sensitive student documents to reporters. \n\nThe surveillance software, developed by companies like Gaggle Safety Management, scans school-issued devices 24/7 for signs of bullying, self-harm, or violence, alerting staff when potential issues are detected. Approximately 1,500 school districts nationwide use Gaggle's technology to track six million students, with Vancouver schools paying $328,036 for three years of service. \n\nWhile school officials maintain the technology has helped counselors intervene with at-risk students, documents revealed LGBTQ+ students were potentially outed to administrators through the monitoring.","contentLength":865,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Dapr‚Äôs microservices runtime now supports AI agents","url":"https://techcrunch.com/2025/03/12/daprs-microservices-runtime-now-supports-ai-agents/","date":1741796789,"author":"Frederic Lardinois","guid":176,"unread":true,"content":"<p>Back in 2019, Microsoft open-sourced Dapr, a new runtime for making building distributed microservice-based applications easier. At the time, nobody was talking about AI agents yet, but as it turns out, Dapr had some of the fundamental building blocks for supporting AI agents built-in from the outset. That‚Äôs because one of Dapr‚Äôs core features is [‚Ä¶]</p><p>¬© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":421,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How I Vibe Coded the Pixel Icon Library Website Without Learning to Code (Thanks, Cursor AI!)","url":"https://hackernoon.com/how-i-vibe-coded-the-pixel-icon-library-website-without-learning-to-code-thanks-cursor-ai?source=rss","date":1741795203,"author":"Devansh Chaudhary","guid":310,"unread":true,"content":"<blockquote><p>As a designer, I've always been fascinated by the intersection of design and tech. The gap between designer and developer has always intrigued me‚ÄîI just never expected to cross it this soon.</p></blockquote><p>The <a href=\"https://pixeliconlibrary.com/\">Pixel Icon Library</a> by HackerNoon started as a fun design project to create pixelated icons that captured the nostalgic essence of HackerNoon‚Äôs design language‚Äîbut transforming a Figma Design file into a fully functional website myself? That was not on my card for 2025 until I discovered Cursor AI. Having the library open-sourced via GitHub, xFigma &amp; NPM, we always wanted to make these icons more accessible to the community. The idea was simple: let‚Äôs build a website where designers and developers could browse, search, and download these icons for their projects.</p><p>\\\nThe challenge? I had literally zero experience with coding.üíÄ</p><p>\\\nEnter Cursor AI, an AI-powered code editor that quickly became my mentor and guide throughout this journey. Here's how I pushed past my limits from design to full-stack with AI assistance and some serious <a href=\"https://hackernoon.com/it-takes-more-than-thinking-humans-put-the-vibe-into-vibe-coding\">vibe coding</a>.</p><h2>Starting from Ground Zero</h2><p>The first step was to establish what I wanted to build and take stock of what I already had:</p><ul><li>A library of pixel icons in SVG format.</li><li>A Figma design for the website.</li><li>A list of features I wanted on the website.</li><li>Absolutely no idea how to bring it to life.</li></ul><h2>Setting Up the Project with Cursor AI</h2><p>\\\nAfter installing Cursor, the next thing was to check for:</p><ul><li> - for the NPM package</li></ul><p>\\\nWith the essentials installed, it was time to get my hands dirty. I cloned the repo to my system, opened the project folder in Cursor AI, and initiated a conversation with the built-in AI assistant.</p><blockquote><p>Having had a solid experience with Claude, I opted for Claude 3.7 Sonnet in Agent Mode to guide me through the process.</p></blockquote><p>\\\nWith the first prompt, I briefly explained what I wanted to build and listed the required features along with a basic structure of the project, asking for suggestions on which framework(s) to prioritize efficiency and speed. Cursor helped with the necessary folder structure and suggested I stick to HTML &amp; Tailwind for the project. It even created the files required for starting the project. Here‚Äôs how the folder structure looked after this:</p><p>The next step was setting up Tailwind &amp; starting with the UI.</p><h2>Setting Up Tailwind CSS &amp; Building The UI</h2><p>To my surprise, Cursor messed up the Tailwind installation and mixed up commands from Tailwind v3.4 &amp; v4.0. So, it was time for me to step up! I went over to <a href=\"https://tailwindcss.com/docs/installation/tailwind-cli\">Tailwindcss Installation Docs</a> and followed these steps:</p><ul><li><p>To install Tailwind, open the terminal &amp; run:</p><p><code>npm install tailwindcss @tailwindcss/cli</code></p></li></ul><ul><li><p>Import Tailwind in src/style.css file:</p></li></ul><ul><li><p>To set up the Build Process, run:</p><p><code>npx @tailwindcss/cli -i ./src/style.css -o ./src/output.css --watch</code></p></li></ul><ul><li><p>Start using Tailwind in the HTML:</p><p><code>&lt;link href=\"/src/output.css\" rel=\"stylesheet\"&gt;</code></p></li></ul><p>\\\nOnce Tailwind was installed and the classes were ready to be used, I started defining colors, fonts, and other atoms for the AI agent before building out more complex molecules, organisms, and pages as per my Figma Design.</p><p>\\\nThen, I installed the Pixel Icon Library NPM Package to use the icons in the project. Here are the steps for installing the NPM Package:</p><ul><li><p><code>npm i @hackernoon/pixel-icon-library</code></p></li></ul><ul><li><p>Import the CSS in your HTML</p><p>(I moved all the necessary icon font files to the /fonts folder for ease of access.)</p><p><code>&lt;link href=\"/fonts/iconfont.css\" rel=\"stylesheet\"&gt;</code></p></li></ul><ul><li><p><code>&lt;i class=\"hn hn-icon-name\"&gt;&lt;/i&gt;</code></p></li></ul><p>\\\nFor the UI, I took an element-by-element approach to keep things organized and make it easy to revert to the previous iteration if needed.</p><ul></ul><p>\\\nI made sure to define paddings, margins, border radius, colors, and dimensions for all these elements while adding hover &amp; click states as well. In addition to CSS properties, I included responsive behavior &amp; interactions in the prompts as well.</p><p>:::tip\nTo generate a preview for all the code I was approving, I used the <a href=\"https://github.com/ritwickdey/vscode-live-server\">Live Server</a> extension. This extension launches a local development server with a live reload feature for static &amp; dynamic pages in one click!</p><h2>Icon Data Challenge &amp; Implementing Search Functionality</h2><p>With the UI Elements in place, it was time for the real challenge: displaying all icons with their details - Icon Name, Icon Type Tag, and SVG Code while ensuring smooth search functionality. To overcome this, Cursor suggested a structured approach:</p><ul><li>Creating a JSON file with icon metadata and SVG code</li><li>Loading the data from the JSON to display it efficiently.</li><li>Implement search based on the icon name.</li><li>Add search filters based on the icon type tags - solid, regular, brands/social-media-icons, purrcats</li></ul><p>\\\nTo further automate this process, I asked Cursor to create a script to add all the icon data to the /data/icons.json file.</p><p>\\\nNow, with the search working, search filters in place, and icon modal working as intended, all that was left to do was add remaining interactions to the UI, thorough testing, and deployment!</p><p>Once I was happy with the UI and thoroughly tested every functionality, it was time for the next challenge - Deployment!!!</p><p>\\\nSince the project was a static site, I needed a hosting solution that was fast, free, and easy to maintain. GitHub Pages was a no-brainer! It offered:</p><ul><li>Seamless integration with the GitHub repo, making deployment effortless.</li><li>It‚Äôs free to use &amp; best suited for static projects like this.</li><li>Updates are super easy - all you need is a commit!</li></ul><p>\\\nBut before I could push all my code to the repository and deploy via GitHub Pages, I needed to clean up the code and check for any production issues. After a quick back and forth with Cursor, and a few tweaks, it was all ready for launch!</p><p>\\\n<strong>Here‚Äôs all you need to do to deploy your project via GitHub Pages:</strong></p><ul><li>Push all your code to the GitHub repo &amp; make sure it‚Äôs public</li><li>Enabling GitHub pages for the repo</li><li>Set the  to ‚Äú‚Äù (The branch where your code is. In my case, it was in website branch)</li><li>Add your custom domain (Like I used: pixeliconlibrary.com)</li><li>Configure DNS (Thanks to <a href=\"https://hackernoon.com/u/richardjohnn\">Richard</a> for helping me out with this)</li><li>Wait for a few minutes &amp; your website will be LIVE!</li></ul><p>From designing pixel-art icons to vibe-coding my way into a fully functional site, this project pushed me beyond my comfort zone in the best way possible. Looking back, it was more than just building a website - it was about widening my creative horizons &amp; realizing that with AI, the line between design and development is blurring faster than ever. <em>And to me, it's like an endless road has opened up in front of my eyes.</em></p><p>:::tip\nOne designer to another: If I can do it, so can you. So, what are you waiting for? Let‚Äôs get building!</p><p><em>Wanna take a peek at the code behind the website? Check out </em></p>","contentLength":6608,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Sakana claims its AI-generated paper passed peer review ‚Äî but it‚Äôs a bit more nuanced than that","url":"https://techcrunch.com/2025/03/12/sakana-claims-its-ai-paper-passed-peer-review-but-its-a-bit-more-nuanced-than-that/","date":1741795202,"author":"Kyle Wiggers","guid":175,"unread":true,"content":"<p>Japanese AI startup Sakana said that its AI generated one of the first peer-reviewed scientific publications. But while the claim isn‚Äôt necessarily untrue, there are caveats to note. The debate swirling around AI and its role in the scientific process grows fiercer by the day. Many researchers don‚Äôt think AI is quite ready to serve [‚Ä¶]</p><p>¬© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":406,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Snap introduces AI Video Lenses powered by its in-house generative model","url":"https://techcrunch.com/2025/03/12/snap-introduces-ai-video-lenses-powered-by-its-in-house-generative-model/","date":1741795200,"author":"Aisha Malik","guid":173,"unread":true,"content":"<p>Snapchat is introducing its first ever video generative AI Lenses, the company told TechCrunch exclusively. The Lenses are powered by Snap‚Äôs in-house built generative video model. The three new AI Video Lenses&nbsp;are available to users on the app‚Äôs premium subscription tier, Snapchat Platinum, which costs $15.99 per month. The launch comes as Snap unveiled an [‚Ä¶]</p><p>¬© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":432,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Moonvalley releases a video generator it claims was trained on licensed content","url":"https://techcrunch.com/2025/03/12/moonvalley-releases-a-video-generator-it-claims-was-trained-on-licensed-content/","date":1741795200,"author":"Kyle Wiggers","guid":174,"unread":true,"content":"<p>Los Angeles-based startup Moonvalley has launched an AI video-generating model it claims is one of the few trained on openly licensed ‚Äî not copyrighted ‚Äî data. Named ‚ÄúMarey‚Äù after cinema trailblazer √âtienne-Jules Marey, the model was built in collaboration with Asteria, a newer AI animation studio. Marey was trained on ‚Äúowned or fully licensed‚Äù source [‚Ä¶]</p><p>¬© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":435,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Garantex administrator arrested in India under extradition law","url":"https://techcrunch.com/2025/03/12/garantex-administrator-arrested-in-india-under-extradition-law/","date":1741794929,"author":"Jagmeet Singh, Lorenzo Franceschi-Bicchierai","guid":172,"unread":true,"content":"<p>Garantex co-founder Aleksej Besciokov was arrested in India's Kerala on Tuesday under the country's extradition law.</p><p>¬© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":179,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ubuntu 25.10 Looks To Make Use Of Rust Coreutils & Other Rust System Components","url":"https://www.phoronix.com/news/Ubuntu-25.10-Rust-Coreutils","date":1741794840,"author":"Michael Larabel","guid":436,"unread":true,"content":"<article>Plans have been drafted to begin using more Rust-rewritten Linux system components within the Ubuntu 25.10 release due out later this year and ahead of next year's all important Ubuntu 26.04 LTS release. Among the Rust components being planned for use in Ubuntu 25.10 is the Rust Coreutils \"uutils\" software...</article>","contentLength":310,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AMD's 3D V-Cache Optimizer Driver For Squeezing More Ryzen 9 9950X3D Performance","url":"https://www.phoronix.com/review/amd-3d-vcache-optimizer-9950x3d","date":1741793400,"author":"Michael Larabel","guid":435,"unread":true,"content":"<article>Merged for the Linux 6.13 kernel was the AMD 3D V-Cache Optimizer driver for being able to influence the kernel's scheduling decisions on AMD processors where only a subset of CCDs have the larger 3D V-Cache. With this new driver users can communicate their cache vs. frequency preference for influencing where new tasks are first placed if on the CCD with the larger L3 cache or with the higher frequency potential. Here is a look at the impact of using the AMD 3D V-Cache Optimizer driver with the new AMD Ryzen 9 9950X3D.</article>","contentLength":524,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Roomba-maker iRobot Warns of Possible Shutdown Within 12 Months","url":"https://hardware.slashdot.org/story/25/03/12/1437241/roomba-maker-irobot-warns-of-possible-shutdown-within-12-months?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1741792800,"author":"msmash","guid":262,"unread":true,"content":"Roomba maker iRobot has warned it may cease operations within 12 months unless it can refinance debt or find a buyer, just one day after launching a new vacuum cleaner line. In its March 12 quarterly report, the company disclosed it had spent $3.6 million to amend terms on a $200 million Carlyle Group loan from 2023, as U.S. revenue plunged 47% in the fourth quarter. \n\n\"Given these uncertainties and the implication they may have on the Company's financials, there is substantial doubt about the Company's ability to continue as a going concern for a period of at least 12 months from the date of the issuance of its consolidated 2024 financial statements,\" the company wrote. \n\nThe robot vacuum pioneer has initiated a formal strategic review after a failed Amazon acquisition, the departure of founder Colin Angle, and layoffs affecting over half its workforce. iRobot cited mounting competition from Chinese manufacturers and expects continued losses for \"the foreseeable future.\"","contentLength":986,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"With Gemini Robotics, Google Aims for Smarter Robots","url":"https://spectrum.ieee.org/gemini-robotics","date":1741791744,"author":"Eliza Strickland","guid":159,"unread":true,"content":"<p>DeepMind launches two new foundation models to help robots reason </p>","contentLength":66,"flags":null,"enclosureUrl":"https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NzAyNTM1Ni9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc3MDc5MzU0NH0.qUSCBPf2eO8a4aagmxU9QqBUARy6BdaMRq9xw3qPOQg/image.jpg?width=600","enclosureMime":"","commentsUrl":null},{"title":"Why Onyx thinks its open source solution will win enterprise search","url":"https://techcrunch.com/2025/03/12/why-onyx-thinks-its-open-source-solution-will-win-enterprise-search/","date":1741791600,"author":"Rebecca Szkutak","guid":171,"unread":true,"content":"<p>Enterprises have troves of internal data and information that employees need to complete their tasks or answer questions for potential customers. But that doesn‚Äôt mean the right information is easy to find. Onyx wants to solve that problem through its internal enterprise search tool. There are other big names in the category, like Glean ‚Äî [‚Ä¶]</p><p>¬© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":413,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Morgan Stanley Cuts iPhone Shipment Forecast on Siri Upgrade Delay, China Tariffs","url":"https://apple.slashdot.org/story/25/03/12/1416206/morgan-stanley-cuts-iphone-shipment-forecast-on-siri-upgrade-delay-china-tariffs?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1741790400,"author":"msmash","guid":261,"unread":true,"content":"Morgan Stanley has reduced its iPhone shipment forecasts after Apple confirmed the delay of a more advanced Siri personal assistant, dampening prospects for accelerating phone upgrades. The investment bank now predicts 230 million iPhone shipments in 2025 (flat year-over-year) and 243 million in 2026 (up 6%), down from previous estimates. \n\nAn upgraded Siri was the most sought-after Apple Intelligence feature among prospective buyers, according to the bank's survey data. \"Access to Advanced AI Features\" appeared as a top-five driver of smartphone upgrades for the first time, with about 50% of iPhone owners who didn't upgrade to iPhone 16 citing the delayed Apple Intelligence rollout as affecting their decision. The firm also incorporated headwinds from China tariffs in its assessment, noting Apple is unlikely to fully offset these costs without broader exemptions.","contentLength":876,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Lithium-ion batteries are remaking Google‚Äôs data centers","url":"https://techcrunch.com/2025/03/12/lithium-ion-batteries-are-remaking-googles-data-centers/","date":1741790395,"author":"Tim De Chant","guid":170,"unread":true,"content":"<p>Google is replacing lead-acid battery backup units with lithium-ion cells, freeing up space for more servers.</p><p>¬© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":172,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Project Europe, a new early-stage fund, will back people under 25 to build the next tech titan","url":"https://techcrunch.com/2025/03/12/project-europe-a-new-early-stage-fund-will-back-under-25s-with-200k-to-build-the-next-tech-titan/","date":1741790178,"author":"Ingrid Lunden","guid":169,"unread":true,"content":"<p>A recurring theme in Europe‚Äôs tech world is that the region needs its own Google or Microsoft. Now comes the launch of a new fund to support this initiative. Project Europe ‚Äî a new fund for founders ‚Äúsolving hard problems with technical solutions‚Äù ‚Äî says it has initially pulled together $10 million from 128 different [‚Ä¶]</p><p>¬© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":397,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Battery manufacturer Northvolt nears the end as it files for bankruptcy in Sweden","url":"https://techcrunch.com/2025/03/12/battery-manufacturer-northvolt-nears-the-end-as-it-files-for-bankruptcy-in-sweden/","date":1741790047,"author":"Tim De Chant","guid":168,"unread":true,"content":"<p>Though the Swedish startup has raised over $14 billion, it has been running short on cash recently.</p><p>¬© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":162,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OpenSSL 3.5 Alpha 1 Released With Server-Side QUIC","url":"https://www.phoronix.com/news/OpenSSL-3.5-Alpha-1","date":1741788266,"author":"Michael Larabel","guid":434,"unread":true,"content":"<article>OpenSSL 3.5 Alpha 1 is out today as the first development milestone on the path to releasing OpenSSL 3.5.0 in April...</article>","contentLength":118,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How La Fourche, an online organic supermarket, is thriving after q-commerce‚Äôs bust","url":"https://techcrunch.com/2025/03/12/how-la-fourche-an-online-organic-supermarket-is-thriving-after-q-commerces-bust/","date":1741788083,"author":"Romain Dillet","guid":167,"unread":true,"content":"<p>La Fourche is just seven years old but it has been quite a rollercoaster for the French startup. During this time, the online grocery retailer has gone through a global pandemic, followed by the rise of venture-backed quick-commerce startups that promised grocery deliveries in less than 15 minutes, followed by the implosion of that vertical. [‚Ä¶]</p><p>¬© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":412,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Amazon, Google and Meta Support Tripling Nuclear Power By 2050","url":"https://hardware.slashdot.org/story/25/03/12/1350256/amazon-google-and-meta-support-tripling-nuclear-power-by-2050?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1741788000,"author":"msmash","guid":260,"unread":true,"content":"Amazon, Alphabet's Google and Meta Platforms on Wednesday said they support efforts to at least triple nuclear energy worldwide by 2050. From a report: The tech companies signed a pledge first adopted in December 2023 by more than 20 countries, including the U.S., at the U.N. Climate Change Conference. Financial institutions including Bank of America, Goldman Sachs and Morgan Stanley backed the pledge last year. \n\nThe pledge is nonbinding, but highlights the growing support for expanding nuclear power among leading industries, finance and governments. Amazon, Google and Meta are increasingly important drivers of energy demand in the U.S. as they build out AI centers. The tech sector is turning to nuclear power after concluding that renewables alone won't provide enough reliable power for their energy needs. Microsoft and Apple did not sign the statement.","contentLength":866,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AMD Ryzen 9 9900X3D Linux Benchmarks Forthcoming","url":"https://www.phoronix.com/news/AMD-Ryzen-9-9900X3D-Linux","date":1741787428,"author":"Michael Larabel","guid":433,"unread":true,"content":"<article>Today marks the retail availability of the AMD Ryzen 9 9900X3D and Ryzen 9 9950X3D processors. At the top of the hour when the new AMD Zen 5 3D V-Cache processors went on sale, I found both the 9900X3D and 9950X3D in-stock and at MSRP pricing... Less than a half hour later, the 9950X3D is now out of stock while as of writing the 9900X3D remains in-stock at major Internet retailers at its $599 price point...</article>","contentLength":410,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pentera nabs $60M at a $1B+ valuation to build simulated network attacks to train security teams","url":"https://techcrunch.com/2025/03/12/pentera-nabs-60m-at-a-1b-valuation-to-build-simulated-network-attacks-to-train-security-teams/","date":1741784423,"author":"Ingrid Lunden","guid":166,"unread":true,"content":"<p>Strong and smart security operations teams are at the heart of any cybersecurity strategy, and today a startup that builds tooling to help keep them on their toes is announcing some funding on the back of a lot of growth. Pentera ‚Äî which has built a system that launches simulations of network attacks to stress [‚Ä¶]</p><p>¬© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":382,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"New Brain Tech Gives Voice to ALS Patients","url":"https://spectrum.ieee.org/als","date":1741784402,"author":"Greg Uyeno","guid":158,"unread":true,"content":"<p>Cognixion‚Äôs headset offers a communication tool for people with locked-in syndrome</p>","contentLength":84,"flags":null,"enclosureUrl":"https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NjcwNzkxNS9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc4NTM4MjYzN30.gmAWwtt2dlg0mZD6jMC0v8N2_D0y_2xo_v7IjK1wTwY/image.jpg?width=600","enclosureMime":"","commentsUrl":null},{"title":"Allstate Insurance Sued For Delivering Personal Info In Plaintext","url":"https://yro.slashdot.org/story/25/03/11/225252/allstate-insurance-sued-for-delivering-personal-info-in-plaintext?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1741784400,"author":"BeauHD","guid":259,"unread":true,"content":"An anonymous reader quotes a report from The Register: New York State has sued Allstate Insurance for operating websites so badly designed they would deliver personal information in plain-text to anyone that went looking for it. The data was lifted from Allstate's National General business unit, which ran a website for consumers who wanted to get a quote for a policy. That task required users to input a name and address, and once that info was entered, the site searched a LexisNexis Risk Solutions database for data on anyone who lived at the address provided. The results of that search would then appear on a screen that included the driver's license number (DLN) for the given name and address, plus \"names of any other drivers identified as potentially living at that consumer's address, and the entire DLNs of those other drivers.\"\n \nNaturally, miscreants used the system to mine for people's personal information for fraud. \"National General intentionally built these tools to automatically populate consumers' entire DLNs in plain text -- in other words, fully exposed on the face of the quoting websites -- during the quoting process,\" the court documents [PDF] state. \"Not surprisingly, attackers identified this vulnerability and targeted these quoting tools as an easy way to access the DLNs of many New Yorkers,\" according to the lawsuit. The digital thieves then used this information to \"submit fraudulent claims for pandemic and unemployment benefits,\" we're told. ... [B]y the time the insurer resolved the mess, crooks had built bots that harvested at least 12,000 individuals' driver's license numbers from the quote-generating site.","contentLength":1656,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Wolf Games, backed by ‚ÄòLaw & Order‚Äô creator, uses AI to create murder mystery games","url":"https://techcrunch.com/2025/03/12/wolf-games-backed-by-law-order-creator-uses-ai-to-create-murder-mystery-games/","date":1741784400,"author":"Lauren Forristal","guid":165,"unread":true,"content":"<p>Elliot Wolf, the executive producer and son of ‚ÄúLaw &amp; Order‚Äù creator Dick Wolf, is entering a new venture aimed at engaging true crime fans.&nbsp; He, along with co-founders Andrew Adashek (CEO) and Noah Rosenberg (CTO), are developing Wolf Games, a new startup that leverages AI to generate daily murder mystery games. The company also [‚Ä¶]</p><p>¬© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":405,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linux 6.15 Set To Include Better Handling For Intel P Or E Core Only Mitigations","url":"https://www.phoronix.com/news/Linux-6.15-P-E-Core-Mitigation","date":1741783612,"author":"Michael Larabel","guid":432,"unread":true,"content":"<article>A set of patches from Intel for utilizing the CPU type for CPU matching as part of the x86 mitigation handling is likely to be part of the upcoming Linux 6.15 kernel. These patches are intended for helping with CPU security mitigations on Intel Core hybrid processors where there are security vulnerabilities affecting only P cores or only E cores but not both sets of CPU cores present in the system...</article>","contentLength":403,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Meta faces publisher copyright AI lawsuit in France","url":"https://techcrunch.com/2025/03/12/meta-faces-publisher-copyright-ai-lawsuit-in-france/","date":1741781461,"author":"Natasha Lomas","guid":164,"unread":true,"content":"<p>Meta is facing an AI copyright lawsuit in France that‚Äôs been brought by authors and publishers who are accusing it of economic ‚Äúparasitism,‚Äù Reuters reports. The French litigation was filed in a Paris court this week by the National Publishing Union (SNE), the National Union of Authors and Composers (SNAC), and the Society of People [‚Ä¶]</p><p>¬© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":409,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ditto lands $82M to synchronize data from the edge to the cloud","url":"https://techcrunch.com/2025/03/12/ditto-lands-82m-to-synchronize-data-from-the-edge-to-the-cloud/","date":1741780920,"author":"Paul Sawers","guid":163,"unread":true,"content":"<p>Ditto, a company that‚Äôs setting out to bring ‚Äúresilient‚Äù connectivity to edge devices, has raised $82 million in a Series B round of funding at a post-money valuation of $462 million, more than double its Series A valuation from 2023. ‚ÄúEdge,‚Äù in the context of Ditto‚Äôs industry, refers to a distributed computing model that brings [‚Ä¶]</p><p>¬© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":411,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mercedes-Benz Drives Toward Solid-State EV Batteries","url":"https://spectrum.ieee.org/mercedes-benz","date":1741780803,"author":"Lawrence Ulrich","guid":157,"unread":true,"content":"<p>Factorial Energy's semi-solid-state cells could be in EVs in a few years</p>","contentLength":72,"flags":null,"enclosureUrl":"https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81Njc2OTUwOS9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc4ODU2NTg0NX0.kWQaB9dF0zGesDWKBhA70-XztMQk1cdzlWVpWCNBs7E/image.jpg?width=600","enclosureMime":"","commentsUrl":null},{"title":"Pok√©mon GO maker Niantic is selling its games division to Scopely for $3.5B","url":"https://techcrunch.com/2025/03/12/pokemon-go-maker-niantic-is-selling-its-games-division-to-scopely-for-3-5b/","date":1741780800,"author":"Ivan Mehta","guid":162,"unread":true,"content":"<p>Mobile gaming giant Scopely on Wednesday said it has agreed to acquire Pok√©mon Go maker Niantic's gaming division for $3.5 billion. </p><p>¬© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":196,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"10 Questions for Every Startup Founder: Brex on Rewriting the Rules of Business Finance","url":"https://hackernoon.com/10-questions-for-every-startup-founder-brex-on-rewriting-the-rules-of-business-finance?source=rss","date":1741779893,"author":"","guid":309,"unread":true,"content":"<p>:::tip\nYou‚Äôre reading an interview from&nbsp;‚Äôs Startup Founder Interview Series! Would you like to take a stab at answering some of these questions? </p><p>The modern spend platform.</p><p>We originally got our start when we saw no one in our YC cohort could get a corporate card. We realized that the payments industry needed to be shaken up - so we pivoted from our original idea and launched Brex to help founders scale in 2017. \\n </p><p>From there, we realized just how much opportunity existed when it came to how companies manage their spend.&nbsp;Existing tools for planning, spending, and tracking company dollars were antiquated and disconnected, leading to tedious processes for founders and finance teams.</p><p>\\\nA fundamental shift was required. That inspired our effort to go beyond just a corporate card and to build an entire platform from the ground up that&nbsp;unified financial services and software, allowing money movement and spend controls to coexist.</p><p>\\\nThis need to optimize spend is even more relevant in today‚Äôs macroeconomic environment, where founders and finance&nbsp;teams are being asked to maximize the ROI of every dollar. We help ensure that they‚Äôre equipped to spend smarter and accordingly grow faster.</p><h2>3. What do you love about your team, and why are you the ones to solve this problem?</h2><p>We‚Äôve got a team of builders who are willing to operate with the utmost intensity to build a generational company that exceeds the expectations of our customers. We talk often at Brex about breaking trade-offs between speed and quality. We need to move fast to bring the future we imagine forward, but we never cut corners in that journey. I am insanely proud of how the team approaches this every day, keeping an eye on the big picture but always obsessing about the details. \\n </p><p>The other thing worth noting is that Brex exists at the intersection of financial services and software, and we‚Äôve got an incredible team of experts from both domains (capital markets&nbsp;&lt;&gt;&nbsp;engineers) charting our path forward. The result is that we‚Äôve got the most advanced global capabilities and payment rails in the market, and we‚Äôre consistently raising the bar in every<a href=\"https://www.brex.com/spring-2025\">&nbsp;product release</a>&nbsp;to make our offering easy to use for smaller companies and customizable at scale for our enterprise customers.</p><h2>4. If you weren‚Äôt building your startup, what would you be doing?</h2><p>I truly can‚Äôt imagine not building Brex - it is so core to who I am.</p><h2>5. At the moment, how do you measure success? What are your metrics?</h2><p>We look at revenue growth driven by both new customer acquisition and net revenue retention. We also set targets around burn rate, sales efficiency, and CAC paybacks with the goal of 2025 being the last year that Brex burns money.</p><p>\\\nBeyond the hard metrics, it is incredibly important to us that our platform can scale.&nbsp;Brex is the only player that can credibly serve companies at every stage of growth, from YC founders starting out to trillion-dollar public companies. We will continue to prioritize this as a core competency - success is serving this wide range of customers for years to come.</p><h2>6. In a few sentences, what do you offer to whom?</h2><p>Brex offers a breadth of products to help businesses spend smarter and make every dollar count including the world‚Äôs smartest corporate card with intuitive expense management, banking, bill pay, accounting automation, travel, and more.</p><h2>7. What‚Äôs most exciting about your traction to date?</h2><p>We‚Äôve been able to empower companies to make smarter spending decisions at every stage of growth‚Äîfrom startups, to mid-size companies, all the way up to leading, global enterprises like Anthropic, Arm, Robinhood, ServiceTitan, Wiz, and more. We‚Äôve seen companies that began with just two people grow into 1,000+ employee enterprises, and Brex has been there with them every step of the way. It's incredibly rewarding to see how Brex has supported them in scaling efficiently and effectively. With Brex, our customers are saving 169,000 hours per month on expense, documentation, and accounting and customers have a 94% employee compliance rate, compared to the industry standard of 70%.</p><p>\\\nBut it's not just about providing tools; it's about being a true strategic partner and helping them fuel their exponential growth. That‚Äôs the part that truly excites us.</p><h2>8. Where do you think your growth will be next year?</h2><p>2024 was a year of sharp improvement at Brex. Revenue growth accelerated almost 3x this year, net revenue retention is up more than 15 points, while the burn rate is down almost 70%. In 2025, we will continue to compound the momentum built this year, growing faster than 2024 and being the last year in which we burn money ‚Äî an important milestone to our future IPO. We will also continue to raise the bar on product quality and innovation, while increasing the pace of customer acquisition with healthy unit economics at scale.</p><h2>9. Tell us about your first paying customer and revenue expectations over the next year.</h2><p>Scale AI was Brex‚Äôs first customer. I vividly remember Henrique, my co-founder, and I walking to their office and physically handing Alex their Brex card. It‚Äôs a moment I‚Äôll never forget.</p><h2>10. What‚Äôs your biggest threat?</h2><p>Focus. The central tension of building a company is obsessing over outcomes, but at the same time, not caring about them at all. Ironically, the more you let go of the outcomes and focus on putting out great work, the more the outcomes come. It‚Äôs the old adage: the score takes care of itself. All we need to do is to put great work in front of customers, and the score will take care of itself.</p>","contentLength":5578,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Scimplify raises $40M to help manufacturers access specialty chemicals","url":"https://techcrunch.com/2025/03/12/scimplify-raises-40m-to-help-manufacturers-access-specialty-chemicals/","date":1741777234,"author":"Jagmeet Singh","guid":161,"unread":true,"content":"<p>Scimplify has raised $40 million in a new equity round backed by Accel to expand its presence in the U.S. and enter new markets.</p><p>¬© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":191,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Can a New Power Link Boost the EU‚Äôs Energy Independence?","url":"https://spectrum.ieee.org/black-sea-energy-link","date":1741777202,"author":"Amos Zeeberg","guid":156,"unread":true,"content":"<p>Undersea cable would supply clean electricity from the Caucasus</p>","contentLength":63,"flags":null,"enclosureUrl":"https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NjY2NzY1My9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc5MDMyNTE5NH0.hL7VXlZjYYVXlowC14gTHx1atEgcdIKzlIAmDPaCrgk/image.jpg?width=600","enclosureMime":"","commentsUrl":null},{"title":"KDE's KWin Wayland & X11 Code Are Now Split, KWin_X11 To Be Maintained Until Plasma 7","url":"https://www.phoronix.com/news/KWin-Wayland-X11-Split","date":1741776829,"author":"Michael Larabel","guid":431,"unread":true,"content":"<article>Yesterday marked the milestone of KWin's kwin_x11 and kwin_wayland code being split up. The Wayland and X11 code for the KWin compositor is now separate from each other but can be co-installable for systems wanting to support both X11 and Wayland environments...</article>","contentLength":262,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Users Hate Search Filters‚ÄîMaybe Let AI Handle It Instead","url":"https://hackernoon.com/users-hate-search-filtersmaybe-let-ai-handle-it-instead?source=rss","date":1741776515,"author":"Oleksandr Rudin","guid":308,"unread":true,"content":"<p>Usually, when you want to find a product on the internet, you face complex forms with many filters. Let‚Äôs take the example of Zillow, the real-estate marketplace website. This is how their interface looks:</p><p>It seems pretty simple, which is great. Things become a little complicated when we click on ‚Äúmore‚Äú and see extended form</p><p>\\\n <img src=\"https://cdn.hackernoon.com/images/null-vt234ky.png\" alt=\"\">This still doesn‚Äôt look complicated. In my opinion, Zillow has a great UI/UX design.</p><p>\\\nMany users find filling out multiple filters frustrating and time-consuming. My idea for this article is to provide an option to the user to have it much simpler using AI. Instead of all these fields, we can have only one, where the user can provide a free text, and all those parameters can be extracted from this field. The design for this can look like this:</p><p>\\\nThis is a good alternative for the complex forms. Yes, the user still needs to write all the details to provide the best match, but it will be much faster than selecting all the fields in the forms.</p><p>Implementing an AI algorithm can be a challenge and can cost some time/money. But what if we keep the old structure of existing search functionality and just parse the free text into filter fields?</p><p>\\\nI got this challenge for one of my clients, and I  am providing you with an adapted version of using OpenAI to do that job:</p><pre><code>openai.chat.completions.create({\n  model: \"gpt-4-turbo\",\n  messages: [\n    {\n      role: \"system\",\n      content: `You are a helpful assistant that parses real-estate related text into a strict JSON format. JSON format should be just a simple string without any addition symbols, for example { \"property_type\": [\"house\"] }`\n    },\n    {\n      role: \"user\",\n      content: `Parse the following real-estate related text into strict JSON format with the following structure:\n        {\n          \"location\": \"Extract the mentioned city, neighborhood, or region. Set empty string if none found.\",\n          \"price_range\": \"Extract the mentioned price range or budget. Return as an object {min: number, max: number}. Set null if none found.\",\n          \"property_type\": \"Extract the type of property (e.g., apartment, house, studio, condo). Set empty array if none found.\",\n          \"bedrooms\": \"Extract the number of bedrooms. Set null if none found.\",\n          \"bathrooms\": \"Extract the number of bathrooms. Set null if none found.\",\n          \"amenities\": \"Extract key amenities mentioned (e.g., parking, balcony, pool, pet-friendly). Set empty array if none found.\",\n          \"keywords\": \"Extract any additional keywords relevant to the real estate context. Set empty array if none found.\"\n        }\n        Text: \"${userFreeTextInput}\"`\n    }\n  ],\n  max_tokens: 500\n});\n</code></pre><p>\\\nIn this example, I am using the  gpt-4-turbo model, which will return strict JSON that I can use as filters. For an MVP, using OpenAI's API is ideal. However, for a production setting with high traffic, a fine-tuned or self-hosted model may be more cost-effective in the long run. The result of the AI request can be configured to have the same as the user can select in traditional filters UI (first image). We can also improve that by adding exceptions, for example, when the user types something like this</p><pre><code>Looking for a 2-bedroom apartment in Texas under $2,500/month, pet-friendly, with a balcony and parking.\nI am not considering Austin or Dallas.\n</code></pre><p>\\\nTo exclude specific locations, we modify the JSON structure to include an  key. The AI should extract places users don‚Äôt want and store them in an array.</p><pre><code>{\n  \"except_location\": \"Extract the cities, neighborhoods, or regions the user wants to exclude. Set empty array if none found.\",\n}\n</code></pre><p>\\\nAnd here we go with obvious benefits by using AI. The user can even ask to exclude all big cities from the list, and the AI can find all big cities in Texas and does the job.</p><p>\\\nInstead of traditional forms, we can give users the  possibility to use a chat, the next level of communication with services. Chat can ask additional questions in case some required information for searching is not found in the free text.</p><p>\\\nThis is similar to real communication with the agent when users come to the office of the real-estate agency. Having more natural interaction improves user experience.</p><p>\\\nSince user preferences are different, I recommend introducing this as a beta option and allowing users to choose their preferred method. In our case, we will run an A/B test comparing AI-powered search against traditional filters. Metrics like time-to-search, conversion rate, and user satisfaction scores will determine which method performs better. In my opinion, in the future, as more people will use AI Chats, they will become more familiar with those types of interfaces and will prefer this option.</p><p>\\\nThe main benefit of using this approach is that we can simply add voice recognition to the input, and this will look very similar to talking to a real agent. Search results can be easily extended by adding more details to the filters.</p><p>\\\nBy collecting anonymized user queries, we can analyze patterns and fine-tune a model to provide better recommendations over time.</p><p>\\\nI hope this article will help your business to provide your customers with a modern idea of communication.</p>","contentLength":5188,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Haiku OS Wrapping Up Its New malloc & Various Performance Optimizations","url":"https://www.phoronix.com/news/Haiku-OS-New-malloc-More-Perf","date":1741776253,"author":"Michael Larabel","guid":430,"unread":true,"content":"<article>The BeOS-inspired Haiku open-source operating system project is out with a new monthly progress report to highlight its latest development accomplishments...</article>","contentLength":157,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"LLVM 20's Great Fortran Language Support With Flang","url":"https://www.phoronix.com/news/LLVM-20-Flang","date":1741775619,"author":"Michael Larabel","guid":429,"unread":true,"content":"<article>With the newly-released LLVM 20.1 compiler stack among the many changes throughout the massive codebase is renaming the \"flang-new\" compiler just to \"flang\". This new Flang compiler front-end has matured quite well over the years to providing robust and reliable Fortran language support within the confines of the LLVM toolchain...</article>","contentLength":332,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Every Truth (And Lie) Told in Netflix's 'Zero Day,' Ranked","url":"https://hackernoon.com/every-truth-and-lie-told-in-netflixs-zero-day-ranked?source=rss","date":1741774893,"author":"Moonlock (by MacPaw)","guid":307,"unread":true,"content":"<p><em>By Mykhailo Pazyniuk, Malware Research Engineer at Moonlock, the cybersecurity division of MacPaw.</em></p><p>\\\nNetflix‚Äôs ‚ÄòZero Day‚Äô places viewers in the midst of a massive cyberattack that cripples the United States. With Robert De Niro starring as a former U.S. President investigating the attack, the series explores themes of political intrigue, digital warfare, and the fragility of modern infrastructure. But just how realistic is ‚ÄòZero Day‚Äô from a cybersecurity perspective?</p><p>\\\nAs a malware research engineer, I watched the show with a critical eye. While some aspects of the cyberattack feel eerily plausible, others stray into sci-fi. Here‚Äôs my breakdown of the ‚ÄòZero Day‚Äô where I'll cover three cyber threats that could realistically happen, and three that are pure fiction (at least for now).</p><h2>Three threats that could happen</h2><p>The series shows several combined attack vectors along with consequences that follow. The main methods include a full-scale attack on critical infrastructure using weaponized malware, as well as a supply chain attack that spreads through fake versions of legitimate software.</p><p>\\\nTo assess the show‚Äôs accuracy, let‚Äôs compare these scenarios with real-life cyberattacks.</p><h3>Cyberattack on the critical infrastructure (Colonial Pipeline, 2021)</h3><p>Cybercriminals disrupting essential services is one of the most realistic aspects of ‚ÄòZero Day‚Äô. Targeted attacks on power grids, water supplies, and hospitals are not just possible ‚Äì they are already happening. The 2021 Colonial Pipeline ransomware attack shut down one of the largest fuel pipelines in the U.S., leading to gas shortages and widespread panic buying.</p><h3>Weaponized malware (Stuxnet, 2010)</h3><p>The show suggests that a cyber attack could be designed to cripple a nation‚Äôs security by sabotaging industrial systems. In reality, malware infection of critical systems have influenced global geopolitics for years. A historical precedent is Stuxnet, a highly sophisticated cyber weapon used to damage Iran‚Äôs nuclear centrifuges.</p><p>\\\nStuxnet closely resembles the attack shown in the series, especially since it also caused physical damage to infrastructure. This type of malware functions like a worm, crawling through networks, spreading across devices, and causing failures in software or hardware ‚Äî remaining persistent for long periods. We can only imagine the dire consequences if such a worm used AI to adapt to its environment.</p><h3>Supply chain attack (NotPetya, 2017)</h3><p>‚ÄòZero Day‚Äô suggests that an attack could rapidly spread through interconnected systems ‚Äî a scenario that is entirely plausible. Today, a single compromised vendor in a supply chain can infect thousands of organizations. Our team  many similar attack techniques while analyzing fake software bundled with stealer implants, tricking users into believing they were using legitimate programs.</p><p>\\\nOne of the most devastating cyberattacks in history, NotPetya, spread through a compromised update for widely used software in Ukraine, causing billions of dollars in damages worldwide.</p><h2>Three threats that are far from reality</h2><p>Netflix excels at storytelling, which is why its shows are so captivating. However, here‚Äôs how Zero Day dramatizes hacking for suspense.</p><h3>Instant and simultaneous system collapse</h3><p>In ‚ÄòZero Day‚Äô, the cyberattack appears to take down everything at once ‚Äî financial markets, emergency services, transportation. While coordinated attacks are possible, real-world cyberattacks don‚Äôt usually spread with such precision. Attacks like NotPetya or SolarWinds took time to propagate, and organizations reacted at different speeds.</p><blockquote><p><em>‚ÄúIf we're talking about a common vulnerability, it would likely be in the baseband or hardware. But with multiple vendors supplying critical infrastructure across the country, this remains unrealistic for now,‚Äù</em> notes Senior Reverse Engineer at MacPaw‚Äôs Moonlock (who chose to remain anonymous).</p></blockquote><h3>Total control with a few keystrokes</h3><p>'Zero Day' relies on a classic Hollywood trope ‚Äî a hacker typing furiously in a dark room, instantly causing systems to crash like dominoes. In reality, cyberattacks take weeks, months, or even years to prepare. Breaching critical infrastructure requires intricate social engineering, vulnerability hunting, lateral movement, and stealth to evade detection. It‚Äôs never as simple as hitting a few keystrokes and watching the world burn.</p><h3>The unstoppable supervirus</h3><p>The show portrays an unstoppable cyber weapon with no way to mitigate its effects. It's true that advanced malware can be highly persistent, but no cyberattack is truly unpatchable. Even the most destructive malware can be taken down with countermeasures, whether through endpoint protection, network segmentation, or manual intervention. The notion \"once it‚Äôs launched, it‚Äôs game over\" is pure fiction.</p><blockquote><p><em>‚ÄúIf the goal is to find vulnerabilities in an infected system, fuzzers are used. They run non-stop on CI servers. But instead of brute-forcing all possible values, they rely on smart mutations. Moreover, not every crash dump leads to an exploitable vulnerability. And to even operate on an infected system in the first place, you‚Äôd already need a vulnerability to execute code. So, you've got something like a time loop in ‚ÄòTerminator‚Äô. Therefore, I‚Äôd say these scenarios aren‚Äôt plausible with the current state of AI development,‚Äù</em> adds Senior Reverse Engineer at MacPaw‚Äôs Moonlock.</p></blockquote><p>While 'Zero Day' takes creative liberties, some of its fictional elements could eventually become real. Advancements in AI-driven attacks, deepfake social engineering, and autonomous malware may one day bring us closer to the threats depicted in the show. AI-assisted hacking tools are already reshaping the threat landscape, making cyberattacks faster and more efficient.</p><p>\\\nFor example, Moonlock Lab‚Äôs team recently discovered  that uses the OpenAI API (ChatGPT) to run phishing campaigns. While the AI still requires well-structured queries to generate personalized phishing content, it significantly simplifies and accelerates the work of threat actors.</p><p>\\\nMoreover, as governments and nation-state actors invest in cyber warfare, the line between fiction and reality continues to blur. AI-driven disinformation campaigns, automated zero-day exploits, and self-spreading malware are no longer far-fetched scenarios. However, at this stage, threat actors primarily use AI for automation and attack preparation ‚Äî not ==‚Äò<em>using AI to adapt the code in the process of execution</em>‚Äô,== as seen in the series.</p><h2>Fiction as a cautionary tale</h2><p>The ‚ÄòZero Day‚Äô series may exaggerate some elements of cyber warfare, but it effectively highlights an important truth ‚Äî our digital infrastructure is vulnerable. While we may not see a Hollywood-style ‚Äòdoomsday virus‚Äô anytime soon, real-world threats like ransomware, critical infrastructure attacks, and AI-driven cybercrime demand our attention and awareness.</p><p>\\\nAs malware researchers, security professionals, and even everyday users, we should learn from both real-world incidents and fictional warnings. Cyber threats are evolving, 'Zero Day' just accelerates the timeline. \\n </p>","contentLength":7133,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Jonathan Riddell Stepping Down From KDE Plasma Release Management","url":"https://www.phoronix.com/news/Riddell-Stepping-Down-Plasma-RM","date":1741774688,"author":"Michael Larabel","guid":428,"unread":true,"content":"<article>Longtime KDE developer who has served with Plasma release management duties, KDE Neon operating system development, and former Kubuntu release manager, among other roles, announced he will be stepping down from his Plasma release management duties...</article>","contentLength":250,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Salesforce to invest $1B in Singapore to boost adoption of AI","url":"https://techcrunch.com/2025/03/12/salesforce-to-invest-1b-in-singapore-to-boost-adoption-of-ai/","date":1741774346,"author":"Kate Park","guid":160,"unread":true,"content":"<p>Salesforce plans to invest $1 billion in Singapore over the next five years as it seeks to fuel the adoption of its AI agent development platform, Agentforce. Salesforce claimed that Agentforce can help alleviate Singapore‚Äôs ongoing labor issues and augment the country‚Äôs workforce and enterprises by creating ‚Äúdigital workforces‚Äù that combine humans with autonomous AI [‚Ä¶]</p><p>¬© 2024 TechCrunch. All rights reserved. For personal use only.</p>","contentLength":446,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Solar Adds More New Capacity To the US Grid In 2024 Than Any Energy Source In 20 Years","url":"https://hardware.slashdot.org/story/25/03/11/2133237/solar-adds-more-new-capacity-to-the-us-grid-in-2024-than-any-energy-source-in-20-years?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1741773600,"author":"BeauHD","guid":258,"unread":true,"content":"AmiMoJo shares a report from Electrek: The U.S. installed 50 gigawatts (GW) of new solar capacity in 2024, the largest single year of new capacity added to the grid by any energy technology in over two decades. That's enough to power 8.5 million households. According to the U.S. Solar Market Insight 2024 Year in Review report (PDF) released today by the Solar Energy Industries Association (SEIA) and Wood Mackenzie, solar and storage account for 84% of all new electric generating capacity added to the grid last year.\n \nIn addition to historic deployment, surging U.S. solar manufacturing emerged as a landmark economic story in 2024. Domestic solar module production tripled last year, and at full capacity, U.S. factories can now produce enough to meet nearly all demand for solar panels in the U.S. Solar cell manufacturing also resumed in 2024, strengthening the U.S. energy supply chain. [...] Total US solar capacity is expected to reach 739 GW by 2035, but the report forecasts include scenarios showing how policy changes could impact the solar market. [...] The low case forecast shows a 130 GW decline in solar deployment over the next decade compared to the base case, representing nearly $250 billion of lost investment.","contentLength":1236,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Data Transformation and Discretization: A Comprehensive Guide","url":"https://hackernoon.com/data-transformation-and-discretization-a-comprehensive-guide?source=rss","date":1741773424,"author":"Aleeza Adnan","guid":306,"unread":true,"content":"<p>\\\nData transformation and discretization are critical steps in the data preprocessing pipeline. They prepare raw data for analysis by converting it into forms suitable for mining, improving the efficiency and accuracy of data mining algorithms. This article dives deep into the concepts, techniques, and practical applications of data transformation and discretization.</p><p>Data transformation involves converting data into appropriate forms for mining. This step is essential because raw data is often noisy, inconsistent, or unsuitable for direct analysis. Common data transformation strategies include:</p><ol><li>: Remove noise from the data (e.g., using binning or clustering).</li><li>: Create new attributes from existing ones (e.g., area = height √ó width).</li><li>: Summarize data (e.g., daily sales ‚Üí monthly sales).</li><li>: Scale data to a smaller range (e.g., 0.0 to 1.0).</li><li>: Replace numeric values with intervals or conceptual labels (e.g., age ‚Üí \"youth,\" \"adult,\" \"senior\").</li><li><strong>Concept Hierarchy Generation</strong>: Generalize data to higher-level concepts (e.g., street ‚Üí city ‚Üí country).</li></ol><ul><li>: Removes noise, inconsistencies, and redundancies.</li><li><strong>Enhances Mining Efficiency</strong>: Reduces data volume and complexity, speeding up algorithms.</li><li><strong>Facilitates Better Insights</strong>: Transforms data into forms that are easier to analyze and interpret.</li></ul><p>Normalization scales numeric attributes to a specific range, such as [0.0, 1.0] or [-1.0, 1.0]. This is particularly useful for distance-based mining algorithms (e.g., k-nearest neighbors, clustering) to prevent attributes with larger ranges from dominating those with smaller ranges.</p><h4><strong>3.1.1 Min-Max Normalization</strong></h4><ul><li><p>v*‚Äô* : Original value of the attribute.</p></li><li><p>min‚Å°A ‚Äã: Minimum value of attribute&nbsp;.</p></li><li><p>max‚Äã: Maximum value of attribute&nbsp;.</p></li><li><p>new_min‚Äã: Minimum value of the new range (e.g., 0.0).</p></li><li><p>new_max‚Äã: Maximum value of the new range (e.g., 1.0).</p></li><li><p>Suppose the attribute \"income\" has a minimum value of $12,000 and a maximum value of $98,000.</p></li><li><p>We want to normalize an income value of $73,600 to the range [0.0, 1.0].</p></li><li><p>The normalized value is .</p></li></ul><h4><strong>3.1.2 Z-Score Normalization</strong></h4><ul><li><p>Suppose the mean income is $54,000 and the standard deviation is $16,000.</p></li><li><p>We want to normalize an income value of $73,600.</p></li><li><p>The normalized value is .</p></li></ul><h4><strong>3.1.3 Decimal Scaling Normalization</strong></h4><ul><li><p>j : Smallest integer such that ( max(|v'|) &lt; 1 ).</p></li><li><p>Suppose the attribute \"price\" has values ranging from -986 to 917.</p></li><li><p>The maximum absolute value is 986.</p></li><li><p>The smallest integer ( j ) such that ( 986 / 10^j &lt; 1 ) is  j = 3 .</p></li><li><p>The normalized value is .</p></li></ul><p>Discretization replaces numeric values with interval or conceptual labels. This is useful for simplifying data and making patterns easier to understand.</p><p>Binning divides the range of an attribute into bins (intervals). There are two main types:</p><ul><li>Divide the range into ( k ) equal-width intervals.</li><li>Example: For the attribute \"age\" with values [12, 15, 18, 20, 22, 25, 30, 35, 40], create 3 bins:<ul></ul></li></ul><ul><li>Divide the range into ( k ) bins, each containing approximately the same number of values.</li><li>Example: For the same \"age\" values, create 3 bins:<ul></ul></li></ul><p>Histograms partition the values of an attribute into disjoint ranges (buckets). The histogram analysis algorithm can be applied recursively to generate a multilevel concept hierarchy.</p><ul><li>For the attribute \"price\" with values [1, 1, 5, 5, 5, 5, 8, 8, 10, 10, 10, 10, 12, 14, 14, 15, 15, 15, 15, 15, 18, 18, 18, 18, 18, 18, 20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 25, 25, 25, 25, 28, 28, 30, 30, 30]:</li><li>Create an equal-width histogram with a bin width of $10:<ul></ul></li></ul><h4><strong>3.2.3 Cluster, Decision Tree, and Correlation Analyses</strong></h4><ul><li>Group similar values into clusters and replace raw values with cluster labels.</li><li>Example: Cluster \"age\" values into \"young,\" \"middle-aged,\" and \"senior.\"</li></ul><ul><li>Use decision trees to split numeric attributes into intervals based on class labels.</li><li>Example: Split \"income\" into intervals that best predict \"credit risk.\"</li></ul><ul><li>Use measures like chi-square to merge intervals with similar class distributions.</li><li>Example: Merge adjacent intervals if they have similar distributions of \"purchase behavior.\"</li></ul><h3><strong>3.3 Concept Hierarchy Generation for Nominal Data</strong></h3><p>Concept hierarchies generalize nominal attributes to higher-level concepts (e.g., street ‚Üí city ‚Üí country). They can be generated manually or automatically based on the number of distinct values per attribute.</p><ul><li>For the attributes \"street,\" \"city,\" \"province,\" and \"country\":<ul><li>Sort by the number of distinct values:</li><li>Country (15) ‚Üí Province (365) ‚Üí City (3567) ‚Üí Street (674,339).</li><li>Country ‚Üí Province ‚Üí City ‚Üí Street.</li></ul></li></ul><h2><strong>4. Practical Applications</strong></h2><ul><li>: Normalize income and age attributes to cluster customers into segments.</li><li>: Discretize purchase amounts into intervals to identify patterns.</li><li>: Use concept hierarchies to generalize transaction locations (e.g., street ‚Üí city ‚Üí country).</li></ul><p>Data transformation and discretization are essential steps in data preprocessing. They improve data quality, enhance mining efficiency, and facilitate better insights. By normalizing, discretizing, and generating concept hierarchies, you can transform raw data into a form that is ready for analysis.</p>","contentLength":5017,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI Wants to Fix Your Network Before It Breaks‚ÄîBut Can You Trust It?","url":"https://hackernoon.com/ai-wants-to-fix-your-network-before-it-breaksbut-can-you-trust-it?source=rss","date":1741773143,"author":"Anna Naumova","guid":305,"unread":true,"content":"<p>Not that long ago, the experience in managing networks and‚ÄÇdata centers was one of humans doing the effort, as admins entered commands, configured routers, balanced loads, and hunted down outages. Fast forward to today, and the AI revolution is a game changer ‚Äî automating fast decision-making, predicting failures before vibrations cause them, and optimizing infrastructure in ways that humans never could.</p><p>\\\nBut how much of this is real? And how much is simply marketing hype?</p><p>\\\nSlicing it‚Äîand slicing it again‚Äîlet‚Äôs take a look at AI in networks and data centers today, who‚Äôs using it and where it makes a difference, as well as what it needs to‚ÄÇdo better.</p><p>Network management has traditionally been a reactive job. Something breaks? An engineer fixes it. Latency spikes? Someone troubleshoots it. However, AI-enabled networking is moving us away from the reactive‚ÄÇfirefighting and toward the proactive self-healing system.</p><h3>Artificial Intelligence Is Redefining Networking.</h3><ul><li><p><strong>Traffic Optimization Automatically:</strong> AI-driven SD-WANs and intent-based networking systems allow bandwidth‚ÄÇto be dynamically provisioned, traffic routed dynamically, and loads balanced across data centers, in real time, based on current demand.</p></li><li><p> ML algorithms can be trained on historical network data that assist AI in predicting hardware failures (e.g. switches, routers, fiber links) before they happen, minimizing failures and downtime that can be expensive.</p></li><li><p><strong>Anomaly Detection &amp; Security:</strong> AI can spot anomalous traffic patterns that might indicate a cyberattack, misconfiguration, or insider threat ‚Äî all common occurrences that traditional rule-based systems tend to miss.</p></li><li><p> Artificial intelligence (AI) ‚Äî based network monitoring tools dynamically prioritize traffic based on real-time business needs, followed by ensuring‚ÄÇthat data-heavy applications can get precedence over other not-so-important traffic.</p></li></ul><h2><strong>üè¢ AI and the Data Center of the Future</strong></h2><p>So, modern data centers are a logistical hellscape ‚Äî you‚Äôre managing‚ÄÇpower, cooling, security, storage, compute, and network, all at the same time. AI is coming to optimize everything to efficiency, scale, and autonomy.</p><h3>Data Centers Are Getting Smarter Thanks to AI.</h3><ul><li><p><strong>Cooling Optimization &amp; Energy Efficiency:</strong> AI reads on temperature, load distribution, and airflow to passively‚ÄÇadjust the cooling power accordingly, which reduces power consumption. (One industry example: Google cut its data center cooling costs to 40% by adopting‚ÄÇDeepMind AI.)</p></li><li><p><strong>Smart Resource Allocation:</strong> This includes workload orchestration with AI, which can intelligently distribute workloads of jobs over multiple servers to decrease resource‚ÄÇwaste and increase efficiency. Predictably, Artificial Intelligence, on the other hand, makes possible scaling that is at a granular level as opposed to a resource allocation of fixed resources, significantly reducing waste of power‚ÄÇand cost.</p></li><li><p><strong>Self-Healing Infrastructure:</strong> And AI detects early-stage hardware degradation before faults occur and initiates proactive mitigation‚ÄÇ(e.g., moving workloads from nodes in failure). Hyperscalers are experimenting with replacing‚ÄÇhardware with AI-driven robotics.</p></li><li><p><strong>Automatic Service Establishment:</strong> AI coordinates what are often complex, parallel changes to the network so that if many changes are needed, then less human labor is involved and the changes get deployed‚ÄÇfaster. Business intent-based configuration of BGP peering, VLAN tagging, or cloud interconnects instead of‚ÄÇscripts by hand is done via AI for example.</p></li></ul><h2>üèôÔ∏è<strong>The Significance of Edge AI in Networking and Data Center Environments</strong></h2><p>As AI workloads are on the hike, putting everything on the cloud is no more‚ÄÇa smart option. Enter edge computing.</p><h3>Why AI is Moving to the Edge.</h3><ul><li> Processing AI workloads closer to‚ÄÇthe user results in timely decisions ‚Äî autonomous vehicles, industrial robotics, smart cities, etc.</li><li> Not every application requires returning the data to a centralized data center ‚Äî edge AI performs‚ÄÇthis task locally.</li><li> Applications like security, fraud detection, and machine vision are all AI-powered and must respond in real time, so tasks at the edge are ideal.</li><li> Tesla‚Äôs Autopilot doesn‚Äôt stream raw video to a cloud data center‚Äîits on-device edge AI processes it‚ÄÇin the moment.</li></ul><h3>How Networks Need to Adapt.</h3><p>Provision‚ÄÇof high-speed low-latency connections between devices, edge nodes, and central data centers is AI at the Edge. 5G and next-gen networking is going to play a huge role‚ÄÇin making AI-enabled edge computing practical.</p><h2><strong>üõë AI is NOT the Solution: Challenges and Risks</strong></h2><p>And even while AI is coming to‚ÄÇrevolutionize networking and data centers, it‚Äôs not a sure thing. Here are some of the most significant‚ÄÇroadblocks:</p><ul><li><strong>AI Still Needs Quality Data.</strong> AI models are only as good as the data they‚Äôre‚ÄÇtrained on. So garbage data‚ÄÇ= garbage predictions.</li><li> AI-based Network Monitoring has too many false positives, where normal traffic is flagged as an anomaly.</li><li> AI-powered automation can occasionally introduce an additional level of complexity,‚ÄÇand so that when things go awry, it‚Äôs even harder for engineers to diagnose the problem. ‚ÄúAI misconfiguration‚Äù is a real threat ‚Äî perhaps an AI auto-optimizes a network that somehow adds‚ÄÇcongestion.</li><li> Attackers are already‚ÄÇleveraging adversarial attacks to evade security systems powered by AI. A poorly trained AI could even create new vulnerabilities instead‚ÄÇof shutting them down.</li></ul><h2><strong>üîÆ How AI Will Change Networks and Data Centers</strong></h2><p>We aren‚Äôt there just yet ‚Äî not fully at the AI-driven, self-optimizing network/data center ‚Äî but we‚ÄÇare moving in that direction. Some trends to watch:</p><ul><li><p> Companies like‚ÄÇJuniper, Cisco, and Arista are using AI-native network controllers so you don‚Äôt even have to adjust anything by hand.</p></li><li><p><strong>AI-Optimized Networking Chips:</strong> Custom AI accelerator chips will pinpoint packet routing and conduct deep packet inspection faster than ever before.</p></li><li><p><strong>AI for Carbon-Neutral Data Centers:</strong> Look for carbon-aware AI‚ÄÇscheduling‚Äîto move workloads to the server farms with the greenest energy mix in real time.</p></li></ul><p>AI is helping networks and data centers to be more clever, speedy, and efficient.</p><p>\\\nSelf-optimizing fabric networks, AI-powered cooling, and predictive maintenance are just some of the ways the industry is working towards‚ÄÇfully autonomous infrastructure.</p><p>\\\nBut AI is not a cure-all ‚Äî bad data, hard-to-reduce errors, and security risks‚ÄÇall need human supervision.</p><p>\\\n AI-driven networking‚ÄÇarchitecture, AI-first chipsets, true self-healing.</p><p>What impact will there‚ÄÇbe from AI on networking and data centers in the future?</p><p>\\\nLeave your comments below!</p>","contentLength":6658,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How to Teach a Tiny AI Model Everything a Huge One Knows","url":"https://hackernoon.com/how-to-teach-a-tiny-ai-model-everything-a-huge-one-knows?source=rss","date":1741772689,"author":"Raviteja Reddy Ganta","guid":304,"unread":true,"content":"<ol><li>Additional experiment on MNIST</li></ol><p>In this article, I will explore the knowledge distillation process in AI ‚Äîhow it works in general, its significance, and the reasons for using it.</p><p>\\\nHow can we compress and transfer knowledge from a bigger model or ensemble of models(which were trained on very large datasets to extract structure from data) to a single small model without much dip in performance?</p><p>\\\nBut why do we want to do this? Why we need a smaller model when a bigger model or ensemble model is already giving great results on test data?</p><p>\\\nAt training time we typically train large/ensemble of models because the main goal is to extract structure from very large datasets. We could also be applying many things like dropout, data augmentation at train times to feed these large models all kinds of data.</p><p>\\\nBut at prediction time our objective is totally different. We want to get results as quickly as possible. So using a bigger/ensemble of models is very expensive and will hinder deployment to large number of users. So, now the question is how can we compress knowledge from this bigger model into a smaller model which can be easily deployed.</p><p>\\\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean from google through their<a href=\"https://arxiv.org/pdf/1503.02531.pdf\">paper</a>&nbsp;came up with a different kind of training called&nbsp;&nbsp;to transfer this knowledge to the smaller model. This is the same technique that hugging face used in their&nbsp;<a href=\"https://arxiv.org/pdf/1910.01108.pdf\">Distill BERT</a>&nbsp;implementation.</p><p>\\\nIf we can train this smaller model to&nbsp;&nbsp;in the same way as a large model, then this smaller model trained this way will do much better than the smaller model trained on the same data but in the normal way. This is one of the&nbsp;&nbsp;principles behind Distillation</p><p>Usually, in Machine learning, a model that learns to discriminate between a large number of classes, the main training objective is to maximize the average log probability of correct answer. For example, take the example of the MNIST dataset where the goal is to classify an image as to whether it‚Äôs 1 or 2 or ‚Ä¶ 9. So if the actual image is 2 then the objective of any model is to maximize&nbsp;&nbsp;(which can be read as probability that a particular image is 2 given the image). But the model also gives probabilities to all incorrect answers even though those probabilities are very small, some of them are much larger than others. Point is that even though these probabilities are small, relative probabilities of incorrect answers tell us a lot about how the model can generalize. To understand it, let‚Äôs have a look at the below example.</p><p>\\\nIn the above figure, this version of 2 was given a probability of 10-6&nbsp;of being a 3 and 10-9&nbsp;of being a 7 whereas for another version it may be the other way around. This is valuable information that defines a rich similarity structure over the data(i. e. it says which 2‚Äôs look like 3‚Äôs and which look like 7‚Äôs) but it has very little influence on the cross-entropy cost function during the transfer stage because the probabilities are so close to zero.</p><p>\\\nBut before we move on to the distillation procedure, let‚Äôs spend time on how the model actually produced output probabilities. This is where softmax activation comes in. Last step of model processing is softmax and this component is what gives output probabilities. Input to softmax is called logits and we design the final layer of NN in such a way that number of hidden units = number of classes we want to classify.</p><p>The formula for calculating softmax is given as</p><p>The above equation gives probabilities for each&nbsp;&nbsp;and the sum of all probabilities overall&nbsp;&nbsp;equals 1. During training time, loss for any single training example is calculated by comparing these softmax probabilities with hard targets(labels) and using backpropagation coefficients are updated until the loss is minimum.</p><p>\\\nAs seen above this softmax gives a high probability to a true label and low probabilities to incorrect labels. We also see that probabilities of incorrect answers even though small, have lot of information hidden in them which helps the model to generalize. We call this&nbsp;</p><h2>3. Distillation Procedure</h2><p>According to the paper, the best way to transfer generalization capabilities of the larger model to a small model is to use class probabilities produced by the cumbersome model as&nbsp;&nbsp;for training the small model.</p><p>\\\nSo the process is as follows:</p><ul><li><p>Take the original training set which was used to train the bigger model then pass that training data through the bigger model and get softmax probabilities over different classes. As seen above, true label will get high probability and incorrect labels will get low probabilities. But we saw these low probabilities have a lot of information hiding in them. So to magnify importance of these probabilities authors of the papers used a variable called Temperature(T) to divide all logits before passing through softmax. This produces a softer probability distribution over classes. We can see below</p></li></ul><p>\\\nThe output of applying softmax with temperature(T) is what we call Soft targets. This process is what authors called. <em>Analogy with removing impurities in water by increasing temperature</em></p><ul><li>Much of information about learned function from large model resides in the ratios of very small probabilities in the soft targets.</li></ul><ol><li>&nbsp;- output from the large model after temperature T has been applied during softmax</li><li>&nbsp;- output from the smaller model after temperature T has been applied during softmax</li><li>&nbsp;- output from smaller model when temperature T = 1(regular softmax)</li><li>&nbsp;- actual targets from training set</li></ol><p>\\\nBelow is a flowchart of the entire training process</p><p>\\\nSo training process for a small model has 2 loss functions. The first loss function takes both soft predictions and soft targets and is the cross-entropy loss function. This is the way generalization ability is transferred from large model to small model by trying to match soft targets. For this loss function, both softmax uses the temperature of ‚ÄòT‚Äô.</p><p>\\\nAuthors also found that using the small model to match true targets helps. This is incorporated in the second cost function. The final cost is a weighted average of these two cost functions with hyper-parameters alpha and beta.</p><p>The authors used the MNIST dataset to test this approach. They used two architectures for this which differs only in the number of hidden units in middle layers. The authors used 2 hidden layer neural network in both cases</p><ol><li>Smaller model which can be viewed as 784 -&gt; 800 -&gt; 800 -&gt; 10 (where 784 is unrolled dimensions of an image, 800 is the number of hidden units with RELU activation and 10 is the number of classes we are predicting). This model gave 146 test errors with no regularization.</li><li>Bigger model which can be viewed as 784 -&gt; 1200 -&gt; 1200 -&gt; 10 (where 784 is unrolled dimensions of an image, 1200 is the number of hidden units with RELU activation and 10 is the number of classes we are predicting). This model is trained on MNIST using dropout, weight-constraints, and jittering input images and this net achieved 67 test errors.</li></ol><p>\\\nCan we transfer this improvement in the bigger model to a small model?</p><p>\\\nAuthors now used both soft targets obtained from the big net and true targets without dropout and no jittering of images i.e, the smaller net was regularized solely by adding the additional task of matching soft targets produced by the large net at a temperature of 20 and the result is.</p><blockquote><p>&nbsp;using 784 -&gt; 800 -&gt; 800 -&gt; 10</p></blockquote><p>\\\nThis shows that soft targets can transfer a great deal of knowledge to the small model, including the knowledge about how to generalize that is learned from translated training data. In other words, the benefit we got from transforming inputs transfers across to the little net even though we are not transforming inputs for the small net.</p><p>\\\nIt‚Äôs well-known fact that transforming inputs by different transformations make the model generalize much better and in our case information about how to&nbsp;&nbsp;is showing up in Dark knowledge and this is hiding in soft targets. None of this information is in True targets. So by using information from soft targets our small net is performing much better.</p><blockquote><p>Big net using soft targets learned similarity metric that learned ‚Äòwhat‚Äôs like what‚Äô and with this knowledge transfer, we are telling the little net ‚Äòwhat‚Äôs like what‚Äô</p></blockquote><p>\\\nAll of above experiment on MNIST is summarized below</p><h2>5. Additional experiment on MNIST</h2><p>In addition, authors also tried omitting examples of digit 3 when training a smaller model using distillation. So from the perspective of the small model, 3 is a mythical digit that it has never seen. Since the smaller model has never seen 3 during training, we expect it to make a lot of errors when encountering 3 in the test set. Despite this, the distilled model only made 206 test errors of which 133 are on 1010 threes in the test set. This clearly shows that generalization capabilities of the large model were transferred to the small model during distillation and this causes the small model to correctly predict 3 in most cases</p><p>\\\nSo moral of the story is.</p><blockquote><p>Transforming input images greatly improves generalization. Transforming the targets also has a similarly large effect and if we can get soft targets from somewhere it‚Äôs much cheaper as we can get the same performance with the smaller model</p></blockquote><ol><li>Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. ‚ÄúDistilling the knowledge in a neural network.‚Äù</li><li>Knowledge distillation by intellabs.</li></ol>","contentLength":9379,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"In-House vs. Agency PR: Which Strategy Boosts Your Brand?","url":"https://hackernoon.com/in-house-vs-agency-pr-which-strategy-boosts-your-brand?source=rss","date":1741771089,"author":"Valeriya Mingova","guid":303,"unread":true,"content":"<p>PR is a crucial component of communication between businesses and clients. Most people agree on that. With well-executed PR, a business can reach new heights, expand its horizons, build audience loyalty, and strengthen trust. But it‚Äôs just as easy to mess things up if handled incorrectly.</p><p>Let‚Äôs start with the fact that I‚Äôm generally against this kind of opposition, even though it‚Äôs very common. \"We have our own team,\" \"We‚Äôd rather hire an in-house PR person,\" \"We do everything ourselves\"‚Äîthese are frequent comments.</p><p>\\\nBut here‚Äôs an obvious analogy. If you need to sell your product, do you hire a traveling salesman, hand him a suitcase with product samples, put him in a cart, and send him off to knock on doors? Of course not. You hire a team of salespeople, provide them with mailing services, landing pages, lead generators, analytics, and so on. Can a salesperson sell on their own? Yes. Will their efficiency increase if you create the right environment? Absolutely. The same applies to PR.</p><p>\\\nLooking at trends in integrating PR into businesses, let‚Äôs examine how Russian IT companies‚Äîtechnological flagships of the economy‚Äîimplement this. Previously, we  that 17% of companies significantly expanded their PR teams. More than 50% of companies want to strengthen their marketing units by involving PR agencies. This indicates that the sector understands that they can‚Äôt survive without PR reinforcements.</p><p>Sometimes, companies have both in-house specialists and external partners working together towards results. Of course, situations vary, and it‚Äôs not always about budget constraints. Let‚Äôs break this down neatly.</p><h3>Pros and Cons of Each Option</h3><p>The most obvious advantage of an in-house PR specialist: they‚Äôre all yours. Always available, and not strictly limited by a contract (just check PR job listings on HH.ru‚Äîthere‚Äôs a ton of responsibilities). You can assign them anything! And they will do something. All for one salary! Whether they have the time, expertise, or resources to do everything effectively is another question.</p><p>\\\nAn in-house PR person is deeply immersed in the business and knows it better than anyone else. That‚Äôs a unique asset. But there are two nuances:</p><ol><li>When will they have time to work with the external world if they‚Äôre always learning about the product and engaging with internal business structures?</li><li>Everyone knows that if you stay in one environment too long, you lose objectivity, start seeing customers through your own lens, and lose critical perspective on the product and business.</li></ol><p>\\\nAn in-house PR person might embark on a vague quest, given ambiguous tasks like \"figure something out.\" If management isn‚Äôt clear on what they want, they hire someone, vaguely outline a task, and then oversee execution with a \"seagull management\" approach‚Äîflying in occasionally to check progress. No news? \"Make something up. Walk around the factory, talk to smart people, follow the director for two days and write down their ideas‚Äîthat‚Äôs what we pay you for!\" Agencies don‚Äôt work like that.</p><p>\\\nAnd here‚Äôs the kicker‚Äîlack of mandatory results is compensated by cost savings. One salary. But this advantage diminishes if there‚Äôs a whole PR team, in which case costs become comparable to those of an agency, where a team also works on your project.</p><p>Should you then hire an agency? Not so fast‚Äîthere‚Äôs more to consider.</p><p>\\\nAgencies have more resources. They handle multiple clients, have broader experience, stronger connections, and more data to understand target audiences and markets. However, agencies must work closely with clients since they lack in-depth product knowledge and internal access. There‚Äôs an adjustment period to fine-tune collaboration.</p><p>\\\nAgencies are results-driven, working with deadlines and specific metrics‚Äîwhat, how much, and how effectively. Sounds great, but only if the client knows what results they want and is ready to cooperate‚Äîapproving materials, providing information, and answering questions promptly. If not, an agency can become an annoying force constantly demanding input to meet deadlines.</p><p>\\\nAgencies are skilled but don‚Äôt like doing much beyond the contract‚Äôs scope. At some point, additional costs may arise‚Äîespecially when a client gets excited and wants more.</p><p>\\\nGood work costs money. Not astronomical amounts, but agencies are always pricier than a single in-house PR specialist. However, if the in-house team grows beyond one person, costs become comparable.</p><p>\\\nFinally, working with an agency can be daunting‚Äînot because they bite, but because they make publicity a reality. Suddenly, people will read about your business, discuss it, scrutinize it. What if something doesn‚Äôt work well or look good enough? This fear of public exposure is more common among business owners than you‚Äôd think.</p><p>A global study shows that brands outsource the following to agencies:</p><ul><li>Pitching to journalists (78% of companies)</li><li>Revising or building media lists (73%)</li><li>Project work for product launches (51%)</li><li>Content creation, including blogs (39%)</li></ul><p>\\\nAdditionally, businesses often hire multiple agencies for different tasks.</p><p>The best approach, if circumstances allow, is a PR team strengthened by a professional agency. Alternatively, an in-house PR manager overseeing a partner agency can be effective. This mix optimally distributes resources, workload, and ensures measurable, predictable results.</p><p>But if you have to choose one? Here‚Äôs how to decide.</p><ul><li>Process matters more than results. Social media is managed, texts are written, publications happen, and requests come in. Ambitious growth may be tough, but for some, that‚Äôs enough.</li><li>You value internal knowledge. The in-house PR person understands your business deeply, maintains great internal relationships, and keeps communications smooth.</li><li>You prefer a slow and steady approach. If there are no major growth ambitions and no clear promotional plans, an in-house PR specialist is a solid, cost-effective solution.</li></ul><ul><li>You operate in a competitive market. Quick, well-planned PR actions are crucial, and predicting their consequences requires expertise and resources.</li><li>You have ambitious growth plans. PR enhances audience interaction, speeds up decision-making, builds loyalty, and creates brand ambassadors. A well-known brand has at least a 30% higher selection probability over an unknown but identical product.</li><li>You don‚Äôt have (or plan to build) an in-house PR team. Many business owners prefer to delegate processes outside their expertise to professionals, making agency selection critical.</li></ul><p>\\\nSome might argue: \"What about companies with full-scale in-house PR teams?\" Yes, having an in-house team is strategically sound. But look at major players‚Äîthey have plenty of partners, including agencies. Strength lies in diversification.</p>","contentLength":6788,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Here Are the 7 Productivity Tips Pro Software Engineers Use to Stay Ahead","url":"https://hackernoon.com/here-are-the-7-productivity-tips-pro-software-engineers-use-to-stay-ahead?source=rss","date":1741768274,"author":"Maksim Zhelezniakov","guid":302,"unread":true,"content":"<p>Modern software engineering is highly competitive and ever-changing space that requires many different skills. It‚Äôs not nearly enough just to know how to code well and implement tasks as you are asked to. You better know well your company‚Äôs business side, its important money-making metrics, learn how to cut corners to achieve results faster, find crucial edge cases the product team is not aware of, communicate expectations and your ideas clearly and test the outcomes diligently. Basically, you need to wear many hats and be a bit of everything for your team and company. That usually comes with some heavy experience, try and error ways and constant learning. But you can always grow professionally and become more productive in the shorter period of time, hopefully even earning a promotion while doing it.</p><p>\\\nIn this article I‚Äôm going to share 7 productivity tips I learned the hard way over the years as a senior iOS engineer. Let‚Äôs see how exactly you can be a better engineer starting today:</p><h2>1. Keep your internal backlog</h2><p>Yes, Jira and other tracking systems are great, but they are designed for your team first, not just for you personally. Everyone uses it to check your progress on sprint tasks, but it‚Äôs not granular and not tailored to your goals.</p><p>\\\nYou don‚Äôt need another sophisticated tracking system here. It can be just a notes app on your device or a simple task manager, to-do list or any other way of structuring your tasks. Here is a step by step example of how it can work:</p><ol><li>Take the sprint tasks from your favorite Jira, look at them and prioritize. What should be your primary task? Hint: usually it‚Äôs something business related, maybe a new feature your team expects to be done by the end of the sprint. What are the tasks that can be safely postponed for later and completed if you have some time left? It‚Äôs always a good idea to clarify and share your understanding with a manager or a product person. It can be done during sprint planning if you have them.</li><li>Break down your tasks by days, allocate more time and efforts for your main ones and leave less space for those you deem not so important.</li><li>Add team meetings and other activities to your list. For example, I find it a good practice to have a dedicated time slot for code reviews every day. This way you don‚Äôt need to rush your comments, and there are more chances to contribute better. I published a separate piece on getting the most out of your PR reviews <a href=\"https://hackernoon.com/code-review-culture-why-you-need-to-have-one\">here</a>.</li><li>Add some other todos to your list that are not included in your sprint. For example, ‚Äúask Tom about a change we did last week to see if there is something I need to do next time‚Äú</li><li>Corporate responsibilities should be in the list, as well. Example: ‚Äúupdate my goals‚Äù, ‚Äúrespond to a company survey request‚Äù, etc.</li><li>Finally, keep track of all these things at the beginning of the day.</li></ol><p>\\\nThis internal todo helps you to not forget about small things you might miss otherwise.</p><h2>2. Be proactive at team meetings</h2><p>It might sound tempting to be present at your team‚Äôs meeting, but at the same time try to finish that annoying task or follow up with someone in Slack. But, believe me, it is not the best time to lose focus. Your team members, especially both engineering manager and product manager expect you to be vocal and present. Share your ideas and opinions, ask questions on why and when, take initiative.</p><p>\\\nIt is a hard work, and I know, there are days you want to sit through quietly with low energy levels. But these efforts usually pay off in the long term.</p><p>Communication is crucial for software engineers. Try to find a balance in what exactly you‚Äôre saying to other people and how.</p><p>\\\nFor example, during a cross-functional team standup where you have a product manager, a designer, a QA engineer ‚Äî meaning, people with different backgrounds and responsibilities, ‚Äî it‚Äôs better to filter out in your speech the things they won‚Äôt understand. Let‚Äôs look at the difference:</p><blockquote><p>Yesterday I was applying a patch to our delivery endpoint, when the Cosmos database got corrupted entities inside. I had to roll it back and change the way we insert the hash values in its tables, adding better sorting algorithms.</p></blockquote><blockquote><p>Yesterday I was having a hard time while working on a delivery task. There was a problem during a rollout which I immediately fixed. It was a beta environment, so we‚Äôre completely fine. In fact, I improved the way we work with it which we will see in our metrics next week.</p></blockquote><p>\\\nNotice how the second version leaves the tech terms out and focuses on the results that are perfectly clear to everyone? To practice this simpler type of communication try to ask yourself these questions:</p><ul><li>What do they really want to know?</li><li>What I am really trying to say?</li></ul><p>\\\nIt is a virtue to explain a complicated tech stuff in simpler terms. But the better you do it, the better is understanding of it by other people. In the end you‚Äôre gonna have less miscommunication problems, less uncertainties and less concerns.</p><p>\\\nAt last, you can always dig deeper into the tech things if it‚Äôs needed or asked. It is the perfect place to do it on the tech syncs with other engineers.</p><h2>4. Build a bridge with your manager</h2><p>This one does not depend entirely on you, of course, but in general, a good manager is someone who advocates you and your stakes in the eyes of the business. They are the ones who discuss promotions for teammates with upper management and make judgements on whether you are ready to take a bigger role and responsibilities. Hence, it‚Äôs in your interests to be in good relationships with your manager.</p><p>\\\nHow exactly can you plan on doing that? Well, try to put yourself into your manager‚Äôs shoes. What are their goals and what does the upper management expect from them? Plan a set of goals for you to achieve in the upcoming performance cycle and commit to them. Present them to the manager, discuss together and see them through. For example, take on a good refactoring project that will improve the product performance. By the way, a good sign of experienced engineer is to be able to delegate some tasks to others, be able to control the process and get the work done.</p><h2>5. Focused work over constant multitasking</h2><p>Let‚Äôs talk about the actual coding routine. Let‚Äôs say, you have a task pending to be done from our first tip. How do you approach it? Everywhere around you there are ‚Äúnoise‚Äú factors: meetings, constant Slack messages, etc. I find it to be more effective to shield yourself from external non-important things and let yourself to be in the context of your task preferably without unnecessary interruptions. Get rid of these non-mandatory meetings if possible and do not rush into answering on every Slack message right away. Of course, it doesn‚Äôt mean you should ignore DMs all the time, but it‚Äôs better to avoid disruptions of your ‚Äúthinking cycle‚Äú. What do I mean by that?</p><p>\\\nHere‚Äôs an example of how it can be done:</p><ul><li><p>You have an important bug to solve. There is a couple of theories you want to try.</p></li><li><p>You‚Äôre working on the first one. You applied a fix and ready to test if it works.</p></li><li><p>Then you receive a Slack message. By skimming through a notification (literally a second of your time) you know that it can wait.</p></li><li><p>You finish testing the theory. It doesn‚Äôt pan out, so next you‚Äôre gonna try the second one.</p></li><li><p>Then you take a break and answer on that message.</p></li><li><p>After coming back to the bug you‚Äôre ready to work on the next theory.</p></li></ul><p>In my experience it‚Äôs much harder to come back to something that you left partially unfinished, just left in the middle of the change. The more time in between the breaks, the harder is to get back that context later, ‚Äúload it into your RAM‚Äú if you will.</p><p>\\\nThis approach doesn‚Äôt mean that it‚Äôs ok to ghost your colleagues. No, but in many cases no one expects you to write back instantly. There‚Äôs definitely some grace courtesy period, so try to not overreach it. Sometimes, if they ask me to check something, and it takes time whereas my task is more important at that moment, I would let them know that I will check that and come back in an hour or so:</p><blockquote><p>Hey, sure, I‚Äôm gonna check this and come back to you in an hour. Hope it‚Äôs ok with you. It‚Äôs just that I have this high-priority task I‚Äôve been dealing with the whole day, and I need to finish it first.</p></blockquote><p>\\\nThis way you‚Äôre giving an actual ETA to your colleague and don‚Äôt leave them hanging while being polite and friendly in your communication.</p><p>I‚Äôve known very talented engineers with diverse technical backgrounds who were of tremendous help to me. It was a delight to learn from these knowledgeable people. But have they done great in their careers moving up the ladder? Maybe earning a promotion, more responsibilities and a bigger paycheck? Surprisingly, not all of them. One of the reasons it happened this way is because almost no one knew about their work except for the people in their small dev teams.</p><p>\\\nHonestly, if you work at a decent company with no micromanaging, no one is watching your pull requests under a microscope 24/7. And that small change that may be saved a company a significant amount of money can easily go unnoticed if not told properly by you to the stakeholders involved. Quite often these people are managers, product owners or heads of departments‚Ä¶ Meaning that they are not the tech people in the slightest. So, I advise you to not just show your golden PR, but present it in a way everyone in the company outside of your ‚Äútech bubble‚Äú would understand the meaning of it and more importantly, the company value. Presentation, nice graphs, real numbers ‚Äî all of that. Which leads us back to the 3rd point about soft skills and communication.</p><p>\\\nThere is a different side to this point that I hear sometimes in complains from fellow engineers. Like that person in your company who is good at self-marketing themselves and makes everything a much bigger thing than it actually was. They might have been praised more by leadership teams just because of their showcase skills. And just because of that it can be looked down upon. But hey, it will be beneficial to pick up a marketing trick or two from them, as well. Yes, ‚Äúselling‚Äù your work is also important.</p><p>\\\nIf your presentation skills are lacking, think about the ways you could improve it. I would make it as one of personal goals with a clear set of action points. See whether your manager can be of help here. Winking at the 4th tip over there :wink:</p><p>Last but not least, do not crunch extra hours. Do not overwork. Period.</p><p>\\\nIt depends on your company culture, but in some places management can be very manipulative in creating the ‚Äúhurry hurry hurry‚Äú type of atmosphere where NOT staying late in front of your screen is frowned upon. The bottom line is that I would avoid such companies in favor of a better work-life balance. In a nutshell, it means that the company does not value its employees and does not care about them.</p><p>\\\nI used to work extra hours before in my early years as an engineer. One time, we were told to release a huge and complicated feature in some fixed amount of time. The deadline was not realistic in a first place. We all knew about it, but for some reason that I already don‚Äôt remember of, the team had to get along with it. We crunched through multiple weekends, working at nights. Can you guess what was the result? We couldn‚Äôt make it, naturally. In the end, no one actually did anything about it, but I needed a month of no-work weekends after it to get recovered. As our team leader nicely said to me: ‚ÄúThis company would take as much of your time as you allow it to‚Äú.</p><p>\\\nAfter that incident and many similar ones before, I put my own personal boundaries of where and when the work that I‚Äôm being paid for starts and stops. Remember, if you work 5/7 officially, you have a number of hours you are paid for. Even if you work as a freelancer, it‚Äôs basically the same thing. There is always a ‚Äúmoney per hour‚Äù relation. If you do extra hours without extra pay, in most cases you do a disservice to yourself. Trust me, all this can easily lead to a burnout at some point that will be hard to come out of.</p><p>\\\nIn some rare occasions, it can be beneficial to do extra time, but only if there is a real profit out of it on the horizon. Extra pay, promotion, extra vacation days or some other form of retribution that is actually worth to do it for. But take it as a rule of thumb: make the most out of your working hours and work hard, but at the end of the day, when your clock is up, leave it to the next day. Value your time.</p><p>I hope that these tips were somewhat of help for you. Maybe some of them you already know about, but others did get you thinking on making improvements with. And that was the reason for writing down these thoughts ‚Äî to share my experience and see if it can be useful to others. Till the next time, keep growing and be better versions of yourselves!</p>","contentLength":12905,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Building Enterprise Angular Apps? You‚Äôre Doing It Wrong (Unless You Use Standalone Components)","url":"https://hackernoon.com/building-enterprise-angular-apps-youre-doing-it-wrong-unless-you-use-standalone-components?source=rss","date":1741767943,"author":"Tom Smykowski","guid":301,"unread":true,"content":"<p>Building scalable, enterprise Angular apps comes with some challenges, one of which is to organize project in a way, that something maintained by two-three developers will be good to be developed by several teams like this.</p><p>\\\nSo the organisation of the project is really important. Especially because an enterprise app can consist of anything between 100 and 500 components and same amount of services.</p><p>\\\nBy the way, my name is Tom Smykowski, I‚Äôm an expert in building scalable, enterprise applications and in this series I‚Äôll be teaching you how to do it!</p><p>\\\nBefore standalone components were introduced, you had to put every component, directive and pipe into a module. Here‚Äôs an example of a module:</p><pre><code>@NgModule({\n  declarations: [AppComponent, HeaderComponent],\n  imports: [CommonModule, FormsModule],\n  providers: [SomeService],\n  bootstrap: [AppComponent],\n})\nexport class AppModule {}\n</code></pre><p>\\\nThere were several problems with this approach. First of all, modules aren‚Äôt always necessary. Other libraries like React doesn‚Äôt require this construct. For a reason, it increases complexity, increases risk of circular dependencies, makes the codebase harder to understand (you have to figure out what module exports your component, especially difficult when working with 3rd party libraries), lazy loading was a bit complex, and well‚Ä¶ there was really a nice pattern to use them.</p><p>\\\nIf you put too much in a module, it gets bloated and your app suffers on performance, because Angular is unable to tree-shake them properly. If you create a module for every component‚Ä¶ what is the best option‚Ä¶ what‚Äôs the point of modules at all?</p><p>\\\nThese issues led to creation of standalone elements in Angular. Now you import a component, service, or a pipe directly where you use it, from where it‚Äôs defined:</p><pre><code>import { Component } from '@angular/core';\nimport { CommonModule } from '@angular/common';\nimport { MeasurementCardComponent } from './measurement-card.component';\nimport { UnitConverterPipe } from './unit-converter.pipe';\nimport { StatusDirective } from './status.directive';\n\n@Component({\n  selector: 'app-factory-dashboard',\n  imports: [\n    CommonModule,\n    MeasurementCardComponent,\n    UnitConverterPipe,\n    StatusDirective,\n  ],\n  // Add other metadata properties here if needed\n})\nexport class FactoryDashboardComponent {\n  // Component logic goes here\n}\n</code></pre><p>\\\nAs you can see, it‚Äôs easier to figure out and find dependencies of the component using standalone entities.</p><p>When you are not there yet with your codebase to use standalone entities, you have to know that in Angular 19 components are standalone by default. You don‚Äôt have to add standalone: true to the decorator:</p><pre><code>@Component({\n  selector: 'app-factory-dashboard',\n  standalone: true,\n  // Add other metadata properties here\n})\n</code></pre><p>\\\nYou can also use the schematic to automatically switch to standalone:</p><pre><code>ng generate @angular/core:standalone\n</code></pre><p>\\\nIt will also change other elements like bootstrapping and routing. So a lot of things will be taken care of automatically.</p><p>\\\nIn my experience the schematics works quite good. If you use some special things in your application, you may need to do some manual adjustments. But as long as you sticked to Angular standards, it should go smoothly.</p><p>\\\nTo summarize by updating and switching to standalone components, your app complexity will drop, maintainability increase, build size will drop and initial load time drop. All of these contribute strongly to building scalable Angular app.</p><p>\\\nIn my experience of working on Angular apps serving millions of users, standalone components are mature, and serving purpose they were designed for. So, I encourage you to switch to standalone entities.</p><p>\\\n<strong>I‚Äôve prepared a free checklist how to make your Angular app scalable and ready for enterprise demand. If you‚Äôre interested, you can get this&nbsp;.</strong></p><p>\\\nIf you have any questions, ask these in the comment section!</p>","contentLength":3899,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Founder of China‚Äôs New AI Model Says His Agent is More Autonomous Than Rivals'","url":"https://hackernoon.com/founder-of-chinas-new-ai-model-says-his-agent-is-more-autonomous-than-rivals?source=rss","date":1741767409,"author":"Akriti Galav","guid":300,"unread":true,"content":"<p>In the ever-changing world of artificial intelligence, there's always buzz about the \"next big thing.\" Yet every now and then, something revolutionary appears that totally just rewrites the rules. Right now, that innovation is Manus AI, a groundbreaking creation by a Chinese startup called Monica.</p><p>\\\nSince its official launch on March 6, 2025, Manus has quickly caught global attention, prompting intense conversations and excitement about what's next in AI technology. But why exactly is everyone talking about Manus?</p><h2>Introducing Manus: An AI That Actually Gets Things Done</h2><p>During Manus‚Äôs recent launch event, Peak, the co-founder and Chief Scientist of Manus, said that they have been quietly building what they believe is the next big evolution in AI. He further said,</p><blockquote><p>\"Today, we're launching an early preview of Manus, the first general AI agent. This isn't just another chat-based workflow but a truly autonomous agent that bridges the gap between conception and execution.\"</p></blockquote><p>\\\nUnlike traditional chatbots, Manus doesn‚Äôt merely answer questions‚Äîit independently completes entire tasks without continuous oversight.</p><p>\\\nPicture this: You‚Äôre planning a two-week trip to Japan and simply instruct Manus, \"Plan my trip and keep the budget under $2,000.\" Within moments, Manus independently scours travel websites, compares flights, arranges accommodations, and compiles a detailed itinerary‚Äîwithout any additional guidance from you.</p><p>\\\nBut the magic doesn't stop at travel planning. Manus can screen job resumes, conduct complex market research, build fully functioning websites, and even analyze financial data in real-time. As Peak confidently showcased during the demonstration:</p><blockquote><p>‚ÄúIn this example, we ask Manus to help screen resumes. I've just sent Manus a zip file containing 10 documents. Since each Manus session has its own virtual computer, it works just like a human‚Äîfirst unzipping the file, reading each r√©sum√© page-by-page, and compiling important information. It then ranks the candidates and, if you prefer, presents everything neatly in a spreadsheet.‚Äù</p></blockquote><p>\\\nManus is also capable of tackling complex research projects. Peak shared another example:</p><blockquote><p>‚ÄúWe asked Manus to filter New York properties based on safety, school quality, and budget constraints. It independently researched safe neighborhoods, investigated middle schools, wrote a Python script to handle budget calculations, and finally delivered a detailed report along with recommended properties.‚Äù</p></blockquote><p>\\\nThis kind of autonomy, performing intricate tasks, even while users are offline, makes Manus stand out.</p><h2>Why Manus AI is Creating So Much Buzz</h2><p>With heavyweights like OpenAI, Google, and Anthropic dominating AI headlines, Manus manages to stand apart for several reasons:</p><ul><li><p>Full Autonomy: Manus doesn‚Äôt need constant prompts. As Peak explained, ‚ÄúWhile other AI stops at generating ideas, Manus delivers results. It's built for long-term projects, continuing its work even after you log off.‚Äù  \\n </p></li><li><p>Complete Transparency: AI is often criticized as a ‚Äúblack box,‚Äù where nobody knows exactly how decisions get made. Manus addresses this head-on. \"We built Manus to openly show each step in its thinking process, making decisions clear and trustworthy,\" Peak noted during the demo.  \\n </p></li><li><p>Real-World Results: Manus doesn't just promise‚Äîit delivers. In recent benchmarks evaluating real-world scenarios, Manus has already surpassed OpenAI‚Äôs famed DeepResearch system. Peak highlighted, ‚ÄúBeyond benchmarks, Manus has proven its capabilities on real-world platforms like Upwork, Fiverr, and Kaggle.‚Äù</p></li></ul><h2>How Did Manus Suddenly Capture Everyone‚Äôs Attention?</h2><p>The rapid rise in Manus‚Äôs popularity echoes recent AI sensations like DeepSeek‚Äôs R1. But Manus might be even bigger. When Manus's demo video surfaced on social media platform X, it quickly went viral‚Äîwithin hours, it had racked up over 200,000 views. Tech enthusiasts worldwide scrambled for invitation codes. The video itself was astonishing: Manus seamlessly jumped between platforms like X and Telegram, managing over fifty screens simultaneously, effortlessly multitasking at a level previously unseen.</p><p>\\\nWhile the AI's capabilities are clear, its origins are still somewhat mysterious. The entrepreneur behind Manus is known only as Peak, a Chinese tech innovator famed previously for creating mammoth mobile browsers. Beyond this, not much is publicly known, adding to the intrigue and speculation around this technology.</p><h2>A Glimpse into the Future</h2><p>Manus isn't widely available yet‚Äîit‚Äôs still in an exclusive preview phase‚Äîbut even now, its impact is undeniable. Peak summed up its broader significance perfectly at the launch event:</p><blockquote><p>‚ÄúThe name Manus comes from the motto ‚ÄòMens et Manus‚Äô‚ÄîMind and Hand. It embodies our belief that knowledge must be applied to make a meaningful impact on the world. Manus aims to extend your capabilities, amplify your impact, and become the hand that brings your mind‚Äôs vision into reality.‚Äù</p></blockquote><p>\\\nAs Manus continues to evolve, it prompts some fascinating questions:</p><ul><li><p>Will fully autonomous AIs soon manage entire areas of our lives?</p></li><li><p>How might tools like Manus reshape our approach to work, creativity, or even daily decisions?</p></li></ul><p>Peak offered an inspiring final note:</p><blockquote><p>‚ÄúThe future isn't just about what AI can do for us‚Äîit's about how we'll collaborate, coexist, and build together.‚Äù</p></blockquote><p>\\\nOne thing is clear: Manus AI isn‚Äôt merely another passing tech trend. It‚Äôs a pivotal moment that could redefine what we believe is possible with artificial intelligence.</p><p>\\\nIn tech, it‚Äôs exactly these kinds of moments‚Äîfilled with uncertainty, potential, and endless possibilities‚Äîthat keep us dreaming, exploring, and innovating. And Manus just gave us a thrilling reason to look forward.</p>","contentLength":5763,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OpenAI Launches $50 million AI fund","url":"https://hackernoon.com/openai-launches-$50-million-ai-fund?source=rss","date":1741766476,"author":"This Week in AI Engineering","guid":299,"unread":true,"content":"<p>\\\nWelcome to the ninth edition of <strong>\"This Week in AI Engineering\"</strong>!</p><p>\\\nOpenAI launched a $50M funding connecting 15 research institutions, Inception Labs released Mercury with speeds 10x faster than current LLMs, Cohere For AI unveiled Aya Vision for multilingual capabilities, and Alibaba's QwQ-32B matches DeepSeek-R1 with far fewer parameters.</p><p>\\\nWith this, we‚Äôll also be talking about some must-know tools to make developing AI agents and apps easier.</p><h3><strong>NextGenAI: OpenAI's $50M Consortium Connecting 15 Research Institutions</strong></h3><p>OpenAI has launched , an alliance uniting 15 leading research institutions with $50M in funding to accelerate scientific breakthroughs and transform education through AI. The initiative provides research grants, compute resources, and API access to support academic innovation across disciplines.</p><ul><li> Direct access for model training, fine-tuning, and application development</li><li> Dedicated compute resources for university-led AI model development</li></ul><ul><li><strong>Cross-Institutional Collaboration:</strong> Shared resources and findings across consortium members</li><li> Student access to hands-on AI model training and application development</li><li> AI-powered manufacturing, energy, and healthcare advancement at Ohio State</li><li> Boston Public Library digitizing public domain materials for broader access</li></ul><ul><li> Harvard and Boston Children's Hospital accelerating rare disease diagnostics</li><li> Duke University conducting metascience research to identify high-impact AI fields</li><li> Texas A&amp;M implementing Generative AI Literacy Initiative</li><li> Oxford digitizing rare texts at Bodleian Library with OpenAI's API</li></ul><p>\\\nThe initiative expands on OpenAI's educational commitment following ChatGPT Edu's launch in May 2024. NextGenAI focuses specifically on providing API-level access and research funding to drive innovations in scientific research, university operations, and educational methodologies.</p><h3><strong>Mercury: Inception Labs Launches 10x Faster Diffusion LLMs</strong></h3><p>Inception Labs has released , the first commercial-scale diffusion large language model (dLLM) family that achieves output speeds faster than DeepSeek Coder V2 Lite, GPT-4o Mini, and Claude 3.5 Haiku. The technology demonstrates breakthrough performance with Mercury Coder running at over 1000 tokens per second on standard NVIDIA H100s.</p><ul><li> Coarse-to-fine diffusion process instead of traditional autoregressive generation</li><li> Transformer-based neural network that modifies multiple tokens in parallel</li><li> Compatible with existing NVIDIA GPUs without requiring specialized chips</li><li> Available via API and on-premise installations with fine-tuning support</li></ul><ul><li> 1109 tokens/second for Mercury Coder Mini vs 59 tokens/second for GPT-4o Mini</li><li> 88.0% for Mercury Coder Mini, matching GPT-4o Mini's 88.0%</li><li> 78.6% for Mercury Coder Mini vs 78.5% for GPT-4o Mini</li><li> 82.2% for Mercury Coder Mini, significantly outperforming GPT-4o Mini's 60.9%</li></ul><ul><li> 20x faster than some frontier models running below 50 tokens/second</li><li> Mercury Coder Small scores 90.0% on HumanEval, tied with Gemini 2.0 Flash-Lite</li><li> Second place in Copilot Arena, outperforming GPT-4o Mini and Gemini 1.5-Flash</li><li> 5-10x reduction in inference costs while maintaining competitive code quality</li></ul><p>\\\nThe models enable new capabilities for latency-sensitive applications that previously required compromising on model quality to meet speed requirements. Mercury's architecture allows continuous refinement of outputs to correct mistakes and hallucinations, similar to approaches used in leading image and video generation systems.</p><h3><strong>Aya Vision: Cohere For AI Launches State-of-the-Art Multilingual Vision Model</strong></h3><p>Cohere For AI has released , an advanced open-weight vision model that significantly expands multilingual and multimodal capabilities across 23 languages spoken by over half the world's population. The model excels in image captioning, visual question answering, and cross-modal translation tasks.</p><ul><li> Available in 8B and 32B parameter configurations</li><li> Processes 23 languages with consistent performance across linguistic domains</li><li> Combines synthetic annotations, translation rephrasing, and multimodal merging</li><li> Unified image and text understanding with cross-modal translation capabilities</li></ul><ul><li> Aya Vision 8B achieves up to 70% win rates against comparable models</li><li> 79% win rate in multilingual vision tasks for the 8B variant</li><li> Aya Vision 8B outperforms Llama-3.2 90B Vision with 63% win rates</li><li> 32B model outperforms models 2x its size (Llama-3.2 90B, Molmo 72B)</li></ul><ul><li> Performance scaling from 40.9% to 79.1% win rates through technical refinements</li><li> Open-sourced Aya Vision Benchmark for multilingual multimodal assessment</li><li> Optimized for researchers with limited computation resources</li><li> Free access via WhatsApp integration for global usability</li></ul><p>\\\nThe release includes open-weights for both model sizes on Kaggle and Hugging Face, continuing Cohere's expansion of multilingual AI research that began with the Aya initiative two years ago. The model builds upon Aya Expanse, supporting research collaboration across 3,000 researchers from 119 countries.</p><p>Alibaba has released , a new open-source reinforcement learning (RL) enhanced language model that achieves performance comparable to DeepSeek-R1 despite using significantly fewer parameters. The model demonstrates that strategic RL applications can dramatically close the performance gap with much larger models.</p><ul><li> 32B parameters versus DeepSeek-R1's 671B (37B activated)</li><li> Two-stage reinforcement learning with outcome-based rewards</li><li> Math and coding task optimization using accuracy verifiers</li><li> General capability enhancement with reward models and rule-based verifiers</li><li> Apache 2.0 open-source availability</li></ul><ul><li> 79.5% accuracy, matching DeepSeek-R1's 79.8%</li><li> 63.4% score compared to DeepSeek-R1's 65.9%</li><li> 73.1% performance versus 71.6% for DeepSeek-R1</li><li> 83.9% accuracy, comparable to R1's 83.3%</li><li> 65.4% score, outperforming R1's 60.3%</li></ul><p>\\\n<strong>Integration Capabilities:</strong></p><ul><li> Built-in agent capabilities for environmental interaction</li><li> Dynamic thought process adjustment based on feedback</li><li> Available through Hugging Face, ModelScope, and Alibaba Cloud DashScope</li><li> Accessible via Qwen Chat with Python integration examples</li></ul><p>\\\nAlibaba's team identifies this as an initial step toward developing more capable AGI systems by combining stronger foundation models with scaled RL and computational resources.</p><ul><li> is a technical platform that automates the creation of user interfaces (UIs) from JSON data. It generates UIs in HTML and Tailwind CSS, allowing users to copy and share the code via links. This tool streamlines UI development by providing a straightforward method to transform data into functional interfaces, enhancing productivity and collaboration among developers.</li><li> is a specialized search engine designed to efficiently scan and index public GitHub repositories. It enables users to quickly find specific code snippets, files, or functionalities within the vast ecosystem of open-source projects. By offering targeted search capabilities, GitSearch helps developers discover and leverage existing code, contributing to increased efficiency and collaboration in software development.</li><li> is an enterprise-grade AI solution designed to automate unit test generation and management for complex Java code. Unlike LLM-driven assistants, it uses reinforcement learning to produce reliable, executable, and correct test code, ensuring IP security through on-premises operation. It integrates into IntelliJ and CI pipelines, generating tests 250x faster than manual methods and increasing code coverage. Diffblue Cover aims to</li><li> is an AI-powered platform designed to help developers understand and modernize complex mainframe codebases. It leverages AI to uncover code insights, generate documentation for languages like COBOL and Assembly, and extract business logic from legacy systems. Swimm provides features such as visualizing program flows, identifying dependencies, and assessing the impact of changes. It aims to reduce mainframe complexity, create missing specs, and ensure secure, compliant, and scalable operations, with options for both cloud and on-premises deployment.</li></ul><p>And that wraps up this issue of \"<strong>This Week in AI Engineering.</strong>\" </p><p>\\\nThank you for tuning in! Be sure to share this with your fellow AI enthusiasts and follow for the latest weekly updates.</p><p>\\\nUntil next time, happy building!</p>","contentLength":8194,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Why Giving Up Is So Easy: 5 Common Reasons People Quit","url":"https://hackernoon.com/why-giving-up-is-so-easy-5-common-reasons-people-quit?source=rss","date":1741766407,"author":"Pawan Pratap Singh","guid":298,"unread":true,"content":"<blockquote><p><strong>‚ÄúNothing comes easy. Success doesn't just drop on your lap. You have to go out and fight for it every day.‚Äù \\n  \\n ‚Äï Dana White</strong></p></blockquote><p>\\\nIt has been proven time and time again that qualities like grit can make all the difference when it comes to achieving success‚Äîno matter how you define it or the kind of success you‚Äôre chasing. However, beyond grit, determination, and drive, there are countless distractions that can stand in your way.</p><p>\\\nThese distractions can crush your dreams and goals in an instant, draining every ounce of ambition and determination from within. They are the very obstacles that cause many of us to abandon our dreams too soon, regardless of how significant our aspirations may be.</p><p>\\\nIn this post, I will share with you the 5 simple reasons why people give up far too easily.</p><h2>1. Surrounded By People Who‚Äôve Given Up</h2><blockquote><p><strong>‚ÄúSurround yourself with people on the same mission as you.‚Äù</strong></p></blockquote><p>\\\nWho better to surround yourself with than people who‚Äôve given up on their life goals and dreams‚Ä¶ Doing this is the dumbest thing you can do if you expect more out of your life. What chance does an aspiring singer have if he/she is surrounded by people who‚Äôve quit on their own dreams and goals?</p><p>\\\nOr worse, these are the types of people they take bullshit advice from? These types of people will tell you things like:</p><ul><li>There‚Äôs no way you‚Äôll be successful.</li><li>If I didn‚Äôt succeed, you won‚Äôt either.</li><li>Nobody in our family/area is successful, so what makes you any different?</li><li>It‚Äôs nice to dream, but it‚Äôs not realistic.</li><li>Just get a basic job, start a family, and settle.</li><li>Your goals are unrealistic.</li></ul><p>\\\nThey‚Äôll tell you things like this because that‚Äôs their reality. And if they don‚Äôt tell you directly, they‚Äôll tell you through their behavior. They‚Äôre not giving you advice, they‚Äôre giving themselves advice. They‚Äôre not talking to you, they‚Äôre talking to a mirror.</p><p>\\\nSo, one way or another, you‚Äôll more than likely give up after a couple of months, or even after a short year‚Äîunless you change your circle of friends, associates, and the people you let into your life every day.</p><h2>2. Reading Too Many ‚Äú30 under 30‚Äù Billionaire Lists</h2><blockquote><p><strong>‚ÄúMoney is only a tool. It will take you wherever you wish, but it will not replace you as the driver.‚Äù</strong></p></blockquote><p>\\\nYou know the ones I‚Äôm talking about. The types of lists Forbes publishes every year or so. There‚Äôs nothing wrong with these lists, in fact, they‚Äôre inspirational, and there‚Äôs something we can learn from those who are on it.</p><p>\\\nBut when you get caught up in the thought of mimicking what they‚Äôve done, as quickly as they‚Äôve done it, you become delusional. The fact is almost 100% of people will never accumulate billions of pounds before they‚Äôre 30 years old. It‚Äôs not that it‚Äôs impossible, but the probability is low.</p><p>\\\nWhen you‚Äôre looking at the world through delusional eyes, you end up disappointed, depressed, failing, and questioning whether you‚Äôre good enough or not. Then your self-esteem starts taking a pounding like it‚Äôs just been in a fight with Mike Tyson.</p><p>\\\nAnd that‚Äôs when the thought of giving up starts creeping in before it becomes your reality. Having high expectations is NOT the same as having delusional expectations. Don‚Äôt get the two definitions confused.</p><h2>3. Not Having A Clear Purpose</h2><blockquote><p><strong>‚ÄúA lack of clarity could put the breaks on any journey to success.‚Äù</strong></p></blockquote><p>\\\nMost of us have no idea why we‚Äôre doing what we‚Äôre doing. I know I‚Äôve been there. Having no clear sense of purpose, or ‚Äúwhy‚Äù behind my actions is what caused me to procrastinate, and quit too fast and too soon.</p><p>\\\nJust imagine how disastrous that would be if the pilot flying tons of people from one country to another had NO idea what he/she was doing. Well, that‚Äôs exactly how disastrous it is to your life, your goals, your aims, and your dreams.</p><p>\\\nThis is <a href=\"https://www.lifelords.com/success/smart-goal-setting/\">why smart goal setting is essential</a>‚Äîit gives you direction and clarity, helping you stay focused on your purpose. Not knowing why, and not having a strong enough ‚Äúwhy‚Äù will cause YOUR plane to crash, burn, and explode until there‚Äôs nothing left of it.</p><p>\\\nThen that‚Äôll cause you to quit, give up, or worse, to never ever dare to try again. Write down WHY you‚Äôre doing what you‚Äôre doing, and WHY it matters. And If it doesn‚Äôt matter, then it‚Äôs your job to find something that does matter.</p><p>\\\nSomething that gives you a strong enough reason to persevere and see it through until completion. Something that you wake up thinking about and go to bed thinking about. And something that keeps you focused on what‚Äôs important instead of what‚Äôs irrelevant.</p><h2>4. Not Willing To Give Up Old Habits For Better Habits</h2><blockquote><p><strong>‚ÄúDon‚Äôt make a habit of choosing what feels good over what‚Äôs actually good for you.‚Äù</strong></p></blockquote><p>\\\nEver since I started my personal development journey 6+ years ago, I‚Äôve given up video games, watching too much TV, the mainstream news, and many other things. If I never gave up those habits, I wouldn‚Äôt even be sat writing out this article.</p><p>\\\nInstead, I‚Äôd be swimming in a pool of bad habits that are poisonous to my goals and my plans. And I would have shot my ambition in the head a long time ago.</p><p>\\\n<strong>Grant Cardone is a great example of this</strong>.</p><p>\\\nUntil the age of 25, he was taking drugs, hanging around druggies, and negative people. Then he completely turned his life around by changing his drug habits, and the types of people and things he surrounded himself with.</p><p>\\\nIf you‚Äôre not willing to give up old habits for habits that support your goals, then you may as well tear up the sheet of paper you wrote your goals down on. Or burn it. Because you‚Äôll only get in your way, waste time, and quit on yourself too soon and too early.</p><blockquote><p><strong>‚ÄúWithout promotion, something terrible happens‚Ä¶ Nothing.‚Äù</strong></p></blockquote><p>\\\nThere was a time when I was terrified of promoting myself. It wasn‚Äôt self-promotion itself that scared me; it was the opinions of other people that scared me.</p><p>\\\nWhat will they think of me? How will they respond? What if they criticize me? What if they don‚Äôt agree? The WHAT IF disease stood in my way and I wasn‚Äôt willing to fight back.</p><p>\\\nIf you don‚Äôt promote yourself, or worse, you promote yourself too little, then there‚Äôs no way you‚Äôll have the stomach to keep persevering and pursuing the things you want.</p><p>\\\nOne way or another you‚Äôll quit on yourself and make excuses as to why things aren‚Äôt working out for you, or why things will NEVER work out for you. You‚Äôll excuse yourself from the fact that you never promoted yourself enough, if at all.</p><p>\\\nNobody will give you what you want if you‚Äôre too afraid to ask for it. And there‚Äôs no achievement without the involvement of other people. Nothing is achieved alone.</p>","contentLength":6692,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Pasifika Web3 Tech Hub Promises Personal Freedom, Financial Sovereignty for Pacific Islands","url":"https://hackernoon.com/pasifika-web3-tech-hub-promises-personal-freedom-financial-sovereignty-for-pacific-islands?source=rss","date":1741766240,"author":"Edwin Liava'a","guid":297,"unread":true,"content":"<p>Today marks a watershed moment for our Pacific Island communities. With immense pride and profound hope, I officially launch the Pasifika Web3 Tech Hub i.e. a groundbreaking initiative designed to unite our cultural heritage with blockchain innovation.</p><p>\\\n<strong>On this day, I am giving you the key to your personal freedom and financial sovereignty.</strong> This isn't merely a technological project, it's a pathway to economic empowerment and cultural preservation for Pacific Islanders everywhere.</p><p>\\\nThe Pasifika Web3 Tech Hub is a DAO-governed, PASIFIKA token-powered marketplace built on Blockchain technology, connecting our island communities to global markets through a decentralized physical infrastructure (DePIN). This platform will enable us to share our data, digital content, traditional knowledge, handicrafts, and local produce with the world on our own terms, preserving our cultural identity while creating unprecedented economic opportunities.</p><h2>Moving Beyond Legacy Systems</h2><p>In recent days, many fellow Tongans have approached me asking if I would consider taking on CEO positions in public enterprises if such roles were advertised. Their queries echo similar questions about the Board of Directors positions I addressed recently.</p><p>\\\nMy response remains consistent i.e. Why would I want to inherit a mess orchestrated by other people? Let them fix their own problems. I have contributed enough already in the utilities sector, fighting uphill battles against outdated structures and resistant bureaucracies.</p><p>I have moved on. I am already in Web3, and I am now offering this future to all citizens.</p><p>\\\nJust as I outlined in my recent <a href=\"https://hackernoon.com/statutory-boards-represent-legacy-systems-that-have-outlived-their-purpose\">piece</a> about the obsolescence of the Public Enterprises Statutory Board of Directors, our progress demands that we leave behind systems that no longer serve their purpose. The traditional corporate structures that have failed to deliver prosperity to our people cannot be the vehicles for our future success.</p><h2>Building Rather Than Repairing</h2><p>Instead of pouring energy into fixing broken systems, I've chosen to build something entirely new i.e. a system designed specifically for our needs, values, and aspirations.</p><p>\\\nThe Pasifika Web3 Tech Hub represents a fundamental shift in thinking. Rather than creating more oversight layers, redundant bureaucracies, or positions of limited influence, we're establishing direct pathways to market, transparent governance, and genuine community ownership.</p><p>\\\nOur decentralized approach eliminates the very inefficiencies I've spent years pointing out in traditional governance structures. Where the current systems create barriers between producers and consumers, our platform creates bridges. Where existing frameworks drain resources through unnecessary administrative layers, our model channels resources directly to value creators.</p><h2>Your Invitation to True Sovereignty</h2><p>Today's launch is an invitation. An invitation to artisans to share their crafts with global markets. An invitation to farmers to receive fair compensation for their produce. An invitation to knowledge keepers to preserve and monetize our cultural wisdom on their own terms.</p><p>But most importantly, it's an invitation to all Pacific Islanders to claim ownership of their digital and economic future.</p><p>\\\nThe journey continues, but now with a clear destination i.e. a Pacific that leverages technology not just to participate in the global economy, but to lead it in ways that honor our values, strengthen our communities, and preserve our unique cultural heritage.</p><p>\\\nTo those who continue asking if I'll return to traditional roles within the existing system, my answer is clear i.e. I've chosen to build the future rather than repair the past. I invite you to join me on this journey toward genuine digital sovereignty and economic empowerment.</p><p>\\\nThe key to your personal freedom and financial sovereignty is now in your hands. The Pasifika Web3 Tech Hub is more than a platform, it's our collective declaration that we will chart our own course in the digital economy, on our own terms, with our cultural values intact.</p><p>\\\nThe Pasifika Web3 Tech Hub starts today, and it begins with us. You can start by reading our Constitution <a href=\"https://github.com/Pasifika-Web3-Tech-Hub/constitution\">here</a> on our GitHub repository.</p><p>\\\nWatch this <a href=\"https://github.com/Pasifika-Web3-Tech-Hub\">space</a> for updates</p><p><em>The Pasifika Web3 Tech Hub is a DAO-governed, PASIFIKA token-powered marketplace built on Blockchain, connecting island communities to global markets through decentralized infrastructure. We unite traditional knowledge with cutting-edge AI to preserve cultural heritage while creating economic opportunities across the Pacific.</em></p>","contentLength":4537,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI Coding Tools Are Bad News for Lazy Programmers","url":"https://hackernoon.com/ai-coding-tools-are-bad-news-for-lazy-programmers?source=rss","date":1741764985,"author":"Maximiliano Contieri","guid":296,"unread":true,"content":"<p><em>How AI is Redefining the Role of Programmers in the Tech Industry</em></p><blockquote><p>TL;DR: AI-generated code outperforms lazy programmers. Software jobs are declining, but centaurs will survive.</p></blockquote><p>\\\nToday, a friend of mine shared his frustration about being asked to fill out a registration form requiring a username and password.</p><p>\\\nThe system was poorly designed and demanded a secure password without explaining what the issue was.</p><p>\\\nThis is a clear example of terrible UX and a lazy programmer unaware of writing error messages with empathy.</p><p>\\\nI told my friend this is unacceptable in today's world since modern AIs can easily solve such problems.</p><p>\\\nI took my phone and said, \"L.M.P.T.F.Y.\" (Let Me Prompt That For You), the modern equivalent to <a href=\"https://en.wiktionary.org/wiki/LMGTFY\">LMGTFY</a>.</p><p>\\\nI quickly wrote a prompt:</p><blockquote><p>Give me the code for a secure password validation site with helpful error messages when the user enters a weak password.</p></blockquote><p>\\\nThe A.I. successfully provided a working page with this code:</p><pre><code>document.getElementById('passwordForm')\n  .addEventListener('submit', function(event) {\n  event.preventDefault(); \n  // Prevent the form from submitting\n  const password = document.getElementById('password').value; \n  // Get the password value\n  const errorMessages = document.getElementById('errorMessages'); \n  // Get the error messages container\n  errorMessages.innerHTML = ''; // Clear previous error messages\n\n  let errors = []; // Array to store error messages\n\n  // Validate minimum length\n  if (password.length &lt; 8) {\n    errors.push('The password must be at least 8 characters long.');\n  }\n\n  // Validate uppercase letters\n    if (!/[A-Z]/.test(password)) {\n      errors.push('The password must contain one uppercase letter.');\n    }\n\n    // Validate lowercase letters\n    if (!/[a-z]/.test(password)) {\n      errors.push('The password must contain one lowercase letter.');\n    }\n\n    // Validate numbers\n    if (!/[0-9]/.test(password)) {\n      errors.push('The password must contain one number.');\n    }\n\n    // Validate special characters\n    if (!/[!@#$%^&amp;*(),.?\":{}|&lt;&gt;]/.test(password)) {\n      errors.push('The password must contain one special character.');\n    }\n\n    // Display errors or success message\n    if (errors.length &gt; 0) {\n      errors.forEach(error =&gt; {\n          const p = document.createElement('p'); \n          // Create a new paragraph element\n          p.textContent = error; \n          // Set the error message as text\n          errorMessages.appendChild(p);\n          // Add the error message to the container\n      });\n    } else {\n        alert('Valid password!'); \n        // Show success message if no errors\n    }\n});\n</code></pre><p>\\\nYou can also try it yourself:</p><p>The AI-generated solution with helpful error messages is bad news for lazy programmers.</p><p>\\\nSoftware development jobs are already declining, and this trend is expected to continue:</p><p>This is something many people have been forecasting.</p><p>\\\nI wrote an article five years ago during the last <a href=\"https://en.wikipedia.org/wiki/AI_winter\">AI Winter</a> predicting this would happen.</p><p>As the great Niels Bohr once <a href=\"https://hackernoon.com/400-thought-provoking-software-engineering-quotes\">said</a>:</p><blockquote><p>Prediction is very difficult, especially about the future.</p></blockquote><p>Now, it's clear: lazy programmers are doomed!</p><p>What can we do as software engineers besides writing mediocre code?</p><p>Soon, there will be a shortage of handy people such as electricians, plumbers, and painters.</p><p>A.I. won't take your job. A developer mastering AI tools will.</p><p>\\\nIn these articles, you can compare the output of many AIs with and without guidance.</p><p>\\\nFor example, the above code has several problems unnoticed by AIs:</p><p>Humans remain invaluable when they know how to harness AI effectively.</p><p>\\\nHere's a video benchmarking some tools:</p><p>Hopefully, my friend will soon complete the password form ‚Äî or better yet developers will deprecate all passwords.</p><p>Also, I hope you'll write solutions like these and get paid as a \"Centaur\"- a developer who masters AI tools to enhance their craft.</p>","contentLength":3821,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Anonymous Sources: Starship Needs a Major Rebuild After Two Consecutive Failures","url":"https://science.slashdot.org/story/25/03/11/2159228/anonymous-sources-starship-needs-a-major-rebuild-after-two-consecutive-failures?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1741762800,"author":"BeauHD","guid":257,"unread":true,"content":"Longtime Slashdot reader schwit1 shares a report from Behind The Black: According to information at this tweet from anonymous sources, parts of Starship will likely require a major redesign due to the spacecraft's break-up shortly after stage separation on its last two test flights. These are the key take-aways, most of which focus on the redesign of the first version of Starship (V1) to create the V2 that flew unsuccessfully on those flights:\n\n- Hot separation also aggravates the situation in the compartment.\n- Not related to the flames from the Super Heavy during the booster turn.\n- This is a fundamental miscalculation in the design of the Starship V2 and the engine section. \n- The fuel lines, wiring for the engines and the power unit will be urgently redone.\n- The fate of S35 and S36 is still unclear. Either revision or scrap. \n- For the next ships, some processes may be paused in production until a decision on the design is made. \n- The team was rushed with fixes for S34, hence the nervous start. There was no need to rush. \n- The fixes will take much longer than 4-6 weeks.\n- Comprehensive ground testing with long-term fire tests is needed. [emphasis mine] \n\nIt must be emphasized that this information comes from leaks from anonymous sources, and could be significantly incorrect. It does however fit the circumstances, and suggests that the next test flight will not occur in April but will be delayed for an unknown period beyond.","contentLength":1454,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The TechBeat: Your Writing Has a Fingerprint‚ÄîAnd This Cutting Edge AI Model Can Identify It (3/12/2025)","url":"https://hackernoon.com/3-12-2025-techbeat?source=rss","date":1741759867,"author":"Techbeat","guid":295,"unread":true,"content":"<p>By <a href=\"https://hackernoon.com/u/superlinked\">@superlinked</a> [ 11 Min read ] \n In this tutorial, we‚Äôll guide you through the process of creating a movie recommendation system using vector databases.  <a href=\"https://hackernoon.com/stop-scrolling-start-building-create-your-own-ai-movie-recommender\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/contactraac\">@contactraac</a> [ 4 Min read ] \n real-world assets (RWAs) will lead the DeFi renaissance and help grow TVLs in 2025 through borrowing, collateralized lending, yield generation. <a href=\"https://hackernoon.com/why-rwas-will-lead-the-defi-renaissance-of-2025\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/benoitmalige\">@benoitmalige</a> [ 4 Min read ] \n Simplify your life with small changes to your space, time, and relationships for a lighter, focused you. <a href=\"https://hackernoon.com/this-5-step-framework-helps-declutter-your-life-from-areas-you-need-to-the-most\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/redact\">@redact</a> [ 3 Min read ] \n Musk claims a 'massive cyberattack' is behind a Twitter/X outage impacting thousands globally, but skepticism arises over the lack of evidence. Learn about the  <a href=\"https://hackernoon.com/x-outage-exposes-musks-poor-digital-hygiene\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/kamalsamaila\">@kamalsamaila</a> [ 4 Min read ] \n Trump flips on crypto, signs order for U.S. Bitcoin reserve worth $17.5B. Visionary move or risky bet? Explore the bold policy shift <a href=\"https://hackernoon.com/did-donald-trump-just-flip-the-script-on-crypto\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/hackernooncontests\">@hackernooncontests</a> [ 4 Min read ] \n 1 month left to enter Round 1 of the Spacecoin Writing Contest! Write about #decentralized-internet, #spacetech, #blockchain-use-case to compete for 15000 USDT! <a href=\"https://hackernoon.com/one-month-left-to-win-your-share-of-15000-usdt-in-round-1-of-the-spacecoin-writing-contest\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/hayday\">@hayday</a> [ 6 Min read ] \n It Takes More than Thinking: Humans Put the Vibe into Vibe Coding. An article about software engineers in a post-AI world, Vibeware and embracing ourselves <a href=\"https://hackernoon.com/it-takes-more-than-thinking-humans-put-the-vibe-into-vibe-coding\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/alexpiskarev\">@alexpiskarev</a> [ 9 Min read ] \n I built and shut down a ‚Ç¨500K neobank for immigrants in Portugal. Here's a post-mortem. <a href=\"https://hackernoon.com/i-built-a-euro500k-neobank-for-immigrantsit-didnt-go-as-planned\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/HuseynG\">@HuseynG</a> [ 7 Min read ] \n MCP (Model Context Protocol) is an open standard that allows AI systems to connect seamlessly with a wide variety of data sources. <a href=\"https://hackernoon.com/model-context-protocol-mcp-the-usb-c-of-ai-data-connectivity\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/4rkal\">@4rkal</a> [ 5 Min read ] \n In this article, I will be showing you how to deploy the GoTTH stack (Go Templ htmx tailwind) to production. <a href=\"https://hackernoon.com/how-to-deploy-go-templ-htmx-tailwindcss-to-production\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/mlsprwr1337\">@mlsprwr1337</a> [ 2 Min read ] \n Procedural generation with isometric Wave Function Collapse (WFC). Generate your own little microcosmos using a browser-based editor. <a href=\"https://hackernoon.com/this-browser-based-editor-generates-stunning-isometric-tile-maps-in-pixel-art-style\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/marcryan\">@marcryan</a> [ 5 Min read ] \n Understand the 4 types of synthetic data‚ÄîImputation, User Creation, Insights Modeling, and Manufactured Outcomes‚Äîto enhance AI, analytics, and market research <a href=\"https://hackernoon.com/everyone-in-ai-loves-synthetic-databut-no-one-can-agree-on-what-it-is\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/mcsee\">@mcsee</a> [ 3 Min read ] \n When you add flags like isTesting, you mix testing and production code. This creates hidden paths that are only active in tests. <a href=\"https://hackernoon.com/code-smell-293-you-should-avoid-adding-istesting-or-similar-flags\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/authoring\">@authoring</a> [ 5 Min read ] \n Using grammatical structures from parsed text, this study explores a new method for detecting authorship, improving accuracy in AI and fake text identification. <a href=\"https://hackernoon.com/your-writing-has-a-fingerprintand-this-cutting-edge-ai-model-can-spot-it\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/hennygewichers\">@hennygewichers</a> [ 6 Min read ] \n From pizza orders to smart home cameras, empathetic AI is now nudging our decisions in real time. Can we keep up? <a href=\"https://hackernoon.com/ai-chatbots-are-getting-too-good-at-making-you-say-yes\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/hackercm5drdgdo0000357l4uxtvhpm\">@hackercm5drdgdo0000357l4uxtvhpm</a> [ 6 Min read ] \n Explore how Agentic AI transitions beyond chatbots into autonomous automation, empowering businesses with innovative workflows, and enhanced productivity. <a href=\"https://hackernoon.com/agentic-ai-is-becoming-a-new-trend-in-silicon-valley\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/truewebber\">@truewebber</a> [ 9 Min read ] \n Discover why using a shared database table is an anti-pattern and how a ‚ÄòContract First‚Äô approach fosters clear ownership and smoother integration. <a href=\"https://hackernoon.com/a-table-as-an-api-illusions-and-reality\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/zbruceli\">@zbruceli</a> [ 11 Min read ] \n Part II of the series: use MCP and Solana AgentKit to build an AI Agent that can trade USD and EUR stablecoins. <a href=\"https://hackernoon.com/building-an-ai-trading-agent-using-anthropics-mcp-part-ii\">Read More.</a></p>","contentLength":3207,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Finding XPath Bugs in XML Document Processors: Testing XPath Functionality and Other Related Work","url":"https://hackernoon.com/finding-xpath-bugs-in-xml-document-processors-testing-xpath-functionality-and-other-related-work?source=rss","date":1741744415,"author":"XPath","guid":294,"unread":true,"content":"<p>While various related approaches to our work exist, to the best of our knowledge, we propose the first general approach to testing XML processors to find logic bugs. As discussed above, the most closely related work proposed testing the index support of SQLServer in the context of XPath and XQuery [41], which, to the best of our knowledge, is the only work that has tackled the test-oracle problem for XML processors, but is limited in scope.</p><p>\\\n<em>Testing XPath functionality.</em> Various approaches to benchmarking XPath implementations or test suites for them have been proposed, the most representative being XPathMark and the W3C qt3 test suite. XPathMark [25] is a benchmark for testing XML processors‚Äô XPath standard 1.0 functionality, containing both correctness as well as performance tests. The W3C qt3 test suite developed by the W3C XQuery and XSLT Working Groups [19] contains around 30,000 tests for XPath and XQuery targeting XPath 3.0 and later versions, which cover a broad range of functions and expressions.</p><p>\\\n<em>XML-related automated synthetic data generation.</em> Previous works have proposed approaches for automatically generating XML-related data, such as XML documents, XPath, and XQuery expressions. Aboulnaga et al. proposed an XML document generator to generate synthetic, but complex, structured XML data by introducing recursion and repetition on tag name assignment and controlling the element frequency distribution [20]. Rychnovsk√Ω and Holubov√° proposed an approach to generate XML documents related to given XPath queries from a specific XML schema to improve query efficiency [37], which is useful for developers to create micro-benchmarks for testing performance over certain XPath expressions. XQGen [42] is a tool for generating XPath queries that conform to a given XML schema, allowing users to specify multiple parameters, such as the percentage of empty queries desired and the percentage of queries with predicates. XPath generated by XQGen includes only direct node tests without introducing complex expressions, such as axes or function transformations. Similarly, the XQuery generator designed by Todic and Uzelac [41] includes XQuery FLWOR expressions, but the logic predicate consists only of simple operations, such as value comparisons. Neither of these works tackled the test oracle problem, and, as indicated by the results in Section 4.3, given their different focus, they cannot be effectively combined with a differential testing oracle.</p><p>\\\n<em>Targeted test case generation.</em> Many testing tools guide their test case generation process to improve testing efficiency, for random approaches such as random byte mutation used in fuzzing approaches generate a large proportion of invalid queries [47]. DynSQL [27] guides the fuzzing process of DBMSs towards increased code coverage and high statement validity. APOLLO [28] is a system for detecting performance regression bugs in DBMSs. It increases the probability of including components from previously encountered performance issues. Cynthia [39] was proposed to test Object Relational Mappers (ORMs) and generates targeted databases dependent on generated abstract SQL queries, which are likely to return non-empty results. Query Plan Guidance (QPG) [22] guides testing towards exploring more unique query plans.</p><p>\\\n The targeted node in XPress was inspired by the pivot row in Pivoted Query Synthesis (PQS) [36], which was originally proposed to test relational DBMSs. PQS‚Äô and XPress‚Äô commonality is that they select a random element, in PQS, a row in the database, while for XPress, a node in an XML document, based on which they generate a query that is guaranteed to fetch the element. However, both the purpose and use of the targeted node and pivot row differ. In PQS, the pivot row is used both for test-case generation and to construct the test oracle, by evaluating an expression and ensuring that it evaluates to true for the pivot row so that it can be used in a query that is guaranteed to fetch the row. Doing so requires a naive reimplementation of all the DBMSs‚Äô operators that should be tested, which incurs a high implementation effort, as highlighted in follow-up work [? ]. In XPress, the targeted node is used only for test-case generation, to improve testing efficiency and to ensure non-empty intermediate results; to this end, XPress uses the XML processor to determine the result of the expression, rather than requiring the reimplementation of operators. In addition, for predicate rectification, XPress provides operator-specific rules, rather than relying on a generic one, aiming to generate more interesting test cases. The high-level idea of a pivot element also inspired other works; for example, recent work on Android testing introduced the concept of a  [40].</p><p>(1) Shuxin Li, Southern University of Science and Technology China and Work done during an internship at the National University of Singapore (shuxin.li.lv@gmail.com);</p><p>(2) Manuel Rigger, National University of Singapore Singapore (rigger@nus.edu.sg).</p>","contentLength":5036,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"An Analysis of BaseX Historical Bug Reports","url":"https://hackernoon.com/an-analysis-of-basex-historical-bug-reports?source=rss","date":1741743566,"author":"XPath","guid":293,"unread":true,"content":"<h2>4.4 Analysis of BaseX Historical Bug Reports</h2><p>Unlike formal verification approaches, automatic testing approaches might miss bugs in the system tested. Due to the lack of ground truth, we cannot generally determine which bugs are overlooked by our approach. However, as a best-effort approach, we studied historical bug reports in order to determine whether XPress could have found them.</p><p>\\\n We analyzed all historical BaseX bug reports in its GitHub bug tracker. We selected BaseX, because the majority of issues are closed (1618 out of 1640). The issue tracker of BaseX is used for confirmed bug reports filtered from reports from the mailing list, and the BaseX maintainers carefully label and document them. For these reasons, it was easy to identify and classify the underlying problem of each bug report.</p><p>\\\n. We manually analyzed all historical bug issues until 2023 Apr 17 in BaseX, which were 1597 issues, after excluding the issues we reported. To confine the study of bug reports within the scope of XPath, we selected bug reports triggered by only XPath expressions. To determine whether a bug could be theoretically found by XPress, we mainly checked three aspects of the reports. For XPress to cover the test case, both the XML document and the XPath expression in the test case should not include any unimplemented functions or language features. Second, we could construct the sections and the predicate tree structure of XPress for involved predicates to form the pattern of the bug-inducing XPath expression. Third, XML processors should disagree on the result set. Note that this is a best-effort approach, because we might both incorrectly conclude that XPress might find a bug (e.g., it might be unlikely that the test case would be generated in practice) or incorrectly conclude that a bug cannot be found even when a different test-case within the reach of XPress would trigger the same underlying bug.</p><p>\\\n. Out of the total 78 bugs that we collected, we identified 20 bugs that could have been detected by XPress. For the other 58 bugs, we identified 4 kinds of bugs that XPress would have failed to find, namely due to (1) unimplemented functionalities (51 cases), (2) invalid inputs where the expected result would be an error (6 cases), (3) processors producing different results (2 cases), and (4) miscellaneous other issues (6 cases). Bugs belonging to more than one group are included in all involved groups. The differential testing oracle fails to detect the bugs with processors producing different results, while we consider the other categories mostly as implementation limitations in test-case generation. Therefore, out of all 78 bugs, 76 bugs (97%) could be detected through differential testing. This further demonstrates the effectiveness of employing a differential testing oracle for XPath-related testing.</p><p>\\\n<em>Unimplemented functionalities.</em> Most uncovered bug reports are due to unimplemented functionalities. Unsupported functions include constructors defined by the XML or XPath language standards, array and map functions, and also constructors of derived datatypes [2], such as xs:NMtokens. Given enough time, it would be straightforward to implement them in XPress. For/while loops, variable declaration, if-else conditional expressions, and self-defined functions are also unimplemented. These could be supported based on approaches that have been proposed in the context of compiler testing [32, 43]. Neither the XML documents nor XPath expressions that XPress constructs involve namespaces, which allow distinguishing items with the same tag name. They could be integrated into the XPress test-case generator. By implementing all these features, an additional 38 bugs (48%) could have been found.</p><p>\\\n Bug reports grouped into expected is error refers to invalid test cases, which are successfully executed instead of throwing an error. XPress constructs both syntactically and semantically valid expressions and therefore could not detect bugs within this category. However, the differential testing oracle could detect these bugs by comparing the errors of the different XML processors.</p><p>\\\n The different result category contains queries for which different processors intentionally produce different results, which shows the limitation of the differential testing oracle. One example is the function id, which selects nodes with xml:id attributes. BaseX takes attributes named as id as xml:id attributes, while Saxon and eXist-DB require an explicit declaration.</p><p>(1) Shuxin Li, Southern University of Science and Technology China and Work done during an internship at the National University of Singapore (shuxin.li.lv@gmail.com);</p><p>(2) Manuel Rigger, National University of Singapore Singapore (rigger@nus.edu.sg).</p>","contentLength":4749,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Index Support in BaseX, eXist-DB, Saxon, and libxml2: Explained","url":"https://hackernoon.com/index-support-in-basex-exist-db-saxon-and-libxml2-explained?source=rss","date":1741743276,"author":"XPath","guid":292,"unread":true,"content":"<p>We are aware of only one automated testing approach that has been proposed to test XML processors [41]. It tackled the test oracle problem by using differential testing by comparing the results of Microsoft‚Äôs SQLServer with and without using indexes. Their approach was specifically designed to test SQLServer‚Äôs index support and is not publicly available. Due to the narrow testing scope, and since the tool is not publicly available, we could not conduct experiments to directly compare the approaches. However, we further extended our tool to support differential testing with index configurations. Both approaches are complementary, as XPress could not only use differential testing among various XML processors, but also create or omit indexes to find additional bugs.</p><p>\\\n<em>Index support in BaseX, eXist-DB, Saxon, and libxml2.</em> Database indexes are data structures built to speed up data retrieval [31] and are DBMS-specific. Not all XML processors are DBMSs‚Äîas in-memory processors, Saxon and libxml2 lack support for indexes. BaseX and eXist-DB both enable structural indexes, such as storing all distinct paths of nodes by default. For value indexes to optimize querying on content values, BaseX creates text index and attribute index automatically. Users can further define additional indexes. Additionally, BaseX provides token indexes, which apply to specific functions, such as contains-token. eXist supports range indexes, which could be defined for specific nodes or attributes to speed up related comparison searches on their contents.</p><p>\\\n. We tested eXist‚Äôs range index and BaseX‚Äôs token index using the XPath expression generation approach as described in Section 3.2. Due to the found unfixed bugs in eXist, we conducted differential testing within eXist by checking the results with and</p><p>\\\nwithout range index definition. For BaseX, we defined a token index and compared its results directly with the results of Saxon.</p><p>\\\n. Throughout the testing method, we detected one additional bug for BaseX[10] and found no additional bugs in eXist. We reported the found bug shown in Figure 9 to the BaseX developers, who quickly fixed it. The query selects all nodes with tag name M in the document which holds attribute v that contains token \"a\". BaseX returned node M without token index, as expected, while unexpectedly returning an empty result set when not using an index. Overall, while the results suggest that using or removing indexes might find additional bugs, doing so had low effectiveness. A potential explanation could be that our test-case generation approach does not consider when indexes could be applied, which might result in low testing efficiency.</p><p>[10] https://github.com/BaseXdb/basex/issues/2222</p><p>(1) Shuxin Li, Southern University of Science and Technology China and Work done during an internship at the National University of Singapore (shuxin.li.lv@gmail.com);</p><p>(2) Manuel Rigger, National University of Singapore Singapore (rigger@nus.edu.sg).</p>","contentLength":2978,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Geothermal Could Power Nearly All New Data Centers Through 2030","url":"https://news.slashdot.org/story/25/03/11/2149222/geothermal-could-power-nearly-all-new-data-centers-through-2030?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1741742700,"author":"BeauHD","guid":255,"unread":true,"content":"An anonymous reader quotes a report from TechCrunch: There's a power crunch looming as AI and cloud providers ramp up data center construction. But a new report suggests that a solution lies beneath their foundations. Advanced geothermal power could supply nearly two-thirds of new data center demand by 2030, according to an analysis by the Rhodium Group. The additions would quadruple the amount of geothermal power capacity in the U.S. -- from 4 gigawatts to about 16 gigawatts -- while costing the same or less than what data center operators pay today. In the western U.S., where geothermal resources are more plentiful, the technology could provide 100% of new data center demand. Phoenix, for example, could add 3.8 gigawatts of data center capacity without building a single new conventional power plant.\n \nGeothermal resources have enormous potential to provide consistent power. Historically, geothermal power plants have been limited to places where Earth's heat seeps close to the surface. But advanced geothermal techniques could unlock 90 gigawatts of clean power in the U.S. alone, according to the U.S. Department of Energy. [...] Because geothermal power has very low running costs, its price is competitive with data centers' energy costs today, the Rhodium report said. When data centers are sited similarly to how they are today, a process that typically takes into account proximity to fiber optics and major metro areas, geothermal power costs just over $75 per megawatt hour. But when developers account for geothermal potential in their siting, the costs drop significantly, down to around $50 per megawatt hour.\n \nThe report assumes that new generating capacity would be \"behind the meter,\" which is what experts call power plants that are hooked up directly to a customer, bypassing the grid. Wait times for new power plants to connect to the grid can stretch on for years. As a result, behind the meter arrangements have become more appealing for data center operators who are scrambling to build new capacity.","contentLength":2037,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Finding XPath Bugs in XML Document Processors: Existing-Generator Baselines and More","url":"https://hackernoon.com/finding-xpath-bugs-in-xml-document-processors-existing-generator-baselines-and-more?source=rss","date":1741740853,"author":"XPath","guid":291,"unread":true,"content":"<p><em>Existing-generator baselines.</em> We considered the only two‚Äîto the best of our knowledge‚Äîapproaches to generate XPath expressions. Neither of them was specifically designed to be combined with a XPath test oracle. XQgen [42] generates XPath queries for micro benchmarking. Its generated predicates only check for sub-element existence. The XQuery generator designed by Todic and Uzelac [41] generates XPath queries for automatically testing index support in DBMSs. Given that indexes apply only to sargable queries (i.e., simple comparisons), the expressions it generates are simple. Both approaches generate XPath expressions based on an XML schema, while XPress generates XPath expressions based on the actual XML document. Based on this, we expect both of them to have low applicability for our differential-testing approach. Given that neither implementations are publicly available, we re-implemented them based on the description in the papers.</p><p>\\\n<em>Self-constructed baselines.</em> We also constructed our own baselines to investigate the efficiency of the separate components of XPress. XPress has two main components, namely (1) the targeted predicate generation by using the targeted node to refer to existing nodes and attributes and (2) the predicate rectification to avoid empty result sets. To evaluate the effect of the components individually, we enabled them individually to test whether they improve XPress‚Äôs bug detection efficiency.</p><p>\\\n. We considered four configurations for our selfconstructed baselines. Apart from our proposed approach introduced in Section 3.2 as (1) T<em>argeted, we derive configuration</em> (2) <em>Targeted without Rectification</em>, (3) <em>Untargeted with Rectification</em>, and (4) <em>Untargeted without Rectification</em>. In (2) Targeted without Rectification, we disable the rectification process, which would otherwise ensure targeted node selection. Since selecting a targeted node for predicate generation guidance always requires at least one node in the result set, we stop generating new sections after an empty result set is produced. In (3) <em>Untargeted with Rectification</em>, we generate predicates without using targeted node information to supply parameters that reference existent context and trigger corner cases for function nodes, while keeping the rectification to ensure that at least one node from the candidate set is included in the result set. In (4) <em>Untargeted without rectification,</em> we remove both components to generate predicates randomly, while omitting rectification.</p><p>\\\n. We set each baseline to run for 24 hours [30]. We repeated each experiment 10 times to account for potential performance deviations, and report the arithmetic mean for all metrics. As our testing target, we selected BaseX 10.4, which is the BaseX version that we first started testing. The reason for selecting BaseX as a representative is that we found most bugs in BaseX and all bugs were fixed, allowing us to determine the number of unique bugs we found in a testing campaign by deduplicating bug-inducing test cases automatically. Specifically, given two bug-inducing test cases, we could determine whether they trigger the same underlying bug by identifying their fix commits; only if their associated fix commit are different, do we consider the bugs unique. This is a best-effort technique, as, for example, one fix commit might address multiple bugs. We disabled the generation of the has-children functions as well as using relative XPath expressions in predicates, as they consistently lead to crashes, triggering known bugs.</p><p>\\\n<em>Results of existing generators.</em> Neither XQGen nor the Combined XML/XQuery generator found bugs in our experiment. This is expected, as previously proposed approaches were not designed for</p><p>\\\nautomated testing. As mentioned above, XQGen generates predicates that only check for element existence. The XQuery generator designed by Todic and Uzelac generates simple predicates that include at most one comparison operator.</p><p>\\\n<em>Results of different configurations.</em> As Figure 8 shows, our proposed approach, Targeted outperforms the other configurations. Within 24 hours, it found the most number of unique bugs (namely 12.5). Both configurations with targeted generation clearly outperformed the untargeted approaches, while rectification shows a similar performance in the speed of bug detection. As shown in Table 3, both targeted generation and rectification reduce the testing throughput, as they obtain intermediate results using the XML processor under test. Despite generating only 50% of the number of test cases as compared to (4) Untargeted without Rectification, (1) Targeted detected 20√ó more bug-inducing test cases and 2√ó more unique bugs. The results show that selecting a target node to guide the XPath generation process improves testing efficiency significantly. As observed above when discussing the small-scope hypothesis, most of the bugs that we found can be reproduced using a single section, explaining the limited effectiveness of rectification. However, we still believe that rectification is an important component, since without it, bugs requiring multiple sections with non-empty results could hardly be found.</p><p>\\\n We collected code coverage for three processors‚Äô core modules for XPress for 24 hours [30] of execution. The result is shown in Table 4. To put the numbers in relation, we collected coverage also for the projects‚Äô test suites; Saxon has no publicly available test suites and is therefore excluded. For the three XML processors, the line coverage ranged from 15% to 20%, and the</p><p>\\\nbranch coverage ranged from 10% to 16%. The coverage percentages are low, which is expected. The main reason for low code coverage is that XML processors typically also have other components than XPath processing. Taking BaseX as an example, around 21% of uncovered code was GUI-related, 10% was due to lack of full-text functionality support, and 5% were database commands. In Saxon, as another example, XSLT modules have not been covered. A further 18% uncovered code in BaseX involved unimplemented functions; it would be straightforward to implement many additional ones, such as math functions, but the many functions available would make this a tedious task. In Section 4.4, we detail unsupported XPath features, implementing which might allow us to find more bugs. XPress‚Äôs test-case generation process primarily aims at generating semantically valid expressions, which results in low error-checking branch coverage, quantifying which is difficult, as the relevant code is spread throughout the code base.</p><p>(1) Shuxin Li, Southern University of Science and Technology China and Work done during an internship at the National University of Singapore (shuxin.li.lv@gmail.com);</p><p>(2) Manuel Rigger, National University of Singapore Singapore (rigger@nus.edu.sg).</p>","contentLength":6823,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Team Behind Las Vegas Sphere Plans 5,000-Capacity 'Mini-Spheres'","url":"https://entertainment.slashdot.org/story/25/03/11/2143232/team-behind-las-vegas-sphere-plans-5000-capacity-mini-spheres?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1741740300,"author":"BeauHD","guid":254,"unread":true,"content":"Sphere Entertainment Co, the company behind the Las Vegas Sphere, said they are considering opening scaled-down versions of the immersive venue in other cities. AV Magazine reports: While this has been been feasible for its high-profile residencies such as U2, the Eagles, Dead &amp; Company and Anyma, smaller venues could attract a broader range of artists who might not have the budget or demand to fill the flagship Las Vegas location. By scaling down the size while retaining the signature technology, Sphere Entertainment Co can offer a similar spectacle at a more sustainable cost for artists and spectators.\n \nThe possibility of mini-Spheres follows news that a full-scale venue will open in the UAE as a result of a partnership between Sphere Entertainment Co and the Department of Culture and Tourism -- Abu Dhabi. Beyond concerts, the Las Vegas Sphere has proven successful with immersive films such as V-U2: An Immersive Concert Film and the Sphere Expeience featuring Darren Aronofsky's Postcard from Earth, which In January passed 1,000 screenings. \"As we enter a new fiscal year, we see significant opportunities to drive our Sphere business forward in Las Vegas and beyond,\" said Dolan. \"We believe we are on a path toward realizing our vision for this next-generation medium and generating long-term shareholder value.\"","contentLength":1332,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GStreamer 1.26 Released With Vulkan Improvements, H.266/VVC + LCEVC + JPEG-XS Support","url":"https://www.phoronix.com/news/GStreamer-1.26-Released","date":1741738790,"author":"Michael Larabel","guid":427,"unread":true,"content":"<article>GStreamer 1.26 is out today as the newest major feature release for this widely-used open-source multimedia framework...</article>","contentLength":120,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AMD Ryzen 9 9950X3D With 3D V-Cache Impresses In Launch Day Testing","url":"https://hardware.slashdot.org/story/25/03/11/2126204/amd-ryzen-9-9950x3d-with-3d-v-cache-impresses-in-launch-day-testing?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1741737720,"author":"BeauHD","guid":253,"unread":true,"content":"MojoKid writes: AMD just launched its latest flagship desktop processors, the Ryzen 9 9950X3D. Ryzen 9 9950X3D is a 16-core/32-thread, dual-CCD part with a base clock of 4.3GHz and a max boost clock of 5.7GHz. There's also 96MB of second-gen 3D V-Cache on board. Standard Ryzen 9000 series processors feature 32MB of L3 cache per compute die, but with the Ryzen 9 9950X3D, one compute die is outfitted with an additional 96MB of 3D V-Cache, bringing the total L3 up to 128MB (144MB total cache). The CCD outfitted with 3D V-Cache operates at more conservative voltages and frequencies, but the bare compute die is unencumbered.\n \nThe Ryzen 9 9950X3D turns out to be a high-performance, no-compromise desktop processor. Its complement of 3D V-Cache provides tangible benefits in gaming, and AMD's continued work on the platform's firmware and driver software ensures that even with the Ryzen 9 9950X3D's asymmetrical CCD configuration, performance is strong across the board. At $699, it's not cheap but its a great CPU for gaming and content creation, and one of the most powerful standard desktop CPUs money can buy currently.","contentLength":1127,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Microsoft is Replacing Remote Desktop With Its New Windows App","url":"https://it.slashdot.org/story/25/03/11/2037221/microsoft-is-replacing-remote-desktop-with-its-new-windows-app?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1741735200,"author":"msmash","guid":252,"unread":true,"content":"Microsoft is ending support of its Remote Desktop app for Windows on May 27th. From a report: If you use the Remote Desktop app to connect to Windows 365, Azure Virtual Desktop, or Microsoft Dev Box machines then you'll have to transition to the Windows app instead. \n\nThe new Windows app, which launched in September, includes multimonitor support, dynamic display resolutions, and easy access to cloud PCs and virtual desktops. Microsoft says \"connections to Windows 365, Azure Virtual Desktop, and Microsoft Dev Box via the Remote Desktop app from the Microsoft Store will be blocked after May 27th, 2025.\"","contentLength":609,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Preprint Sites bioRxiv and medRxiv Launch New Era of Independence","url":"https://science.slashdot.org/story/25/03/11/2031217/preprint-sites-biorxiv-and-medrxiv-launch-new-era-of-independence?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1741732800,"author":"msmash","guid":251,"unread":true,"content":"A new chapter has begun for two of the world's most popular preprint platforms, bioRxiv and medRxiv, with the launch of a non-profit organization that will manage them, their co-founders announced today. From a report: The servers allow researchers to share manuscripts for free before peer review and have become an integral part of publishing biology and medical research. Until now, they had been managed by Cold Spring Harbor Laboratory (CSHL) in New York. The new organization, named openRxiv, will have a board of directors and a scientific and medical advisory board. It is supported by a fresh US$16-million grant from the Chan Zuckerberg Initiative (CZI), the projects' main financial backer. \n\n\"It's just exciting to see this key piece of infrastructure really get the attention that it deserves as a dedicated initiative,\" says Katie Corker, executive director of ASAPbio, a scientist-driven non-profit organization, which is based in San Francisco, California. Preprints are \"the backbone of the scientific publishing ecosystem, maybe especially at the current moment, when there's a lot of worries about who has control of information.\" \n\nThe launch of openRxiv \"reflects a maturation of the projects,\" which started as an experiment at CSHL, says Richard Sever, a co-founder of both servers and chief science and strategy officer at openRxiv. It has \"become so important that they should have their own organization running them, which is focused on the long-term sustainability of the servers, as opposed to being a side project within a big research institution,\" says Sever.","contentLength":1591,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OpenAI pushes AI agent capabilities with new developer API","url":"https://arstechnica.com/ai/2025/03/openai-pushes-ai-agent-capabilities-with-new-developer-api/","date":1741725737,"author":"Benj Edwards","guid":479,"unread":true,"content":"<p>The AI industry is doing its best to will \"agents\"‚Äîpieces of AI-driven software that can perform multistep actions on your behalf‚Äîinto reality. Several tech companies, <a href=\"https://arstechnica.com/information-technology/2024/12/google-goes-agentic-with-gemini-2-0s-ambitious-ai-agent-features/\">including Google</a>, have emphasized agentic features recently, and in January, OpenAI CEO Sam Altman <a href=\"https://arstechnica.com/information-technology/2025/01/sam-altman-says-we-are-now-confident-we-know-how-to-build-agi/\">wrote</a> that 2025 would be the year AI agents \"join the workforce.\"</p><p>OpenAI is working to make that promise happen. On Tuesday, OpenAI <a href=\"https://openai.com/index/new-tools-for-building-agents/\">unveiled</a> a new \"<a href=\"https://platform.openai.com/docs/quickstart?api-mode=responses\">Responses API</a>\" designed to help software developers create AI agents that can perform tasks independently using the company's AI models. The Responses API will eventually replace the current <a href=\"https://platform.openai.com/docs/api-reference/assistants\">Assistants API</a>, which OpenAI plans to retire in the first half of 2026.</p><p>With the new offering, users can develop custom AI agents that <a href=\"https://platform.openai.com/docs/guides/tools-file-search\">scan company files</a> with a file search utility that rapidly checks company databases (with OpenAI promising not to train its models on these files) and <a href=\"https://platform.openai.com/docs/guides/tools-computer-use\">navigates websites</a>‚Äîsimilar to functions available through <a href=\"https://arstechnica.com/ai/2025/01/openai-launches-operator-an-ai-agent-that-can-operate-your-computer/\">OpenAI's Operator</a> agent, whose underlying Computer-Using Agent (CUA) model developers can also access to enable automation of tasks like data entry and other operations.</p>","contentLength":1123,"flags":null,"enclosureUrl":"https://cdn.arstechnica.net/wp-content/uploads/2025/03/agent_test-1152x648.jpg","enclosureMime":"","commentsUrl":null},{"title":"Apple patches 0-day exploited in ‚Äúextremely sophisticated attack‚Äù","url":"https://arstechnica.com/security/2025/03/apple-patches-0-day-exploited-in-extremely-sophisticated-attack/","date":1741724771,"author":"Dan Goodin","guid":478,"unread":true,"content":"<p>Apple on Tuesday patched a critical zero-day vulnerability in virtually all iPhones and iPad models it supports and said it may have been exploited in ‚Äúan extremely sophisticated attack against specific targeted individuals‚Äù using older versions of iOS.</p><p>The vulnerability, tracked as CVE-2025-24201, resides in Webkit, the browser engine driving Safari and all other browsers developed for iPhones and iPads. Devices affected include the iPhone XS and later, iPad Pro 13-inch, iPad Pro 12.9-inch 3rd generation and later, iPad Pro 11-inch 1st generation and later, iPad Air 3rd generation and later, iPad 7th generation and later, and iPad mini 5th generation and later. The vulnerability stems from a bug that wrote to out-of-bounds memory locations.</p><p>‚ÄúImpact: Maliciously crafted web content may be able to break out of Web Content sandbox,‚Äù Apple wrote in a bare-bones <a href=\"https://support.apple.com/en-us/122281\">advisory</a>. ‚ÄúThis is a supplementary fix for an attack that was blocked in iOS 17.2. (Apple is aware of a report that this issue may have been exploited in an extremely sophisticated attack against specific targeted individuals on versions of iOS before iOS 17.2.)‚Äù</p>","contentLength":1144,"flags":null,"enclosureUrl":"https://cdn.arstechnica.net/wp-content/uploads/2025/03/iPhone-16e-notch-1152x648.jpg","enclosureMime":"","commentsUrl":null},{"title":"Intel Overclocking Watchdog Driver Posted For The Linux Kernel","url":"https://www.phoronix.com/news/Intel-Overclocking-Watchdog","date":1741716600,"author":"Michael Larabel","guid":426,"unread":true,"content":"<article>An unexpected patch on the Linux kernel mailing list today by a Siemens engineer is implementing a driver for the Intel Over-Clocking Watchdog...</article>","contentLength":145,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Nouveau On NVIDIA Turing GPUs & Newer Will Now Prefer NVK+Zink For OpenGL","url":"https://www.phoronix.com/news/Nouveau-Turing-Zink-NVK-OpenGL","date":1741711200,"author":"Michael Larabel","guid":425,"unread":true,"content":"<article>As a sign of the times for both the NVK open-source NVIDIA Vulkan driver within Mesa and the generic Zink OpenGL-on-Vulkan code, with next quarter's Mesa 25.1 release when using a NVIDIA Turing GPU or newer with the Nouveau driver stack it will now default to using Zink atop NVK for OpenGL rather than the existing NVC0 Gallium3D driver...</article>","contentLength":340,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"CrossOver 25.0 Announced - Built Atop Wine 10.0 For Linux & macOS","url":"https://www.phoronix.com/news/CrossOver-25.0-Released","date":1741706379,"author":"Michael Larabel","guid":424,"unread":true,"content":"<article>CodeWeavers that continues to be the largest patron to the development of the open-source Wine software announced today CrossOver 25.0 as the newest version of their commercial downstream...</article>","contentLength":190,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Google's Pixel Watch Knows When Your Heart Stops Beating","url":"https://spectrum.ieee.org/heart-attack-smartwatch","date":1741703403,"author":"Elissa Welle","guid":155,"unread":true,"content":"<p>The feature set to roll out in the US this month will call 911 if you can‚Äôt</p>","contentLength":77,"flags":null,"enclosureUrl":"https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NjcwNTY3Mi9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc3Mjg3MTEyOH0.GzE5L4Fb8ueiISIYEvX3bBMzEfsD14la9xuOopm36Mk/image.jpg?width=600","enclosureMime":"","commentsUrl":null},{"title":"AMD Ryzen 9 9950X3D Delivers Excellent Performance For Linux Developers, Creators & Technical Computing","url":"https://www.phoronix.com/review/amd-ryzen-9-9950x3d-linux","date":1741698000,"author":"Michael Larabel","guid":423,"unread":true,"content":"<article>Ahead of tomorrow's availability of the Ryzen 9 9900X3D and Ryzen 9 9950X3D CPUs in retail channels, today the embargo lifts on being able to deliver Ryzen 9 9950X3D reviews and performance benchmarks. Simply put, for Linux creators, developers, enthusiasts, and others running technical computing workloads and other similar tasks on their desktop, the Ryzen 9 9950X3D with its 16 cores / 32 threads and 144MB total cache makes for an excellent desktop CPU. In this review are around 400 Linux benchmarks looking at the captivating performance and competitive power efficiency of the AMD Ryzen 9 9950X3D.</article>","contentLength":605,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GNOME Dash To Panel Extension Development Being \"Passed On\"","url":"https://www.phoronix.com/news/GNOME-Dash-To-Panel-Pause","date":1741697554,"author":"Michael Larabel","guid":422,"unread":true,"content":"<article>The GNOME Dash To Panel extension that allows moving the dash into the GNOME main panel has proven popular with GNOME desktop users for an integrated icon taskbar and status panel on GNOME Shell. Unfortunately though one of the main developers to Charles Gagnon is \"passing on\" development of the extension moving forward...</article>","contentLength":324,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Microsoft‚Äôs Muse AI Edits Video Games on the Fly","url":"https://spectrum.ieee.org/ai-video-games","date":1741694403,"author":"Matthew S. Smith","guid":154,"unread":true,"content":"<p>Muse is a proof of concept for more consistent AI gameplay</p>","contentLength":58,"flags":null,"enclosureUrl":"https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81Njc1NjE4Ny9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc2OTg5MjY5MH0.b8svwZw9hhFzoCD1y3INt5Zdwo9jCK1sB1mO7oKUG_E/image.jpg?width=600","enclosureMime":"","commentsUrl":null},{"title":"Why extracting data from PDFs is still a nightmare for data experts","url":"https://arstechnica.com/ai/2025/03/why-extracting-data-from-pdfs-is-still-a-nightmare-for-data-experts/","date":1741691736,"author":"Benj Edwards","guid":477,"unread":true,"content":"<p>For years, businesses, governments, and researchers have struggled with a persistent problem: How to extract usable data from Portable Document Format (PDF) files. These digital documents serve as containers for everything from scientific research to government records, but their rigid formats <a href=\"https://arxiv.org/html/2410.21169v1\">often trap the data inside</a>, making it difficult for machines to read and analyze.</p><p>\"Part of the problem is that PDFs are a creature of a time when print layout was a big influence on publishing software, and PDFs are more of a 'print' product than a digital one,\" <a href=\"https://merrill.umd.edu/directory/derek-willis\">Derek Willis</a>, a lecturer in Data and Computational Journalism at the University of Maryland, wrote in an email to Ars Technica. \"The main issue is that many PDFs are simply pictures of information, which means you need Optical Character Recognition software to turn those pictures into data, especially when the original is old or includes handwriting.\"</p><p><a href=\"https://en.wikipedia.org/wiki/Computational_journalism\">Computational journalism</a> is a field where traditional reporting techniques merge with data analysis, coding, and algorithmic thinking to uncover stories that might otherwise remain hidden in large datasets, which makes unlocking that data a particular interest for Willis.</p>","contentLength":1182,"flags":null,"enclosureUrl":"https://cdn.arstechnica.net/wp-content/uploads/2025/03/digitizing_a_book_header_3-1152x648.jpg","enclosureMime":"","commentsUrl":null},{"title":"UK Greenlights Amazon Kuiper, Starlink Faces New Rival","url":"https://spectrum.ieee.org/starlink-internet-kuiper-competition","date":1741690803,"author":"Margo Anderson","guid":153,"unread":true,"content":"<p>Satellite broadband tech squares off, while also trying to avoid collisions</p>","contentLength":75,"flags":null,"enclosureUrl":"https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NjYzMTg2Ni9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc3MDc2MjA5Nn0.bmrElVf6-ehF4bT2-fpNTfzZKS4uhqwZHxx_YwvZ1bM/image.jpg?width=600","enclosureMime":"","commentsUrl":null},{"title":"COBOL Language Frontend Merged For GCC 15 Compiler","url":"https://www.phoronix.com/news/GCC-15-Merges-COBOL","date":1741688520,"author":"Michael Larabel","guid":421,"unread":true,"content":"<article>A big albeit late feature landed today for the upcoming GCC 15 compiler... The COBOL programming language front-end has been merged!..</article>","contentLength":134,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linux Kernel Patches Posted For The ESWIN EIC7700 SoC + SiFive HiFive Premier P550","url":"https://www.phoronix.com/news/Linux-Patches-EIC7700-HiFive","date":1741687851,"author":"Michael Larabel","guid":420,"unread":true,"content":"<article>Patches were posted to the Linux Kernel Mailing List this morning for wiring up the ESWIN EIC7700 RISC-V SoC support and the most notable board using this SoC so far, the SiFive HiFive Premier P550...</article>","contentLength":200,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Servo Makes Improvements To Its Demo Browser & Embedding API","url":"https://www.phoronix.com/news/Servo-February-2025","date":1741686939,"author":"Michael Larabel","guid":419,"unread":true,"content":"<article>The Servo open-source web engine is out with its February 2025 status update to highlight work on the engine itself as well as its demo browser and embed API capabilities for using Servo by other applications...</article>","contentLength":211,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AMD Announces The EPYC Embedded 9005 Series","url":"https://www.phoronix.com/news/AMD-EPYC-Embedded-9005-Turin","date":1741684290,"author":"Michael Larabel","guid":418,"unread":true,"content":"<article>Since last year we have continued to be impressed by the AMD EPYC 9005 \"Turin\" server processors while today they are announcing the EPYC Embedded 9005 line-up. The AMD EPYC Embedded 9005 Series processors are much like the EPYC 9005 series processors but with a few differences...</article>","contentLength":281,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FreeBSD 13.5 Released With Device Driver Updates & Fixes","url":"https://www.phoronix.com/news/FreeBSD-13.5-Released","date":1741654405,"author":"Michael Larabel","guid":417,"unread":true,"content":"<article>FreeBSD 13.5 is out today as the final update to the FreeBSD 13 series. Users should begin making plans for upgrading to the current FreeBSD 14 stable series or eyeing the future FreeBSD 15.0 release...</article>","contentLength":202,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OpenZFS 2.3.1 Released With Linux 6.13 Compatibility, Many Fixes","url":"https://www.phoronix.com/news/OpenZFS-2.3.1-Released","date":1741639981,"author":"Michael Larabel","guid":416,"unread":true,"content":"<article>Building off the big OpenZFS 2.3 feature release from January, OpenZFS 2.3.1 is out today with Linux 6.13 kernel compatibility as well as various bug fixes...</article>","contentLength":158,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Mir 2.20 Brings Focus Stealing Prevention, Workaround/Quirk Fixes","url":"https://www.phoronix.com/news/Mir-2.20-Released","date":1741633348,"author":"Michael Larabel","guid":415,"unread":true,"content":"<article>Mir 2.20 is out today as the newest version of this Canonical-developed Wayland compositor and set of libraries for developing Wayland-based shells...</article>","contentLength":150,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["tech"]}