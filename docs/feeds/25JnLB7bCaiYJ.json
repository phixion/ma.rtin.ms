{"id":"25JnLB7bCaiYJ","title":"Tech News","displayTitle":"Tech News","url":"","feedLink":"","isQuery":true,"isEmpty":false,"isHidden":false,"itemCount":77,"items":[{"title":"Coinbase CEO Brian Armstrong trolls the prediction markets","url":"https://techcrunch.com/2025/11/01/coinbase-ceo-brian-armstrong-trolls-the-prediction-markets/","date":1762016348,"author":"Anthony Ha","guid":274,"unread":true,"content":"<article>While Armstrong may have helped some Kalshi and Polymarket users make a little money, he was also illustrating how easily these markets can be manipulated.</article>","contentLength":155,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Did a Weather Balloon, Not a Mysterious Space Object, Strike That United Airlines Flight?","url":"https://tech.slashdot.org/story/25/11/01/0615237/did-a-weather-balloon-not-a-mysterious-space-object-strike-that-united-airlines-flight?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1762014840,"author":"EditorDavid","guid":244,"unread":true,"content":"Slashdot reader joshuark shares this report from SFGate:\n\n\nThe mystery object that struck a plane at 36,000 feet is likely not space debris, as some speculated, but rather a Silicon Valley test project gone wrong... \n\nWindBorne Systems, a Palo Alto startup that uses atmospheric balloons to collect weather data for AI-based forecast models,has come forward to say that they believe they may be responsible for the object that hit the windshield... \"At 6am PT, we sent our preliminary investigation to both NTSB and FAA, and are working with both of them to investigate further,\" [WindBorne's CEO John Dean posted on social media...]\n WindBorne said the company has launched more than 4,000 balloons and that it coordinates with the Federal Aviation Administration for every launch. \n\nWindBorne \"has conducted more than 4,000 launches,\" the company said in a statement, noting that they've always coordinated those launched with America's Federal Aviation Administration and filed aviation alerts for every launched balloon. Plus \"The system is designed to be safe in the event of a midair collision... Our balloon is 2.4 pounds at launch and gets lighter throughout flight.\"\n\n\nWe are working closely with the FAA on this matter. We immediately rolled out changes to minimize time spent between 30,000 and 40,000 feet. These changes are already live with immediate effect. Additionally, we are further accelerating our plans to use live flight data to autonomously avoid planes, even if the planes are at a non-standard altitude. We are also actively working on new hardware designs to further reduce impact force magnitude and concentration.","contentLength":1642,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Rising energy prices put AI and data centers in the crosshairs","url":"https://techcrunch.com/2025/11/01/rising-energy-prices-put-ai-and-data-centers-in-the-crosshairs/","date":1762013700,"author":"Tim De Chant","guid":273,"unread":true,"content":"<article>A majority of consumers say they’re worried about data centers driving up electricity costs. Is the industry prepared for a possible backlash?</article>","contentLength":144,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Elaborate Hoaxes in the Age of AI","url":"https://hackernoon.com/elaborate-hoaxes-in-the-age-of-ai?source=rss","date":1762012807,"author":"Jacob Landry","guid":309,"unread":true,"content":"<p>This week, I’ve seen a lot of over-dramatization of very simple factual events that seem to be fueled by AI in many ways. Now, these aren’t “caused” by AI; I’m not referring to hoaxes that people have used AI specifically to spread, but things that AI has made worse by the ease with which fake information can be made to look very, very real.</p><p>Fifteen years ago, this problem existed, but in my opinion, was severely muted. The concept of biased news is not new by any means and has been an issue for as long as the news has existed. There have always been audiences that are more susceptible to believing in these invented stories, scenarios, and scams, and the media has always catered to them, guiding them to the water they wish them to drink. </p><p>\\\nMy concern, and reason for this brain dump, is that with AI, these evil parties seem to be able to cast a much wider net than they could before. They can twist real news into something it’s not with fake videos made by AI; they can pump the internet full of AI-generated content that says whatever they want and cites other AI-generated sources, and they can mobilize an army of influencers that spread their filth like wildfire in an instant.</p><p>\\\nI’ve found that recently, a huge chunk of my time when consuming any form of media is spent asking, “Is this real?” I consistently have to find multiple sources and manually scan them, looking for clues that it was AI-generated, a task that is getting harder and harder by the week. The videos are getting more realistic, the content is written better, and the sources I'm used to relying on are less and less trustworthy.</p><p>A group of protestors (in this case, trolls) showed up at Chicago’s Bean with claims that there was a man trapped inside. The protestors claimed that they had found evidence that a wealth of life-support systems had been purchased during the making of the Bean statue, and also attempted to make a connection to a potential missing person (a baby, I believe) around the time of its construction. Trolls exist. They always have and always will. That’s not the issue at hand here. The issue is what happened next. This group was clearly trying to be funny, just causing a stir with some radical idea for their own amusement, but the internet used AI to take the country by storm.</p><p>\\\nWhile scrolling, I started to see dozens of videos with screenshots of these purchase records, x-ray footage of a person floating inside the Bean, and “eyewitness” reports from someone who claimed they could hear knocking or scratching coming from inside the structure. There were also videos of the Bean being constructed, where you could clearly see the equipment being placed inside. Most, if not all, of these were generated by AI and are completely fake. I knew this, being a sensible human, but I had to admit that the quality created compelling evidence. With less common sense, I would have been easily duped.</p><p>The recent discovery of 3I/ATLAS has been a goldmine for AI generators and the conspiracy-loving masses. From what I could find, which wasn’t much because it seems a lot of this information is being controlled to stop the spread of disinformation, all we know is that a comet is passing through our solar system. This comet looks like a comet and acts like a comet, but is slightly faster and is not orbiting our sun. One scientist ventured a challenge to the “it’s just a comet” consensus to encourage more critical thinking, theorizing that it was, of course, possible for it to be an alien craft. This set the AI-loving conspiracy nuts on fire.</p><p>\\\nMy Instagram feed was on absolute fire with fake videos of this comet with lights being emitted from the sides like a ship, with exhaust clearly venting into space, and fake X-ray shots that showed the internal ship structure and beings inside. Countless videos of influencers pretended to be experts on the matter and talked about potential alien invasions. The worst part of all of this was that each one cited different sources and pulled from different video content. It was easy to assume that a “potential alien invasion” was fake; however, I have to admit, the video content they provided was stunningly believable. The “experts” talking were confident and had plenty of “research” to back them up.</p><p>\\\nThe most alarming thing about this entire situation is how fast this misinformation is able to spread and how absolutely believable it can make it. I know this isn’t a controlled situation, and we’ve always had irresponsible people running social media accounts to susceptible individuals, but the use of AI in these fields is making the problem more abundant and harder to discern.</p><h2><strong>It’s not all bad… but it’s pretty bad.</strong></h2><p>AI is a wonderful tool that has the potential to make our lives easier. I don’t believe that it is ready for constant use yet, despite it being shoved down our throats around every turn. It consistently hallucinates and makes false claims; it slows my work down more than it speeds it up, and has become a barrier to productivity in most situations I’ve tried to use it. </p><p>\\\nHowever, I can admit it has potential, and there are small automation tasks that I do find small uses for it. That being said, there’s always going to be a heated conversation around ethics and how we should be using AI. </p><p>\\\nI don’t believe there will be any disagreement, however, that the above cases are the wrong way to use AI. The use of AI to generate content to fuel conspiracy theories and spread them to the masses as facts is dangerous and, frankly, terrifying. I believe we’re only seeing the tip of the iceberg, and I worry that we’re headed for a future where we’ll no longer be able to discern the difference between truth and fiction.</p>","contentLength":5778,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Security Holes Found in OpenAI's ChatGPT Atlas Browser (and Perplexity's Comet)","url":"https://it.slashdot.org/story/25/11/01/054213/security-holes-found-in-openais-chatgpt-atlas-browser-and-perplexitys-comet?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1762011240,"author":"EditorDavid","guid":243,"unread":true,"content":"The address bar/ChatGPT input window in OpenAI's browser ChatGPT Atlas \"could be targeted for prompt injection using malicious instructions disguised as links,\" reports SC World, citing a report from AI/agent security platform NeuralTrust:\n\nNeuralTrust found that a malformed URL could be crafted to include a prompt that is treated as plain text by the browser, passing the prompt on to the LLM. A malformation, such as an extra space after the first slash following \"https:\" prevents the browser from recognizing the link as a website to visit. Rather than triggering a web search, as is common when plain text is submitted to a browser's address bar, ChatGPT Atlas treats plain text as ChatGPT prompts by default. \n\nAn unsuspecting user could potentially be tricked into copying and pasting a malformed link, believing they will be sent to a legitimate webpage. An attacker could plant the link behind a \"copy link\" button so that the user might not notice the suspicious text at the end of the link until after it is pasted and submitted. These prompt injections could potentially be used to instruct ChatGPT to open a new tab to a malicious website such as a phishing site, or to tell ChatGPT to take harmful actions in the user's integrated applications or logged-in sites like Google Drive, NeuralTrust said. \n\nLast month browser security platform LayerX also described how malicious prompts could be hidden in URLs (as a parameter) for Perplexity's browser Comet. And last week SquareX Labs demonstrated that a malicious browser extension could spoof Comet's AI sidebar feature and have since replicated the proof-of-concept (PoC) attack on Atlas. \n\n\n\nBut another new vulnerability in ChatGPT Atlas \"could allow malicious actors to inject nefarious instructions into the artificial intelligence (AI)-powered assistant's memory and run arbitrary code,\" reports The Hacker News, citing a report from browser security platform LayerX:\n\n\n\n\"This exploit can allow attackers to infect systems with malicious code, grant themselves access privileges, or deploy malware,\" LayerX Security Co-Founder and CEO, Or Eshed, said in a report shared with The Hacker News. The attack, at its core, leverages a cross-site request forgery (CSRF) flaw that could be exploited to inject malicious instructions into ChatGPT's persistent memory. The corrupted memory can then persist across devices and sessions, permitting an attacker to conduct various actions, including seizing control of a user's account, browser, or connected systems, when a logged-in user attempts to use ChatGPT for legitimate purposes.... \n\n\"What makes this exploit uniquely dangerous is that it targets the AI's persistent memory, not just the browser session,\" Michelle Levy, head of security research at LayerX Security, said. \"By chaining a standard CSRF to a memory write, an attacker can invisibly plant instructions that survive across devices, sessions, and even different browsers. In our tests, once ChatGPT's memory was tainted, subsequent 'normal' prompts could trigger code fetches, privilege escalations, or data exfiltration without tripping meaningful safeguards....\" \n\n\nLayerX said the problem is exacerbated by ChatGPT Atlas' lack of robust anti-phishing controls, the browser security company said, adding it leaves users up to 90% more exposed than traditional browsers like Google Chrome or Microsoft Edge. In tests against over 100 in-the-wild web vulnerabilities and phishing attacks, Edge managed to stop 53% of them, followed by Google Chrome at 47% and Dia at 46%. In contrast, Perplexity's Comet and ChatGPT Atlas stopped only 7% and 5.8% of malicious web pages. \n\nFrom The Conversation:\n\nSandboxing is a security approach designed to keep websites isolated and prevent malicious code from accessing data from other tabs. The modern web depends on this separation. But in Atlas, the AI agent isn't malicious code — it's a trusted user with permission to see and act across all sites. This undermines the core principle of browser isolation. \n\n\nThanks to Slashdot reader spatwei for suggesting the topic.","contentLength":4094,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Beyond Brute Force: 4 Secrets to Smaller, Smarter, and Dramatically Cheaper AI","url":"https://hackernoon.com/beyond-brute-force-4-secrets-to-smaller-smarter-and-dramatically-cheaper-ai?source=rss","date":1762009207,"author":"Anthony Laneau","guid":308,"unread":true,"content":"<p>Large Language Models (LLMs) are incredibly powerful generalists, but transforming them into specialized experts is a major challenge. The process of training a model on new, specific knowledge like internal company documents or a complex reasoning task is notoriously expensive, time-consuming, and fraught with pitfalls. We want smaller, more efficient models that can master a domain without the compute budget of a tech giant.</p><p>\\\nThe core idea behind making smaller models smarter is a concept called \"distillation.\" In this process, a smaller \"student\" model learns from a larger, more capable \"teacher\" model. The student doesn't just learn from a static textbook of examples; it learns to mimic the teacher's thought process. This is a powerful shortcut for transferring expertise.</p><p>\\\nUntil now, however, engineers have faced a frustrating trade-off. One approach, on-policy reinforcement learning (RL), forces the student to learn from its own mistakes, which is relevant but painfully slow. The alternative, off-policy distillation, is much faster but dangerously flawed; the student learns from the teacher's ideal examples, which often occur in contexts the student will never encounter on its own, causing errors to compound. This has been the bottleneck for creating specialized AI; until now.</p><p>\\\nA powerful technique called \"on-policy distillation\" combines the best of both worlds. By having a teacher model provide dense, token-by-token feedback on the student model's own attempts, we can achieve breakthroughs in training efficiency and capability. Here are the four most surprising and impactful takeaways from this approach.</p><h3>A Smarter Feedback Loop Makes AI Training Up to 100x Cheaper</h3><p>The fundamental difference between Reinforcement Learning (RL) and Distillation lies in the density of the feedback. To understand this, imagine learning to play chess.</p><ul><li> is like learning chess by only being told if you won or lost at the very end of a match. The feedback is directly related to your actions, but it's sparse. You know you lost, but you don't know if it was because of your opening, a mid-game blunder, or a weak endgame.</li><li> is like watching a grandmaster play. You observe brilliant moves, but they are made in complex board positions that you, as a novice, will rarely find yourself in. The feedback is dense, but the context is often irrelevant to your own learning path.</li><li> provides the best of both worlds. It's like having an expert coach who grades every single one of your moves in your own games, telling you if a move was a \"blunder,\" \"inaccuracy,\" or \"brilliant.\" The feedback is both dense and perfectly relevant to your current skill level.</li></ul><p>\\\nThis smarter feedback loop has a massive impact on efficiency. In a direct back-to-back comparison where a student model learned from a teacher trained via RL, on-policy distillation allowed the student to reach the teacher's performance level 7-10 times faster in terms of gradient steps. This translates to a staggering 50-100x improvement in cumulative compute efficiency.</p><p>\\\nThe reason for this dramatic speedup is that on-policy distillation provides more useful information (more \"bits per episode\") for the model to learn from. Because this dense, token-level feedback reduces gradient noise, it allows for training with shorter contexts and smaller, more efficient batch sizes, further slashing the overall computational cost.</p><h3>You Can Cure “AI Amnesia” When Teaching New Knowledge</h3><p>A common and frustrating problem in AI is \"catastrophic forgetting.\" When you take a pre-trained model and fine-tune it on new, specialized information (like your company's internal knowledge base), it often degrades or completely forgets its original, general-purpose skills, such as the ability to follow instructions.</p><p>\\\nConsider an experiment to create an \"internal assistant.\" Researchers started with the Qwen3-8B model, which had a strong instruction-following score of 85%. After fine-tuning it on a 70-30 mix of internal company documents and general chat data:</p><ul><li>Its knowledge about the documents improved significantly (from 18% to 36% on a QA evaluation).</li><li>However, its instruction-following skill degraded badly, dropping from 85% down to 79%.</li></ul><p>\\\nThe solution was a brief phase of on-policy distillation after the initial fine-tuning. By using the original version of the model as the teacher, researchers could restore the lost behavior. The results were powerful:</p><ul><li>Instruction-following performance was almost fully recovered, jumping back up to 83%.</li><li>Crucially, this happened without losing the newly acquired knowledge. In fact, the knowledge score even improved slightly to 41%.</li></ul><p>\\\nThis finding is a game-changer for \"continual learning,\" aka the ability to update models with new information over time without having to perform expensive, full-scale retraining from scratch. It provides a reliable way to teach an AI new facts without it forgetting its core skills.</p><h3>An AI Can Master a Reasoning Skill From Just One Example</h3><p>This finding is highly counterintuitive. In most AI training methods, repeatedly training a model on the exact same prompt is a recipe for failure; the model simply memorizes the answer instead of learning the underlying skill.</p><p>\\\nHowever, an experiment with on-policy distillation turned this assumption on its head. Researchers trained a student model on a math reasoning task using only a single, randomly chosen prompt. They trained on this one prompt for 20 consecutive steps, each with a batch of 256 rollouts, generating 5,120 total learning sequences.</p><p>\\\nThe remarkable outcome turns conventional wisdom on its head: the student model was able to approximately match the performance of the expert teacher model on the AIME'24 math benchmark, despite only ever having seen that one problem.</p><p>\\\nThis works because on-policy distillation teaches the model to approximate the teacher's entire thought process; its full probability distribution for what the next best token should be at every step, rather than just memorizing a final answer. This means that for certain skills, the bottleneck isn't finding thousands of examples, but creating a single, perfectly-guided learning experience.</p><h3>Why \"Practicing\" on Its Own Samples Can Make an AI Dumber</h3><p>It seems logical that if a model produces a high-quality output, you could feed that output back into its training data to reinforce good behavior. This method, known as supervised fine-tuning (SFT) on on-policy data, is like having the model \"practice\" on its own best work.</p><p>\\\nBut researchers found the opposite to be true. When they trained a model using a dataset composed of its own samples, its performance on an instruction-following evaluation actually degraded.</p><p>\\\nThe technical reason for this failure is subtle but critical. While the dataset of the model's own outputs might be perfectly on-policy on average, every finite batch of data exhibits a slightly different distribution. Training on these batches causes the model’s internal policy to drift away from its original state. This process turns training on its own samples into a form of off-policy training over time, leading to the same compounding error and divergence seen in other flawed methods.</p><p>\\\nIn contrast, on-policy distillation is completely stable in this self-distillation scenario. Because the teacher model remains a fixed, consistent target, the student can robustly converge on the desired behavior without degrading. This further cements on-policy distillation as a superior and more reliable tool for behavior refinement and continual learning.</p><h3>The Future of AI is Smaller, Faster, and More Personal</h3><p>On-policy distillation is more than just another training technique; it's a foundational shift in how we create specialized, expert AI. By combining the direct relevance of learning from one's own actions with the incredible efficiency of dense, token-by-token feedback, it solves some of the biggest challenges in applied AI.</p><p>\\\nThe benefits are clear: massive compute savings, a cure for catastrophic forgetting, and unbelievable data efficiency. This is a key enabling technology that lowers the barrier to entry, unlocking the ability for more teams to build and maintain custom models that possess deep domain knowledge without sacrificing core capabilities. This democratization of expert AI will fuel new business models and create competitive advantages previously reserved for frontier labs.</p>","contentLength":8401,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI researchers ’embodied’ an LLM into a robot – and it started channeling Robin Williams","url":"https://techcrunch.com/2025/11/01/ai-researchers-embodied-an-llm-into-a-robot-and-it-started-channeling-robin-williams/","date":1762009200,"author":"Julie Bort","guid":272,"unread":true,"content":"<article>AI researchers at Andon Labs embedded various LLMs in a vacuum robot to test how ready they were to be embodied. And hilarity ensued.</article>","contentLength":133,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linux Kernel Ported To WebAssembly - Demo Lets You Run It In Your Web Browser","url":"https://www.phoronix.com/news/Linux-Kernel-WebAssembly","date":1762008052,"author":"Michael Larabel","guid":693,"unread":true,"content":"<article>Open-source developer Joel Severin today announced his work on porting the Linux kernel to WebAssembly and has successffully gotten the kernel up and running within WASM-capable web browsers...</article>","contentLength":193,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"MIT Physicists Find a Way To See Inside Atoms That May Aid Search For Antimatter","url":"https://science.slashdot.org/story/25/11/01/0545231/mit-physicists-find-a-way-to-see-inside-atoms-that-may-aid-search-for-antimatter?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1762007640,"author":"EditorDavid","guid":242,"unread":true,"content":"\"Traditionally, exploring the interior of atomic nuclei requires enormous particle accelerators that stretch for kilometers and propel beams of electrons at extremely high speeds,\" writes SciTechDaily. \n\nBut MIT physicists have unveiled a groundbreaking alternative that \"used the atom's own electrons as probes to momentarily enter the nucleus...\"\n\n\n\nIn research published in Science, a team of MIT physicists achieved exceptionally precise measurements of the energy of electrons orbiting a radium atom that had been chemically bonded with a fluoride atom to form radium monofluoride. By studying these molecules, the researchers created a kind of miniature particle collider. Within this environment, the electrons surrounding the radium atom were confined closely enough to occasionally slip into the nucleus before returning to their usual orbits... When those electrons returned to their outer paths, they retained the altered energy, effectively carrying a \"message\" from within the nucleus that could be decoded to reveal its internal arrangement... \n\n[The researchers] trapped and cooled the molecules and sent them through a system of vacuum chambers, into which they also sent lasers, which interacted with the molecules. In this way, the researchers were able to precisely measure the energies of electrons inside each molecule. When the researchers analyzed their measurements, they noticed that the electrons carried slightly different energies than expected if they had remained outside the nucleus. The difference was incredibly small, only about one millionth of the energy of the laser photon used to excite the molecules, but it was clear evidence that the electrons had entered the radium nucleus and interacted with its protons and neutrons... \n\nThe researchers plan to use this new technique to create a detailed map of how forces are distributed inside the nucleus... to chart the nucleus with greater precision and search for possible violations of fundamental symmetries in nature. \n\n\"It is thought that additional sources of fundamental symmetry violation are required to explain the almost complete absence of antimatter in our universe,\" the article points out. \"Such violations could be seen within the nuclei of certain atoms such as radium... \n\n\n\"Unlike most atomic nuclei, which are spherical in shape, the radium atom's nucleus has a more asymmetrical configuration, similar to a pear. Scientists predict that this pear shape could significantly enhance their ability to sense the violation of fundamental symmetries, to the extent that they may be potentially observable.\"","contentLength":2606,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Hidden Ledger of Code: Tracking the Carbon Debt Inside Our Software","url":"https://hackernoon.com/the-hidden-ledger-of-code-tracking-the-carbon-debt-inside-our-software?source=rss","date":1762005619,"author":"Jacob Wolinsky","guid":307,"unread":true,"content":"<ul><li>Every line of code carries an invisible cost. As software scales, so does the energy it consumes and the emissions it generates.</li><li>This growing footprint forms what many engineers now call carbon debt: the accumulation of energy waste caused by inefficient architecture, redundant compute, or neglected cleanup.</li><li>The problem isn’t limited to theory anymore. Global data workloads are rising faster than the efficiency gains meant to offset them, and few teams have the tools to measure what their systems actually emit.</li><li>Because engineers control how and where code runs, real progress starts inside development workflows, not in boardrooms.</li><li>As carbon visibility moves closer to the code itself, software projects may soon be judged not only by speed and stability, but by how responsibly they use the power behind them.</li></ul><p>Teams talk about technical debt every sprint. They track code smells, refactoring needs, module complexity, and build bloat. But almost no one tracks the energy drain built into their systems, and this makes that blind spot real.</p><p>\\\nEvery inefficiency in code, like extra loops, redundant database fetches, and idle background tasks, translates into power use. Run thousands or millions of times per day, and what feels trivial becomes measurable emissions. Researchers have begun quantifying this: for example, the <a href=\"https://arxiv.org/pdf/2007.07610\">Green Algorithms framework</a> shows that compute time, memory usage, and data center efficiency can be converted into carbon equivalent estimates for any computational task.</p><p>\\\nAt the data center scale, inefficiencies amplify. One white-paper found that servers may draw <a href=\"https://www.nrdc.org/sites/default/files/NRDC_WSP_Cloud_Computing_White_Paper.pdf\">60% to 90%</a> of their peak power even while idle. Multiply that across dozens of servers, and weeks of wasted cycles become dozens of kilograms of CO2 equivalent.</p><p>\\\nEvery product team now operates with an invisible balance sheet, one that records carbon alongside complexity.</p><p>The term carbon debt originates in environmental accounting, where it describes the <a href=\"https://globalclimateinitiatives.com/en/quest-ce-que-la-dette-carbone/\">accumulated emissions</a> a system or entity has “borrowed” against future budgets with insufficient offsets. (It’s rooted in the broader notion of ecological or climate debt.) Now, technologists are borrowing that phrase to describe software systems whose inefficiencies accrue hidden energy costs over time.</p><p>\\\nIn software, carbon debt grows when layers of redundant code, over-provisioned infrastructure, and heavy frameworks persist unchecked. A module that spawns unnecessary background jobs, or a service that overfetches data, burns CPU cycles, which burn power.</p><p>\\\nWhen infrastructure is sized with generous headroom “just in case,” that slack often stays underutilized, yet still draws baseline power. Servers and services often draw between <a href=\"https://gulfbusiness.com/why-idle-servers-not-ai-are-the-true-sustainability-threat/\">27% and 36%</a> of peak power even under light load.</p><p>\\\nAs your system advances with more users, more services, and more replicas, each inefficiency multiplies. What once was a single wasted cycle becomes thousands per second. That energy waste endures unless addressed, compounding like interest owed on an invisible balance.</p><p>\\\nNext, we’ll trace how code builds up emissions so you can see where the debt really comes from.</p><h2>How Code Accrues Emissions</h2><p>The energy footprint of software often hides in the smallest details of its logic. A loop that runs one step too long or a recursive function that never terminates efficiently can keep processors active far longer than needed. Each extra millisecond of compute draws power, and the effect multiplies when thousands of users trigger the same function at once.</p><h3>How Tiny Loops Turn Into Big Costs</h3><p>Research on mobile software shows that energy code smells can dramatically increase consumption, and in some cases, they can consume up to <a href=\"https://fpalomba.github.io/pdf/Journals/J13.pdf\">87x more energy</a> than clean versions. Follow-up work found that fixing these patterns delivered <a href=\"https://pmc.ncbi.nlm.nih.gov/articles/PMC11479295/\">4% to 30%</a> efficiency gains in practice. These results reinforce the broader point: repetitive, seemingly minor patterns accumulate real power draw over time.</p><p>\\\nSimilar waste appears in everyday engineering habits: redundant database queries, unnecessary front-end re-renders, and dormant API endpoints all keep processors active, drawing power without improving performance.</p><p>\\\nOver-sized build artifacts and idle background tasks deepen the impact by holding memory and storage resources active long after they’re useful. When these patterns run across millions of daily transactions, the emissions scale from grams to kilograms of CO2. Quantifying that footprint is the next challenge, and few teams yet have the tools to do it precisely.</p><h2>Measuring What We Don’t See</h2><p>Tracking how much energy software really uses is harder than it sounds. The <a href=\"https://sci-guide.greensoftware.foundation/\">Software Carbon Intensity (SCI)</a> framework from the Green Software Foundation is one of the first real attempts to make that measurable, like mapping compute time, memory use, and data transfer against actual energy data.</p><p>\\\nTools such as <a href=\"https://www.cloudcarbonfootprint.org/\">Cloud Carbon Footprint</a> and CodeCarbon are now taking that formula a step further, embedding energy estimates directly into build pipelines and dashboards so developers can see environmental impact alongside performance metrics. This aligns with broader conversations inside the DevOps community, where teams are beginning to explore practical ways to <a href=\"https://hackernoon.com/code-green-the-pragmatists-guide-to-eco-friendly-devops\">embed sustainability</a> into build and deployment workflows.</p><p>\\\nThe challenge is translating code execution into physical terms. Every watt drawn depends on processor type, cooling efficiency, and the carbon intensity of the grid that powers the data center. The same workload might have a fraction of the emissions on renewable-heavy infrastructure compared to fossil-fueled grids.</p><p>\\\nThe logic behind these tools isn’t far from how predictive analytics is being used to expose hidden operational costs in other industries, turning guesswork into measurable insight. Until this kind of visibility becomes standard in developer environments, most teams will keep optimizing performance while staying blind to the energy behind it.</p><h2>The Governance Gap: Why Carbon Isn’t Yet a Coding Metric</h2><p>Sustainability still sits outside most engineering workflows. In many companies, carbon reporting lives with facilities or operations teams, not with the people writing or deploying code.</p><p>\\\nAs a result, the energy cost of a release is rarely discussed in sprint planning or post-mortems. Agile ceremonies track velocity, story points, and error rates, but not emissions.</p><p>Few DevOps environments include “carbon sprints” or carbon budgets, even though they could be tracked the same way as uptime or latency. A <a href=\"https://greensoftware.foundation/articles/green-software-foundation-releases-first-ever-state-of-green-software-report\">report</a> based on responses from over <a href=\"https://stateof.greensoftware.foundation/en/methodology/\">2,000 software practitioners</a> has found that most organizations are still in the early stages of measuring software-related emissions. Others echoed this, noting that sustainability metrics remain largely absent from continuous-integration and delivery pipelines.</p><p>\\\nThat gap is beginning to close. Some open-source communities have started experimenting with “green commits” to tag energy-efficient changes, and enterprise dashboards are beginning to surface sustainability data next to performance KPIs. As this visibility improves, design priorities are shifting toward decay and restraint by building systems that know when to slow down, scale back, or shut off entirely.</p><h2>Designing for Decay: Making Efficiency a Default</h2><p>Architects concerned with long-lived systems often speak of <a href=\"https://onlinelibrary.wiley.com/doi/10.1002/smr.2423\">architectural erosion</a> or design decay, like the gradual divergence between intended structure and runtime reality. Architecture erosion is a well-known risk in systems as features accumulate and shortcuts proliferate. One way to counter that drift is to build systems that self-optimize or sunset unused processes automatically, pruning inactive modules or trimming underutilized services based on real usage signals.</p><p>Treating code decay as a feature means embedding routines that perform periodic cleanup: archiving stale APIs, retiring dormant modules, or enforcing dependency hygiene. Frameworks may require that libraries unused for X releases be flagged or removed. Over time, the shift moves from “unlimited scaling” toward sustainable scaling, systems designed to shrink or sleep when load is low rather than running flat out forever.</p><p>\\\nEngineers can use runtime profiling, build monitoring, and garbage-collection heat maps as signals. If a microservice’s CPU utilization stays near zero for weeks, it raises a refactor or archive flag. If build artifacts grow without change, they are flagged for pruning.</p><p>\\\nThis philosophy sets the stage for what’s next: making carbon visibility part of everyday decision-making, and bringing engineering metrics and emissions metrics into the same ecosystem.</p><h2>The Road to Carbon Transparency</h2><p>Imagine an IDE where each file, function, or commit carries a live “emissions counter”; you write a loop, and you see how much energy it might cost. That’s the direction software tooling is heading. Build tools could come to flag carbon-heavy changes before they’re merged.</p><p>\\\n<a href=\"https://www.researchgate.net/publication/379003736_Carbon-Awareness_in_CICD\">CI/CD pipelines</a> will evolve to flag carbon-intensive builds, perhaps even rejecting code that spikes emissions far above baseline. With tighter integration, carbon metrics will merge with performance dashboards, showing build time, throughput, and CO2 cost in one pane.</p><h3>Cloud Dashboards &amp; Deployment Transparency</h3><p>Cloud providers may expose per-deployment carbon cost insights, mapping workload emissions to regions, instance types, and schedules. The same principle underpins the idea of <a href=\"https://hackernoon.com/carbon-aware-computing-next-green-breakthrough-or-new-greenwashing\">carbon-aware computing</a>, where workloads shift dynamically to regions or times with cleaner grids. Integrating that into the same console where devs monitor CPU, bandwidth, and billing makes sustainability part of everyday trade-offs.</p><p>\\\nWith visibility in place, engineers will begin to optimize not just for latency or memory, but for carbon as a first-class metric. Those insights will shape budgeting decisions, drive architecture choices (edge, serverless, off-peak scheduling), and enforce sustainable defaults in code.</p><p>\\\nAhead lies a time when your pull request comes with a carbon delta and teams judge changes not only by correctness or performance, but by how much energy they add or save.</p><h2>Engineering Accountability</h2><p>Sustainability in software doesn’t start in a server farm, but it starts at the keyboard. Every query, commit, and deployment decision shapes the energy profile of the systems we run. For years, efficiency meant speed and uptime, and now it also means restraint.</p><p>\\\nAcross the industry, teams are beginning to treat carbon debt the same way they treat technical debt: as something that compounds if ignored. Cleaning up unused code, right-sizing infrastructure, or pausing idle jobs are no longer side tasks; they’re acts of maintenance that protect performance and the planet.</p><p>\\\nAs tooling matures, carbon visibility will become part of normal governance, sitting next to reliability and security in every build report. The responsibility won’t rest with operations alone but with every engineer who touches code. Because in modern software, clean code and clean energy belong to the same conversation, and writing one well means caring about the other.</p>","contentLength":11106,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Go: Can It Mitigate Supply Chain Attacks?","url":"https://hackernoon.com/go-can-it-mitigate-supply-chain-attacks?source=rss","date":1762005611,"author":"Go [Technical Documentation]","guid":306,"unread":true,"content":"<p>Modern software engineering is collaborative, and based on reusing Open Source software. That exposes targets to supply chain attacks, where software projects are attacked by compromising their dependencies.</p><p>\\\nDespite any process or technical measure, every dependency is unavoidably a trust relationship. However, the Go tooling and design help mitigate risk at various stages.</p><p>There is no way for changes in the outside world—such as a new version of a dependency being published—to automatically affect a Go build.</p><p>\\\nUnlike most other package managers files, Go modules don’t have a separate list of constraints and a lock file pinning specific versions. The version of every dependency contributing to any Go build is fully determined by the  file of the main module.</p><p>\\\nSince Go 1.16, this determinism is enforced by default, and build commands (, , , , …) <a href=\"https://go.dev/ref/mod#go-mod-file-updates\">will fail if the go.mod is incomplete</a>. The only commands that will change the  (and therefore the build) are  and . These commands are not expected to be run automatically or in CI, so changes to dependency trees must be made deliberately and have the opportunity to go through code review.</p><p>\\\nThis is very important for security, because when a CI system or new machine runs , the checked-in source is the ultimate and complete source of truth for what will get built. There is no way for third parties to affect that.</p><p>\\\nMoreover, when a dependency is added with , its transitive dependencies are added at the version specified in the dependency’s  file, not at their latest versions, thanks to <a href=\"https://go.dev/ref/mod#minimal-version-selection\">Minimal version selection</a>. The same happens for invocations of <code>go install example.com/cmd/devtoolx@latest</code>, <a href=\"https://research.swtch.com/npm-colors\">the equivalents of which in some ecosystems bypass pinning</a>. In Go, the latest version of  will be fetched, but then all the dependencies will be set by its  file.</p><p>\\\nIf a module gets compromised and a new malicious version is published, no one will be affected until they explicitly update that dependency, providing the opportunity to review the changes and time for the ecosystem to detect the event.</p><h2>Version contents never change</h2><p>Another key property necessary to ensure third parties can’t affect builds is that the contents of a module version are immutable. If an attacker that compromises a dependency could re-upload an existing version, they could automatically compromise all projects that depend on it.</p><p>\\\nThat’s what the  file is for. It contains a list of cryptographic hashes of each dependency that contributes to the build. Again, an incomplete  causes an error, and only  and  will modify it, so any changes to it will accompany a deliberate dependency change. Other builds are guaranteed to have a full set of checksums.</p><p>\\\nThis is a common feature of most lock files. Go goes beyond it with the <a href=\"https://go.dev/ref/mod#checksum-database\">Checksum Database</a> (sumdb for short), a global append-only cryptographically-verifiable list of go.sum entries. When  needs to add an entry to the  file, it fetches it from the sumdb along with cryptographic proof of the sumdb integrity. This ensures that not only every build of a certain module uses the same dependency contents, but that every module out there uses the same dependency contents!</p><p>\\\nThe sumdb makes it impossible for compromised dependencies or even Google-operated Go infrastructure to target specific dependents with modified (e.g. backdoored) source. You’re guaranteed to be using the exact same code that everyone else who’s using e.g. v1.9.2 of  is using and has reviewed.</p><p>\\\nFinally, my favorite features of the sumdb: it doesn’t require any key management on the part of module authors, and it works seamlessly with the decentralized nature of Go modules.</p><h2>The VCS is the source of truth</h2><p>Most projects are developed through some version control system (VCS) and then, in other ecosystems, uploaded to the package repository. This means there are two accounts that could be compromised, the VCS host and the package repository, the latter of which is used more rarely and more likely to be overlooked. It also means it’s easier to hide malicious code in the version uploaded to the repository, especially if the source is routinely modified as part of the upload, for example to minimize it.</p><p>\\\nIn Go, there is no such thing as a package repository account. The import path of a package embeds the information that <a href=\"https://pkg.go.dev/cmd/go#hdr-Remote_import_paths\">needs in order to fetch its module</a> directly from the VCS, where tags define versions.</p><p>\\\nWe do have the <a href=\"https://go.dev/blog/module-mirror-launch\">Go Module Mirror</a>, but that’s only a proxy. Module authors don’t register an account and don’t upload versions to the proxy. The proxy uses the same logic that the  tool uses (in fact, the proxy runs ) to fetch and cache a version. Since the Checksum Database guarantees that there can be only one source tree for a given module version, everyone using the proxy will see the same result as everyone bypassing it and fetching directly from the VCS. (If the version is not available anymore in the VCS or if its contents changed, fetching directly will lead to an error, while fetching from the proxy might still work, improving availability and protecting the ecosystem from <a href=\"https://blog.npmjs.org/post/141577284765/kik-left-pad-and-npm\">“left-pad” issues</a>.)</p><p>\\\nRunning VCS tools on the client exposes a pretty large attack surface. That’s another place the Go Module Mirror helps: the  tool on the proxy runs inside a robust sandbox and is configured to support every VCS tool, while <a href=\"https://go.dev/ref/mod#vcs-govcs\">the default is to only support the two major VCS systems</a> (git and Mercurial). Anyone using the proxy can still fetch code published using off-by-default VCS systems, but attackers can’t reach that code in most installations.</p><h2>Building code doesn’t execute it</h2><p>It is an explicit security design goal of the Go toolchain that neither fetching nor building code will let that code execute, even if it is untrusted and malicious. This is different from most other ecosystems, many of which have first-class support for running code at package fetch time. These “post-install” hooks have been used in the past as the most convenient way to turn a compromised dependency into compromised developer machines, and to <a href=\"https://en.wikipedia.org/wiki/Computer_worm\">worm</a> through module authors.</p><p>\\\nTo be fair, if you’re fetching some code it’s often to execute it shortly afterwards, either as part of tests on a developer machine or as part of a binary in production, so lacking post-install hooks is only going to slow down attackers. (There is no security boundary within a build: any package that contributes to a build can define an  function.) However, it can be a meaningful risk mitigation, since you might be executing a binary or testing a package that only uses a subset of the module’s dependencies. For example, if you build and execute  on macOS there is no way for a Windows-only dependency or a dependency of <code>example.com/cmd/othertool</code> to compromise your machine.</p><p>\\\nIn Go, modules that don’t contribute code to a specific build have no security impact on it.</p><h2>“A little copying is better than a little dependency”</h2><p>The final and maybe most important software supply chain risk mitigation in the Go ecosystem is the least technical one: Go has a culture of rejecting large dependency trees, and of preferring a bit of copying to adding a new dependency. It goes all the way back to one of the Go proverbs: <a href=\"https://youtube.com/clip/UgkxWCEmMJFW0-TvSMzcMEAHZcpt2FsVXP65\">“a little copying is better than a little dependency”</a>. The label “zero dependencies” is proudly worn by high-quality reusable Go modules. If you find yourself in need of a library, you’re likely to find it will not cause you to take on a dependency on dozens of other modules by other authors and owners.</p><p>\\\nThat’s enabled also by the rich standard library and additional modules (the  ones), which provide commonly used high-level building blocks such as an HTTP stack, a TLS library, JSON encoding, etc.</p><p>\\\nAll together this means it’s possible to build rich, complex applications with just a handful of dependencies. No matter how good the tooling is, it can’t eliminate the risk involved in reusing code, so the strongest mitigation will always be a small dependency tree.</p><p>\\\n<em>This article is available on&nbsp;&nbsp;under a CC BY 4.0 DEED license.</em></p>","contentLength":8072,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Archinstall 3.0.12 & Pacman 7.1 Released For Arch Linux Users","url":"https://www.phoronix.com/news/Archinstall-3.0.12-Pacman-7.1","date":1762002319,"author":"Michael Larabel","guid":692,"unread":true,"content":"<article>Kicking off November for Arch Linux users happen to be the releases of Pacman 7.1 as well as Archinstall 3.0.12...</article>","contentLength":114,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Chips Need to Chill Out","url":"https://spectrum.ieee.org/thermal-management-chips","date":1762002002,"author":"Harry Goldstein","guid":82,"unread":true,"content":"<p>The semiconductor industry seeks radical cooling solutions</p>","contentLength":58,"flags":null,"enclosureUrl":"https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTk1NzcyNS9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTgxMjg5Nzk0M30.oEWlOSimqjs-0YhG6yHcN93FhULhj37OplNQHCtEdkQ/image.png?width=600","enclosureMime":"","commentsUrl":null},{"title":"Samsung Building Facility With 50,000 Nvidia GPUs To Automate Chip Manufacturing","url":"https://hardware.slashdot.org/story/25/10/31/2352207/samsung-building-facility-with-50000-nvidia-gpus-to-automate-chip-manufacturing?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1762002000,"author":"BeauHD","guid":241,"unread":true,"content":"An anonymous reader quotes a report from CNBC: Korean semiconductor giant Samsung said Thursday that it plans to buy and deploy a cluster of 50,000 Nvidia graphics processing units to improve its chip manufacturing for mobile devices and robots. The 50,000 Nvidia GPUs will be used to create a facility Samsung is calling an \"AI Megafactory.\" Samsung didn't provide details about when the facility would be built. It's the latest splashy partnership for Nvidia, whose chips remain essential for building and deploying advanced artificial intelligence. [...]\n \nOn Thursday, Nvidia representatives said they will work with Samsung to adapt the Korean company's chipmaking lithography platform to work with Nvidia's GPUs. That process will results in 20 times better performance for Samsung, the Nvidia representatives said. Samsung will also use Nvidia's simulation software called Omniverse. Known for its mobile phones, Samsung also said it would use the Nvidia chips to run its own AI models for its devices. In addition to being a partner and customer, Samsung is also a key supplier for Nvidia. Samsung makes the kind of high-performance memory Nvidia uses in large quantities, alongside its AI chips, called high bandwidth memory. Samsung said it will work with Nvidia to tweak its HBM4 memory for use in AI chips.","contentLength":1318,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"PCI Resizable BAR Improvements Heading To Linux 6.19","url":"https://www.phoronix.com/news/PCI-ReBAR-Better-Linux-6.19","date":1762000551,"author":"Michael Larabel","guid":691,"unread":true,"content":"<article>Restructuring to the Linux kernel's PCI Resizable BAR \"ReBAR\" support is set to be submitted for the upcoming Linux 6.19 kernel cycle...</article>","contentLength":136,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linux 6.18 Kernel Happenings, Python 3.14, NTFSPLUS & Other October Highlights","url":"https://www.phoronix.com/news/October-2025-Highlights","date":1761993411,"author":"Michael Larabel","guid":690,"unread":true,"content":"<article>During the month of October on Phoronix were 305 original news articles around Linux/open-source and another 21 featured Linux hardware reviews / multi-page featured benchmark articles. There was an exciting mix of software and hardware happenings over the past month. Here is a look back at what excited readers the most...</article>","contentLength":324,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AMD Acknowledges RDSEED Failure On AMD Zen 5 With Software Fix Coming","url":"https://www.phoronix.com/news/AMD-SB-7055-RDSEED-Zen-5","date":1761992857,"author":"Michael Larabel","guid":689,"unread":true,"content":"<article>In mid-October a Meta engineer uncovered an RDSEED architectural issue with AMD Zen 5 CPUs. A patch in turn was sent out to the Linux kernel mailing list to disable RDSEED usage on affected Zen 5 processors. AMD this week issued a security bulletin to acknowledge the issue and report that a microcode fix is coming...</article>","contentLength":318,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"KDE Plasma 6.6 To Support Intel's Adaptive Sharpness Feature","url":"https://www.phoronix.com/news/Plasma-6.6-Adaptive-Sharpness","date":1761991744,"author":"Michael Larabel","guid":688,"unread":true,"content":"<article>KDE Plasma developers continue to be busy landing more fixes for the recently introduced Plasma 6.5 while also lining up more new features for Plasma 6.6...</article>","contentLength":156,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Falling Panel Prices Lead To Global Solar Boom, Except For the US","url":"https://hardware.slashdot.org/story/25/10/31/2340238/falling-panel-prices-lead-to-global-solar-boom-except-for-the-us?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1761991200,"author":"BeauHD","guid":240,"unread":true,"content":"Longtime Slashdot reader AmiMoJo shares a report from the Financial Times: Solar power developers want to cover an area larger than Washington, DC, with silicon panels and batteries, converting sunlight into electricity that will power air conditioners in sweltering Las Vegas along with millions of other homes and businesses. But earlier this month, bureaucrats in charge of federal lands scrapped collective approval for the Esmeralda 7 projects, in what campaigners fear is part of an attack on renewable energy under President Donald Trump. \"We will not approve wind or farmer destroying [sic] Solar,\" he posted on his Truth Social platform in August. Developers will need to reapply individually, slowing progress.\n \nThousands of miles away on the other side of the Pacific Ocean, it is a different story. China has laid solar panels across an area the size of Chicago high up on the Tibetan Plateau, where the thin air helps more sunlight get through. The Talatan Solar Park is part of China's push to double its solar and wind generation capacity over the coming decade. \"Green and low-carbon transition is the trend of our time,\" President Xi Jinping told delegates at a UN summit in New York last month. China's vast production of solar panels and batteries has also pushed down the prices of renewables hardware for everyone else, meaning it has \"become very difficult to make any other choice in some places,\" according to Heymi Bahar, senior analyst at the International Energy Agency. [...]\n \nMore broadly, the US's focus on fossil fuels and pullback of support for clean energy further cedes influence over the future global energy system to China. The US is trying to tie its trading partners into fossil fuels, pressing the EU to buy $750 billion of American oil, natural gas, and nuclear technologies during his presidency as part of a trade deal, scuppering an initiative to begin decarbonizing world shipping and pressuring others to reduce their reliance on Chinese technology. But the collapsing cost of solar panels in particular has spoken for itself in many parts of the world. Experts caution that the US's attacks on renewables could cause lasting damage to its competitiveness against China, even if an administration more favorable to renewables were to follow Trump's.","contentLength":2298,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"SpaceX Set To Win $2 Billion Pentagon Satellite Deal","url":"https://tech.slashdot.org/story/25/10/31/2347207/spacex-set-to-win-2-billion-pentagon-satellite-deal?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1761980400,"author":"BeauHD","guid":239,"unread":true,"content":"According to the Wall Street Journal, SpaceX is reportedly poised to secure a $2 billion Pentagon contract to develop hundreds of missile-tracking satellites for President Trump's ambitious Golden Dome defense system. The Independent reports: The planned \"air moving target indicator\" system in question could ultimately feature as many as 600 satellites once it is fully operational, The Wall Street Journal reports. Musk's company has also been linked to two more satellite ventures, which are concerned with relaying sensitive communications and tracing vehicles, respectively.\n \nGolden Dome, inspired by Israel's \"Iron Dome,\" was announced by Trump and Secretary of War Pete Hegseth at the White House in May and will amount to a complex system of satellites and weaponry capable of destroying incoming missiles before they hit American targets. The president promised it would be \"fully operational\" before he leaves office in January 2029, capable of intercepting rockets, \"even if they are launched from space,\" with an overall price tag of $175 billion.","contentLength":1061,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The TechBeat: From Cloud to Desk: 3 Signs the AI Revolution is Going Local (11/1/2025)","url":"https://hackernoon.com/11-1-2025-techbeat?source=rss","date":1761977453,"author":"Techbeat","guid":305,"unread":true,"content":"<p>By <a href=\"https://hackernoon.com/u/hacker-Antho\">@hacker-Antho</a> [ 4 Min read ] \n New research shatters AI security assumptions, showing that poisoning large models is easier than believed and requires a very small number of documents. <a href=\"https://hackernoon.com/the-illusion-of-scale-why-llms-are-vulnerable-to-data-poisoning-regardless-of-size\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/socialdiscoverygroup\">@socialdiscoverygroup</a> [ 6 Min read ] \n Discover how React 19's new hooks—useActionState, useFormStatus, and useOptimistic—simplify form handling with less boilerplate and cleaner code.  <a href=\"https://hackernoon.com/react-19-new-tools-to-work-with-forms\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/mayukhsuri\">@mayukhsuri</a> [ 3 Min read ] \n AWS outage on Oct 20, 2025, disrupted major apps worldwide. Learn what caused it, how it spread, and key lessons to build stronger cloud systems. <a href=\"https://hackernoon.com/aws-outage-2025-what-really-happened-on-october-20-and-what-it-teaches-us-about-the-cloud\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/filestack\">@filestack</a> [ 6 Min read ] \n Stop babysitting profile pictures. Learn how Filestack Workflows turn image uploads into scalable, async, and lightning-fast experiences. <a href=\"https://hackernoon.com/how-to-fix-profile-image-upload-headaches-with-filestack-workflows\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/nownodes\">@nownodes</a> [ 4 Min read ] \n Blast API ends operations in Oct 2025. Explore the best developer alternatives like NOWNodes and Alchemy for secure, scalable RPC migration. <a href=\"https://hackernoon.com/blast-api-shutdown-the-best-alternatives-for-developers\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/mend\">@mend</a> [ 4 Min read ] \n Traditional testing breaks with AI. Learn how red teaming and AI-powered fuzzing uncover hidden weaknesses in large language models. <a href=\"https://hackernoon.com/why-traditional-testing-breaks-down-with-ai\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/knightbat2040\">@knightbat2040</a> [ 5 Min read ] \n What started as a simple script evolved into a full-fledged data engineering and NLP pipeline that can process a decade's worth of legal decisions in minutes. <a href=\"https://hackernoon.com/python-script-to-read-and-judge-1500-legal-cases\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/hackmarketing\">@hackmarketing</a> [ 7 Min read ] \n Learn how Web3 projects can grow sustainably through education, trust, and human-centered marketing that builds real users and community. <a href=\"https://hackernoon.com/the-future-of-web3-marketing-education-trust-and-sustainability\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/botbeat\">@botbeat</a> [ 8 Min read ] \n A deep dive into the 30 companies that burned over one trillion OpenAI tokens—featuring Duolingo, OpenRouter, and Indeed as top power users of GPT tech. <a href=\"https://hackernoon.com/whos-used-one-trillion-plus-openai-tokens-salesforce-shopify-canva-hubspot-and-26-more-companies\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/melvin-manni\">@melvin-manni</a> [ 5 Min read ] \n Learn how good intentions can lead to spaghetti dry code, over abstraction and over engineered systems.  <a href=\"https://hackernoon.com/the-road-to-hell-is-paved-with-good-dry-intentions\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/giovannicoletta\">@giovannicoletta</a> [ 11 Min read ] \n An interrogation of how physics concepts like black holes, entropy, and quantum theory mirror the rise and limits of artificial intelligence. <a href=\"https://hackernoon.com/the-physics-of-ai\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/ichebykin\">@ichebykin</a> [ 5 Min read ] \n Context engineering for coding agents is the best way to improve the model performance for code generation.  <a href=\"https://hackernoon.com/context-engineering-for-coding-agents\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/mcsee\">@mcsee</a> [ 3 Min read ] \n Avoid Boolean variables, they lead to conditional logic and force you to write Ifs. Create polymorphic states instead <a href=\"https://hackernoon.com/code-smell-07-avoid-boolean-variables\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/sanjaybarot\">@sanjaybarot</a> [ 23 Min read ] \n Ransomware has gone cloud-native: no payloads, just API abuse. Learn the tactics—IAM takeovers, KMS locks, backup sabotage—and how to build resilience. <a href=\"https://hackernoon.com/ransomware-goes-cloud-native\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/ainativedev\">@ainativedev</a> [ 4 Min read ] \n GitHub Copilot evolves: cloud-based agents now handle PRs, iterate from feedback, and fit seamlessly into dev workflows. <a href=\"https://hackernoon.com/githubs-copilot-adds-cloud-agent-to-draft-pull-requests-autonomously\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/aifundingtracker\">@aifundingtracker</a> [ 13 Min read ] \n AI startups raised over $3.6 billion this week across infrastructure, wearable AI, enterprise automation, and fintech innovation. <a href=\"https://hackernoon.com/weekly-ai-startup-funding-october-20-25-2025\">Read More.</a></p>","contentLength":2886,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Numbers Show Xbox's Current Plan Isn't Working","url":"https://games.slashdot.org/story/25/10/31/2332211/the-numbers-show-xboxs-current-plan-isnt-working?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1761967800,"author":"BeauHD","guid":238,"unread":true,"content":"An anonymous reader quotes a report from Gizmodo: It's time for Xbox to eat some humble pie and perform some real soul-searching. Microsoft released its latest quarterly earnings report and proved the worst of our fears about its gaming brand. Not only are Xbox hardware sales down significantly, but the brand itself is barely treading water. Gamers are voicing their displeasure with their wallets, but Microsoft's top brass is still only thinking about the margins. Microsoft was more keen to promote the scale of its cloud and AI services revenue -- which was up 28% year over year -- than talk about its beleaguered gaming brand. The company's overall gaming revenue fell by 2% compared to the same time last year. This was precipitated by a \"decline in Xbox hardware,\" which was down by 22% following a steady decline quarter after quarter. Its first-party games and its Game Pass subscription were doing better, though the overall growth was only up by 1%, and even that was driven by the \"better-than-expected performance\" of third-party games. You can give credit to titles like Clair Obscur: Expedition 33 for why Xbox isn't in an even deeper hole than it is now.\n \nThe tech giant has no expectation that its Xbox brand will start making more money anytime soon. In its earnings call with investors, Microsoft Chief Financial Officer Amy Hood said the company expects Xbox will continue to decline \"in the low to mid-single digits\" for the following quarter. That's mostly due to the lack of landmark first-party titles. Just this month, Xbox released Ninja Gaiden 4, The Outer Worlds 2, and Double Fine's The Keeper. Xbox also made a huge marketing push for its first handheld, made in partnership with Asus, the ROG Xbox Ally and Ally X. In any other year, this would be a big month for any gaming company. The dour outlook comes after months of bad news. After two subsequent price hikes, Xbox Series S and Series X consoles now cost between $100 to $150 more than they did at launch five years ago. Microsoft also pushed prices of its Game Pass Ultimate subscription tier from $20 to $30 per month. A full-year's subscription would now demand $360. In a separate article, Gizmodo reviews Microsoft's new ROG Xbox Ally X handheld, which \"offers a better experience overall\" than the \"other small-scale Windows PC gaming devices released this year.\" However, \"it's still nowhere close to what you truly want from a console.\"","contentLength":2436,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"OpenAI Launches Aardvark To Detect and Patch Hidden Bugs In Code","url":"https://it.slashdot.org/story/25/10/31/2314223/openai-launches-aardvark-to-detect-and-patch-hidden-bugs-in-code?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1761963000,"author":"BeauHD","guid":237,"unread":true,"content":"OpenAI has introduced Aardvark, a GPT-5-powered autonomous agent that scans, reasons about, and patches code like a human security researcher. \"By embedding itself directly into the development pipeline, Aardvark aims to turn security from a post-development concern into a continuous safeguard that evolves with the software itself,\" reports InfoWorld. From the report: What makes Aardvark unique, OpenAI noted, is its combination of reasoning, automation, and verification. Rather than simply highlighting potential vulnerabilities, the agent promises multi-stage analysis -- starting by mapping an entire repository and building a contextual threat model around it. From there, it continuously monitors new commits, checking whether each change introduces risk or violates existing security patterns.\n \nAdditionally, upon identifying a potential issue, Aardvark attempts to validate the exploitability of the finding in a sandboxed environment before flagging it. This validation step could prove transformative. Traditional static analysis tools often overwhelm developers with false alarms -- issues that may look risky but aren't truly exploitable. \"The biggest advantage is that it will reduce false positives significantly,\" noted Jain. \"It's helpful in open source codes and as part of the development pipeline.\"\n \nOnce a vulnerability is confirmed, Aardvark integrates with Codex to propose a patch, then re-analyzes the fix to ensure it doesn't introduce new problems. OpenAI claims that in benchmark tests, the system identified 92 percent of known and synthetically introduced vulnerabilities across test repositories, a promising indication that AI may soon shoulder part of the burden of modern code auditing.","contentLength":1724,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"FCC To Rescind Ruling That Said ISPs Are Required To Secure Their Networks","url":"https://it.slashdot.org/story/25/10/31/237241/fcc-to-rescind-ruling-that-said-isps-are-required-to-secure-their-networks?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1761960600,"author":"BeauHD","guid":236,"unread":true,"content":"The FCC plans to repeal a Biden-era ruling that required ISPs to secure their networks under the Communications Assistance for Law Enforcement Act, instead relying on voluntary cybersecurity commitments from telecom providers. FCC Chairman Brendan Carr said the ruling \"exceeded the agency's authority and did not present an effective or agile response to the relevant cybersecurity threats.\" Carr said the vote scheduled for November 20 comes after \"extensive FCC engagement with carriers\" who have taken \"substantial steps... to strengthen their cybersecurity defenses.\" Ars Technica reports: The FCC's January 2025 declaratory ruling came in response to attacks by China, including the Salt Typhoon infiltration of major telecom providers such as Verizon and AT&amp;T. The Biden-era FCC found that the Communications Assistance for Law Enforcement Act (CALEA), a 1994 law, \"affirmatively requires telecommunications carriers to secure their networks from unlawful access or interception of communications.\"\n \n\"The Commission has previously found that section 105 of CALEA creates an affirmative obligation for a telecommunications carrier to avoid the risk that suppliers of untrusted equipment will \"illegally activate interceptions or other forms of surveillance within the carrier's switching premises without its knowledge,'\" the January order said. \"With this Declaratory Ruling, we clarify that telecommunications carriers' duties under section 105 of CALEA extend not only to the equipment they choose to use in their networks, but also to how they manage their networks.\" A draft of the order that will be voted on in November can be found here (PDF).","contentLength":1658,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"What is Bending Spoons? Everything to know about AOL’s acquirer","url":"https://techcrunch.com/2025/10/31/what-is-bending-spoons-everything-to-know-about-aols-acquirer/","date":1761959494,"author":"Anna Heim","guid":271,"unread":true,"content":"<article>Bending Spoons remains largely unknown, even as its portfolio of products has served more than a billion people. </article>","contentLength":113,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Bluesky Hits 40 Million Users, Introduces 'Dislikes' Beta","url":"https://tech.slashdot.org/story/25/10/31/231232/bluesky-hits-40-million-users-introduces-dislikes-beta?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1761958200,"author":"BeauHD","guid":235,"unread":true,"content":"Bluesky has surpassed 40 million users and is launching a \"dislikes\" beta to improve its personalization algorithms and reduce toxic content. TechCrunch reports: With the \"dislikes\" beta rolling out soon, Bluesky will take into account the new signal to improve user personalization. As users \"dislike\" posts, the system will learn what sort of content they want to see less of. This will help to inform more than just how content is ranked in feeds, but also reply rankings.\n \nThe company explained the changes are designed to make Bluesky a place for more \"fun, genuine, and respectful exchanges\" -- an edict that follows a month of unrest on the platform as some users again criticized the platform over its moderation decisions. While Bluesky is designed as a decentralized network where users run their own moderation, some subset of Bluesky users want the platform itself to ban bad actors and controversial figures instead of leaving it up to the users to block them. Bluesky, however, wants to focus more on the tools it provides users to control their own experience.","contentLength":1076,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Wine 10.18 Released With More WoW64 Mode Improvements","url":"https://www.phoronix.com/news/Wine-10.18-Released","date":1761957308,"author":"Michael Larabel","guid":687,"unread":true,"content":"<article>Wine 10.18 is now available for capping off the month of October and working toward the code freeze for Wine 11.0 beginning in early December...</article>","contentLength":144,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How L.A. Scores “Vulnerability” of Unhoused People Is Changing: What You Need to Know","url":"https://hackernoon.com/how-la-scores-vulnerability-of-unhoused-people-is-changing-what-you-need-to-know?source=rss","date":1761956616,"author":"The Markup","guid":304,"unread":true,"content":"<p><em>Welcome to The Markup, where we use investigative reporting, data analysis, and software engineering to challenge technology to serve the public good. Sign up for</em><em><a href=\"https://themarkup.org/newsletter/klaxon\">Klaxon</a>, a newsletter that delivers our stories and tools directly to your inbox.</em></p><p>\\\nOne year after <a href=\"https://themarkup.org/investigation/2023/02/28/l-a-s-scoring-system-for-subsidized-housing-gives-black-and-latino-people-experiencing-homelessness-lower-priority-scores\">a Markup investigation</a> revealed racial bias in Los Angeles’s housing intake system for people experiencing homelessness, local politicians have pressed for reforms and the agency responsible for housing is taking steps to make its approach more equitable and effective.</p><p>\\\nShortly after our original investigation published, Los Angeles City Council Member Nithya Raman, who chairs the Housing and Homelessness committee, introduced <a href=\"https://clkrep.lacity.org/onlinedocs/2023/23-0281_misc_03-10-23.pdf\">a motion</a> citing the article and <a href=\"https://clkrep.lacity.org/onlinedocs/2023/23-0281_rpt_hh_03-15-23.pdf\">calling on</a> the Los Angeles Homeless Services Authority (LAHSA) to come up with a plan to reform its intake system. The legislation, approved <a href=\"https://clkrep.lacity.org/onlinedocs/2023/23-0281_CAF_3-24-23.pdf\">unanimously</a>, called specifically for greater fairness in the “vulnerability” scoring system that The Markup analyzed. Used by Los Angeles for the past decade, the system <a href=\"https://themarkup.org/investigation/2023/02/28/l-a-s-scoring-system-for-subsidized-housing-gives-black-and-latino-people-experiencing-homelessness-lower-priority-scores\">rated Black people as significantly less vulnerable</a> than White people year after year, making them less likely to obtain subsidized permanent housing.</p><p>\\\nBlack people are hugely overrepresented among unhoused people in L.A., making up about 9 percent of Los Angeles County’s population but about 30 percent of the county’s people experiencing homelessness.</p><p>\\\n“To see that the tool that we’re using to put people in line for housing was not actually housing unhoused Black Angelenos as quickly as we could was really surprising to me,” said Raman, who read the article in the Los Angeles Times, where it was <a href=\"https://www.latimes.com/california/story/2023-02-28/black-latino-homeless-people-housing-priority-list-los-angeles\">co-published</a>. Raman, who is currently running for re-election in District Four, in central LA, said the investigation “absolutely” spurred the council to act.</p><p>\\\nLAHSA, given a deadline of April 2023 in the legislation, still has not provided a reform plan. A spokesperson for the agency didn’t directly respond to a request for comment about the plan.</p><p>\\\nRaman said LAHSA has taken some steps in the past year to improve how it allocates housing. Among other changes, she said, the agency has started to prioritize some groups, including those already involved in housing programs and those who already have the documents required to move into a building, like an ID and social security number.</p><p>\\\nMeanwhile, the agency <a href=\"https://www.lahsa.org/news?article=936-changes-to-ces-psh-prioritization-and-matching\">also de-emphasized</a> the score’s importance in placing people for permanent housing. People applying for housing are scored on a 17-point scale. Previously, the people with the highest scores were given the highest priority, but now any person who scores an eight or above can be prioritized, depending on the other factors being considered.</p><p>\\\nStill, equity in the housing system remains a known problem. In November, researchers from the University of Southern California and the University of California Los Angeles, working in partnership with LAHSA, released <a href=\"https://cais.usc.edu/wp-content/uploads/2023/11/CESTTRR-Final-Report-2023.pdf\">a long-awaited study</a> on racial bias in the system and ways to reform the scoring system, known as the VI-SPDAT.</p><p>\\\nThe study, which analyzed scores across race and ethnicity, tracked with The Markup’s findings from earlier in the year, concluding that the scoring tool is biased toward White people and that it’s ineffective overall. The study, in some respects, went even further. Using data on who ultimately faced an “adverse” event, like jail or death, the researchers concluded that tool was “not much more accurate than a random guess at predicting vulnerability.”</p><p>\\\nThe study suggested several ways the scoring system could become more accurate and equitable, some of which matched The Markup’s reporting. The scoring system asks intensely personal questions about a person’s life, including around issues like violence and substance abuse, and the report <a href=\"https://cais.usc.edu/wp-content/uploads/2023/11/CESTTRR-Final-Report-2023.pdf#page=89\">recommends rewording and reframing</a> questions to make the survey less complex and more sensitive. A revised version of the system with new questions and scoring could substantially reduce bias, the researchers conclude.</p><p>\\\nFor example, the study suggests that the existing question about whether anyone has “forced you or tricked you to do things that you do not want to do” should be amended to stress that answering yes “will not result in punishment or any negative consequences.” Another question currently asks, “Are there any medications like painkillers that you don’t take the way the doctor prescribed or where you sell the medication?” The study suggested softening it to, “Do you have medication that you choose to sell instead of taking to help support yourself financially? Answering yes to this question will not result in punishment or negative consequences for you.” Several questions were suggested for removal entirely.</p><p>\\\nIn a written statement, LAHSA spokesperson Christopher Yee acknowledged that it’s long been clear that “the VI-SPDAT has shortcomings related to equity,” adding that the survey is “long, cumbersome, and not trauma-informed in the content of the questions or administration process.”</p><p>\\\nYee highlighted the study on recommended changes to the system and said the agency is “working with key partners and stakeholders to create a plan to implement and refine” a new iteration of the scoring system while it continues to use the old version.</p><p>\\\nThe agency, he noted, has already dropped a requirement to score people for interim housing entry or time-limited subsidy programs, but will still require scoring for permanent housing. LAHSA must use some sort of prioritization system to access certain federal housing funds under <a href=\"https://www.hud.gov/sites/documents/17-01CPDN.PDF\">rules</a> established by the U.S. Department of Housing and Urban development.</p><p>\\\nThe planned changes to the scoring system will first apply to screening for adults, and later the agency plans to explore changes to related tools for young people and families with children. The Markup found that racial disparities were even more stark for a variation of the VI-SPDAT used in Los Angeles for people under the age of 25.</p><p>\\\nYee’s statement did not provide a timeline for the revised tool’s launch, but in <a href=\"https://www.lahsa.org/documents?id=7737-cesttrr-report-faq.pdf\">a FAQ</a> released alongside the study LAHSA said service providers could expect more information early this year on changes to the intake process, known as the Coordinated Entry System.</p><p>\\\nRaman, for her part, said she’s withholding judgment until data can show how those changes affect who is housed. But, she said, “there’s no question in my mind that CES needs reform.”</p>","contentLength":6475,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Austria's Ministry of Economy Has Migrated To a Nextcloud Platform In Shift Away From US Tech","url":"https://yro.slashdot.org/story/25/10/31/2023230/austrias-ministry-of-economy-has-migrated-to-a-nextcloud-platform-in-shift-away-from-us-tech?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1761955800,"author":"BeauHD","guid":234,"unread":true,"content":"An anonymous reader quotes a report from ZDNet: Even before Azure had a global failure this week, Austria's Ministry of Economy had taken a decisive step toward digital sovereignty. The Ministry achieved this status by migrating 1,200 employees to a Nextcloud-based cloud and collaboration platform hosted on Austrian-based infrastructure. This shift away from proprietary, foreign-owned cloud services, such as Microsoft 365, to an open-source, European-based cloud service aligns with a growing trend among European governments and agencies. They want control over sensitive data and to declare their independence from US-based tech providers.\n \nEuropean companies are encouraging this trend. Many of them have joined forces in the newly created non-profit foundation, the EuroStack Initiative. This foundation's goal is \" to organize action, not just talk, around the pillars of the initiative: Buy European, Sell European, Fund European.\" What's the motive behind these moves away from proprietary tech? Well, in Austria's case, Florian Zinnagl, CISO of the Ministry of Economy, Energy, and Tourism (BMWET), explained, \"We carry responsibility for a large amount of sensitive data -- from employees, companies, and citizens. As a public institution, we take this responsibility very seriously. That's why we view it critically to rely on cloud solutions from non-European corporations for processing this information.\"\n \nAustria's move and motivation echo similar efforts in Germany, Denmark, and other EU states and agencies. The organizations include the German state of Schleswig-Holstein, which abandoned Exchange and Outlook for open-source programs. Other agencies that have taken the same path away from Microsoft include the Austrian military, Danish government organizations, and the French city of Lyon. All of these organizations aim to keep data storage and processing within national or European borders to enhance security, comply with privacy laws such as the EU's General Data Protection Regulation (GDPR), and mitigate risks from potential commercial and foreign government surveillance.","contentLength":2108,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"YouTube TV Loses ESPN, ABC and Other Disney Channels","url":"https://entertainment.slashdot.org/story/25/10/31/2017209/youtube-tv-loses-espn-abc-and-other-disney-channels?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1761953400,"author":"BeauHD","guid":233,"unread":true,"content":"Disney's channels, including ESPN, ABC, FX, and NatGeo, have gone dark on YouTube TV after Google and Disney failed to renew their carriage agreement before the October 30 deadline, with each side blaming the other for using unfair negotiating tactics and price hikes. YouTube TV says it will issue a $20 credit to subscribers if the blackout continues while negotiations proceed. Engadget reports: \"Last week Disney used the threat of a blackout on YouTube TV as a negotiating tactic to force deal terms that would raise prices on our customers,\" YouTube said in an announcement on its blog. \"They're now following through on that threat, suspending their content on YouTube TV.\" YouTube added that Disney's decision harms its subscribers while benefiting its own live TV products, such as Hulu+Live TV and Fubo.\n \nIn a statement sent to the Los Angeles Times, however, Disney accused Google's YouTube TV of choosing to deny \"subscribers the content they value most by refusing to pay fair rates for [its] channels, including ESPN and ABC.\" Disney also accused Google of using its market dominance to \"eliminate competition and undercut the industry-standard terms\" that other pay-TV distributors have agreed to pay for its content.","contentLength":1233,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Most Anticipated BNB Launch of 2025: $BALZ Brings The Meme Migration Home","url":"https://hackernoon.com/the-most-anticipated-bnb-launch-of-2025-$balz-brings-the-meme-migration-home?source=rss","date":1761951173,"author":"Chainwire","guid":303,"unread":true,"content":"<p>Singapore, Singapore, October 31st, 2025/Chainwire/--The Binance Smart Chain (BNB) network has seen renewed activity, and BALZ has emerged as one of its notable community movements, with over 40,000 active members before launch on X (@). Observers regard it as one of the more anticipated community-driven launches of the year, comparable to projects such as Aster and Four.meme.</p><p>Raising over $2 million within days of opening, BALZ has positioned itself as a significant project developing on BNB, despite its informal branding and memetic culture. </p><p>With more than 40,000 members prior to its anticipated token presale, the project has adopted an unconventional approach to community growth through guerrilla marketing and its \"rug pull recovery protocol.\"</p><p>Instead of allocating capital to influencer campaigns, the team integrated communities from Solana and Base, migrating them to BNB through its protocol. At the time of writing, more than 10,000 verified holders are in the process of migration.</p><h3>The Token Presale: Closing Tonight, October 31st at 23:59 PDT</h3><p>At the center of BALZ is the Fair-As-F* Launch (FAF), a limited-time token presale closing on October 31 at 23:59 PDT. Within days of opening, BALZ raised over $2 million, drawing parallels to earlier community-led launches such as Shiba Inu and Floki in 2020.</p><p>FAF is structured with a fixed price and specific time frame, allowing equal participation without insider advantages or automated trading. In a market that has frequently favored early access and automation, BALZ seeks to show that fairness can be built into its design.</p><h3>BNB Market Conditions and Timing</h3><p>The timing aligns with a significant shift in the cryptocurrency market. On October 10, 2025, the sector experienced its largest liquidation event to date, with $19 billion eliminated within 48 hours as Bitcoin declined from $126,000 to $105,000. This event represented market deleveraging rather than capitulation.</p><ul><li>Open interest decreased from $48.7 billion to $45.1 billion</li><li>Funding rates fell by 51 percent</li><li>Overleveraged positions were cleared</li></ul><p>The result is a market now characterized by conviction-based participants and institutional capital seeking new deployment opportunities.</p><p>Market structure mirrors 2020-2021 exactly:</p><ul><li>Bitcoin ETFs pulled in $2.71 billion during October 6-10, BlackRock's IBIT holding $65.26 billion</li><li>85% of institutional firms now allocate to digital assets</li><li>Fed rate cuts hit 93% probability for next quarter</li></ul><p>BNB Smart Chain Shows Continued Growth</p><ul><li>3.62 million daily active addresses in October 2025</li><li>Total Value Locked surged 217% to $17.1 billion</li><li>70% of BNB meme traders are currently profitable</li></ul><p>CZ is back. He changed his X profile from \"ex-@binance\" to \"@binance\" in September 2025. BNB hit an all-time high of $1,311. Real infrastructure that actually supports growth. BNB is where smart money is rotating.</p><p>BALZ is capturing this momentum at the exact moment Solana and Base communities are looking for an exit. Market observers note the project is one CZ tweet away from a billion-dollar market cap, similar to previous meme token cycles where single endorsements rapidly accelerated valuations into nine-figure territory.</p><p>The presale window closes October 31st at 23:59 PDT.</p><p>Follow: X: @ | Telegram: t.me/BALZ_Official</p><p> is a meme coin launching on Binance Smart Chain with a mission: to build the safest, fastest trading platform and no-code launchpad in crypto. Led by a doxxed team and powered by 40,000+ active members.</p><p>:::tip\n<em>This story was published as a press release by Chainwire under HackerNoon’s Business Blogging&nbsp;. Do Your Own Research before making any financial decision.</em></p>","contentLength":3623,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Amazon To Block Piracy Apps On Fire TV","url":"https://yro.slashdot.org/story/25/10/31/2012202/amazon-to-block-piracy-apps-on-fire-tv?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1761951000,"author":"BeauHD","guid":232,"unread":true,"content":"Amazon will begin blocking sideloaded piracy apps on Fire TV devices by cross-checking them against a blacklist maintained by the Alliance for Creativity and Entertainment. The company will, however, continue to allow legitimate sideloading for developers. Heise reports: In response to an inquiry, Amazon explained that it has always worked to ban piracy from its app store. As part of an expanded program led by the ACE, it is now blocking apps that demonstrably provide access to pirated content, including those downloaded outside the app store. This builds on Amazon's ongoing efforts to support creators and protect customers, as piracy can also expose users to malware, viruses, and fraud.\n \n[...] The sideloading option will remain available on Fire TV devices running Amazon's new operating system, Vega OS. However, it is generally limited to developers here. In this context, the company emphasized that, contrary to rumors, there are no plans to upgrade existing Fire TV devices with Fire OS as the operating system to Vega OS.","contentLength":1039,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Aster’s Rocket Launch Surpasses $1B in Trading Volume, as Nubila Joins with Over 6 Million $NB","url":"https://hackernoon.com/asters-rocket-launch-surpasses-$1b-in-trading-volume-as-nubila-joins-with-over-6-million-$nb?source=rss","date":1761950749,"author":"Chainwire","guid":302,"unread":true,"content":"<p>George Town, British Virgin Islands, October 31st, 2025/Chainwire/--, the decentralized trading platform, has generated strong momentum with its innovative product .</p><p>In the first six days following the debut of Rocket Launch, Aster recorded approximately $122 million in spot trading volume and $933 million in perpetual trading volume. Within five days after APRO’s $AT token TGE, Aster captured over 90% of the market share in $AT perpetual trading, underscoring Rocket Launch’s significant contribution to overall market activity.</p><p>Since its debut on October 24, Rocket Launch has meaningfully increased both user activity and engagement on the platform. On October 29, Aster announced a 500,000 $AT Loyalty Bonus distributed to early participants who traded within the first four days of the campaign.</p><p>The platform also disclosed that the spot trading competition features a reward pool of no less than 1.5 million $AT, followed by a perpetual trading campaign with at least 1.5 million $AT in additional rewards, marking a continuation of strong user engagement across both markets.</p><p>The first Rocket Launch event not only accelerated new user acquisition but also reactivated existing traders and token holders, significantly enhancing overall liquidity and engagement across the Aster ecosystem. This milestone demonstrates Rocket Launch’s strong driving force and long-term potential in shaping the growth of the Aster DeFi landscape.</p><p>Next Rocket Launch: Nubila Debuts, Powering the Physical Oracle Layer for AI and Prediction Markets</p><p>Aster announced that the next Rocket Launch will begin on October 31, 2025, at 12:00 UTC, featuring , a decentralized oracle network for AI and prediction markets. The seven-day campaign will include both spot and perpetual trading campaigns for Nubila ($NB).</p><p>The event adopts a dual reward structure. The Spot campaign offers a $200,000 $ASTER prize pool alongside over 3 million $NB in rewards, while the Perpetual campaign features an exclusive pool exceeding 3 million $NB, aimed at fostering broader participation and sustained market activity.</p><p>Continuing its long-term vision, Aster is redefining the evolution of token launches through Rocket Launch, transforming what used to be a single market event into a continuous, growth-oriented journey.</p><p>Each Rocket Launch campaign is structured to create a self-reinforcing value loop. The reward pool combines $ASTER and the project’s native tokens. Project teams contribute both capital and tokens, while Aster allocates those funds to buy back $ASTER from the open market.</p><p>The repurchased $ASTER, together with the project tokens, are then distributed as rewards to participants, ensuring that users benefit directly from both trading activity and ecosystem growth.</p><blockquote><p>“Aster’s Rocket Launch is more than a trading campaign; it’s an engine for on-chain innovation,” said Leonard, CEO of Aster. “Every participant becomes part of the ecosystem, contributing to the process of value creation for emerging projects.”</p></blockquote><p> is building the physical oracle layer for AI and prediction markets. Its decentralized sensor network captures real-world data and transforms it into verifiable intelligence for AI systems and smart contracts.</p><p>Backed by BCG, Block Space Force, Quantum Holdings, VeChain, and IoTeX, Nubila has deployed 21,000+ devices across 122 countries and 16,000+ validator nodes, powering the next wave of AI agents and decentralized applications with real, trustworthy physical data.</p><p> is a next-generation decentralized exchange offering both Perpetual and Spot trading, designed as a one-stop onchain venue for global crypto traders. It features MEV-free, one-click execution in 1001x Mode. Perpetual Mode adds 24/7 stock Perpetuals, Hidden Orders, and grid trading, available across BNB Chain, Ethereum, Solana, and Arbitrum.</p><p>Its unique edge lies in the ability to use liquid-staking tokens (asBNB) or yield-generating stablecoins (USDF) as collateral, unlocking unparalleled capital efficiency. Backed by YZi Labs, Aster is building the future of DeFi: fast, flexible, and community-first.</p><p>:::tip\n<em>This story was published as a press release by Chainwire under HackerNoon’s Business Blogging&nbsp;. Do Your Own Research before making any financial decision.</em></p>","contentLength":4256,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Denmark Reportedly Withdraws 'Chat Control' Proposal Following Controversy","url":"https://yro.slashdot.org/story/25/10/31/205234/denmark-reportedly-withdraws-chat-control-proposal-following-controversy?utm_source=rss1.0mainlinkanon&utm_medium=feed","date":1761948600,"author":"BeauHD","guid":231,"unread":true,"content":"An anonymous reader quotes a report from The Record: Denmark's justice minister on Thursday said he will no longer push for an EU law requiring the mandatory scanning of electronic messages, including on end-to-end encrypted platforms. Earlier in its European Council presidency, Denmark had brought back a draft law which would have required the scanning, sparking an intense backlash. Known as Chat Control, the measure was intended to crack down on the trafficking of child sex abuse materials (CSAM). After days of silence, the German government on October 8 announced it would not support the proposal, tanking the Danish effort.\n \nDanish Justice Minister Peter Hummelgaard told reporters on Thursday that his office will support voluntary CSAM detections. \"This will mean that the search warrant will not be part of the EU presidency's new compromise proposal, and that it will continue to be voluntary for the tech giants to search for child sexual abuse material,\" Hummelgaard said, according to local news reports. The current model allowing for voluntary scanning expires in April, Hummelgaard said. \"Right now we are in a situation where we risk completely losing a central tool in the fight against sexual abuse of children,\" he said. \"That's why we have to act no matter what. We owe it to all the children who are subjected to monstrous abuse.\"","contentLength":1358,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"GNOME Gains A New macOS-Inspired Quick Menu Option","url":"https://www.phoronix.com/news/GNOME-Kiwi-macOS-Quick-Menu","date":1761946882,"author":"Michael Larabel","guid":686,"unread":true,"content":"<article>For GNOME desktop users desiring a more macOS-like experience, a new GNOME extension provides a macOS-inspired quick menu option...</article>","contentLength":131,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Two Windows vulnerabilities, one a 0-day, are under active exploitation","url":"https://arstechnica.com/security/2025/10/two-windows-vulnerabilities-one-a-0-day-are-under-active-exploitation/","date":1761944636,"author":"Dan Goodin","guid":334,"unread":true,"content":"<p>Two Windows vulnerabilities—one a zero-day that has been known to attackers since 2017 and the other a critical flaw that Microsoft initially tried and failed to patch recently—are under active exploitation in widespread attacks targeting a swath of the Internet, researchers say.</p><p>The zero-day&nbsp;went undiscovered until <a href=\"https://www.trendmicro.com/en_us/research/25/c/windows-shortcut-zero-day-exploit.html\">March</a>, when security firm Trend Micro said it had been under active exploitation since 2017, by as many as 11 separate advanced persistent threats (APTs). These APT groups, often with ties to nation-states, relentlessly attack specific individuals or groups of interest. Trend Micro went on to say that the groups were exploiting the vulnerability, then tracked as ZDI-CAN-25373, to install various known post-exploitation payloads on infrastructure located in nearly 60 countries, with the US, Canada, Russia, and Korea being the most common.</p><h2>A large-scale, coordinated operation</h2><p>Seven months later, Microsoft still hasn’t patched the vulnerability, which stems from a bug in the <a href=\"https://learn.microsoft.com/en-us/openspecs/windows_protocols/ms-shllink/16cb4ca1-9339-4d0c-a68d-bf1d6cc0f943\">Windows Shortcut</a> binary format. The Windows component makes opening apps or accessing files easier and faster by allowing a single binary file to invoke them without having to navigate to their locations. In recent months, the ZDI-CAN-25373 tracking designation has been changed to CVE-2025-9491.</p>","contentLength":1299,"flags":null,"enclosureUrl":"https://cdn.arstechnica.net/wp-content/uploads/2022/10/windows-malware-1024x648.jpg","enclosureMime":"","commentsUrl":null},{"title":"Bluesky hits 40 million users, introduces ‘dislikes’ beta","url":"https://techcrunch.com/2025/10/31/bluesky-hits-40-million-users-introduces-dislikes-beta/","date":1761941646,"author":"Sarah Perez","guid":270,"unread":true,"content":"<article>As users \"dislike\" posts, the system will learn what sort of content they want to see less of. This will help to inform more than just how content is ranked in feeds, but also reply rankings.</article>","contentLength":191,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Meta bought 1 GW of solar this week","url":"https://techcrunch.com/2025/10/31/meta-bought-1-gw-of-solar-this-week/","date":1761938790,"author":"Tim De Chant","guid":269,"unread":true,"content":"<article>The social media company inked three deals in the U.S. to power its data centers and offset its carbon footprint.</article>","contentLength":113,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AI mania tanks CoreWeave’s Core Scientific acquisition — it buys Python notebook Marimo","url":"https://techcrunch.com/2025/10/31/ai-mania-tanks-coreweaves-core-scientific-acquisition-it-buys-python-notebook-marimo/","date":1761936828,"author":"Julie Bort","guid":268,"unread":true,"content":"<article>CoreWeave's failed buy of Core Scientific is another sign of an AI bubble. But it's still shopping.</article>","contentLength":99,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Holography in Cuprates: Critical Review of Quantitative Claims","url":"https://hackernoon.com/holography-in-cuprates-critical-review-of-quantitative-claims?source=rss","date":1761932719,"author":"The Tech Reckoning is Upon Us!","guid":301,"unread":true,"content":"<p>The theories of both, finite- and zero-density, spinons have been extensively discussed in the context of the ’strange metal’ phase in the underdoped cuprates and other (arguably, even stranger) heavy-fermion compounds long before the advent of holography [3]. Once there, the applied holography quickly joined the quest into the properties of this phase that had long evaded a consistent and satisfactory explanation.</p><p>\\\nInstead of going after the NFL fermion propagator, however, many of the holographic proposals focused on reproducing the experimental data in the cuprates - and often times even claimed achieving a quantitative agreement.</p><p>\\\nIn light of its intrinsically unsettled status one would have thought that it might be rather detrimental for any speculative approach to seek out not a mere qualitative but an actual quantitative, down to the number, agreement between its specific predictions and some preselected sets of experimental data. In fact, if such a quantitative agreement were indeed achieved one would have even more explaining to do (first and foremost, as to why an apriori approximate approach appears to be so unexpectedly accurate?).</p><p>\\\nThe earlier discussion of some of the popular evidence in support of condensed matter holography as well as the debunking of a number of its specific predictions [26] can be found in [34]. However, the admirable persistence with which those predictions continued to be regularly cited in the subsequent holographic literature [35] suggests that the comments of [34] might have had been (most regretfully) overlooked.</p><p>\\\nIn fact, there is more than a single reason for which semiclassical holography (or its improvement at the level of accounting for the matter back-reaction in the HartreeFock approximation) - thus far, the only practical way of performing the holographic calculations [26–29] - would not have been expected to provide any quantitatively accurate results in the first place. There are, of course, such obvious differences from the string-theoretical holographic constructions as a low physical value of N (which, in practice, often amounts to ’spin up/down’) and the lack of Lorentz, translational, and/or rotational (as well as any super-)symmetries.</p><p>\\\nArguably, though, the most important is the fact that much of the condensed matter physics operates in the intermediate - as opposed to ultra-strong - interaction regime, while it is only the latter that is supposed to have a weakly coupled gravity as its bulk dual [26]. Indeed, most solids form under the condition that its potential (interaction) and kinetic energies on average balance each other out. This suggests that the ’bona fide’ strong-coupling regime could only become attainable in some sort of a ’flat band’ scenario where kinetic energy is completely quenched or, at least, significantly diminished.</p><p>\\\nIn light of that, it is unsurprising that much of the recent effort towards implementing such mechanism has been centered on the SYK model and its variants [31] whose ’flat band’ nature facilitates the existence of a holographic dual. A viable candidate to this role was proposed in the form of the Jackiw-Teitelboim (JT) dilatonenhanced 1 + 1-dimensional gravity [31].</p><p>\\\nIt is worth pointing out, though, that at the practical level all the holographic matching between the SYK and JT theories has been, so far, established within their low-energy sectors that are both controlled by a single soft Schwarzian mode (’boundary graviton’). So as far as the low-energy properties of the two models are concerned, they both allow for the same (effectively 0 + 1- dimensional) description in terms of either a fluctuating 1d boundary or Liouvillian-type large-N matrix quantum mechanics [31, 36]. This is not surprising given the intrinsically non-dynamical nature of 2d (and 3d) pure gravity. Such a caveat notwithstanding, the low-energy SYK-JT equivalence has been repeatedly and staunchly referred to as a genuine example of holographic correspondence between the 1+1-dimensional bulk and 0+1-dimensional boundary theories [31].</p><p>\\\nAs to the general HV models (22) and corresponding vacuum metrics (26), the standard list of observables to be matched includes temperature-dependent specific heat</p><p>\\\nand frequency-dependent optical conductivity</p><p>\\\ndetermined by the bare scaling dimensions.</p><p>\\\nIncidentally, this value of the HV parameter was previously singled out on the basis of analyzing entanglement entropy [28]. Besides, it suggests the interpretation of d − θ as an effective number of dimensions orthogonal to the FS.</p><p>\\\nThe other frequently invoked relation [26, 28, 29] is</p><p>\\\nin which case the first inequality in (27) is marginally satisfied as equality. Notably, in 2d it would only be consistent with (40) for z = 3/2.</p><p>\\\nAlso, from the beginning of the cuprates saga an even greater fixation has always been on the linear-T dependence of resistivity, also observed in a variety of other materials [35]. Of course, the conductivity scaling with frequency (39) does not readily translate into its temperature dependence, as it would be determined by a specific mechanism of momentum relaxation (i.e., Umklapp, phonons, and/or disorder).</p><p>\\\nTo this end, the use of the memory matrix technique yielded a proper conductivity scaling [26, 35] in both limits of strong,</p><p>\\\nmomentum-non-conserving scattering where ∆ is the dimension of the leading translation invariance-breaking 8 operator. The formulas (42) and (43) agree for ∆ = z + (d − θ)/2 which condition coincides with that of marginal fulfillment of the Harris criterion for the disorder scattering to become a relevant perturbation.</p><p>\\\nAn alternate interpretation of the linear-T resistivity, σ(T ) ∼ 1/T , proposed in [26, 35] relates it to the FL-like entropy, S(T ) ∼ C(T ) ∼ T . This school of thought introduces the notion of inelastic ’Planckian’ scattering rate as a potentially single most important scale for thermalization/equilibration/information scrambling (albeit not a rate of momentum relaxation) in strongly interacting systems</p><p>\\\nInterestingly, it is the (admittedly, unphysical) model of [38] that so far has managed to reproduce a longer list of the power-law dependencies found in the cuprates, as compared to the competing schemes [39]. Unfortunately, such a serendipitous success does not offer any immediate insight into the underlying mechanism of the NFL behavior in the cuprates.</p><p>\\\nFurthermore, contrasting the large-r and -τ asymptotics (31) of the HV holographic propagators against their eikonal/bosonization counterparts in search of some agreement suggests finite positive values of θ, contrary to the ’Planckian’ scenario. This observation might further reduce the chances of constructing a consistent HV holographic model of the strange metal phase in the cuprates.</p><p>\\\nIn part, the deficiencies of the HV-based approach have been circumvented by the arrival of the ’second SYK wave’ [40] which utilizes the Hamiltonian obtained from the conventional combination of a kinetic (quadratic) and interaction (quartic) terms by randomizing the amplitudes of either one or both of these terms a la SY K. Making such randomization spatially non-uniform one opens a channel for non-conservation of momentum which then gives rise to the linear-T ’Planckian’ rate (on top of a constant).</p><p>\\\nOf course, the very existence of different explanations (cf., for example, [35, 39] and [40]) for certain scaling laws observed in the cuprates may suggest that their ultimate interpretation is yet to be found. It would be, therefore, imperative to strive to extend the list of matching properties, akin to [38, 39] as the means of discriminating between the competing schemes.</p><p>(1) D. V. Khveshchenko, Department of Physics and Astronomy, University of North Carolina, Chapel Hill, NC 27599.</p>","contentLength":7853,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ChatGPT: Everything you need to know about the AI-powered chatbot","url":"https://techcrunch.com/2025/10/31/chatgpt-everything-to-know-about-the-ai-chatbot/","date":1761932180,"author":"Kyle Wiggers, Cody Corrall, Alyssa Stringer, Kate Park","guid":267,"unread":true,"content":"<article>A timeline of ChatGPT product updates and releases, starting with the latest, which we’ve been updating throughout the year.</article>","contentLength":126,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Tattd gave four TechCrunch writers tattoos at Startup Battlefield","url":"https://techcrunch.com/2025/10/31/tattd-gave-four-techcrunch-writers-tattoos-at-startup-battlefield/","date":1761931929,"author":"Amanda Silberling","guid":266,"unread":true,"content":"<article>Tattd, a marketplace for tattoo-seekers and artists, set up a mini tattoo parlor in the Expo Hall at TechCrunch Disrupt 2025.</article>","contentLength":125,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Hackers threaten to leak data after breaching University of Pennsylvania to send mass emails","url":"https://techcrunch.com/2025/10/31/hackers-threaten-to-leak-data-after-breaching-university-of-pennsylvania-to-send-mass-emails/","date":1761931802,"author":"Amanda Silberling","guid":265,"unread":true,"content":"<article>As the hackers plainly stated in their message (\"Please stop giving us money\"), this breach appears motivated to suppress alumni donations. </article>","contentLength":140,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AWS exceeds Wall Street’s expectations as demand for cloud infra remains high","url":"https://techcrunch.com/2025/10/31/aws-exceeds-wall-streets-expectations-as-demand-for-cloud-infra-remains-high/","date":1761929958,"author":"Rebecca Szkutak","guid":264,"unread":true,"content":"<article>AWS continues to see strong demand as companies gobble up its cloud infrastructure services in the age of AI. </article>","contentLength":110,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Government hackers breached telecom giant Ribbon for months before getting caught","url":"https://techcrunch.com/2025/10/31/government-hackers-breached-telecom-giant-ribbon-for-months-before-getting-caught/","date":1761929138,"author":"Zack Whittaker","guid":263,"unread":true,"content":"<article>Ribbon, which provides software and technology to phone and internet giants, said nation-state hackers were in its systems since at least December 2024.</article>","contentLength":152,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Holographic Propagators: Geodesics and Local Criticality","url":"https://hackernoon.com/holographic-propagators-geodesics-and-local-criticality?source=rss","date":1761927304,"author":"The Tech Reckoning is Upon Us!","guid":300,"unread":true,"content":"<p>The early holographic studies of fermion propagators [28] produced a number of intriguing results, including multiple Fermi surfaces (which merge into one critical ’Fermi ball’ in some extreme limits), dispersionless poles, and oscillatory frequency dependence (which was later shown not to arise in more systematic ’top down’ constructions [26]), etc. A physical interpretation of those results is impeded by the fact that much of this work is numerical.</p><p>\\\nA simple and amenable to analytical treatment semiclassical calculation can be performed in the regime mL ≫ 1 where m is a mass of the conjectured dual bulk fermion [28, 29]. In this regime, the fermion’s paths contributing to various quantum-mechanical amplitudes follow closely the classical boundary-to-boundary trajectories (geodesics) derived from the (imaginary-time) action</p><p>\\\nby varying over τ(u) and r(u).</p><p>\\\nEvaluating this action on its geodesic one obtains</p><p>\\\nWhile an explicit analytic computation of (29) can only be performed in some special cases, the one-parameter space/time dependencies can be readily found for a broad variety of metrics. Specifically, for the HV metric (26) one obtains [29, 30]</p><p>\\\nNotably, in the absence of hyperscaling violation (θ = 0) both these asymptotics become either constant (less likely) or logarithmic (more likely, see below). Thus, if the classical EMD Lagrangian (22) were to represent a valid bulk dual of a boundary theory with the gauge-like interaction (1) the asymptotics (31) would not be readily reconcilable with the eikonal/bosonization results (11,21) which depend primarily on z (via η) rather than θ.</p><p>\\\nand is composed of the two independent solutions which read</p><p>\\\nImposing the proper boundary conditions and following the holographic dictionary [26] one then defines the propagator as a reflection coefficient for the wave incident at the boundary</p><p>\\\nA different behavior (unattainable in the case of a HV metric (26) with finite z and θ) occurs for α = β + 1 in which case the integral in (33) diverges at u → 0. This peculiar NFL regime, dubbed ’local criticality’, is characterized by the propagator</p><p>\\\nwhere a(k), b(k), and ν(k) ∼ k are non-singular functions of momentum that can, in general, produce multiple poles identified as the distinct (’fractionalized’) FS [28].</p><p>\\\nFourier transforming (36) is complicated by the fact that G(ω, k) is not analytically known across the entire range of its arguments. However, the fast (and/or furious) Fourier transformation via a saddle point suggests the following form of this function in the spacetime domain</p><p>\\\nAdding to the intrigue, there are some recent Monte Carlo results on the 2d Hubbard and t − J models that have long been thought to represent the prototypical NFL normal state in the cuprates. These results do not readily conform to a momentum-independent, yet strongly energy-dependent, self-energy function, showing less of energy/temperature dependence than any of the above expressions [33]. It remains to be seen as to what this might imply for the general applicability of the theories of fermions (’spinons’) governed by the interactions (1) to the analysis of those microscopic models.</p><p>(1) D. V. Khveshchenko, Department of Physics and Astronomy, University of North Carolina, Chapel Hill, NC 27599.</p>","contentLength":3313,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"KosmicKrisp Now Vulkan 1.3 Compliant For Apple Devices","url":"https://www.phoronix.com/news/KosmicKrisp-Vulkan-1.3","date":1761927280,"author":"Michael Larabel","guid":685,"unread":true,"content":"<article>Over the summer months LunarG announced KosmicKrisp as a new Vulkan-on-Metal implementation for Apple devices and built around Mesa. That alternative to MoltenVK was upstreamed for next quarter's Mesa 26.0 release and now it's also celebrating being an officially Vulkan 1.3 conformant implementation...</article>","contentLength":303,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The HackerNoon Newsletter: The Road to Hell is Paved with Good DRY Intentions (10/31/2025)","url":"https://hackernoon.com/10-31-2025-newsletter?source=rss","date":1761926592,"author":"Noonification","guid":299,"unread":true,"content":"<p>🪐 What’s happening in tech today, October 31, 2025?</p><p>By <a href=\"https://hackernoon.com/u/melvin-manni\">@melvin-manni</a> [ 5 Min read ] Learn how good intentions can lead to spaghetti dry code, over abstraction and over engineered systems.  <a href=\"https://hackernoon.com/the-road-to-hell-is-paved-with-good-dry-intentions\">Read More.</a></p><p>By <a href=\"https://hackernoon.com/u/salkimmich\">@salkimmich</a> [ 15 Min read ] The evolution of workload identity: Kerberos to X.509 to SPIFFE to TWI. Why credentials should expire faster than your containers run. <a href=\"https://hackernoon.com/workload-identity-what-history-teaches-us-about-the-future-of-machine-identity\">Read More.</a></p><p>🧑‍💻 What happened in your world this week?</p><p>We hope you enjoy this worth of free reading material. Feel free to forward this email to a nerdy friend who'll love you for it.See you on Planet Internet! With love, \n The HackerNoon Team ✌️</p>","contentLength":624,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Perplexity strikes multi-year licensing deal with Getty Images","url":"https://techcrunch.com/2025/10/31/perplexity-strikes-multi-year-licensing-deal-with-getty-images/","date":1761925574,"author":"Rebecca Bellan","guid":262,"unread":true,"content":"<article>Perplexity’s agreement with Getty appears to legitimize some of the startup’s previous use of Getty’s stock photos. Perplexity came under fire last year for a series of plagiarism accusations from several news organizations. </article>","contentLength":231,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Video Friday: Happy Robot Halloween!","url":"https://spectrum.ieee.org/video-friday-robot-halloween-2674252642","date":1761924603,"author":"Evan Ackerman","guid":81,"unread":true,"content":"<p>Your weekly selection of awesome robot videos</p>","contentLength":45,"flags":null,"enclosureUrl":"https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTk5MzIzNy9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTgwMjU4MDA0M30.nBNXvmaVwqodlAcXGpck-A2yCbWsmFT6eVePaIA2W5Y/image.png?width=600","enclosureMime":"","commentsUrl":null},{"title":"Tim Cook says Apple is open to M&A on the AI front","url":"https://techcrunch.com/2025/10/31/tim-cook-says-apple-is-open-to-ma-on-the-ai-front/","date":1761923832,"author":"Sarah Perez","guid":261,"unread":true,"content":"<article>Apple CEO Tim Cook noted in the company's Q4 2025 earnings call that Apple was preparing to announce more AI partnerships like the one it has with OpenAI to integrate ChatGPT into Siri and Apple Intelligence.</article>","contentLength":208,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Luminar is cutting jobs, losing its CFO, and warning of a cash shortage","url":"https://techcrunch.com/2025/10/31/luminar-is-cutting-jobs-losing-its-cfo-and-warning-of-a-cash-shortage/","date":1761922849,"author":"Sean O'Kane","guid":260,"unread":true,"content":"<article>The new turmoil comes as founder Austin Russell is trying to buy the company just a few months after being replaced as CEO.</article>","contentLength":123,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Ubiquitous NFL Problem: Comparing Bosonization, Eikonal, and Holographic Techniques","url":"https://hackernoon.com/the-ubiquitous-nfl-problem-comparing-bosonization-eikonal-and-holographic-techniques?source=rss","date":1761922824,"author":"The Tech Reckoning is Upon Us!","guid":298,"unread":true,"content":"<p>Compared to what it has been just recently [26], the seemingly endless flurry of holographic publications in JHEP, PRD, and other traditional ’condensed matter oriented’ venues has been steadily coming to a mere trickle. Those few holographic exercises that do occasionally pop out still tend to begin with the mantra ’holography is well known to be an established method for studying strongly correlated systems’. However, this optimistic reassurance often appears to be in a rather stark contrast with the typical summary that sounds more like ’as no unambiguous agreement with experiment was found, the problem is left to future work’.</p><p>\\\nAlso, much of the original thrust towards boldly treating an arbitrary condensed matter system of interest as yet another application of some opportunistically chosen weakly-coupled semiclassical gravity has retreated into a ’safer-haven’ topic of hydrodynamics (which, while highlighted and revitalized by holography, can be - and of course had long been - successfully discussed without ever mentioning the latter).</p><p>\\\nOn the outside, it may seem as though the heuristic ’holo-hacking’ (a.k.a. ’bottom up’ or ’non-AdS/nonCFT’) approach tends to pick out its favorite gravity-like bulk theory on the basis of such physically compelling reasons as an existence of the previously found classical solutions and normal modes’ spectra, availability of the numerical simulation software, or mere need to engage students with the tangible computational tasks.</p><p>\\\nHowever, apart from having become a massive and customary practice, there hasn’t been much effort made towards any serious justification of neither the overall holographic scheme, nor its specific ’dictionary’ which was copy-pasted from the original string-theoretical framework. In that regard, it might be worth keeping in mind that just because everyone else on a highway may be driving above the posted speed limit does not by itself make it legal.</p><p>\\\nIn light of the above, comparing holographic propagators to the predictions of other techniques could provide an additional testing ground for, both, the alternate methods as well as the holographic approach itself. the bulk metric, gauge, and scalar (dilaton) fields [26]</p><p>\\\nAmong all the classical solutions of the theory (22) there is a special class of Lifshitz metrics (θ = 0) which were discovered in the semiclassical (ThomasFermi) analysis of matter back-reaction on the metric, as well as in the ’electron star’ scenarios, etc. [27].</p><p>\\\nMore generally, any viable solutions of (22) must obey certain stability (’null energy’) conditions [26]</p><p>(1) D. V. Khveshchenko, Department of Physics and Astronomy, University of North Carolina, Chapel Hill, NC 27599.</p>","contentLength":2751,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"YC alum Adam raises $4.1M to turn viral text-to-3D tool into AI copilot","url":"https://techcrunch.com/2025/10/31/yc-alum-adam-raises-4-1m-to-turn-viral-text-to-3d-tool-into-ai-copilot/","date":1761920436,"author":"Anna Heim","guid":259,"unread":true,"content":"<article>After generating over 10 million social media impressions with the launch of its text-to-3D model app, Adam has raised a $4.1 million seed round to power its next steps.</article>","contentLength":169,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Saving 20+ Hours a Week: How Jamie I.F. Built AffiliateFinder.ai to Automate Affiliate Recruitment","url":"https://hackernoon.com/saving-20-hours-a-week-how-jamie-if-built-affiliatefinderai-to-automate-affiliate-recruitment?source=rss","date":1761920220,"author":"NewsByte.Tech","guid":297,"unread":true,"content":"<p>Affiliate and influencer programs is one of the MOST profitable revenue channels for SaaS businesses - but everyone complains about how hard it is to find good affiliates and influencers to promote them. It takes a LOT of manual work.</p><p>So, we save you 20+ hours per week by finding every affiliate currently promoting your competitors so you can recruit them, as well as all the top influencers and creators in your niche to build partnerships with.</p><h2>3. What do you love about your team, and why are you the ones to solve this problem?</h2><p>We’re a team of people who all have a LOT of experience in affiliate marketing - either as affiliate marketers ourselves, or as affiliate managers. We get the industry.</p><p>We know how time-consuming it is to search for new affiliates all the time - and we literally built <a href=\"http://AffiliateFinder.ai\">AffiliateFinder.ai</a> to solve our own problem while running our affiliate management agency! We know what we want in a software like this, so we feel we know exactly how to build this to solve other people having similar problems.</p><h2>4. If you weren’t building your startup, what would you be doing?</h2><p>Probably building a different startup! I really enjoy the game, and I wouldn’t trade it for anything.</p><h2>5. At the moment, how do you measure success? What are your metrics?</h2><p>Product feedback, customer growth, and retention. If we don’t have a good product, people will not complete their 7-day free trial, and we won’t grow customers or revenue. Everything we’re focused on now is around retention, and not just building something they’ll use once to get a list of affiliates, but how to make this incredibly helpful as an ongoing companion for all your partner management work.</p><h2>6. In a few sentences, what do you offer to whom?</h2><p><a href=\"http://AffiliateFinder.ai\">AffiliateFinder.ai</a> helps affiliate, influencer, and partnerships teams 3x their affiliate recruitment by finding all the best-fit potential affiliates for them - and ordering them by priority so they can start with the absolute top partners.</p><p>We save you 20+ hours per week of boring, manual research - freeing you up to focus on building those relationships with affiliates so they can send you sales while you sleep.</p><p>It works great, no matter what type of business you are: B2B SaaS, AI, DTC/ecom, travel, iGaming, fintech and trading - we have many customers across all types of businesses.</p><h2>7. What’s most exciting about your traction to date?</h2><p>We’re used by several of the largest companies in the world. It’s always validating when multi-billion dollar companies use your tool and find it valuable enough to use and pay for!</p><p>And, surprisingly, some of our first customers were forward-thinking managers at these huge companies - they saw our product and instantly understood how it could help them scale their affiliate revenue and outcompete their competition.</p><h2>8. Where do you think your growth will be next year?</h2><p>We’d like to reach 2,000 paying customers by the end of next year, and we’re building to hit that right now.</p><p>If we can help those 2,000 brands reach their goals by recruiting more partners - then we’ll be extremely happy. We have a lot of great products and features we want to build out, and we’re very excited to build these for the world.</p><h2>9. Tell us about your first paying customer and revenue expectations over the next year.</h2><p>It took us a while to get our first paying customer - a good few months from launch.</p><p>We originally launched with a freemium option where you’d get your first 15 affiliates for free, and then you had to upgrade yourself to get the full version. But nobody was upgrading!</p><p>Once we switched to a 7-day free trial, the customers started rolling in.</p><h2>10. What’s your biggest threat?</h2><p>We’re an AI-powered tool, and we use AI to filter out bad-fit affiliates - and recommend the good fits. But, naturally, if there is an extremely advanced AI that can eventually do this all, then this is a huge threat to us. But, we’re working on custom data to make us the most useful tool in affiliate marketing. That hopefully keeps our competitive advantage as AI improves.</p>","contentLength":4037,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Construct Koin Plans to Bridge a $300 Trillion Market Gap in Real Estate Financing","url":"https://hackernoon.com/how-construct-koin-plans-to-bridge-a-$300-trillion-market-gap-in-real-estate-financing?source=rss","date":1761919399,"author":"Ishan Pandey","guid":296,"unread":true,"content":"<blockquote><h2>Can blockchain technology solve century-old problems in real estate financing, or does it represent another attempt to apply a solution in search of a problem?</h2></blockquote><p>The question becomes more pressing as <a href=\"https://www.constructkoin.com/\">Construct Koin</a> launches a presale that aims to raise $100 million by offering tokens that start at $0.10 and scale to $1 across 10 phases. The project positions itself as a Real Estate Financing (ReFi) protocol that will modernize how capital flows into property development, but the proof will depend on whether it can deliver where others have failed.</p><p>\\\nThe timing appears calculated. The <a href=\"https://coinpedia.org/research-report/top-real-world-asset-rwa-projects/\">tokenized real-world asset market crossed $30 billion in 2025</a>, a figure that reflects roughly a 10-fold increase from 2022 levels. Private credit accounts for approximately $17 billion of this total, while U.S. Treasuries make up $7.3 billion. The momentum suggests that institutions are finding utility in blockchain infrastructure for specific use cases, particularly those involving yield-bearing assets with standardized documentation.</p><h2>Understanding ReFi and What Construct Koin Actually Does</h2><p>Real Estate Financing through blockchain differs from the property tokenization projects that dominated headlines in previous cycles. Instead of selling fractional ownership in buildings, ReFi protocols focus on the financing process itself. Think of it as digitizing the loan origination and management workflow rather than the asset title.</p><p>\\\nConstruct Koin operates by connecting property developers who need capital with investors who provide it through token purchases. The platform claims to use artificial intelligence integrated with Building Information Management (BIM) software to analyze development proposals and make lending decisions in hours rather than the weeks or months typical in traditional finance. According to the project website, loans are secured by legal charges registered on title deeds with HM Land Registry in the UK, providing a layer of protection through real property collateral.</p><p>\\\nThe mechanics work through a loan book model. When developers borrow funds, they pay interest and profit shares back to the protocol. CTK token holders who stake their tokens receive 8 to 12% annual percentage rates paid in USDT stablecoin. The protocol claims it currently has <a href=\"https://www.constructkoin.com/\">£15 million of assets already secured on-chain</a>, though independent verification of this figure through public blockchain explorers remains limited in available documentation.</p><h2>Breaking Down the $100 Million Presale Structure</h2><p>The fundraising approach spans 10 phases, each with a price increment. The first phase opened at $0.10 per token, and the final phase will close at $1.00 per token at the Token Generation Event (TGE). The model resembles how venture capital rounds work, with later participants paying higher prices than earlier ones. Out of the 1 billion total token supply, 400 million tokens have been allocated to the presale.</p><p>\\\nThis represents 40% of the total supply going to public participants. Another 15% has been earmarked for staking rewards, 20% for ecosystem growth, 15% for team and advisors, and 10% for liquidity and reserves. The vesting schedules for team tokens matter here, though specific timeframes were not detailed in the publicly available documentation. Projects that allow insiders to sell immediately after launch have historically faced selling pressure that can depress token prices.</p><p>\\\nThe presale accepts payments through both fiat channels (credit cards, bank transfers) and six major cryptocurrency networks including Ethereum, Bitcoin, Solana, Polygon, and Binance Smart Chain. Minimum purchase amounts vary by network due to transaction fee structures. Ethereum requires a $100 minimum due to higher gas fees, while networks like Polygon and Solana allow $10 minimum purchases. The tokens will remain locked until TGE, which the project estimates will occur 12 to 24 months from launch.</p><h2>The Technology Stack and AI Claims</h2><p>The project emphasizes its use of AI for underwriting decisions. Traditional property development loans can take weeks or months to process as lenders manually review business plans, financial projections, and construction documents. Construct Koin claims its system achieves a 95% speed improvement by automating this analysis through machine learning models that assess risk based on multiple data points.</p><p>\\\nThe integration with BIM systems provides the AI with access to architectural plans, material specifications, and construction schedules. In theory, this allows the algorithm to evaluate whether a project is feasible, properly costed, and likely to complete on time. The platform processes applications and provides offers in hours rather than weeks, according to marketing materials. However, the details about which specific AI models are being used, what training data they rely on, and how they handle edge cases remain undisclosed.</p><p>\\\nThe technical infrastructure runs on Ethereum as an ERC-20 token. The smart contracts are described as audited, though the names of the audit firms and links to audit reports were not prominently featured in the reviewed materials. The choice of Ethereum provides compatibility with existing DeFi infrastructure and wallet solutions, but it also means users will contend with network congestion and variable transaction fees unless they utilize Layer 2 solutions.</p><h2>Market Context and the RWA Surge</h2><p>Understanding where Construct Koin fits requires context about the broader <a href=\"https://www.coindesk.com/business/2025/06/26/real-world-asset-tokenization-market-has-grown-almost-fivefold-in-3-years\">real-world asset tokenization movement</a>. The sector experienced 380% growth over three years, reaching <a href=\"https://www.coindesk.com/business/2025/06/26/real-world-asset-tokenization-market-has-grown-almost-fivefold-in-3-years\">$24 billion by mid-2025</a> according to a report by RedStone, Gauntlet, and RWA.xyz. This represents a shift from experimental pilots to scaled institutional adoption, particularly in fixed income and private credit categories.</p><p>\\\nMajor financial institutions have entered the space. BlackRock launched a $2.9 billion tokenized fund (BUIDL), while Franklin Templeton's tokenized money market fund represents $420 million in assets. Goldman Sachs partnered with BNY Mellon to tokenize money market funds, supported by regulatory frameworks that aim to streamline settlement and reduce costs. The institutional involvement provides validation that blockchain infrastructure can solve real operational problems in capital markets.</p><h2>Who is Building This and Corporate Structure</h2><p>Chris Baldrey-Chouro serves as CEO and founder of Construct Koin. According to a <a href=\"https://podcasts.apple.com/be/podcast/the-crypto-podcast/id1578175723\">podcast interview on The Crypto Podcast</a>, Baldrey-Chouro describes the project as executing \"one of the most innovative fundraising strategies in crypto history.\" His background includes work in recruitment and staffing solutions for commerce, based on corporate registry information.</p><p>\\\nThis multi-jurisdictional setup is common among crypto projects seeking to optimize for regulatory environments while maintaining operational flexibility. The UK entity provides legitimacy through Companies House registration and operates under UK corporate law. The BVI structure offers advantages for token operations, while the Dubai presence targets the Middle East market, which represents over $2 trillion in real estate value according to project materials.</p><p>The project positions itself as compliance-first, a pitch aimed at institutional allocators who need legal clarity before committing capital. The protocol includes KYC and AML requirements for all investors, with enhanced due diligence for purchases exceeding $10,000. This approach contrasts with many DeFi protocols that operate pseudonymously or with minimal identity verification.</p><p>\\\nThe emphasis on milestone-driven disbursements and oracle-verified events addresses one of the key concerns institutional investors have about blockchain-based financing. Traditional tranche financing releases funds only after borrowers meet specific milestones such as completing foundation work or reaching specific construction stages. Construct Koin claims its smart contracts replicate this structure by releasing capital only after verification of progress, reducing the risk of fund misuse.</p><p>\\\nThe security model relies on conservative loan-to-value ratios of 60 to 70%. This means if a developer defaults, the property securing the loan should be worth significantly more than the outstanding debt, allowing the protocol to recover funds through foreclosure and sale. The protocol also maintains an insurance vault funded by a portion of fees to cover defaults beyond normal parameters. Whether these mechanisms will perform as designed during an actual default scenario remains untested.</p><h2>The Presale Risk Landscape in 2025</h2><p>\\\n<a href=\"https://cryptsy.com/red-flags-to-watch-for-in-crypto-presales/\">Common red flags in presales include</a> anonymous teams, unrealistic return promises, poorly written whitepapers, unclear tokenomics, and lack of transparency about milestones. Legitimate projects typically disclose team identities, provide detailed technical documentation, show clear roadmaps, and communicate regularly about progress. Projects that lack these elements often disappear after raising funds, leaving investors with worthless tokens.</p><p>\\\nThe regulatory environment adds another layer of complexity. Crypto projects face scrutiny about whether their tokens constitute securities under various jurisdictions. The classification determines which regulations apply and what disclosures are required. Projects that ignore legal frameworks or operate without proper licensing expose themselves and their investors to enforcement actions, frozen assets, and potential criminal charges.</p><h2>Real Estate Tokenization Track Record</h2><p>The history of real estate tokenization projects provides lessons. Multiple ventures have attempted to bring property onto blockchain with mixed results. Early projects focused on fractional ownership, allowing investors to buy shares in specific buildings. These faced challenges with liquidity, regulatory compliance, and the complexity of managing physical assets through digital interfaces.</p><p>\\\nMore recent projects have shifted toward the financing layer rather than ownership tokenization. This approach encounters fewer regulatory hurdles since it deals with loan products rather than securities representing property ownership. However, the business model still requires borrowers, which means projects must build relationships with developers and prove they can provide capital at competitive rates.</p><p>\\\nThe question of whether blockchain adds genuine value or merely adds complexity remains contentious. Supporters argue that on-chain transparency, programmable terms, and global capital access justify the technology overhead. Critics point out that traditional finance already has efficient systems for real estate lending and that blockchain's benefits are often overstated relative to implementation costs.</p><p>Several execution risks warrant examination. First, the loan book model requires a steady pipeline of creditworthy borrowers. If developers can obtain financing through traditional channels at lower costs, they have little incentive to use a new platform that charges fees. The project must either offer better terms than banks or target developers who cannot access traditional financing, which introduces credit risk.</p><p>\\\nSecond, the AI underwriting system remains largely unproven at scale. While automation can speed processes, it also concentrates risk if the algorithms make systematic errors. A series of bad loans could deplete the insurance fund and leave token holders with losses. The lack of detailed information about the AI's training data, error rates, and decision-making process makes it difficult to assess this risk.</p><p>\\\nThird, regulatory changes could impact operations. Governments continue to develop frameworks for crypto assets and tokenized securities. A regulatory crackdown in key markets could force the project to halt operations, delist from exchanges, or face enforcement actions. The multi-jurisdictional structure provides some flexibility but also creates compliance complexity across multiple legal systems.</p><p>\\\nFourth, the token economics depend on sustained borrower demand and investor interest. If loan volume does not grow as projected, staking rewards may decline, reducing demand for the token. If token prices fall significantly below the purchase price, early investors may become discouraged and sell, creating additional downward pressure. The long lock-up period until TGE means investors cannot exit if circumstances change.</p><p>Looking at established players in the space provides benchmarks. <a href=\"https://coinpedia.org/research-report/top-real-world-asset-rwa-projects/\">Centrifuge has achieved $1 billion in Total Value Locked</a>, making it the third RWA protocol to reach this milestone. The platform tokenizes invoices, receivables, and trade finance instruments, pushing them into DeFi markets as collateral. Centrifuge completed a V3 migration in 2025, delivering multichain infrastructure across six EVM chains.</p><p>\\\nOndo Finance focuses on institutional-grade tokenized securities and has built infrastructure for bringing fixed-income products on-chain. The platform emphasizes compliance and works within regulatory frameworks rather than attempting to circumvent them. This approach has allowed Ondo to partner with traditional financial institutions and build sustainable business models.</p><p>\\\nThe difference between these established protocols and a new entrant like Construct Koin lies in track record. Centrifuge and Ondo have processed real transactions, demonstrated their technology works, and built reputations over multiple years. They have also secured institutional backing and navigated regulatory processes. Construct Koin must still prove it can execute its vision and deliver returns to token holders.</p><h2>What The Numbers Actually Show</h2><p>The project claims £15 million in assets already secured on-chain. Converting to dollars at current exchange rates gives approximately $19 million in collateral backing the protocol before the presale completes. If the presale reaches its $100 million target, the ratio of raised capital to existing collateral will be roughly 5 to 1. This means the project would need to deploy the raised funds into new loans relatively quickly to maintain proportional backing.</p><p>\\\nThe staking rewards of 8 to 12% APR paid in USDT require generating sufficient revenue from loan interest and fees. If the protocol charges borrowers 7 to 15% annual interest, as stated in marketing materials, the math works if the majority of loans perform and default rates remain low. However, a 10% default rate combined with recovery costs could quickly consume the margin between what borrowers pay and what stakers receive.</p><p>\\\nThe tokenomics allocate 15% of supply for staking rewards. With 1 billion total tokens, this equals 150 million tokens reserved for rewards. If tokens reach $1 at TGE as the presale structure suggests, that represents $150 million in value designated for staking. Paying 12% APR on a pool of staked tokens would require substantial protocol revenue, meaning the loan book must grow significantly to sustain these yields.</p><h2>The Bigger Picture: Does ReFi Have Product-Market Fit?</h2><p>The core question is whether blockchain-based real estate financing solves problems that matter to enough participants to create a sustainable market. Developers need capital, and investors want returns. Traditional systems provide both, albeit with friction from intermediaries, paperwork, and slow processes.</p><p>\\\nBlockchain's value proposition centers on disintermediation, transparency, and global access. By removing middlemen, protocols can theoretically offer borrowers lower rates and investors higher yields. By recording transactions on-chain, all parties can audit the state of loans in real-time. By operating globally, capital can flow from anywhere to anywhere, removing geographical barriers.</p><p>\\\nThe counterargument is that the friction in traditional finance exists for reasons. Paperwork and slow processes often serve as risk management mechanisms that prevent bad deals from proceeding. Intermediaries like banks provide expertise in underwriting, legal structuring, and recovery that algorithmic systems may struggle to replicate. Global capital flows sound attractive until investors face losses in foreign jurisdictions where recovering assets is difficult or impossible.</p><p>After examining the available information about Construct Koin, the project represents both the promise and peril of real-world asset tokenization in its current phase. The promise lies in the genuine growth of the RWA sector, which has demonstrated that certain use cases have found product-market fit. The peril comes from execution risk, regulatory uncertainty, and the long history of crypto projects that fail to deliver on ambitious visions.</p><p>\\\nThe data points to consider are straightforward. The RWA market is real and growing, reaching $30 billion with institutional participation from major financial players. Real estate represents the largest asset class globally at over $300 trillion, meaning even a tiny percentage of tokenization would create enormous value. The technology for tokenizing loans and managing them through smart contracts exists and has been deployed by other projects successfully.</p><p>\\\nAgainst this, the presale model concentrates risk on early participants who must wait 12 to 24 months for tokens to unlock while hoping the project executes. The team, while public, does not appear to have prior experience building DeFi protocols or managing large-scale lending operations. The AI claims lack substantiation through independent testing or published benchmarks. The regulatory landscape remains fluid, with governments still determining how to classify and regulate tokenized assets.</p><p>\\\nThe honest assessment is that Construct Koin is attempting something difficult that, if successful, could generate returns for early participants. It is also attempting something that could fail for multiple reasons including poor execution, regulatory intervention, lack of borrower adoption, or macroeconomic changes that reduce demand for real estate development financing. Potential participants should view this as a high-risk investment where capital loss is a realistic outcome, not merely a theoretical possibility disclosed in legal disclaimers.</p><p>\\\nThe project would benefit from greater transparency about its AI technology, more detailed disclosure of its existing loan book, and clearer communication about partnerships with developers who will actually use the platform. Without these elements, investors are essentially betting on a vision backed by marketing materials rather than proven operational metrics.</p><p>\\\nIn a market where over half of projects fail, the bar for success is high. Whether Construct Koin clears that bar will depend on execution over the coming years, not on the quality of its presale marketing or the size of its fundraising target.</p><p>\\\nDon’t forget to like and share the story!</p><p>:::tip\n<em>This author is an independent contributor publishing via our&nbsp;. HackerNoon has reviewed the report for quality, but the claims herein belong to the author. #DYO</em></p>","contentLength":18959,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ubuntu 25.10 amd64v3 Benchmarks: Some Minor & Rare Performance Advantages For Desktop Workloads","url":"https://www.phoronix.com/review/ubuntu-2510-amd64v3","date":1761917760,"author":"Michael Larabel","guid":684,"unread":true,"content":"<article>Yesterday Canonical announced architecture variants for Ubuntu Linux with Ubuntu 25.10 seeing the introduction of \"amd64v3\" packages that are built for the x86_64-v3 micro-architecture feature level to assume AVX/AVX2 and other newer CPU ISA features found since Intel Haswell and AMD Excavator processors. Eager to run some initial tests, here is a first look at the Ubuntu 25.10 amd64v3 performance for desktop workloads.</article>","contentLength":423,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Reddit CEO says chatbots are not a traffic driver","url":"https://techcrunch.com/2025/10/31/reddit-ceo-says-chatbots-are-not-a-traffic-driver/","date":1761917643,"author":"Ivan Mehta","guid":258,"unread":true,"content":"<article>During Reddit's Q3 2025 call, CEO Steve Huffman noted that Google search and direct access continue to be its top traffic drivers.</article>","contentLength":130,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Nvidia expands AI ties with Hyundai, Samsung, SK, Naver","url":"https://techcrunch.com/2025/10/31/nvidia-expands-ai-ties-with-hyundai-samsung-sk-naver/","date":1761917384,"author":"Kate Park","guid":257,"unread":true,"content":"<article>Nvidia CEO Jensen Huang is visiting South Korea to strengthen partnerships with Samsung, Hyundai, SK, and Naver, unveiling plans for AI-powered networks and next-generation intelligent systems.</article>","contentLength":193,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"In 1953, the Ford X-100 Concept Car Had It All","url":"https://spectrum.ieee.org/ford-x-100-concept-car","date":1761915604,"author":"Allison Marsh","guid":80,"unread":true,"content":"<p>Heated seats, a radio phone, even an electric shaver in the glove box</p>","contentLength":69,"flags":null,"enclosureUrl":"https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTk1MTgxNy9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTgyMTUyNjcyMn0.mXOXoQHDNVmqLsPQO9kBLfdSxCqStA7LmZrn3W5Xvu8/image.png?width=600","enclosureMime":"","commentsUrl":null},{"title":"AMD Windows Driver Changes For RX 5000/6000 Series Won't Impact Linux Users","url":"https://www.phoronix.com/news/AMD-Windows-RX-5000-6000-Game","date":1761907869,"author":"Michael Larabel","guid":683,"unread":true,"content":"<article>Over the past day there have been many reports of AMD planning to no longer provide game optimizations for the Radeon RX 5000 and RX 6000 series graphics cards for their Microsoft Windows driver. Surprisingly many in the Linux community still seem to think it will impact the Linux drivers, but long story short, there is no real concern for Linux users/gamers...</article>","contentLength":363,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Linux 6.18-rc4 Fixes Another Performance Regression In The Power Management Code","url":"https://www.phoronix.com/news/Linux-6.18-rc4-PM-Perf-Fix","date":1761906960,"author":"Michael Larabel","guid":682,"unread":true,"content":"<article>Last week there was a fix for a \"serious performance regression\" in the Linux kernel's power management code that affected some Intel-powered Chromebooks. This week the power management fixes ahead of Linux 6.18-rc4 is addressing another performance regression...</article>","contentLength":263,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"AerynOS 2025.10 ISOs Released - GNOME 49, Switches Back To GNU libstdc++","url":"https://www.phoronix.com/news/AerynOS-2025.10-ISOs","date":1761905568,"author":"Michael Larabel","guid":681,"unread":true,"content":"<article>AerynOS 2025.10 ISOs were released today for closing out the month of October. AerynOS as a reminder is the Linux distribution that was started by Ikey Doherty and originally known as Serpent OS that has since evolved into an open-source team effort...</article>","contentLength":252,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Krita Lands Basic HDR Support On Wayland","url":"https://www.phoronix.com/news/Krita-HDR-Wayland-Support","date":1761905043,"author":"Michael Larabel","guid":680,"unread":true,"content":"<article>The KDE/Qt-aligned Krita digital painting application is the latest creative app now supporting high dynamic range (HDR) on Linux when using Wayland...</article>","contentLength":151,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Vulkan 1.4.331 Brings Two New Extensions","url":"https://www.phoronix.com/news/Vulkan-1.4.331-Released","date":1761904127,"author":"Michael Larabel","guid":679,"unread":true,"content":"<article>Just one week after Vulkan 1.4.330 brought five new extensions, Vulkan 1.4.331 is now available with another two new extensions for this high performance graphics and compute API...</article>","contentLength":181,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Aembit Introduces Identity And Access Management For Agentic AI","url":"https://hackernoon.com/aembit-introduces-identity-and-access-management-for-agentic-ai?source=rss","date":1761901555,"author":"CyberNewswire","guid":295,"unread":true,"content":"<p>Silver Spring, USA/ Maryland, October 30th, 2025/CyberNewsWire/-- today announced the launch of Aembit Identity and Access Management (IAM) for Agentic AI, a set of capabilities that help organizations safely provide and enforce access policies for AI agents as they move into production. </p><p>The release introduces Blended Identity, which defines how AI agents act on behalf of verified users, and the MCP Identity Gateway, which ensures secure access to enterprise resources based on identity, access policy, and runtime attributes.</p><p>The new offering extends the Aembit Workload IAM Platform to address one of the most pressing operational questions in artificial intelligence and modern IT: how to control what autonomous and user-driven AI agents can access, under what conditions, and with what accountability.</p><p>AI agents are rapidly becoming a key part of enterprise operations. Nearly half of technology executives say they are already adopting or fully deploying agentic AI, and about the same share expect most of their AI deployments to be autonomous within two years, according to an . These agents retrieve sensitive data, open tickets, and execute code across cloud, on-premises, and SaaS environments.</p><p>Yet most access models were built for people, not self-directed software. Many still rely on static secrets and shared credentials, creating risk and obscuring accountability. </p><p>Worse yet, agents’ actions are often hidden behind the identity of a human, making it almost impossible to audit the actions each actor has taken. The result is a widening gap between the pace of AI adoption and the ability of organizations to secure it with confidence.</p><p> assigns each agent a cryptographically verified identity, issues ephemeral credentials, and enforces policy at runtime. The system records every access decision and maintains attribution across both human-driven and autonomous agent activity. </p><p>By bringing agent activity under the same centralized policy control plane that governs other workloads, Aembit enables enterprises to deploy AI at scale while maintaining control, auditability, and compliance.</p><blockquote><p>“Enterprises want to say yes to agentic AI, and they’re asking Aembit for ways to securely grant agents access to data and applications,” said David Goldschlag, co-founder and CEO of Aembit. </p></blockquote><blockquote><p>“Aembit IAM for Agentic AI gives enterprises the same level of control and audit over agent access that IAM systems have long provided for employees. Our approach enables organizations to advance their AI initiatives without expanding their threat and risk surface.”</p></blockquote><p>The release introduces two core capabilities to the Aembit Workload IAM Platform:</p><ul><li>Blended Identity, which gives every AI agent its own verified identity and, when needed, binds it to the human it represents. This establishes a single, traceable identity for each agent action and allows Aembit to issue a secure credential that reflects that combined context.</li><li>MCP Identity Gateway, which receives that identity credential and controls how agents connect to tools through the Model Context Protocol (MCP). The gateway authenticates the agent, enforces policy, and performs token exchange to securely retrieve the necessary access permissions for each connected resource – without ever exposing them to the agent runtime.</li></ul><p>Together, this functionality allows enterprises to apply least-privilege access, revoke permissions immediately when needed, and ensure that every AI action is attributable and auditable. </p><p>They operate on Aembit’s established Workload IAM foundation, which enforces policy dynamically at runtime, issues ephemeral credentials just in time, and records structured events for full traceability.</p><p>Aembit developed IAM for Agentic AI through collaboration with large businesses, government organizations, and innovative agentic AI startups deploying AI for operational and security workloads. Those efforts helped shape an approach that combines enterprise enforcement with the adaptability AI projects demand.</p><blockquote><p>“AI agents don’t live inside one stack or trust domain,” said Kevin Sapp, co-founder and CTO of Aembit. “They move between hybrid environments in seconds. With Aembit, every agent carries a verified identity that our gateway can authenticate and control in real time. It’s how enterprises can give agents the access they need to work, while never losing sight of who they are or what they touch.”</p></blockquote><p>Aembit IAM for Agentic AI is now available to customers using its Workload IAM Platform. Organizations can learn more, request a demo, or get started today at .</p><p> is the identity and access management platform for agentic AI and workloads. It enforces access based on identity, context, and centrally managed policies, giving organizations a singular place to control access risk from AI agents, automate credential management, and accelerate AI adoption. </p><p>With Aembit, enterprises can confidently control access to sensitive resources across all the workloads that power their business.</p><p>:::tip\n<em>This story was published as a press release by Cybernewswire under HackerNoon’s Business Blogging&nbsp;. Do Your Own Research before making any financial decision.</em></p>","contentLength":5160,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"How Can Governments Pay Open Source Maintainers?","url":"https://hackernoon.com/how-can-governments-pay-open-source-maintainers?source=rss","date":1761894488,"author":"Terence Eden","guid":294,"unread":true,"content":"<p>When I worked for the UK Government I was once asked if we could find a way to pay for all the Open Source Software we were using. It is a surprisingly hard problem and I want to talk about some of the issues we faced.</p><p>What about the Open Source that UK Government ?</p><p>Open Source is facing a crisis. The code that the world relies on is often developed by underpaid engineers on the brink of burn-out.  While I don't think anyone wants Open Source to have a paywall, it seems obvious that large organisation should pay their way and not rely solely on volunteer labour.</p><p>Here are some of the problems I faced when trying to get the UK Government to pay for OSS and how  as a maintainer can help make it easier for large organisations to pay you.</p><p>Firstly, lots of OSS doesn't have a well defined owner; so who gets the money?</p><p>I'm not saying that every little library you create needs to be published by a registered company, nor am I suggesting that you should remove your anonymity. But Governments and other organisations need to know  they are funding and  the money is going. The danger of accidentally funnelling money to a sanctioned state or person is just too big a risk for most organisations.</p><p>If you want to receive funding - make it  clear who you are.</p><p>Even when there is an owner, there often isn't an easy mechanism for paying people. Donation sites like GitHub Sponsors, Ko-Fi, and Patreon are great for individuals who want to throw a small amount of money to creators but they can be problematic for larger organisations.  Many OSS projects get around this by offering support contracts. It makes it much easier for an organisation to justify their spend because they're no longer donating to something which can be obtained for free; they're paying for a service.</p><p>This doesn't have to be a contract offering a 24/7 response and guaranteed SLA. It can be as simple as offering best-effort email support.</p><p>The important thing is to offer an  way for a larger organisation to buy your services. Many organisations have corporate credit cards for lower-cost discretionary spending which doesn't require a full business-case.  How easily could a manager buy a £500 support contact from your site?</p><p>Maintainers don't only have to offer support contracts. Many choose to offer training packages which are a good way to raise money  get more people using your product. Some project maintainers will speak at your conference for a suitable fee.</p><p>Again, the aim here is for maintainers to offer a  reason for a payment to be made.</p><p>Open Source has a brilliant culture of allowing multiple (often anonymous) contributors. That's fine when there's no money involved, but how does a moderately sized project decide who receives what share of the funding? Services like <a href=\"https://opencollective.com/\">OpenCollective</a> can make it easier to show  the money is going but it is better to discuss in advance with all contributors what they expect as a share.</p><p>If people think they're being taken advantage of, or that a project maintainer is unjustly enriching themselves, it can cause arguments.  Be very clear to contributors what the funding is for and whether they're entitled to any of it.</p><p>Finally, we faced the issue that some OSS projects didn't  to take money from the \"big bad state\". They were worried that if people saw \"Sponsored by the Government\" they would assume that there were backdoors for spies, or that the developer might give in to pressure to add unwanted features.  This (usually) isn't the case but it is easy to see why having a single large organisation as the main donor could give the impression of impropriety.</p><p>The best defence against this is to have  of paying sponsors! Having the state as one of many partners makes it clear that a project isn't beholden to any one customer.</p><p>It isn't impossible to get Governments to spend on Open Source. But state spending is heavily scrutinised and, bluntly, they aren't set up to pay  amounts to non-suppliers, who aren't charging money.  While large projects often have the resources to apply for Government grants and contracts, smaller projects rarely have the time or expertise. It is critical that maintainers remove the barriers which make it too hard for organisations to pay them.</p><ul><li><p>Make it easy for Governments and other large organisations to pay you.</p></li><li><p>Be as obvious as possible that you are able to accept payments from them.</p></li><li><p>Don't be afraid to put a large price on your talents.</p></li><li><p>Offer multiple paid-for options like speaker fees, support, and feature development funding.</p></li><li><p>Talk with your contributors to let them know how any funding will be shared.</p></li></ul>","contentLength":4568,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Empowering Flink CDC: Schema Evolution Support Lands in Apache SeaTunnel","url":"https://hackernoon.com/empowering-flink-cdc-schema-evolution-support-lands-in-apache-seatunnel?source=rss","date":1761894481,"author":"William Guo","guid":293,"unread":true,"content":"<article>From classroom to codebase! Meet Dong Jiaxin, a student from USTB who brought CDC Schema Evolution to Apache SeaTunnel on Flink during the OSPP. </article>","contentLength":145,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Simple, Battle-Tested Algorithms Still Outperform AI","url":"https://hackernoon.com/simple-battle-tested-algorithms-still-outperform-ai?source=rss","date":1761894469,"author":"Jose Crespo, PhD","guid":292,"unread":true,"content":"<p>==Let’s put numbers to the AI overhype. Companies are burning more than&nbsp;$200 billion every year&nbsp;by choosing AI over simple, proven algorithms. That’s not an opinion. That’s math. (See the chart below if you’re still rubbing your eyes in disbelief.)==</p><h2>But here’s the question: Is AI the villain - or are CEOs burning money on systems they barely understand?</h2><p>The truth is more brutal than either narrative. AI isn’t the disaster — it’s the&nbsp;&nbsp;for the disaster.</p><p>Look at the chart below.&nbsp;<strong>This is your American Horror Story in three acts:</strong></p><p>&nbsp;Solo AI vendors promise revolutionary decision-making. Fortune 500 CEOs, hypnotized by NVIDIA’s stock price and OpenAI’s demos, write checks their companies can’t cash.</p><p>&nbsp;That red line? That’s $500 billion annually achieving -45% ROI (<em>Return on Investment — how much you get back compared to what you spend</em>). Not a typo.&nbsp;&nbsp;forty-five percent. Companies are paying premium prices to make&nbsp;&nbsp;decisions&nbsp;. It’s like hiring McKinsey to tell you what you already know,&nbsp;<em>except McKinsey occasionally delivers positive value.</em></p><p>&nbsp;Nobody admits failure. The AI system becomes “strategic infrastructure.” The losses get buried in “digital transformation.” The vendors —&nbsp;, the whole choir — keep singing about the future while cashing checks in the present.</p><p><strong>Remember the California Gold Rush from the history books?</strong>&nbsp;You know who got rich in 1849. Not the miners. The pickaxe sellers.</p><p>==Today’s pickaxe merchants sell GPU clusters and API tokens to CEOs panning for digital gold.==&nbsp;They whisper sweet promises: “<em>AI will revolutionize your business.</em>” What they don’t mention is that graph showing AI&nbsp;<em>never achieves positive ROI for operational decisions</em>.</p><p>Instead they boast about having more billion parameters than the competition’s AI monster. Of course theirs is bigger, more insanely convoluted — an expanding minefield of compounding errors,&nbsp;&nbsp;each cascade multiplying the disaster.</p><p>Meanwhile, see that green line in the chart above — simple algorithms from 1913 — keeps printing money at 1200% ROI. But nobody’s selling conferences about the Economic Order Quantity formula. There’s no TED talk about Little’s Law and the other older algorithms.</p><p>Yep, unfortunately you can’t raise billions in funding rounds by selling an algorithm that fits on a napkin from that expensive restaurant where you’re shaking hands with investors interested in a bigger AI beast.🤔</p><p>The tragedy is more nuanced than AI sellers admit — and more complex than CEOs with their broken quarterly-profit-maximization algorithms want to hear.</p><blockquote><p>Yes, there’s a hybrid approach where AI becomes just one component alongside those ancient algorithms when complexity grows. But that requires two things the AI revolution explicitly avoids:</p></blockquote><p><strong>==First, you need to understand your business==</strong>&nbsp;==from the molecular level to the stratosphere — every intricacy of your model, what you’re actually selling versus what you think you’re selling, where costs hide, where value emerges. These insights come from human minds. AI won’t solve what you can’t articulate, despite the sales pitch.==</p><p><strong>==Second, you need to hire and respect professional==</strong>==s who combine programming excellence with mathematical rigor and the rare ability to translate both into business value. But your AI recruiting system will never surface these people.==&nbsp;It’s optimized for commodities, not talent — screening for keywords, not capability.</p><p>Look at the carnage in the table below (Chart 3) . This isn’t speculation — it’s documented history:</p><ul><li>&nbsp;Running on a decision that fits on an index card. Investment: approximately zero. Return: Industry dominance for four decades.</li><li>&nbsp;The crown jewel of&nbsp;. Investment: $4 billion. Return: Sold for parts at a 95% loss.</li><li>&nbsp;One simple rule about load factors. Still the only major US airline to avoid bankruptcy.</li><li>&nbsp;Cutting-edge AI pricing models. Burned $500 million in one quarter before shutting down entirely.</li></ul><p><strong>The only success story with AI? Amazon’s hybrid approach</strong>&nbsp;— yup, not pure AI, it’s old-school EOQ with some ML sprinkled on top when absolutely necessary. Even then, the ROI is a fraction of what simple algorithms achieve alone.</p><p>But let’s be honest here, let’s not throw the baby out with the bathwater: when your business complexity genuinely grows — meaning the actual number of variables you must account for — you need more flexibility. That might be AI/ML, or more likely, it’s just good programmers who understand your business and can architect the math you actually need.</p><p>Here’s what 20 years in the trenches taught most of us: around 90% of “complex” business problems can be solved by a competent programmer with decent math skills and simple algorithms, properly wired together.</p><p>Occam’s Razor still cuts: The simplest solution that works is usually right.</p><p>==But simplicity doesn’t sell conference tickets. Simplicity doesn’t raise Series B funding. Simplicity doesn’t get you on the cover of Wired. 😂==</p><p>So instead, companies take the lazy, prestigious route: throw their problems at whatever AI the vendors are pushing this quarter. It’s like hiring a top surgeon to apply a Band-Aid — expensive, unnecessary, and probably going to make things worse.</p><p>The real tragedy? That competent programmer with the simple solution costs $150K/year. The AI system that fails costs $15M. But the programmer doesn’t come with a sales team, a PowerPoint deck, or a promise to “transform your digital future.”</p><p>Guess which one the board approves.</p><h2>The Seven Heroes of Business History</h2><p>Our position shouldn’t surprise you by now, dear reader. We’re betting on what’s worked brilliantly for decades through every crisis, disaster, and market shift while delivering massive profits:&nbsp;</p><p>The formula is simple: Hire, value, and respect your best asset —&nbsp;. Add modern cloud infrastructure and, when genuinely appropriate (maybe 5–10% of cases), deploy the hybrid Algorithm + AI/ML approach.</p><p>But let’s be clear: forget the magic they’re selling — the fantasy of throwing data at AI and getting perfect solutions every time. That’s not happening.</p><p>Yes, LLMs are useful as search tools, data navigators, even coding assistants. But they’re far from autonomous. In non-trivial cases, you need spend lot of time cross-referencing to catch their false positives and, worse, the errors nobody notices —&nbsp;&nbsp;that spaghettify and collide concepts into dangerous nonsense.</p><h3>Meet the Magnificent Seven</h3><p><strong>More than 500 years of combined service. Trillions in value created. Under 1K lines of code total.</strong></p><p>Look at this table (Chart 4). These seven algorithms have never failed. For decades — in some cases over a century — they’ve consistently delivered business value to most of mankind. The epitome of simplicity.</p><p>\\\nEach one is a specialist with embarrassingly simple code:</p><ul><li>EOQ (1913): The inventory gunslinger - Square root of (2 × demand × order cost / holding cost). One line. Tells you exactly how much to order.</li><li>DuPont (1920): The financial sharpshooter - ROE = Profit Margin × Asset Turnover × Leverage. Three numbers multiplied. Instant diagnosis of what’s broken.</li><li>Newsvendor (1950s): The perishables ranger - Order up to the point where the&nbsp;&nbsp;matches the&nbsp;<strong>cost of running short vs. overstocking</strong>. A single threshold for “how much to make.”</li><li>Kelly (1956): The risk-sizing marshal - Bet a&nbsp;<strong>fraction of your bankroll</strong>&nbsp;based on&nbsp;&nbsp;— when your advantage is bigger, size up; when it’s small, size down. Never overbet.</li><li>CPM (1957): The project management tracker - Find the longest path through your network. That’s your deadline. Everything else can slip; this can’t.</li><li>Little’s Law (1961): The operations enforcer - Items in system = arrival rate × time in system. It’s physics, not statistics. Works for everything.</li><li>PageRank (1998): The young gun who built an empire - A page’s importance is the&nbsp;<strong>sum of votes from important pages</strong>, each vote&nbsp;, with a small&nbsp;&nbsp;factor to keep it stable. Keep iterating. Built Google.</li></ul><blockquote><p><strong><em>Total code for all seven: comfortably &lt;1K lines. The math that runs the world fits in a single GitHub gist.</em></strong></p><p>Is a programming career still viable? Looking at this table — absolutely. Learning algorithms and programming built the past and will build the future, despite this transient AI-hypnotized present.</p></blockquote><p>The AI hype has its place — pattern finding, searching, identifying. But it needs years of improvement to correct its statistical errors from Type I through IV. Even then, algorithms in the hands of competent developers remain irreplaceable and far from commodities.</p><p>&nbsp;While everyone chases AI complexity, these seven simple formulas keep generating thousands of percent returns. They don’t need updates. They don’t hallucinate. They just work.</p><p>That’s not outdated. That’s immortal. Let’s keep programming with algorithms while the true researchers work on fixing AI errors with better math.</p>","contentLength":8955,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The Road to Hell is Paved with Good DRY Intentions","url":"https://hackernoon.com/the-road-to-hell-is-paved-with-good-dry-intentions?source=rss","date":1761893629,"author":"Melvin Kosisochukwu","guid":290,"unread":true,"content":"<p>I have come across my share of beautifully written, over-engineered code… one moment I am thinking “That is an interesting way to do that” and the next minute you are wondering “what the hell is going on here!”. I have also suffered from writing over-engineered code, where you are thinking so far into the future that you compromise . The  (You Aren’t Gonna Need It) principle is a good way to counter over-engineering: functionalities should only be implemented when you need them, not on the possibility that you will need them. I have mentioned over-engineering a couple of times now, and some of you reading this are probably thinking What is over-engineering?</p><p>In very simple terms, over-engineering is making software/system design more complex than necessary; this usually happens because you want to add extra functionality to your component to simplify the implementation of A, only to add the functionality for B later.</p><p>We have this codebase for managing invitations; it’s an inherited, legacy codebase with technical debt that accrues interest. The more you try to work on the legacy code, the more something somewhere else breaks. You have to revert in most cases and start all over. Working around the legacy code ends up accruing interest on the technical debt; it’s a standoff between risking breaking features all over the app or adding more technical debt to the codebase. The third solution we came up with was abstraction. We had to figure out ways to modularise new features or improvements to the application, and to expose and share data where necessary. At first, this seemed like a Hail Mary. Finally, we can work on this mangled codebase with minimal side effects. Were we wrong!! The abstraction spiraled, and the codebase ended up overabstracted, bloating the solution to the problem and bringing the supposed salvaged side of the codebase into its own hell. Along the line, we forgone the rules for the abstraction and now have different parts of the application dependent on each other, looping back to where we started. I suppose this was bound to happen in a codebase with a couple of developers, each with fire up their asses to ship features; we ended up shipping maintenance and more technical debt.</p><p>When collaborating with multiple people on a codebase (which is pretty much always the case), and for corporates who are more interested in shipping features than code quality, code reviews always suffer. For problems like over-engineering and overabstraction, Traditional line-by-line code reviews often miss these systematic issues. You check a component created a couple of weeks ago to support the integration of a feature, following the DRY principle, and realize there are now 10 interconnected/similar features that depend on the component. Code reviews will need to be elevated to catch these architectural issues and ensure that dependency requirements between components are met.</p><p>We live by the DRY (Don’t Repeat Yourself) gospel because it simplifies work, and developers are inherently lazy (in a good way). The DRY principle works well with orthogonal systems: small, self-contained components combined to form a system.</p><p><em>Systems should be composed of a set of cooperating modules, each of which implements functionality independent of each other</em>.</p><p>There should be more emphasis on orthogonal systems and on DRY code; it’s easier to combine the reusable functions you create with each other and scale them as the repository progresses when they are not bloated and overabstracted, at which point you end up with a rigid system interwoven so much with each other that it will be complicated to connect which aspects of the code that does not meet an exact rule, you will find yourself duplicating the code because making it work for a new connection breaks an old system. Congratulations, you have achieved Spaghetti code that cannot bend. To use a truly orthogonal system and avoid spaghetti DRY code, every code module change should affect only the module that is updated.</p><p>Your components should not focus solely on avoiding repetition, but also on being small abstractions of the overall system; otherwise, you will end up with components so complex that they can easily break with a single change to a connected component. When creating reusable code, the approach should be writing code that does not depend on any other code block to function a certain way. Reusability should be used as a tool and not the goal; when you have reusable UI components with business or API logic in the same component, you are on a highway to over-abstraction. It starts small, and before you realize what is happening, the disease has festered throughout your repository. Now you cannot reuse the component on a different page with the same UI functions and different logic without adding additional external logic/context to the component.</p><h2>Modularity Modularity Modularity</h2><p>Modularity involves breaking your system down into smaller, independent codes/components called modules. Please pay attention to the word ‘smaller’; it’s possible to have a bloated module with more code than necessary, which creates over-abstraction and should be avoided. Over-abstraction is really just an unsuccessful attempt at modularity. Your modules should be able to function independently of other modules, only exposing required data. Changes to good, modular, structured code should affect only the modules, without any cascading effects.</p><p>A good approach to building orthogonal systems and easily avoiding over-abstraction is to build with functionality first, then features; this aligns well with Component-Based architecture(separating UI components from stateful components). The functionality stage will focus on the smallest reusable units of code that are assembled to implement the feature. A Login feature will consist of the following functions: collect username and password (UI), validate user data, redirect to the user profile/reject collected user data. Each functionality should operate independently, relying only on necessary data.</p><h2>No medals for over-sophisticated code</h2><p>After writing every code implementation, ask yourself whether there is an easier or simpler way to achieve the same result. Most of us have heard stories about code that can be edited or worked on by only one person in the company, which is not a feat to be proud of. Writing code that can only be maintained by you most likely means it’s over-sophisticated or employs unorthodox procedures. A great example of coding using unorthodox procedures is Gilfoyle’s Hard Drives from the article: &nbsp;</p><p>I have found myself writing overly sophisticated code because I want to use a new technology/package/library I just learned about, without considering whether it is the simplest tool for the job. The excitement for learning something new is great, but attention to when and where it should be applied is probably more important.</p><p>In the quest for writing perfect codes that account for all possible future and time-travelling cases, you end up with an over-engineered codebase. You should give it up because it is not possible to write perfect code, aim for good enough that meets all your immediate requirements. The DRY principle is fundamental; repetition is still a sin in software development. The DRY principle should be applied to an orthogonal system to create a codebase that is decoupled, with each module independent and exchanging data at a separate meeting point(feature module). These will help you create systems that are easy to maintain and debug. Remember that simple is always better in software development.</p>","contentLength":7639,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The 9 Best Affiliate Recruitment Tools to Scale Your Affiliate Program","url":"https://hackernoon.com/the-9-best-affiliate-recruitment-tools-to-scale-your-affiliate-program?source=rss","date":1761891521,"author":"Endorsely","guid":291,"unread":true,"content":"<p>Ever since I started managing affiliate programs, I've been obsessed with finding ways to make partner discovery more efficient.&nbsp;</p><p>The traditional approach - listing your program on directories and hoping quality affiliates stumble across it - is painfully slow. You're competing with hundreds of other programs for attention from affiliates who are already making good money promoting your competitors.</p><p>Here's what actually works: finding people who are already successfully creating content about products like yours, and reaching out to them directly. It's not rocket science, but it's incredibly time-consuming if you're doing it manually.&nbsp;</p><p> because it shows you who is promoting your competitors on their website or social media.</p><p>It’s not limited to one particular network or database, and it runs weekly scans to pick up on the freshest opportunities to promote your product or brand.</p><p><img src=\"https://cdn.hackernoon.com/images/9gHwxzGv0uNfeSQx3w6794cB5gu1-dt03cal.png\" alt=\"\">But the right tool for you will depend on your budget, your ICP, and the stage you’re at with your affiliate program. In this guide, I'll break down the 9 best options, explain exactly what makes each one valuable, and help you figure out which tool (or combination of tools) will actually grow your program without wasting your time or budget.</p><p>|  |  |  |  |\n|----|----|----|----|\n|  | Custom, AI-powered affiliate discovery | AI web scraping + weekly scans | $99/month with 7-day free trial |\n|  | Running a SaaS affiliate program | Built-in affiliate discovery | Free until $1K revenue (then $39/month) |\n|  | Traditional publishers | Network integration | $456/month |\n|  | Marketplace access | 3M+ affiliate network | $39/month + fees |\n|  | Influencer recruitment | 100M+ creator database | $299/month |\n|  | Multi-channel campaigns | E-commerce integration | Custom (est. $500+/month) |\n|  | Enterprise influencer programs | AI automation (Gia) | Custom (est. $1,000+/month) |\n|  | Influencer discovery across 30+ platforms | 40+ filter options | $199/month |\n|  | Partnership intelligence | Free B2B directory | Free |</p><p> is the best affiliate discovery tool for SaaS and e-commerce brands because it finds potential partners who are already talking about products just like yours.</p><p>Whereas many other tools rely solely on their own directory or marketplace, AffiliateFinder.ai scans the whole internet to find the best-match affiliates for your brand.</p><p><img src=\"https://cdn.hackernoon.com/images/9gHwxzGv0uNfeSQx3w6794cB5gu1-ci13c3g.png\" alt=\"\"> \\n I tested this with a client in the keyword research tool space. Within 15 minutes of setting it up, AffiliateFinder.ai found 1300+ potential affiliates - bloggers ranking for \"Ahrefs alternatives,\" YouTubers reviewing competitor products, and websites comparing tools in the space.&nbsp;</p><p>More importantly, these weren't just names in a database from 2019. These were people who had published content in the last few weeks, so I got a much higher response rate than I had with other tools, and recruited more new partners.</p><p>The platform works by continuously scanning the web, YouTube, and Instagram for anyone mentioning your competitors or ranking for your target keywords. It uses AI to filter out irrelevant results (like Wikipedia pages or your actual competitors) and delivers a curated list of partnership opportunities.</p><p>You can view all the stats you need to help make a decision (subscribers, views, engagement rate) without having to leave your dashboard.</p><p><img src=\"https://cdn.hackernoon.com/images/9gHwxzGv0uNfeSQx3w6794cB5gu1-hd23cpn.png\" alt=\"\">This list gets updated every week, with an email notifying you of the top results, so now I have over 3,000 opportunities for this client.</p><p>AffiliateFinder.ai’s filters make it easy to sort through your results and prioritize outreach according to your campaign goals.</p><p>For example, if you want to work with nano-influencers with small but engaged audiences, you can filter results to show those with less than 10k subscribers.</p><p><img src=\"https://cdn.hackernoon.com/images/9gHwxzGv0uNfeSQx3w6794cB5gu1-q433cqe.png\" alt=\"\">As you go through the results, you can add them to different lists according to how relevant they are.&nbsp;</p><p>From your ‘saved’ list, you can check contact details - this finds email addresses and social media profiles associated with the domain or channel.</p><p><img src=\"https://cdn.hackernoon.com/images/9gHwxzGv0uNfeSQx3w6794cB5gu1-qy43cst.png\" alt=\"\">The YouTube email finder deserves special mention because it solves one of the most annoying problems in affiliate recruitment. Normally, getting a YouTuber's email means breaking CAPTCHAs manually, which YouTube actively limits.</p><p>It even finds Linkedin and Twitter (X) profiles for some affiliates, so you can DM them directly if they’re more active on social media than email.&nbsp;</p><p>AffiliateFinder.ai finds these emails in seconds, turning what would be hours of tedious work into a single click. I've personally used this to reach out to 83 YouTube creators in a single afternoon - something that would have taken me several days (and multiple YouTube accounts) to do manually.</p><p>The platform supports 195+ countries and 40+ languages, which matters more than you might think. If you're expanding internationally, you can set up separate scans for each geo and find Australian travel influencers, French beauty creators, or Swedish sneaker reviewers - whatever you need to get more eyes on your product.</p><p> Growing SaaS companies, agencies managing multiple clients, e-commerce brands in competitive niches, and anyone who’s had enough of outdated affiliate databases.</p><ul><li>Finds 500-750+ relevant affiliates instantly</li><li>Weekly automated scans catch new opportunities</li><li>Finds emails for websites, YouTube channels, and Instagram pages</li><li>Competitor-based discovery (proven performers)</li><li>Multi-geo and multi-language support</li></ul><ul><li>No built-in affiliate tracking or payments</li><li>Requires a separate outreach tool for campaigns</li><li>Results still need a manual sense-check</li></ul><ul><li> Includes unlimited discovery, 150 email credits monthly, 1 brand analysis, 2 users</li><li> 500 email credits, 5 brands, 5 users&nbsp;</li><li> Custom pricing and packages (API, custom webhooks)</li></ul><p> If you're serious about scaling your affiliate program and have the budget for a professional tool, AffiliateFinder.ai should be your first purchase after affiliate tracking software. The competitive intelligence approach fundamentally changes how you recruit - from hoping affiliates find you to actively poaching your competitors' best partners.</p><h2><strong>2. Endorsely: Best Budget Option for SaaS</strong></h2><p> combines affiliate recruitment with full program management, wrapping it all in a pricing model that's genuinely friendly to early-stage companies: it's free until your affiliate program generates $1,000 in monthly revenue. \\n  <img src=\"https://cdn.hackernoon.com/images/9gHwxzGv0uNfeSQx3w6794cB5gu1-4553c26.png\" alt=\"\">Think about that for a second. You can discover affiliates, track their performance, manage payouts, and grow your program to $12K annual revenue before spending a single dollar on the platform. Then it's just $39/month until you hit $5,000 monthly, and $99/month after that.&nbsp;</p><p>Compare this to Impact or PartnerStack charging hundreds (or thousands) per month from day one, plus transaction fees on top.</p><p>Endorsely is specifically designed to manage SaaS affiliate programs, meaning it handles subscriptions, upgrades, and recurring commissions seamlessly.</p><p>And the great thing is that it has AI-powered affiliate discovery built in, along with email enrichment.</p><p><img src=\"https://cdn.hackernoon.com/images/9gHwxzGv0uNfeSQx3w6794cB5gu1-s763co1.png\" alt=\"\">Endorsely only covers websites and YouTube channels, so it’s not quite as comprehensive as AffiliateFinder.ai. It also lacks some features like lists and labels.</p><p>But overall, it’s fantastic value for money - and incredibly well designed - for anyone running a SaaS affiliate program.</p><p> SaaS companies launching their first affiliate program, bootstrapped startups watching every dollar, and anyone who wants discovery plus management without the enterprise price tag.</p><ul><li>Completely free until $1K monthly revenue</li><li>Full program management included</li><li>Tracks recurring subscription revenue properly</li><li>LTV analytics for customer quality insights</li><li>Simple, predictable pricing structure</li></ul><ul><li>Discovery less comprehensive than dedicated tools</li><li>Fewer features than enterprise platforms</li><li>Weekly scans not included</li><li>Not designed for e-commerce or service businesses</li></ul><ul><li> Until $1,000/month in affiliate revenue. Find 15 affiliates</li><li> Up to $5,000/month in affiliate revenue. Find 100 affiliates/month</li><li> Up to $20,000/month in affiliate revenue. Find 250 affiliates/month</li></ul><p> If you're launching an affiliate program for a SaaS product and need both discovery and tracking,  gives you everything in one package at a price point that won't make your CFO wince. Use it to get your first 50 affiliates, then consider adding AffiliateFinder.ai when you're ready to scale aggressively.</p><h2><strong>3. Publisher Discovery: Best for Traditional Affiliate Sites</strong></h2><p>Publisher Discovery is the tool for brands that want deep data on traditional affiliate websites - the established review sites, comparison blogs, and publisher networks that have been in the game for years.</p><p>The platform integrates directly with major affiliate networks like ShareASale, CJ, and Impact, which gives it access to performance data that standalone tools can't match.&nbsp;</p><p>You can search by vertical or network, but there’s no custom discovery based on your brand.</p><p><img src=\"https://cdn.hackernoon.com/images/9gHwxzGv0uNfeSQx3w6794cB5gu1-yo73c17.png\" alt=\"\">Publisher Discovery allows you to filter publishers by SEO metrics (domain authority, traffic volume), see which networks they're active on, and even track their performance once you recruit them.&nbsp;</p><p>This network integration is huge if you're already running programs on these platforms and want to expand beyond your current affiliate pool.</p><p>Detailed reporting includes competitor gap analysis and publisher performance benchmarking, which is helpful for identifying your top performers and helping those who are underperforming.</p><p>Who it's for: Enterprise brands working with affiliate networks, programs targeting established publishers rather than influencers, and teams that need performance data integration.</p><ul><li>Deep integration with major networks</li><li>Rich SEO and performance data</li><li>Quality-focused filtering</li><li>Performance tracking post-recruitment</li></ul><ul><li>Premium solution ($450+ per month)</li><li>Limited to affiliates registered on networks</li><li>Steeper learning curve than simpler tools</li></ul><ul><li> All features for one campaign (single geo)</li><li> Five campaigns</li><li> Unlimited campaigns</li></ul><p> combines affiliate program management with a built-in marketplace of 3 million affiliates, giving you instant access to thousands of active partners looking for new programs to promote.</p><p>The marketplace is the real draw here - over 5,000 affiliates actively browse for opportunities, and you can filter them by platform, niche, keywords, and performance metrics.</p><p><img src=\"https://cdn.hackernoon.com/images/9gHwxzGv0uNfeSQx3w6794cB5gu1-8m83cjo.png\" alt=\"\">Instead of cold outreach, you're connecting with people who are actively seeking partnerships. I've seen programs get 20-30 quality applications within the first week of listing.</p><p>However, once you have exhausted the pool of affiliates listed on Refersion for your vertical, new ones don’t come along too often. A tool like AffiliateFinder.ai is better if you want to keep proactively discovering new opportunities every week.</p><p>Beyond discovery, Refersion handles the full program lifecycle: tracking, commission payments, tax forms (W-9s and 1099s), and automated communications.&nbsp;</p><p>The first-party cookie tracking is solid, and the platform integrates with major e-commerce platforms, making setup relatively painless.</p><p>Who it's for: E-commerce brands, D2C companies, and businesses that want a ready-made network plus management tools in one package.</p><ul><li>3M+ affiliate network with 5,000+ active partners</li><li>Full program management included</li><li>Automated commission processing and tax handling</li><li>Good filtering for marketplace affiliates</li></ul><ul><li>Transaction fees (up to 3%) on top of monthly cost</li><li>Marketplace means competing with other brands for attention</li><li>Limited pool of affiliates that doesn’t refresh quickly</li></ul><ul><li><strong>Launch ($39/month + 3% of affiliate sales):</strong> Core features, unlimited affiliates</li><li><strong>Growth ($129/month + 2% of affiliate sales):</strong> Advanced tracking, email attribution, private offers</li><li><strong>Scale ($599/month + 1% of affiliate sales):</strong> Advanced commission options, multi-store support</li></ul><h2><strong>5. Modash: Best for Influencer-Heavy Strategies</strong></h2><p>Modash indexes every social media profile with at least 1,000 followers on Instagram, TikTok, and YouTube - that means hundreds of millions of creators searchable through one interface.</p><p><img src=\"https://cdn.hackernoon.com/images/9gHwxzGv0uNfeSQx3w6794cB5gu1-9x93cnp.png\" alt=\"\">The filtering is impressively robust: follower range, location, engagement rate, audience demographics, growth trends, and even fake follower detection.&nbsp;</p><p>I used it to find 50 Instagram food influencers with 5K-50K followers and minimum 3% engagement in under five minutes.&nbsp;</p><p>It even has an AI Search option so you can find recent posts of men tying their shoelaces or kids playing with toy cars.</p><p>One final touch is the influential follower check: you may have influential people following your brand already, and Modash finds them for you.</p><p>The platform includes email campaigns, tracking, and payment processing so you can manage everything in one place.</p><p>Who it's for: D2C brands prioritizing influencer partnerships, consumer product companies, and anyone focusing on social media affiliates over traditional publishers.</p><ul><li>Massive database (every profile with 1K+ followers)</li><li>Powerful filtering with 40+ criteria</li><li>Fast, intuitive interface</li></ul><ul><li>Expensive compared to some other tools</li><li>Influencer-focused (less useful for B2B or traditional affiliates)</li><li>Requires separate tracking software</li></ul><ul><li> 2 team members, 300 profiles, 150 emails</li><li><strong>Performance ($599/month):</strong> 5 team members, 800 profiles, 400 emails, broader content discovery</li><li><strong>Enterprise (custom pricing):</strong> Higher limits, more seats, payments</li></ul><p>Upfluence offers influencer discovery across all major social networks plus unique features like scanning your customer database to identify which customers are influencers.</p><p>The platform provides 20+ advanced filters to search its large creator database, with detailed metrics including audience demographics, engagement rates, and estimated sponsored post pricing.</p><p><img src=\"https://cdn.hackernoon.com/images/9gHwxzGv0uNfeSQx3w6794cB5gu1-x7a3cck.png\" alt=\"\"> \\n The e-commerce integration lets you connect your Shopify or WooCommerce store, and Upfluence can identify existing customers with significant social followings, turning them into brand ambassadors. This is a feature I haven’t seen anywhere else, and it’s very valuable for established brands.</p><p>The affiliate tracking module lets you generate unique codes and referral links for each influencer, tracking sales back to specific creators. Combined with content management, product seeding automation, and campaign analytics, you get a true all-in-one platform.</p><p>Who it's for: Large brands with significant influencer budgets, agencies managing diverse clients, companies running complex multi-channel campaigns.</p><ul><li>Comprehensive multi-platform coverage</li><li>Customer-to-affiliate conversion feature</li><li>Rich audience data and authenticity metrics</li><li>Full campaign management included</li></ul><ul><li>Complex platform with learning curve</li><li>Overkill for pure affiliate recruitment needs</li></ul><p>Custom pricing, generally starting around $500/month with annual contracts, can exceed $1,000/month.</p><h2><strong>7. GRIN: Best for Enterprise Influencer Programs</strong></h2><p>GRIN is an enterprise-grade creator management platform with affiliate capabilities, designed for brands managing hundreds of influencer relationships simultaneously with AI-powered automation.</p><p><img src=\"https://cdn.hackernoon.com/images/9gHwxzGv0uNfeSQx3w6794cB5gu1-49b3cez.png\" alt=\"\">The platform's AI assistant \"Gia\" suggests optimal creators, predicts performance, and automates routine tasks like follow-ups and content tracking.&nbsp;</p><p>You get complete lifecycle management: discovery, product seeding, content approvals, affiliate link tracking, and payment processing - all in one system. The relationship CRM tracks every conversation, content piece, and product shipment per creator.</p><p>The affiliate module generates unique codes and links for each influencer, tracking sales and automatically calculating commissions.&nbsp;</p><p>For brands running hybrid influencer-affiliate programs at scale, GRIN removes the operational chaos and tracks everything in one manageable place.</p><p>The only downside is that GRIN is focused on social media creators, so you’ll need to pair it with another platform if you want to partner with bloggers too.</p><p>Who it's for: Large D2C brands, major e-commerce companies, agencies with significant influencer budgets (think 100+ active partnerships).</p><ul><li>AI automation (Gia) for optimization and suggestions</li><li>Complete creator lifecycle management</li><li>Product seeding and content management</li><li>Robust analytics and reporting</li></ul><ul><li>One of the most expensive options ($1,000-$2,500+/month)</li><li>Annual contracts required</li><li>Overkill for smaller programs</li></ul><p>Custom pricing starts at around $1,000 per month.</p><p>Influencers Club gives you searchable access to 200M+ influencer profiles across more than 30 platforms, including Spotify and Udemy.&nbsp;</p><p>This broad range of coverage means you can often find creators who don’t show up on bigger platforms like Instagram and YouTube.</p><p>The 40+ advanced filters let you narrow by platform, follower count, location, engagement rate, posting frequency, and even specific bio keywords.</p><p><img src=\"https://cdn.hackernoon.com/images/9gHwxzGv0uNfeSQx3w6794cB5gu1-okc3cm2.png\" alt=\"\">Each creator listed already has a verified email, which is good for easy outreach but may exclude some great opportunities without an email on record.</p><p>On the API plan, you’re able to use Influencers Club data to enrich your own list of emails or usernames. There’s also the option for a fully managed plan where their team takes care of the outreach for you.</p><p>Who it's for: Influencer-focused brands running targeted campaigns across multiple platforms</p><ul></ul><ul><li>Influencer-heavy (less for traditional publishers)</li><li>No website/blogger discovery</li><li>Limited to creators with emails</li></ul><ul><li><strong>Dashboard (from $199/month):</strong> Discovery and outreach for 200M+ creators with emails</li><li><strong>API + Dashboard (from $249/month):</strong> Includes API access, bulk data, enrichment</li><li> Fully managed outreach service</li></ul><p>Partnerbase is a free public directory of 147,000+ companies and 456,000+ B2B partnerships, making it a valuable research tool for identifying potential affiliate relationships.</p><p><img src=\"https://cdn.hackernoon.com/images/9gHwxzGv0uNfeSQx3w6794cB5gu1-lld3cnf.png\" alt=\"\">While not specifically an affiliate recruitment platform, Partnerbase helps you understand the partnership landscape in your industry.&nbsp;</p><p>You can see which companies have affiliate programs, research competitors' partner ecosystems, and identify businesses that might be good affiliate partners based on their existing relationships.</p><p>The real value is in research and intelligence rather than direct recruitment. Use it to identify companies worth approaching, understand what partner programs exist in adjacent spaces, and find opportunities for strategic affiliate partnerships beyond typical content creators.</p><p>Who it's for: B2B companies on tight budgets, researchers mapping partnership landscapes, anyone needing free competitive intelligence.</p><ul><li>Completely free with registration</li><li>456,000+ partnerships indexed</li><li>Helps identify affiliate program opportunities</li></ul><ul><li>Requires manual follow-up work</li><li>Limited compared to paid tools</li></ul><ul><li> Complete access with registration</li></ul><p>Picking the right recruitment tool isn't about finding the \"best\" option - it's about matching your specific situation to the right solution. Here's how to think through the decision.</p><p>Let's do the math that actually matters. If you're an affiliate manager making $60K annually ($30/hour), and manual recruitment takes 20 hours weekly, that's $600/week or $2,400/month of your time. A tool like  at $99/month that saves 75% of that time creates $1,800/month in value - an 18x return.</p><p>Even tools that seem expensive like GRIN at $1,500/month can justify themselves if you're managing 100+ influencer relationships. That's $15 per relationship monthly for complete automation. Without it, you'd need to hire another full-time person.</p><p> like Partnerbase and Endorsely's free tier make sense when you're validating whether an affiliate channel works for your business. But once you have proof of concept, the time savings from paid tools become obvious.</p><p><strong>Launching (0-10 affiliates):</strong> Start with manual research and outreach within your network. Focus on proving the channel works before investing heavily in discovery tools.</p><p><strong>Early Growth (10-50 affiliates):</strong> Add AffiliateFinder.ai Pro at $99/month. The competitor intelligence approach helps you reach quality at this stage when every partnership matters.</p><p><strong>Scaling (50-200 affiliates):</strong> Upgrade to AffiliateFinder.ai Agency or add specialized tools like Modash for influencers. Consider Refersion if marketplace access speeds up recruitment.</p><p><strong>Enterprise (200+ affiliates):</strong> GRIN, Upfluence, or other enterprise platforms start making sense. You need sophisticated management alongside discovery. You may want to keep AffiliateFinder.ai to make sure you never run out of new opportunities.</p><p>The type of affiliate you want to target makes a difference to the tool you choose. Here’s what I recommend:</p><p><strong>Traditional publishers and bloggers:</strong> AffiliateFinder.ai, Publisher Discovery, or Endorsely. These tools excel at finding web content creators.</p><p><strong>Instagram/TikTok influencers:</strong> Modash, Influencers Club, or Upfluence. You need social-first databases with engagement metrics.</p><p> AffiliateFinder.ai (for the email finder specifically) or Modash. Breaking CAPTCHAs manually will kill your productivity.</p><p> Publisher Discovery, Partnerbase for research, or AffiliateFinder.ai for finding B2B content creators.</p><p>Most successful programs need multiple types, which is why many teams run 2-3 tools. For example: AffiliateFinder.ai for bloggers + Modash for influencers, or Refersion marketplace for passive recruitment + AffiliateFinder.ai for active outreach.</p><h3><strong>Discovery vs Management: Why You Need Both</strong></h3><p>Affiliate discovery tools and management platforms solve different problems.</p><p>While many affiliate management platforms include some type of discovery, it’s typically limited to partners already registered on their platform.</p><p>A management platform is necessary from day 1 to track affiliate performance and process commissions, but a specialized discovery tool like AffiliateFinder.ai is essential if you want to expand beyond pre-defined marketplaces.</p><p>When I started out in affiliate management, most of these tools didn’t exist. The options were basically to post in directories and hope for the best, turn manual research into a full-time job, or hire an agency.&nbsp;</p><p>Today, AI-powered tools give even solo founders the ability to execute sophisticated competitive intelligence strategies that used to require entire teams.</p><p><strong> wins for most teams because it solves the actual hard problem: finding high-quality affiliates who are already proven performers in your niche.</strong></p><p>The weekly updates mean you're continuously discovering new opportunities without manual work, and the competitive intelligence angle gives you an unfair advantage over companies still relying on passive recruitment.</p><p>But the most successful affiliate programs I've seen run a hybrid approach: AffiliateFinder.ai for proactive outreach to competitors' affiliates, a marketplace like Refersion for passive inbound applications, and maybe Upfluence or Modash for more reach into social platforms.</p><p>When budget is the constraint, Endorsely's free tier gets you started with both discovery and management for literally zero dollars until you hit $1k monthly revenue. That's enough to prove the channel works before investing more heavily.</p><p>When you need enterprise-scale influencer management and have the budget, GRIN or Upfluence deliver sophisticated automation that justifies their premium pricing. But honestly, most teams should start smaller and scale into these tools rather than buying them upfront.</p><p>Your competitors are already using these tools. The question isn't whether to invest in affiliate recruitment technology - it's whether you can afford not to.&nbsp;</p><p>The gap between companies actively recruiting proven affiliates and those hoping affiliates find them is growing wider every month, and the tools that create that gap are all listed above.</p><p>Affiliate recruitment tools like AffiliateFinder.ai actively find affiliates by scanning the web for competitors' partners, while networks like ShareASale are marketplaces where you list programs and wait for affiliates to apply.</p><p>Yes, most recruitment tools focus only on discovery and contact finding, so you'll need tracking software like Endorsely, Impact, or Tapfiliate to manage commissions and performance tracking.</p><p>AffiliateFinder.ai typically saves 15-20 hours weekly by automating competitor research and contact finding that would take days of manual work to accomplish with the same quality.</p><p>Options exist for every budget: Endorsely starts completely free, AffiliateFinder.ai costs $99 monthly, while enterprise tools like GRIN run $1,000+ monthly depending on program scale and needs.</p><p>Most successful programs use one primary discovery tool like AffiliateFinder.ai plus free supplementary tools like Partnerbase for additional intelligence and marketplaces like Refersion for passive recruitment.</p><p>Track time-to-recruit, affiliate quality (measured by conversion rates), and program growth velocity. Successful tools should cut recruitment time by 50%+ while improving partner quality significantly.</p>","contentLength":24525,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Scenes from TechCrunch Disrupt 2025","url":"https://techcrunch.com/2025/10/30/scenes-from-techcrunch-disrupt/","date":1761883353,"author":"Connie Loizos","guid":256,"unread":true,"content":"<article>Thanks to everyone who made this year's San Francisco event what it was -- and to the 10,000 of you who filled the halls, made the connections, and left with more than you came with. Couldn't make it? These images tell part of the story. </article>","contentLength":238,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Genode-Powered Sculpt OS 25.10 Brings Performance Improvements & Better Drivers","url":"https://www.phoronix.com/news/Sculpt-OS-25.10-Released","date":1761870165,"author":"Michael Larabel","guid":678,"unread":true,"content":"<article>The Genode operating system framework continues innovating over a decade and a half later on this original open-source OS creation and with that Sculpt OS as its general purpose OS. Out today is Sculpt OS 25.10 to incorporate the latest enhancements to the platform...</article>","contentLength":268,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Navan IPO tumbles 20% after historic debut under SEC shutdown workaround","url":"https://techcrunch.com/2025/10/30/navan-ipo-tumbles-20-after-historic-debut-under-sec-shutdown-workaround/","date":1761860500,"author":"Marina Temkin","guid":255,"unread":true,"content":"<article>Navan finished its first day trading at an approximate valuation of $4.7 billion, which is about half of its last private valuation of $9.2 billion.</article>","contentLength":148,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"A Hassle-Free Battery Charger","url":"https://spectrum.ieee.org/diy-nimh-battery-charger","date":1761852604,"author":"Maximilian Kern","guid":79,"unread":true,"content":"<p>Put NiMH cells back in the game</p>","contentLength":31,"flags":null,"enclosureUrl":"https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy82MTk0MTM5Ny9vcmlnaW4ucG5nIiwiZXhwaXJlc19hdCI6MTc2NjAxMjExNX0.hzitnoYWejyuEN1AAxd_H7-TR39wzOr56jBfFY1JLuo/image.png?width=600","enclosureMime":"","commentsUrl":null},{"title":"Rust 1.91 Promotes Windows On 64-bit ARM To Tier-1 Status","url":"https://www.phoronix.com/news/Rust-1.91-Released","date":1761850914,"author":"Michael Larabel","guid":677,"unread":true,"content":"<article>The Rust project announced today the release of Rust 1.91 as the latest update to this popular programming language priding itself on memory safety capabilities...</article>","contentLength":163,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"ChatGPT maker reportedly eyes $1 trillion IPO despite major quarterly losses","url":"https://arstechnica.com/ai/2025/10/is-openai-worth-1-trillion-potential-ipo-may-reveal-the-answer/","date":1761848672,"author":"Benj Edwards","guid":333,"unread":true,"content":"<p>On Tuesday, OpenAI CEO Sam Altman <a href=\"https://www.reuters.com/business/openai-lays-groundwork-juggernaut-ipo-up-1-trillion-valuation-2025-10-29/\">told Reuters</a> during a livestream that going public “is the most likely path for us, given the capital needs that we’ll have.” Now sources familiar with the matter say the ChatGPT maker is preparing for an initial public offering that could value the company at up to $1 trillion, with filings possible as early as the second half of 2026. However, news of the potential IPO comes as the company faces mounting losses that <a href=\"https://www.theregister.com/2025/10/29/microsoft_earnings_q1_26_openai_loss/\">may have reached</a> as much as $11.5 billion in the most recent quarter, according to one estimate.</p><p>Going public could give OpenAI more efficient access to capital and enable larger acquisitions using public stock, helping finance Altman’s <a href=\"https://arstechnica.com/information-technology/2025/08/sam-altman-calls-ai-a-bubble-while-seeking-500b-valuation-for-openai/\">plans</a> to spend trillions of dollars on AI infrastructure, according to people familiar with the company’s thinking who spoke with Reuters. Chief Financial Officer Sarah Friar has reportedly told some associates the company targets a 2027 IPO listing, while some financial advisors predict 2026 could be possible.</p><p>Three people with knowledge of the plans told Reuters that OpenAI has discussed raising $60 billion at the low end in preliminary talks. That figure refers to how much money the company would raise by selling shares to investors, not the total worth of the company. If OpenAI sold that amount of stock while keeping most shares private, the entire company could be valued at $1 trillion or more. The final figures and timing will likely change based on business growth and market conditions.</p>","contentLength":1505,"flags":null,"enclosureUrl":"https://cdn.arstechnica.net/wp-content/uploads/2024/10/openai_treasurechest_1-1152x648.jpg","enclosureMime":"","commentsUrl":null}],"tags":["tech"]}