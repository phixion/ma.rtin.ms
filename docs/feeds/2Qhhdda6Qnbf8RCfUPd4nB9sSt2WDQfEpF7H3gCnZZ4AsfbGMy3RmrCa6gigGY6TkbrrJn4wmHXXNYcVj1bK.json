{"id":"2Qhhdda6Qnbf8RCfUPd4nB9sSt2WDQfEpF7H3gCnZZ4AsfbGMy3RmrCa6gigGY6TkbrrJn4wmHXXNYcVj1bK","title":"top scoring links : rust","displayTitle":"Reddit - Rust","url":"https://www.reddit.com/r/rust/top/.rss?sort=top&t=day&limit=6","feedLink":"https://www.reddit.com/r/rust/top/?sort=top&t=day&limit=6","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":6,"items":[{"title":"Why does clippy encourage `String::push('a')` over `String::push_str(''a\")`?","url":"https://www.reddit.com/r/rust/comments/1r5lqer/why_does_clippy_encourage_stringpusha_over/","date":1771179485,"author":"/u/MediumInsect7058","guid":350,"unread":true,"content":"<p>One thing that has always been annoying me is clippy telling me to use  instead of <code>String::push_str(s: &amp;str)</code> to append a single character . To me this makes no sense. Why should my program decode a utf-8 codepoint from a 32 bit char instead of just copying over 1-4 bytes from a slice? </p><p>I did some benchmarks and found  to be 5-10% faster for appending a single byte string. </p><p>Not that this matters much but I find clippy here unnecessarily opinionated with no benefit to the program.</p>","contentLength":481,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"The next Chrome/Edge releases will credit the ~150 Rust crates they use","url":"https://chromium-review.googlesource.com/c/chromium/src/+/7514149","date":1771177427,"author":"/u/fintelia","guid":351,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://www.reddit.com/r/rust/comments/1r5ku44/the_next_chromeedge_releases_will_credit_the_150/"},{"title":"I built tokio-fsm: proc macro for compile-time validated async state machines","url":"https://www.reddit.com/r/rust/comments/1r5cpml/i_built_tokiofsm_proc_macro_for_compiletime/","date":1771156103,"author":"/u/shree_ee","guid":346,"unread":true,"content":"<p>Tired of writing the same event loop + channel + timeout boilerplate for every stateful async workflow. tokio-fsm discovers states/events from your code and validates transitions at compile time. I am inspired by the work I found myself doing recently and thought there is a gap, plus I love compile-time macros.</p><p>impl Connection { type Context = ConnectionCtx; type Error = std::io::Error;</p><pre><code>#[on(state = Idle, event = Connect)] async fn start(&amp;mut self) -&gt; Transition&lt;Connecting&gt; { Transition::to(Connecting) } #[on(state = Connecting, event = Success)] #[state_timeout(duration = \"30s\")] async fn connected(&amp;mut self) -&gt; Transition&lt;Active&gt; { Transition::to(Active) } </code></pre><p>Invalid transitions = compile errors. Unreachable states = compile errors. Built-in timeouts, channels, background tasks.</p><ul><li>API ergonomics (does <code>#[on(state = X, event = Y)]</code> feel natural?)</li><li>Missing features for real-world usage</li></ul><p>Issues/PRs welcome. Still learning Rust ecosystem best practices.</p>","contentLength":951,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Salvo vs Axum — why is Axum so much more popular?","url":"https://www.reddit.com/r/rust/comments/1r5bqhy/salvo_vs_axum_why_is_axum_so_much_more_popular/","date":1771152560,"author":"/u/Sensitive-Raccoon155","guid":349,"unread":true,"content":"<p>I’ve been playing with both Salvo and Axum lately, and something I can’t wrap my head around is why Axum is so much more popular.</p><p>From a developer experience point of view, Salvo feels surprisingly complete. A lot of the things I usually need are already there, and I don’t have to think too much about adding extra crates for common backend tasks. With Axum, I often end up assembling the stack myself, which isn’t bad, just different.</p><p>I can’t really figure out why Axum gets so much more attention while Salvo barely comes up in discussions. From what I’ve seen so far, Salvo feels pretty capable and well thought out. Maybe I’m missing something, maybe not.</p><p>What do you all think about this?</p>","contentLength":705,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"I made a noise generator TUI","url":"https://www.reddit.com/r/rust/comments/1r5aluk/i_made_a_noise_generator_tui/","date":1771148379,"author":"/u/Aggressive-Smell-432","guid":348,"unread":true,"content":"<p>I’ve been wanting a TUI for something like this for a long time. I wasn't sure why one didn't exist yet, so I made it myself.</p><p>I tried to keep it minimal, but it can also download more sounds directly using yt-dlp. I think it is pretty much feature-complete now, though I would like to add more default sounds in the future.</p><p>here is a link to the repo</p>","contentLength":350,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Silverfir-nano: a Rust no_std WebAssembly interpreter hitting ~67% of single-pass JIT","url":"https://www.reddit.com/r/rust/comments/1r552pe/silverfirnano_a_rust_no_std_webassembly/","date":1771128800,"author":"/u/mbbill","guid":347,"unread":true,"content":"<p>I’ve been building Silverfir-nano, a WebAssembly 2.0 interpreter focused on speed + tiny footprint.</p><ul><li>67% of a single-pass JIT (Wasmtime Winch)</li><li>43% of a full-power Cranelift JIT (Wasmer Cranelift)</li></ul><p> // see below</p><p>Edit1: regarding the 200kb size, copy-pasting reply below.</p><p>&gt;you are going to run ahead of time and then generate more optimized handlers based on that</p><p>Not exactly, fusion is mostly based on compiler-generated instruction patterns and workload type, not on one specific app binary. Today, across most real programs, compiler output patterns are very similar, and the built-in fusion set was derived from many different apps, not a single target. That is why the default/built-in fusion already captures about ~90% of the benefit for general code. You can push it a bit further in niche cases, but most users do not need per-app fusion.</p><p>On the benchmark/build question: the headline numbers are from the fusion-enabled configuration, not the ultra-minimal ~200KB build. The ~200KB profile is for maximum size reduction (for example embedded-style constraints), and you should expect roughly ~40% lower performance there (still quite fast tbh, basically wasm3 level).</p><p>Fusion itself is a size/perf knob with diminishing returns: the full fusion set is about ~500KB, but adding only ~100KB can already recover roughly ~80% of the full-fusion performance. The ~1.1MB full binary also includes std due to the WASI support, so if you do not need WASI you can save several hundred KB more.</p><p>So number shouldn't be 200KB but 700KB for maximum performance. thanks for pointing out.</p>","contentLength":1571,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["dev","reddit","rust"]}