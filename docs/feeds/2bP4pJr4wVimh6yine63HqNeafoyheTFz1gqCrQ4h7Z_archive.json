{"id":"2bP4pJr4wVimh6yine63HqNeafoyheTFz1gqCrQ4h7Z","title":"AlgoMaster Newsletter","displayTitle":"Dev - Algomaster","url":"https://blog.algomaster.io/feed","feedLink":"https://blog.algomaster.io/","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":6,"items":[{"title":"12 OOP Concepts EVERY Developer Should Know","url":"https://blog.algomaster.io/p/12-oop-concepts-every-developer-should-know","date":1770870446,"author":"Ashish Pratap Singh","guid":446,"unread":true,"content":"<p><strong>Object-Oriented Programming (OOP)</strong> gives you a practical way to structure software around real-world “things” like Users, Orders, Payments, and Notifications.</p><p>Instead of scattering data across variables and wiring behavior through unrelated functions, you <strong>bundle state and behavior</strong> into self-contained units. That makes code easier to reason about, extend, test, and maintain as the project grows.</p><p>But OOP is not just about writing classes. It is about understanding a small set of  that help you model complexity, control change, and avoid turning your codebase into a fragile mess.</p><p>In this article, we’ll cover <strong>12 OOP concepts every developer should know</strong>, with real-world examples and code. These concepts also appear frequently in . I’ve also included links to help you explore each concept in more depth.</p><p>A  is a blueprint that defines the structure and behavior of objects. It specifies what data something will hold (fields) and what actions it can perform (methods).</p><blockquote><p> Think of it like an architectural blueprint for a house. The blueprint specifies the number of rooms, doors, and windows. But you can’t live in a blueprint. You need to build an actual house from it.</p></blockquote><pre><code>public class User {\n    private String username;\n    private String email;\n    private String role;\n\n    public User(String username, String email, String role) {\n        this.username = username;\n        this.email = email;\n        this.role = role;\n    }\n\n    public boolean isAdmin() {\n        return \"ADMIN\".equals(role);\n    }\n\n    public String getDisplayName() {\n        return username + \" (\" + role + \")\";\n    }\n}</code></pre><p>In the above example, the  class bundles , , and  together with the methods that operate on them.</p><p>But a class by itself doesn’t do anything. It’s just a template. To actually use it, you need to create objects.</p><p>An  is a concrete instance of a class. It has actual values for the fields defined in the class.</p><p>If the class is a template, each object is a filled-in copy. You can create many objects from the same class, and each one is independent.</p><pre><code>// Creating objects from the User class\nUser alice = new User(\"alice\", \"alice@example.com\", \"ADMIN\");\nUser bob = new User(\"bob\", \"bob@example.com\", \"DEVELOPER\");\nUser carol = new User(\"carol\", \"carol@example.com\", \"DEVELOPER\");\n\nalice.isAdmin();          // true\nbob.isAdmin();            // false\nalice.getDisplayName();   // alice (ADMIN)</code></pre><p>Each object has its own copy of the fields. Changing ‘s role doesn’t affect . They’re independent instances built from the same template.</p><p>Classes and objects let you group related data and behavior together. But in larger systems, you often need to define what behaviors must exist without specifying how they work. That’s where interfaces come in.</p><p>An  is a contract. It defines a set of methods that a class must implement, without specifying how they should work.</p><p>Think about payment processing in an e-commerce app. You need to charge customers, but you don’t want to be locked into a single payment provider. So, you define a contract that says “any payment gateway must support charging and refunding,” and then Stripe, PayPal, Razorypay or any future provider can plug in.</p><pre><code>public interface PaymentGateway {\n    PaymentResult charge(String customerId, double amount);\n    PaymentResult refund(String transactionId);\n}\n\npublic class StripeGateway implements PaymentGateway {\n    private String apiKey;\n\n    public StripeGateway(String apiKey) {\n        this.apiKey = apiKey;\n    }\n\n    @Override\n    public PaymentResult charge(String customerId, double amount) {\n        // Stripe-specific API call\n        System.out.println(\"Charging $\" + amount + \" via Stripe\");\n        return new PaymentResult(true, \"txn_stripe_123\");\n    }\n\n    @Override\n    public PaymentResult refund(String transactionId) {\n        System.out.println(\"Refunding \" + transactionId + \" via Stripe\");\n        return new PaymentResult(true, transactionId);\n    }\n}\n\npublic class PayPalGateway implements PaymentGateway {\n    @Override\n    public PaymentResult charge(String customerId, double amount) {\n        // PayPal-specific API call\n        System.out.println(\"Charging $\" + amount + \" via PayPal\");\n        return new PaymentResult(true, \"txn_paypal_456\");\n    }\n\n    @Override\n    public PaymentResult refund(String transactionId) {\n        System.out.println(\"Refunding \" + transactionId + \" via PayPal\");\n        return new PaymentResult(true, transactionId);\n    }\n}</code></pre><p>The beauty of interfaces is that your checkout service can work with  without knowing whether it’s talking to Stripe or PayPal. Swapping providers means changing one line of configuration, not rewriting your business logic.</p><p>Interfaces tell you  classes must do. The four pillars of OOP tell you  to design those classes well.</p><p> is the practice of <strong>bundling data and methods</strong> together in a class while restricting direct access to the internal data. You expose a controlled public interface and hide everything else.</p><p>Consider a rate limiter. Other parts of your system only need to ask “can this user make another request?” They shouldn’t be able to directly mess with the internal counters or reset the time window.</p><p>Here’s what happens without encapsulation:</p><pre><code>public class RateLimiter {\n    public int requestCount;       // Anyone can modify directly\n    public long windowStartTime;   // Anyone can reset the window\n    public int maxRequests;\n}\n\nRateLimiter limiter = new RateLimiter();\nlimiter.requestCount = -100;       // Invalid state\nlimiter.windowStartTime = 0;       // Window broken</code></pre><pre><code>public class RateLimiter {\n    private int requestCount;\n    private long windowStartTime;\n    private final int maxRequests;\n    private final long windowSizeMs;\n\n    public RateLimiter(int maxRequests, long windowSizeMs) {\n        this.maxRequests = maxRequests;\n        this.windowSizeMs = windowSizeMs;\n        this.windowStartTime = System.currentTimeMillis();\n        this.requestCount = 0;\n    }\n\n    public boolean allowRequest() {\n        resetWindowIfExpired();\n        if (requestCount &lt; maxRequests) {\n            requestCount++;\n            return true;\n        }\n        return false;\n    }\n\n    public int getRemainingRequests() {\n        resetWindowIfExpired();\n        return maxRequests - requestCount;\n    }\n\n    private void resetWindowIfExpired() {\n        long now = System.currentTimeMillis();\n        if (now - windowStartTime &gt;= windowSizeMs) {\n            requestCount = 0;\n            windowStartTime = now;\n        }\n    }\n}</code></pre><p>Now nobody can corrupt the internal state. The only way to interact with the limiter is through  and . The window-reset logic is completely internal. If you later switch from a fixed window to a sliding window algorithm, none of the calling code needs to change.</p><p>Encapsulation hides a class’s internal data. But there’s a closely related concept that hides complexity at a higher level.</p><p> is about <strong>hiding unnecessary complexity</strong> and exposing only what the user needs. While encapsulation hides data, abstraction hides implementation details.</p><blockquote><p> Think about sending a message through Slack. You type a message and hit send. Behind the scenes, there’s WebSocket management, message serialization, retry logic, delivery confirmation, and push notifications. You don’t deal with any of that. The complexity is abstracted away behind a simple action.</p></blockquote><p>In code, abstraction typically uses abstract classes or interfaces to define simplified interactions:</p><pre><code>public abstract class CloudStorage {\n    // What the caller sees - one simple method\n    public String upload(String fileName, byte[] data) {\n        validate(fileName, data);\n        String path = generatePath(fileName);\n        String url = doUpload(path, data);\n        logUpload(fileName, url);\n        return url;\n    }\n\n    // Each provider implements its own upload logic\n    protected abstract String doUpload(String path, byte[] data);\n\n    private void validate(String fileName, byte[] data) {\n        if (fileName == null || data.length == 0) {\n            throw new IllegalArgumentException(\"Invalid file\");\n        }\n    }\n\n    private String generatePath(String fileName) {\n        return \"uploads/\" + System.currentTimeMillis() + \"/\" + fileName;\n    }\n\n    private void logUpload(String fileName, String url) {\n        System.out.println(\"Uploaded \" + fileName + \" to \" + url);\n    }\n}\n\npublic class S3Storage extends CloudStorage {\n    @Override\n    protected String doUpload(String path, byte[] data) {\n        // AWS SDK calls, multipart upload, encryption...\n        return \"https://s3.amazonaws.com/bucket/\" + path;\n    }\n}\n\npublic class GcsStorage extends CloudStorage {\n    @Override\n    protected String doUpload(String path, byte[] data) {\n        // Google Cloud SDK calls, resumable upload...\n        return \"https://storage.googleapis.com/bucket/\" + path;\n    }\n}</code></pre><p>The caller just invokes . They don’t need to know about path generation, validation, or provider-specific SDK calls. All that complexity is abstracted away.</p><p>Abstraction simplifies how you interact with objects. But what if multiple classes share the same data and behavior? That’s where inheritance steps in.</p><p> lets a new class (child)  from an existing class (parent), inheriting its fields and methods. The child class can reuse the parent’s code, add new behavior, or override existing behavior.</p><p>In an event-driven system, every event needs a timestamp, an event ID, and a source. But each specific event type carries its own payload. Instead of duplicating the common fields in every event class, you define them once in a base class.</p><pre><code>public class DomainEvent {\n    protected String eventId;\n    protected String source;\n    protected long timestamp;\n\n    public DomainEvent(String source) {\n        this.eventId = UUID.randomUUID().toString();\n        this.source = source;\n        this.timestamp = System.currentTimeMillis();\n    }\n\n    public String getEventId() {\n        return eventId;\n    }\n\n    public long getTimestamp() {\n        return timestamp;\n    }\n}\n\npublic class UserRegisteredEvent extends DomainEvent {\n    private String userId;\n    private String email;\n\n    public UserRegisteredEvent(String userId, String email) {\n        super(\"user-service\");\n        this.userId = userId;\n        this.email = email;\n    }\n\n    public String getUserId() {\n        return userId;\n    }\n}\n\npublic class OrderPlacedEvent extends DomainEvent {\n    private String orderId;\n    private double totalAmount;\n\n    public OrderPlacedEvent(String orderId, double totalAmount) {\n        super(\"order-service\");\n        this.orderId = orderId;\n        this.totalAmount = totalAmount;\n    }\n\n    public String getOrderId() {\n        return orderId;\n    }\n}</code></pre><p> and  both get , , , and  from  without writing that code again. They also add their own unique fields.</p><p>Use inheritance when there’s a clear  relationship. A . A . Avoid inheriting just to reuse code. If there’s no natural “is-a” relationship, use composition instead.</p><p>Inheritance lets classes share structure and behavior. But what happens when you call the same method on different child classes and get different results? </p><p>Polymorphism means “many forms.” It allows objects of different types to be treated through a common interface, with each type providing its own behavior.</p><ul><li><p> (method overloading): same method name, different parameters</p></li><li><p> (method overriding): same method signature, different implementations in child classes</p></li></ul><p>Runtime polymorphism is the more powerful concept. Imagine a notification system that sends alerts through different channels:</p><pre><code>public interface NotificationChannel {\n    void send(String recipient, String message);\n}\n\npublic class EmailChannel implements NotificationChannel {\n    @Override\n    public void send(String recipient, String message) {\n        // SMTP setup, HTML formatting, attachment handling...\n        System.out.println(\"Email to \" + recipient + \": \" + message);\n    }\n}\n\npublic class SlackChannel implements NotificationChannel {\n    @Override\n    public void send(String recipient, String message) {\n        // Slack API call, channel lookup, markdown formatting...\n        System.out.println(\"Slack to #\" + recipient + \": \" + message);\n    }\n}\n\npublic class SmsChannel implements NotificationChannel {\n    @Override\n    public void send(String recipient, String message) {\n        // Twilio API, phone number validation, character limits...\n        System.out.println(\"SMS to \" + recipient + \": \" + message);\n    }\n}\n\n// Polymorphism in action\nList&lt;NotificationChannel&gt; channels = List.of(\n    new EmailChannel(), new SlackChannel(), new SmsChannel()\n);\n\nfor (NotificationChannel channel : channels) {\n    channel.send(\"ops-team\", \"Server CPU above 90%\");\n    // Each channel sends the alert its own way\n}</code></pre><p>The loop doesn’t know or care whether it’s sending an email, a Slack message, or an SMS. It calls  on each one, and the right implementation runs automatically. If you add a  tomorrow, the loop works without any changes.</p><p>This is the real power of polymorphism: you can write code that works with abstractions, and it automatically handles new types as they’re added.</p><p>Now that we understand how individual classes are structured and designed, let’s look at how objects relate to each other.</p><p> represents a “knows-about” relationship between objects. Both objects exist independently. Neither owns or controls the other.</p><p>Think of a developer and a repository on GitHub. A developer contributes to multiple repositories, and a repository has multiple contributors. But if a developer deletes their account, the repository still exists. And if a repository is archived, the developer keeps working on other things.</p><pre><code>public class Developer {\n    private String username;\n    private List&lt;Repository&gt; repositories;\n\n    public Developer(String username) {\n        this.username = username;\n        this.repositories = new ArrayList&lt;&gt;();\n    }\n\n    public void contributeTo(Repository repo) {\n        repositories.add(repo);\n    }\n}\n\npublic class Repository {\n    private String name;\n    private List&lt;Developer&gt; contributors;\n\n    public Repository(String name) {\n        this.name = name;\n        this.contributors = new ArrayList&lt;&gt;();\n    }\n\n    public void addContributor(Developer dev) {\n        contributors.add(dev);\n    }\n}\n\n// Both objects are created independently\nDeveloper dev = new Developer(\"alice\");\nRepository repo = new Repository(\"payment-service\");\n\n// They reference each other, but neither owns the other\ndev.contributeTo(repo);\nrepo.addContributor(dev);</code></pre><p>The key here is independence. Both  and  are created outside of each other and just hold references. Deleting one doesn’t affect the other.</p><p>Association is the most general type of relationship. But sometimes, one object is part of another. That brings us to aggregation.</p><p> is a specialized form of association that represents a “has-a” relationship where the whole contains parts, but the parts can exist independently.</p><p>Think of a team and its microservices. A team owns multiple microservices, but if the team is reorganized, the services don’t disappear. They get reassigned to a different team.</p><pre><code>public class Team {\n    private String name;\n    private List&lt;Microservice&gt; services;\n\n    public Team(String name) {\n        this.name = name;\n        this.services = new ArrayList&lt;&gt;();\n    }\n\n    // Services are created outside and assigned to the team\n    public void addService(Microservice service) {\n        services.add(service);\n    }\n\n    public void removeService(Microservice service) {\n        services.remove(service);\n    }\n}\n\npublic class Microservice {\n    private String name;\n    private String repoUrl;\n\n    public Microservice(String name, String repoUrl) {\n        this.name = name;\n        this.repoUrl = repoUrl;\n    }\n}\n\n// Microservice exists independently\nMicroservice paymentService = new Microservice(\"payment-service\", \"github.com/org/payments\");\n\n// Team references the service but doesn't own it\nTeam platformTeam = new Team(\"Platform\");\nplatformTeam.addService(paymentService);\n\n// Service can be reassigned to another team\nTeam checkoutTeam = new Team(\"Checkout\");\ncheckoutTeam.addService(paymentService);</code></pre><p>The team has services, but services have their own lifecycle. They exist before being assigned to a team and continue to exist after being reassigned.</p><p>In aggregation, parts can survive without the whole. But what if the parts are so tightly coupled to the whole that they shouldn’t exist independently? </p><p> is a strong form of “has-a” where the whole owns the parts entirely. When the whole is destroyed, the parts are destroyed with it. The parts have no meaning outside of the whole.</p><p>Think of an order and its line items. Each line item (2x T-Shirt, 1x Laptop) only exists as part of that specific order. If the order is cancelled and deleted, the line items go with it. A line item floating around without an order makes no sense.</p><pre><code>public class Order {\n    private String orderId;\n    private List&lt;LineItem&gt; lineItems;  // Order creates and owns line items\n\n    public Order(String orderId) {\n        this.orderId = orderId;\n        this.lineItems = new ArrayList&lt;&gt;();\n    }\n\n    // Order creates the line item internally\n    public void addItem(String productId, String productName, int quantity, double price) {\n        lineItems.add(new LineItem(productId, productName, quantity, price));\n    }\n\n    public double getTotal() {\n        return lineItems.stream()\n            .mapToDouble(LineItem::getSubtotal)\n            .sum();\n    }\n\n    public void cancel() {\n        lineItems.clear();  // Line items destroyed with the order\n        System.out.println(\"Order \" + orderId + \" cancelled\");\n    }\n}\n\npublic class LineItem {\n    private String productId;\n    private String productName;\n    private int quantity;\n    private double unitPrice;\n\n    // Package-private: only Order should create line items\n    LineItem(String productId, String productName, int quantity, double unitPrice) {\n        this.productId = productId;\n        this.productName = productName;\n        this.quantity = quantity;\n        this.unitPrice = unitPrice;\n    }\n\n    double getSubtotal() {\n        return quantity * unitPrice;\n    }\n}\n\n// Order creates line items internally - they don't exist outside\nOrder order = new Order(\"ORD-001\");\norder.addItem(\"SKU-100\", \"Mechanical Keyboard\", 1, 149.99);\norder.addItem(\"SKU-200\", \"USB-C Hub\", 2, 39.99);\nSystem.out.println(order.getTotal());  // 229.97\norder.cancel();  // All line items destroyed</code></pre><p>Notice the difference from aggregation: in composition, the whole creates its parts internally ( inside ). In aggregation, parts are passed in from outside.</p><p>Composition is about ownership and lifecycle control. But not all relationships involve ownership. Sometimes one object just temporarily uses another. </p><p>Dependency is the weakest relationship between classes. It represents a temporary “uses-a” connection where one class uses another, typically as a method parameter, local variable, or return type, but doesn’t hold a long-term reference to it.</p><p>Think of a deployment pipeline. The pipeline uses a logger to record what’s happening, but it doesn’t own the logger or keep it around as part of its state. It just uses it during execution and moves on.</p><pre><code>public class DeploymentService {\n    // Dependency: uses HttpClient temporarily, doesn't store it\n    public DeploymentResult deploy(String serviceName, String version, HttpClient client) {\n        String url = \"https://deploy.internal/\" + serviceName;\n        HttpResponse response = client.post(url, Map.of(\"version\", version));\n\n        if (response.getStatusCode() == 200) {\n            return new DeploymentResult(true, \"Deployed \" + serviceName + \" v\" + version);\n        }\n        return new DeploymentResult(false, \"Deployment failed: \" + response.getBody());\n    }\n}\n\npublic class HttpClient {\n    public HttpResponse post(String url, Map&lt;String, String&gt; body) {\n        // HTTP connection setup, request serialization, TLS...\n        System.out.println(\"POST \" + url);\n        return new HttpResponse(200, \"OK\");\n    }\n}\n\n// DeploymentService uses HttpClient but doesn't own or store it\nDeploymentService deployer = new DeploymentService();\nHttpClient client = new HttpClient();\ndeployer.deploy(\"payment-service\", \"2.4.1\", client);</code></pre><p> depends on , but only during the  call. It doesn’t store the client as a field. Once the method returns, the relationship is gone.</p><p>Dependency is the weakest of the object relationships. The last concept in our list brings us full circle, connecting interfaces back to the classes that implement them.</p><p>Realization is the relationship between an interface and the class that implements it. The class “realizes” the contract defined by the interface by providing concrete implementations of all its methods.</p><p>We already saw this with  in the interfaces section. Let’s look at another example, a cache store:</p><pre><code>public interface CacheStore {\n    void put(String key, String value, int ttlSeconds);\n    String get(String key);\n    void evict(String key);\n}\n\npublic class RedisCache implements CacheStore {\n    private String connectionUrl;\n\n    public RedisCache(String connectionUrl) {\n        this.connectionUrl = connectionUrl;\n    }\n\n    @Override\n    public void put(String key, String value, int ttlSeconds) {\n        // Redis SETEX command with TTL\n        System.out.println(\"Redis SET \" + key + \" EX \" + ttlSeconds);\n    }\n\n    @Override\n    public String get(String key) {\n        // Redis GET command\n        System.out.println(\"Redis GET \" + key);\n        return null;  // Simplified\n    }\n\n    @Override\n    public void evict(String key) {\n        // Redis DEL command\n        System.out.println(\"Redis DEL \" + key);\n    }\n}</code></pre><p>Each class promises to fulfill the  contract. Your application code depends on , so you can use Redis in production, an in-memory map in tests, and Memcached in a different environment, all without changing a single line of business logic.</p><p>Realization is what makes polymorphism through interfaces possible. It’s the bridge between abstract contracts and concrete behavior.</p><p>Here’s how all 12 concepts relate to each other:</p><p>These 12 concepts form the foundation of object-oriented design. You don’t need to use all of them in every project, but understanding each one and knowing when to apply it will make you a better software engineer and help you tackle Low-Level Design interviews with confidence.</p><p>If you found it valuable, hit a like ❤️ and consider subscribing for more such content every week.</p><p>If you have any questions/suggestions, feel free to leave a comment.</p><div data-attrs=\"{&quot;url&quot;:&quot;https://blog.algomaster.io/p/12-oop-concepts-every-developer-should-know?utm_source=substack&amp;utm_medium=email&amp;utm_content=share&amp;action=share&quot;,&quot;text&quot;:&quot;Share&quot;}\" data-component-name=\"CaptionedButtonToDOM\"><div><p>This post is public so feel free to share it.</p></div></div>","contentLength":22646,"flags":null,"enclosureUrl":"https://substackcdn.com/image/fetch/$s_!GcX3!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F069b5b1c-ea4e-4c1e-b77b-640b236b8d83_2638x1320.png","enclosureMime":"","commentsUrl":null},{"title":"I created a comprehensive resource to master Concurrency Interviews","url":"https://blog.algomaster.io/p/concurrency-interview-resource","date":1770300384,"author":"Ashish Pratap Singh","guid":445,"unread":true,"content":"<p>I’m excited to announce the launch of my , built to be the most complete, structured, and high-quality resources for concurrency interview prep available on the internet.</p><p>It covers <strong>concurrency and synchronization fundamentals</strong>,, and <strong>25 commonly asked interview problems</strong> organized by category, each with detailed explanations and implementations. The content supports 5 languages<strong>: Java, Python, C++, C#, and Go</strong>.</p><p>I’ve kept a meaningful portion of the course (around ~50%) as free. For full access, you can .</p><h3>25 Interview Problems (and growing)</h3><p>The course includes <strong>25 concurrency interview problems</strong>, with plans to add more over time.</p><p>For each problem, you get:</p><ul><li><p>The core concurrency challenges</p></li><li><p>Multiple synchronization approaches (so you learn trade-offs, not just one solution)</p></li><li><p>Clean implementations across supported languages</p></li></ul><h3>Language Specific Deep-Dives</h3><p>Concurrency is deeply language-dependent. Each language has its own primitives, libraries, and best practices.</p><p>That’s why the course includes dedicated deep-dive chapters for each language, covering the key concurrency primitives and standard libraries.</p><p>Concurrency is easier to learn when you can visualize what threads are doing.</p><p>This course includes  (flowcharts, sequence diagrams, state diagrams, and more) to make the behavior of threads, locks, and coordination mechanisms feel intuitive.</p><p>There are quizzes after chapters to test and reinforce your understanding.</p><p>There are interactive simulations to help you better understand the multi-threading concepts. I plan to add more simulations in coming weeks.</p><p>For any questions related to content or subscription, please reply to this email or reach out at </p>","contentLength":1652,"flags":null,"enclosureUrl":"https://substackcdn.com/image/fetch/$s_!QD7M!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c970fb5-a9f2-46c3-bd80-e8408439a6c6_2506x1736.png","enclosureMime":"","commentsUrl":null},{"title":"7 Graph Algorithms You Should Know for Coding Interviews in 2026","url":"https://blog.algomaster.io/p/7-graph-algorithms-you-should-know","date":1770177782,"author":"Ashish Pratap Singh","guid":444,"unread":true,"content":"<p>In this post Shayan will share  to know if you are preparing for coding interviews.</p><p>Graphs come up in about 35-40% of coding interviews at major tech companies because so many real systems are graphs: social networks, map routing, dependency chains, web crawlers. If you don’t know core graph patterns, you’ll struggle in the interview.</p><p>I’ve seen candidates get stuck on problems like “Number of Islands” simply because they hadn’t practiced basic grid traversal. These problems become straightforward once you know the patterns.</p><p>In this post, I’ll show you 7 graph algorithms that cover about 85% of graph-related interview questions. For each one, you’ll learn what it does, when to use it, how it works, and which LeetCode problems to practice.</p><blockquote><p> BFS explores a graph . It visits every node at distance 1 from the start, then distance 2, then distance 3, and so on.</p></blockquote><p>Use BFS when the problem is naturally about  or :</p><ul><li><p>Finding the shortest path in an unweighted graph</p></li><li><p>Finding all nodes within K distance</p></li><li><p>Any problem that asks for “minimum steps” or “shortest path” without weights</p></li></ul><p>BFS uses a . You start by adding the source node. Then you repeat: remove the front node, process it, and add all its unvisited neighbors to the back of the queue.</p><p>The queue enforces the level-by-level order. By the time you reach a node, you’ve visited all closer nodes first. This guarantees the first path you find is the shortest.</p><pre><code>queue = [start]\nvisited = {start}\n\nwhile queue is not empty:\n    node = queue.pop_front()\n    for neighbor in node.neighbors:\n        if neighbor not in visited:\n            visited.add(neighbor)\n            queue.push_back(neighbor)</code></pre><h4>Grid Problems (Flood Fill)</h4><p>BFS works well on 2D grids. </p><p>Think of grids like this:</p><ul><li><p>Its neighbors are usually  (sometimes diagonals too)</p></li></ul><p>Problems like “Number of Islands” and “Rotting Oranges” are grid-based BFS.</p><p>For flood fill, you start at a cell and spread to all connected cells matching a condition. You stop at boundaries or cells that don’t match. This is the algorithm behind the paint bucket tool in image editors.</p><h4>Practice these LeetCode problems:</h4><blockquote><p> DFS explores a graph  before it backtracks. If a node has multiple neighbors, DFS fully explores the first neighbor’s branch, then returns and tries the next.</p></blockquote><p>DFS is the right tool when you care about <strong>reachability, structure, or exhaustive exploration</strong>, not minimum distance:</p><ul><li><p>Finding connected components</p></li><li><p>Path finding (when you don’t need the shortest path)</p></li></ul><p>A good mental model is a maze.</p><p>You choose a path, keep walking, and mark where you’ve been. When you hit a dead end, you backtrack to the last fork and try a different direction.</p><ul><li><p>Either an explicit stack you manage yourself</p></li><li><p>Or recursion, which uses the call stack implicitly</p></li></ul><p>Because the “most recent” node is processed next, DFS naturally pushes deeper into the graph.</p><pre><code>function dfs(node):\n    if node in visited:\n        return\n    visited.add(node)\n\n    for neighbor in node.neighbors:\n        dfs(neighbor)</code></pre><p>DFS is one of the most common ways to detect cycles, but the exact rule depends on the graph type (directed vs undirected graphs).</p><p>For undirected graphs: if you reach a visited node that isn’t your immediate parent, you’ve found a cycle.</p><p>For directed graphs: you use three states (unvisited, in-progress, completed). If you reach an in-progress node, you’ve found a back edge, which means a cycle.</p><pre><code>// Directed graph cycle detection\nstate = [UNVISITED] * n\n\nfunction hasCycle(node):\n    state[node] = IN_PROGRESS\n\n    for neighbor in node.neighbors:\n        if state[neighbor] == IN_PROGRESS:\n            return true  // cycle found\n        if state[neighbor] == UNVISITED:\n            if hasCycle(neighbor):\n                return true\n\n    state[node] = COMPLETED\n    return false</code></pre><p><strong>Practice these LeetCode problems:</strong></p><blockquote><p> Dijkstra’s Algorithm finds the <strong>shortest path in a weighted graph</strong> where all edge weights are . Unlike BFS, it works when edges have different costs.</p></blockquote><p>Reach for Dijkstra when you see  and the question is about :</p><ul><li><p>Shortest path with weighted edges</p></li><li><p>Navigation and routing (Google Maps)</p></li><li><p>Network routing with latency costs</p></li><li><p>Any problem mentioning “minimum cost path”</p></li></ul><p>BFS doesn’t work on weighted graphs because one step doesn’t equal one unit of distance. A direct path might cost 10 while a two-step path costs 2.</p><p>Dijkstra fixes this by always expanding the <strong>currently cheapest known node first</strong>:</p><ul><li><p>Maintain  = best known distance from the source</p></li><li><p>Use a <strong>min-heap / priority queue</strong> keyed by distance</p></li><li><p>Pop the node with the smallest distance, then relax its edges</p></li></ul><p>If the graph has only non-negative weights, the first time a node is popped with its best distance, that distance is final.</p><pre><code>dist = [infinity] * numNodes\ndist[source] = 0\npq = [(0, source)]  // (distance, node)\n\nwhile pq is not empty:\n    d, node = pq.pop_min()\n\n    if d &gt; dist[node]:\n        continue  // found a better path already\n\n    for (neighbor, weight) in node.edges:\n        newDist = d + weight\n        if newDist &lt; dist[neighbor]:\n            dist[neighbor] = newDist\n            pq.push((newDist, neighbor))</code></pre><h4>Sample Problem: Network Delay Time</h4><p>You have n network nodes. You’re given travel times as directed edges (u, v, w) where w is the time. You send a signal from node k. How long until all nodes receive it?</p><p>To solve this, you run Dijkstra from node k. Your answer is the maximum distance among all reachable nodes. If any node is unreachable, you return -1.</p><h4>Practice these LeetCode problems:</h4><blockquote><p> Topological sort orders nodes in a <strong>Directed Acyclic Graph (DAG)</strong> so that for every edge , node  appears  node . In plain terms: it gives you an execution order that respects dependencies. That’s why it shows up so often in scheduling-style problems.</p></blockquote><p>Topological sort is the default pattern when you see dependency language:</p><ul><li><p>Course scheduling with prerequisites</p></li><li><p>Build systems and dependency resolution</p></li><li><p>Any problem that mentions “prerequisites” or “dependencies”</p></li></ul><h4>How it works (Kahn’s Algorithm):</h4><p>Kahn’s algorithm is the BFS-style way to do topological sorting.</p><p> A node’s in-degree is the number of edges pointing to it. If a node has in-degree 0, it has no dependencies and you can process it.</p><p>You start by adding all nodes with in-degree 0 to a queue. You process them one by one. When you process a node, you decrement the in-degree of its neighbors. If a neighbor’s in-degree drops to 0, you add it to the queue.</p><p>If you process all nodes, you have a valid topological order. If the queue empties before you’ve processed all nodes, you’ve found a cycle.</p><pre><code>inDegree = count incoming edges for each node\nqueue = all nodes with inDegree 0\nresult = []\n\nwhile queue is not empty:\n    node = queue.pop()\n    result.append(node)\n\n    for neighbor in node.outgoing:\n        inDegree[neighbor] -= 1\n        if inDegree[neighbor] == 0:\n            queue.push(neighbor)\n\nif len(result) &lt; numNodes:\n    return \"cycle detected\"</code></pre><h4>Sample Problem: Course Schedule II</h4><p>You have numCourses courses. Some have prerequisites: [0, 1] means you take course 1 before course 0. You need to return any valid order to finish all courses, or an empty array if impossible.</p><p>To solve this, you build a directed graph where edge b→a means “take b before a”. Then run Kahn’s algorithm. If you can’t process all courses, you’ve found a cyclic dependency.</p><h4>Practice these LeetCode problems:</h4><blockquote><p> Union-Find is a data structure for tracking a collection of elements split into <strong>disjoint (non-overlapping) sets</strong>. It supports two core operations:</p><ul><li><p> which set does  belong to?</p></li><li><p> merge the sets containing  and </p></li></ul><p>Once you have these, you can answer connectivity questions like “are these two nodes connected?” efficiently.</p></blockquote><p>Union-Find shines when you’re adding connections over time and need fast connectivity checks:</p><ul><li><p>Dynamic connectivity queries</p></li><li><p>Detecting cycles in undirected graphs</p></li><li><p>Grouping elements as edges are added</p></li></ul><p>Each set has a leader (representative). Two elements are in the same set if they have the same leader. You store a parent array where parent[i] points to i’s parent. The root is the leader (where parent[i] = i).</p><p>To make operations extremely fast in practice, Union-Find uses two standard optimizations:</p><ol><li><p>: When you find the leader, you make each node on the path point directly to the leader.</p></li><li><p>: When merging two sets, attach the smaller tree under the larger one to keep trees shallow.</p></li></ol><pre><code>parent = [0, 1, 2, ..., n-1]  // each node is its own leader\nrank = [0] * n\n\nfunction find(x):\n    if parent[x] != x:\n        parent[x] = find(parent[x])  // path compression\n    return parent[x]\n\nfunction union(x, y):\n    px, py = find(x), find(y)\n    if px == py:\n        return false  // already connected\n\n    if rank[px] &lt; rank[py]:\n        parent[px] = py\n    else if rank[px] &gt; rank[py]:\n        parent[py] = px\n    else:\n        parent[py] = px\n        rank[px] += 1\n\n    return true</code></pre><h4>Sample Problem: Redundant Connection</h4><p>You have a graph that started as a tree (connected, no cycles), then one edge was added. You need to find the edge that can be removed to restore the tree.</p><p>To solve this using union find, process edges one by one. For each edge (u, v), you check if u and v are already connected using find(). If yes, this edge creates a cycle. If no, you union them. The first edge that connects already-connected nodes is your answer.</p><h4>Practice these LeetCode problems:</h4><blockquote><p> A spanning tree connects all nodes in a graph using exactly n-1 edges with no cycles. The minimum spanning tree is the one with the smallest total edge weight.</p></blockquote><p>MST comes up whenever you need to connect a set of nodes with minimum total cost:</p><ul><li><p>Connecting all nodes with minimum cost</p></li><li><p>Network design (cables, roads, pipelines)</p></li><li><p>Approximation algorithms for NP-hard problems</p></li></ul><h4>How it works (Kruskal’s Algorithm):</h4><p>Kruskal’s is the most common MST approach in interviews because it’s simple and pairs naturally with Union-Find.</p><p>You sort all edges by weight. You process them in order. For each edge, you use Union-Find to check if it connects two different components. If yes, you add it to the MST. If no, you skip it to avoid a cycle.</p><pre><code>edges.sort(by weight)\nmst = []\nuf = UnionFind(n)\n\nfor edge in edges:\n    u, v, weight = edge\n    if uf.find(u) != uf.find(v):\n        uf.union(u, v)\n        mst.append(edge)\n\n    if len(mst) == n - 1:\n        break\n\nreturn mst</code></pre><h4>Sample Problem: Min Cost to Connect All Points</h4><p>You’re given points on a 2D plane. The cost to connect two points is their Manhattan distance. You return the minimum cost to connect all points.</p><p>To solve this, you treat each pair of points as a potential edge. Generate all edges, sort by weight, and run Kruskal’s. The MST gives you the minimum total cost.</p><h4>Practice these LeetCode problems:</h4><blockquote><p> A graph is  if you can split its nodes into two groups such that <strong>every edge connects nodes from different groups</strong>. No edge is allowed within the same group.</p><p>A simple way to remember it: Can you  the graph so that no two adjacent nodes share the same color?</p></blockquote><p>Bipartite checks appear whenever the problem is about dividing things into two compatible sets:</p><ul><li><p>Team or group assignments</p></li><li><p>Checking for odd-length cycles (a graph is bipartite if and only if it has no odd-length cycles)</p></li><li><p>Scheduling with conflicts</p></li></ul><p>You can use  to try 2-coloring the graph.</p><p>Pick a start node and assign it color 0. Assign all its neighbors color 1. Assign their neighbors color 0. Continue alternating.</p><p>If you ever find an edge where both endpoints have the , the graph is  bipartite.</p><pre><code>color = [-1] * n  // uncolored\n\nfunction isBipartite(start):\n    queue = [start]\n    color[start] = 0\n\n    while queue is not empty:\n        node = queue.pop()\n        for neighbor in node.neighbors:\n            if color[neighbor] == -1:\n                color[neighbor] = 1 - color[node]\n                queue.push(neighbor)\n            else if color[neighbor] == color[node]:\n                return false\n\n    return true</code></pre><h4>Sample Problem: Is Graph Bipartite?</h4><p>You’re given an undirected graph. You return true if it’s bipartite.</p><p>Because the graph may be disconnected, run BFS/DFS from :</p><ul><li><p>Start a new traversal, try to 2-color that component</p></li><li><p>If any component fails, return </p></li><li><p>If all components succeed, the graph is bipartite</p></li></ul><h4>Practice these LeetCode problems:</h4><p>You now have 7 algorithms that cover most graph problems in coding interviews. Here’s how to retain them:</p><ol><li><p> Start with BFS and DFS. They’re the foundation the others build on.</p></li><li><p><strong>Match patterns to algorithms.</strong> “Shortest path” without weights → BFS. “Prerequisites” or “dependencies” → topological sort. “Minimum cost to connect” → MST.</p></li><li><p> Don’t just read the code. Write it yourself. Debug it. That’s how you learn.</p></li><li><p><strong>Practice with time limits.</strong> In interviews, you have 20-30 minutes per problem. Get comfortable working under that constraint.</p></li></ol><p>For a structured approach to learn, feel free to take a look at our roadmaps. It covers these algorithms plus topics like Strongly Connected Components, Lowest Common Ancestor, and Network Flow.</p>","contentLength":12897,"flags":null,"enclosureUrl":"https://substackcdn.com/image/fetch/$s_!1puy!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd166816b-b57e-40e3-ac90-33b31795d92e_1280x926.jpeg","enclosureMime":"","commentsUrl":null},{"title":"Polling vs. Long Polling vs. SSE vs. WebSockets vs. Webhooks","url":"https://blog.algomaster.io/p/polling-vs-long-polling-vs-sse-vs-websockets-webhooks","date":1770091373,"author":"Ashish Pratap Singh","guid":443,"unread":true,"content":"<p>Whether you are chatting with a friend or playing an online game, updates show up in real time without hitting .</p><p>Behind these seamless experiences lies a key engineering decision: <strong>how does the server notify the client (or another system) when new data is available?</strong></p><p>The traditional HTTP was built around a simple request-response flow: <strong>the client asks, the server answers</strong>. But in real-time systems, the server often needs to push updates proactively, sometimes continuously.</p><p>That’s where communication models like <strong>Long Polling, Server-Sent Events (SSE), WebSockets, and Webhooks</strong> come in.</p><p>In this article, we’ll break down how each one works, it’s pros and cons, where it fits best, and how to choose the right approach for a  or a .</p><p>Let's start with the most straightforward approach.</p><p> is the simplest approach to getting updates from a server. The client sends requests to the server at regular intervals, checking if anything has changed.</p><p>Think of it like refreshing your email inbox every few minutes to check for new messages.</p><ol><li><p>Client sends an HTTP request to the server</p></li><li><p>Server responds immediately with current data (or empty response)</p></li><li><p>Client waits for a fixed interval (e.g., 5 seconds)</p></li><li><p>Client sends another request</p></li></ol><p>Notice something wasteful here? The client keeps asking even when nothing has changed. Three out of four requests in this diagram returned empty responses. </p><p>In real applications, this ratio is often much worse. You might make 100 requests before getting a single meaningful update.</p><h3>Example: Weather Dashboard</h3><p>Imagine you’re building a weather dashboard. Weather data doesn’t change that frequently, maybe every 15-30 minutes at most. </p><p>Polling makes sense here:</p><pre><code>setInterval(async () =&gt; {\n    const response = await fetch('/api/weather?city=london');\n    const weather = await response.json();\n    updateDashboard(weather);\n}, 60000); // Poll every minute</code></pre><p>Every minute, your client asks for the current weather. The server responds with temperature, humidity, conditions, and so on.</p><ul><li><p> Just a regular HTTP request in a loop. No special protocols or libraries needed.</p></li><li><p> Any HTTP client can do polling. No firewall or proxy issues.</p></li><li><p> Each request is independent. The server doesn’t need to maintain any connection state.</p></li><li><p> Standard HTTP requests that show up in network logs and dev tools.</p></li></ul><ul><li><p> Most requests return empty responses when nothing has changed. This wastes bandwidth and server resources.</p></li><li><p> Updates are delayed by the polling interval. If you poll every 10 seconds, updates can take up to 10 seconds to reach the client.</p></li><li><p> 10,000 clients polling every second means 10,000 requests per second, even when nothing is happening.</p></li><li><p><strong>Trade-off between latency and efficiency:</strong> Shorter intervals mean faster updates but more wasted requests. Longer intervals mean fewer requests but slower updates.</p></li></ul><ul><li><p> Weather data, daily reports, or anything that changes infrequently</p></li><li><p> MVPs, internal tools, or situations where simplicity matters more than efficiency</p></li><li><p> When you need to support older clients or environments that can’t use modern techniques</p></li><li><p> When delays of several seconds (or minutes) are acceptable</p></li></ul><p>Polling is a reasonable starting point, but you’ll quickly feel its limitations as your application grows. If you need faster updates without drowning your server in requests, that’s where long polling comes in.</p><p> improves on regular polling by having the server hold the request open until new data is available (or a timeout occurs). Instead of the client repeatedly asking “anything new?”, the server waits and responds only when there’s something to report.</p><p>This was the technique that powered early real-time applications like Facebook Messenger before WebSockets became widely supported.</p><ol><li><p>Client sends an HTTP request to the server</p></li><li><p>Server holds the connection open (doesn’t respond immediately)</p></li><li><p>When new data arrives, server sends the response</p></li><li><p>Client immediately sends another request</p></li><li><p>If no data arrives within the timeout period, server sends an empty response and client reconnects</p></li></ol><p>The key insight is that the server only responds when it has something meaningful to say. This eliminates the wasted “nothing new” responses of regular polling.</p><h3>Example: Chat Application</h3><p>Consider a chat app built with long polling. When you open a conversation, your browser sends a request like:</p><pre><code>GET /api/messages?conversation=123&amp;after=msg_999</code></pre><p>The server checks if there are any messages newer than . If not, instead of returning an empty response, it holds the connection and waits. </p><p>When someone sends a new message to that conversation, the server immediately responds with the new message. Your client receives it, renders it in the chat window, and immediately opens a new connection to wait for the next message.</p><p>There’s an important detail here: the . HTTP connections can’t stay open forever. Proxies, load balancers, and browsers all have limits. So the server needs to respond eventually, even if nothing happened.</p><p>A typical timeout is 30 seconds. If 30 seconds pass with no new data, the server sends an empty response, the client immediately reconnects, and the wait continues.</p><ul><li><p> Updates arrive almost instantly when they happen, without waiting for a polling interval.</p></li><li><p> Responses almost always contain useful data, not empty “nothing new” responses.</p></li><li><p><strong>Works through proxies and firewalls:</strong> Uses standard HTTP, so it works in restrictive network environments where WebSockets might be blocked.</p></li><li><p> No protocol upgrade, no special handling for connection state.</p></li></ul><ul><li><p> Each waiting client holds a connection open on the server. With 10,000 clients, you need 10,000 open connections.</p></li><li><p><strong>Timeout handling complexity:</strong> You need to handle timeouts, reconnection logic, and edge cases like the client receiving data just as the timeout expires.</p></li><li><p> If multiple events happen quickly, they may get batched together or delivered out of order.</p></li><li><p> Every response requires a new request, and each request carries HTTP headers. This overhead adds up.</p></li></ul><ul><li><p><strong>Chat applications (historically):</strong> Before WebSocket support was universal, long polling powered most chat systems</p></li><li><p> When WebSockets aren’t available due to proxy or firewall restrictions</p></li><li><p><strong>Server-initiated updates:</strong> When clients mostly receive data rather than send it</p></li><li><p> Works well for hundreds or thousands of concurrent connections, but gets expensive at massive scale</p></li></ul><p>Long polling feels like “almost real-time,” but it’s still request-response at heart. The client still initiates every exchange. What if the server could just push data to clients whenever it wants? That’s exactly what Server-Sent Events enable.</p><p> is a standard that allows a server to push updates to a client over a single, long-lived HTTP connection. Unlike polling where the client keeps asking, SSE opens a connection once and the server sends data whenever it has something to share.</p><p>Think of it like subscribing to a news feed. You sign up once, and updates stream to you automatically.</p><ol><li><p>Client opens a connection using the  API</p></li><li><p>Server keeps the connection open and sends events as they occur</p></li><li><p>Events are sent as plain text in a specific format</p></li><li><p>If the connection drops, the browser automatically reconnects</p></li><li><p>Server can send different event types for clients to handle differently</p></li></ol><p>The data flows in one direction: from server to client. If the client needs to send data back, it uses separate HTTP requests.</p><h3>Example: Stock Price Dashboard</h3><pre><code>const eventSource = new EventSource('/api/stock-prices');\n\neventSource.addEventListener('price_update', (event) =&gt; {\n    const data = JSON.parse(event.data);\n    updatePriceDisplay(data.symbol, data.price);\n});\n\neventSource.addEventListener('alert', (event) =&gt; {\n    showNotification(JSON.parse(event.data).message);\n});</code></pre><p>The server streams price updates as they happen. The client handles different event types (price updates vs alerts) with separate listeners.</p><p>SSE uses a simple text-based format that’s easy to read and debug:</p><pre><code>event: price_update\ndata: {\"symbol\": \"AAPL\", \"price\": 150.25}\nid: 12345\n\nevent: price_update\ndata: {\"symbol\": \"GOOG\", \"price\": 2840.50}\nid: 12346</code></pre><p>The  field enables resumption. If the connection drops, the browser sends  in the reconnection request, and the server can replay missed events.</p><ul><li><p> The browser’s  API handles connections, reconnection, and parsing automatically.</p></li><li><p> If the connection drops, the browser reconnects automatically with the last event ID.</p></li><li><p> Multiple SSE streams can share a single TCP connection, reducing overhead.</p></li><li><p><strong>Lower overhead than polling:</strong> One connection, no repeated request headers, no wasted “nothing new” responses.</p></li><li><p><strong>Text-based and debuggable:</strong> Events are plain text, easy to inspect in browser dev tools.</p></li></ul><ul><li><p> Data flows server to client only. The client can’t send data over the same connection.</p></li><li><p> Browsers limit connections per domain (typically 6 for HTTP/1.1). This matters if you need multiple SSE streams.</p></li><li><p> SSE is text-only. Binary data must be base64 encoded, which adds overhead.</p></li><li><p><strong>Less flexible than WebSockets:</strong> No custom subprotocols or extensions.</p></li></ul><ul><li><p> News feeds, social media timelines, activity streams</p></li><li><p> Stock tickers, analytics dashboards, monitoring systems</p></li><li><p> Push notifications, alerts, status updates</p></li><li><p><strong>Any scenario where data flows one way:</strong> From server to client, with no client-to-server messages needed</p></li></ul><p>SSE works great when you only need server-to-client communication. But many applications need bidirectional communication, where both sides send and receive messages freely. That’s where WebSockets shine.</p><p> represent a fundamental departure from HTTP’s request-response model. Instead of one side asking and the other answering, WebSockets establish a persistent, bidirectional channel where either party can send messages at any time.</p><p>This is the technology behind real-time chat apps like Slack and Discord, multiplayer games, and collaborative tools like Google Docs.</p><p>A WebSocket connection starts as HTTP, then “upgrades” to a different protocol:</p><ol><li><p>Client initiates a WebSocket handshake via HTTP upgrade request</p></li><li><p>Server accepts and upgrades the connection from HTTP to WebSocket</p></li><li><p>Both sides can now send messages freely</p></li><li><p>Connection stays open until either side closes it</p></li></ol><p>The initial HTTP handshake looks like this:</p><pre><code>GET /chat HTTP/1.1\nHost: server.example.com\nUpgrade: websocket\nConnection: Upgrade\nSec-WebSocket-Key: dGhlIHNhbXBsZSBub25jZQ==\nSec-WebSocket-Version: 13\nOrigin: https://example.com</code></pre><pre><code>HTTP/1.1 101 Switching Protocols\nUpgrade: websocket\nConnection: Upgrade\nSec-WebSocket-Accept: s3pPLMBiTxaQ9kYGzzhZRbK+xOo=</code></pre><p>Once upgraded, the connection is no longer HTTP. It’s now a WebSocket connection, a binary framing protocol optimized for low-overhead messaging.</p><h3>Example: Chat Application</h3><p>Here’s how a chat client might work:</p><pre><code>const socket = new WebSocket('wss://chat.example.com/room/123');\n\nsocket.onopen = () =&gt; {\n    socket.send(JSON.stringify({\n        type: 'join',\n        username: 'alice'\n    }));\n};\n\nsocket.onmessage = (event) =&gt; {\n    const data = JSON.parse(event.data);\n    if (data.type === 'message') {\n        displayMessage(data.user, data.text);\n    } else if (data.type === 'typing') {\n        showTypingIndicator(data.user);\n    }\n};\n\n// Send a message\nfunction sendMessage(text) {\n    socket.send(JSON.stringify({\n        type: 'message',\n        text: text\n    }));\n}</code></pre><p>Both client and server can send messages at any time. The server can broadcast typing indicators while the client sends messages, all over the same connection.</p><ul><li><p> Messages arrive with minimal latency. No polling, no waiting.</p></li><li><p> Both sides can send messages independently, without waiting for a response.</p></li><li><p> After the initial handshake, message framing is minimal (as little as 2 bytes per frame).</p></li><li><p> WebSockets can send binary data directly, unlike SSE.</p></li><li><p> You can negotiate application-level protocols (like STOMP or MQTT) over WebSocket.</p></li></ul><ul><li><p> The server must maintain state for each connection. This complicates horizontal scaling since you need sticky sessions or a pub/sub layer.</p></li><li><p> You need to handle disconnections, reconnections, heartbeats, and timeouts manually.</p></li><li><p><strong>Proxy and firewall issues:</strong> Some corporate proxies don’t handle WebSocket upgrades properly, blocking connections.</p></li><li><p> Debugging is harder. Messages don’t show up in standard HTTP logs. You need WebSocket-aware tools.</p></li><li><p> Each connection consumes server memory and file descriptors. At massive scale, this adds up.</p></li></ul><ul><li><p> Slack, Discord, WhatsApp Web</p></li><li><p> Real-time game state synchronization</p></li><li><p> Google Docs, Figma, Notion</p></li><li><p> Where milliseconds matter</p></li><li><p> Auctions, live streaming with chat</p></li></ul><p>The rule of thumb: if you truly need bidirectional real-time communication, WebSockets are probably your answer. But if data only flows one way (server to client), SSE is simpler. And if you’re building server-to-server integrations, there’s yet another approach.</p><p>Everything we’ve discussed so far involves browsers or mobile apps talking to servers.</p><p> flip the script entirely: they’re how servers talk to other servers. Instead of clients pulling data from servers, webhooks let one server push data to another when events occur.</p><p>Think of webhooks as “reverse APIs.” Instead of you calling an API to get data, the API calls you when something happens.</p><ol><li><p>You register a callback URL with the provider (e.g., Stripe, GitHub)</p></li><li><p>You specify which events you want to receive</p></li><li><p>When those events occur, the provider sends an HTTP POST to your URL</p></li><li><p>Your server processes the event and responds with 200 OK</p></li><li><p>If delivery fails, the provider typically retries with exponential backoff</p></li></ol><h3>Example: Stripe Payment Webhooks</h3><p>When integrating Stripe, you configure a webhook endpoint in their dashboard:</p><pre><code>https://api.yourapp.com/webhooks/stripe</code></pre><p>And you subscribe to events like , <code>customer.subscription.created</code>, or .</p><p>When a customer’s payment succeeds, Stripe sends you something like:</p><pre><code>{\n  \"id\": \"evt_1PoJkD2eZvKYlo2CmOJbvwD9\",\n  \"type\": \"payment_intent.succeeded\",\n  \"created\": 1713681701,\n  \"data\": {\n    \"object\": {\n      \"id\": \"pi_3JhdNe2eZvKYlo2C1IqojYg9\",\n      \"amount\": 4999,\n      \"currency\": \"usd\",\n      \"customer\": \"cus_xyz789\",\n      \"metadata\": {\n        \"order_id\": \"order_12345\"\n      }\n    }\n  }\n}</code></pre><p>Your server receives this, verifies the signature (to ensure it really came from Stripe), and then does whatever needs doing: update the order status, send a receipt, decrement inventory, notify fulfillment.</p><p>Webhooks create a publicly accessible endpoint that anyone could try to call. You must verify that requests actually come from the expected source.</p><p>Most providers include a signature in the headers:</p><pre><code>Stripe-Signature: t=1617000000,v1=abc123def456...</code></pre><p>You verify this by computing an HMAC of the request body using your webhook secret:</p><pre><code>expected = hmac.new(secret, payload, hashlib.sha256).hexdigest()\nif not hmac.compare_digest(expected, signature):\n    return 403  # Reject the request</code></pre><ul><li><p> Events arrive immediately when they happen, no polling delay.</p></li><li><p> You only receive HTTP calls when something actually happens.</p></li><li><p> The provider doesn’t need to know your internal architecture. It just calls your URL.</p></li><li><p> No persistent connections to maintain. Each event is a single HTTP request.</p></li><li><p> Easy to debug, log, and monitor with standard tools.</p></li></ul><ul><li><p><strong>Requires public endpoint:</strong> Your server must be accessible from the internet. Local development requires tools like ngrok.</p></li><li><p><strong>Delivery isn’t guaranteed:</strong> If your server is down, you miss events. You need idempotent handlers and event replay mechanisms.</p></li><li><p> Providers retry failed deliveries, so your handler must be idempotent (processing the same event twice should be safe).</p></li><li><p> A public endpoint increases attack surface. Signature verification is essential.</p></li><li><p> If events arrive faster than you can process them, you need queuing and rate limiting.</p></li></ul><ul><li><p> Stripe, PayPal, Square notifications</p></li><li><p> GitHub push events triggering builds</p></li><li><p><strong>Third-party integrations:</strong> Any external service that needs to notify your system</p></li><li><p><strong>Microservices communication:</strong> Event-driven architectures between internal services</p></li><li><p> Receiving notifications about account changes, security events</p></li></ul><p>The pattern is clear: when one server needs to notify another about events, webhooks are almost always the right approach. They’re not for browser clients (which can’t expose public endpoints), but for backend-to-backend communication, they’re the gold standard.</p><p>With five options, how do you pick the right one? Consider these four factors:</p><h3>Question 1: Who Needs to Talk to Whom?</h3><p>This is the most fundamental question.</p><p><strong>Server notifying another server?</strong> Webhooks. There’s rarely a good reason to use anything else for backend-to-backend event notification.</p><p><strong>Server pushing updates to clients (one-way)?</strong> SSE is usually the best choice. It’s simpler than WebSockets and handles reconnection automatically.</p><p><strong>Truly bidirectional client-server communication?</strong> WebSockets. If both sides need to send messages freely, nothing else really works.</p><h3>Question 2: How Fast Do Updates Need to Arrive?</h3><p><strong>Minutes or hours are acceptable?</strong> Polling is fine. Don’t over-engineer.</p><p> Long polling or SSE both work well.</p><p> WebSockets. There’s nothing faster for browser-to-server communication.</p><h3>Question 3: What Infrastructure Constraints Exist?</h3><p><strong>Corporate firewalls blocking WebSockets?</strong> Fall back to long polling or SSE.</p><p><strong>Need to support old browsers or restricted environments?</strong> Polling is the safest choice.</p><p><strong>Horizontal scaling is critical?</strong> SSE or webhooks scale more easily than WebSockets.</p><p><strong>Running serverless or can’t maintain persistent connections?</strong> Webhooks for server-to-server. Polling for client-to-server.</p><h3>Question 4: What’s the Acceptable Complexity Budget?</h3><p>Every approach has a complexity cost.</p><ul><li><p> Almost zero complexity. Anyone can implement it.</p></li><li><p> Low client complexity (browser handles it), moderate server complexity.</p></li><li><p> Moderate complexity on both sides. Edge cases are tricky.</p></li><li><p> High complexity. Connection management, scaling, state synchronization.</p></li><li><p> Moderate complexity. Security verification, idempotency, error handling.</p></li></ul><p>Match the tool to the actual requirements.</p><p>If you found it valuable, hit a like ❤️ and consider subscribing for more such content every week.</p><p>If you have any questions/suggestions, feel free to leave a comment.</p>","contentLength":17922,"flags":null,"enclosureUrl":"https://substackcdn.com/image/fetch/$s_!DkxM!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbf447e18-ccb7-4f9c-8e09-2a4d51989ac8_2068x1366.png","enclosureMime":"","commentsUrl":null},{"title":"How to Scale a System from 0 to 10 million+ Users","url":"https://blog.algomaster.io/p/scaling-a-system-from-0-to-10-million-users","date":1769660280,"author":"Ashish Pratap Singh","guid":442,"unread":true,"content":"<p>Scaling is a complex topic, but after working at  on services handling millions of requests and scaling my own  from scratch, I’ve realized that most systems evolve through a surprisingly similar set of stages as they grow.</p><p>The key insight is that <strong>you should not over-engineer from the start</strong>. Start simple, identify bottlenecks, and scale incrementally.</p><p>In this article, I’ll walk you through <strong>7 stages of scaling a system</strong> from zero to 10 million users and beyond. Each stage addresses the specific bottlenecks that show up at different growth points. You’ll learn what to add, when to add it, why it helps, and the trade-offs involved.</p><p>Whether you’re building an app or website, preparing for system design interviews, or just curious about how large-scale systems work, understanding this progression will sharpen they way you think about architecture.</p><blockquote><p>The user ranges and numbers mentioned in this article are approximate and intended to illustrate a scaling journey. Actual thresholds will vary depending on your product, workload characteristics, and traffic patterns.</p></blockquote><p>When you’re just starting out, your first priority is simple: <strong>ship something and validate your idea</strong>. Optimizing too early at this stage wastes time and money on problems you may never face.</p><p>The simplest architecture puts everything on a : your web application, database, and any background jobs all running on the same machine.</p><blockquote><p>This is how Instagram started. When Kevin Systrom and Mike Krieger launched the first version in 2010, 25,000 people signed up on day one.</p><p>They didn’t over-engineer upfront. With a small team and a simple setup, they scaled in response to real demand, adding capacity as usage grew, rather than building for hypothetical future traffic.</p></blockquote><h3>What This Architecture Looks Like</h3><p>In practice, a single-server setup means:</p><ul><li><p>A web framework (Django, Rails, Express, Spring Boot) handling HTTP requests</p></li><li><p>A database (PostgreSQL, MySQL) storing your data</p></li><li><p>Background job processing (Sidekiq, Celery) for async tasks</p></li><li><p>Maybe a reverse proxy (Nginx) in front for SSL termination</p></li></ul><p>All of these run on one virtual machine. Your cloud provider bill might be $20-50/month for a basic VPS (DigitalOcean Droplet, AWS Lightsail, Linode).</p><h3>Why This Works for Early Stage</h3><p>At this stage, simplicity is your biggest advantage:</p><ul><li><p>: One server means one place to deploy, monitor, and debug.</p></li><li><p>: A single $20-50/month Virtual Private Server (VPS) can comfortably handle your first 100 users.</p></li><li><p>: No distributed systems complexity to slow down development.</p></li><li><p>: All logs are in one place, and there are no network issues between components.</p></li><li><p>: You can trace every request end to end because there’s only one execution path.</p></li></ul><h3>The Trade-offs You’re Making</h3><p>This simplicity comes with trade-offs you accept knowingly:</p><p>You’ll know it’s time to evolve when you notice these signs:</p><ul><li><p><strong>Database queries slow down during peak traffic</strong>: The app and database compete for the same CPU and memory. One heavy query can drag down API latency for everyone.</p></li><li><p><strong>Server CPU or memory consistently exceeds 70-80%</strong>: You’re approaching the limits of what a single machine can reliably handle.</p></li><li><p><strong>Deployments require restarts and cause downtime</strong>: Even short interruptions become noticeable, and users start to complain.</p></li><li><p><strong>A background job crash takes down the web server</strong>: Without isolation, non-user-facing work can impact the user experience.</p></li><li><p><strong>You can’t afford even brief downtime</strong>: Your product has become critical enough that even maintenance windows stop being acceptable.</p></li></ul><p>At some point, the server starts to struggle under the weight of doing everything. That’s when it’s time for your first architectural split.</p><p>As traffic grows, your single server starts struggling. The web application and database compete for the same CPU, memory, and disk I/O. A single heavy query can spike latency and slow down every API response.</p><p>The first scaling step is simple: <strong>separate the database from the application server</strong>.</p><p>This two-tier architecture gives you several immediate benefits:</p><ul><li><p>Application and database no longer compete for CPU/memory. Each can use 100% of their allocated resources.</p></li><li><p>Upgrade the database (more RAM, faster storage) without touching the app server.</p></li><li><p>Database server can sit in a private network, not exposed to the internet.</p></li><li><p><strong>Specialized Optimization: </strong>Tune each server for its specific workload. High CPU for app server, high I/O for database.</p></li><li><p>Database backups don’t affect application performance since they run on a different machine.</p></li></ul><h3>Managed Database Services</h3><p>At this stage, most teams use a managed database like , , , or  (I use Supabase at ).</p><p>Managed services typically handle:</p><ul><li><p>Automated backups (daily snapshots, point-in-time recovery)</p></li><li><p>Security patches and updates</p></li><li><p>Basic monitoring and alerts</p></li><li><p>Optional read replicas (we’ll cover these later)</p></li><li><p>Failover to standby instances</p></li></ul><p>The cost difference between self-hosting and managed is usually small once you factor in engineering time. A managed PostgreSQL instance might cost  than a raw VM, but it can save hours of maintenance every week. Those hours are better spent shipping features.</p><p>The main reasons to self-manage a database are:</p><ul><li><p>Cost optimization at very large scale</p></li><li><p>Specific configurations that managed services don’t support</p></li><li><p>Compliance requirements that prohibit managed services</p></li><li><p>You’re building a database product</p></li></ul><p>For most teams, managed services are the right choice until your database bill grows into the <strong>thousands of dollars per month</strong>.</p><p>One often-overlooked improvement at this stage is connection pooling. Each database connection consumes resources:</p><ul><li><p>Memory for the connection state (typically 5-10MB per connection in PostgreSQL)</p></li><li><p>File descriptors on both app and database servers</p></li><li><p>CPU overhead for connection management</p></li></ul><p>Opening a new connection is expensive too. Between the TCP handshake, SSL negotiation, and database authentication, you can add  of overhead per request.</p><p>A connection pooler like  (for PostgreSQL) keeps a small set of database connections open and reuses them across requests.</p><p>With 1,000 users, you might have 100 concurrent connections hitting your API. Without pooling, that’s 100 database connections consuming resources. With pooling, 20-30 actual database connections can efficiently serve those 100 application connections through connection reuse.</p><p><strong>Connection pooling modes:</strong></p><ul><li><p>: One pool connection per client connection (most compatible, least efficient)</p></li><li><p>: Connection returned to the pool after each transaction (best balance for most apps)</p></li><li><p>: Connection returned after each statement (most efficient, but can break features)</p></li></ul><p>Most applications work best with , which often improves connection efficiency by .</p><h3>Network Latency Considerations</h3><p>Separating the database introduces network latency. When app and database were on the same machine, “network” latency was essentially zero (loopback interface). Now every query adds 0.1-1ms of network round-trip time.</p><p>For most applications, this is negligible. But if your code makes hundreds of database queries per request (an anti-pattern, but common), this latency adds up. The solution isn’t to put them back on the same machine, but to optimize your query patterns:</p><ul><li><p>Batch queries where possible</p></li><li><p>Use JOINs instead of N+1 query patterns</p></li><li><p>Cache frequently accessed data</p></li><li><p>Use connection pooling to avoid repeated connection setup overhead</p></li></ul><p>With the database on its own server, you’ve bought yourself room to grow. But you’ve also created a new single point of failure: the application server is now the weak link. What happens when it goes down, or when it simply can’t keep up with demand?</p><p>Your separated architecture handles load better now, but you’ve introduced a new problem: your single application server is now a . If it crashes, your entire application goes down. And as traffic grows, that one server can’t keep up.</p><p>The next step is to run <strong>multiple application servers</strong> behind a .</p><p>The load balancer sits in front of your servers and distributes incoming requests across them. If one server fails, the load balancer detects this (via health checks) and routes traffic only to healthy servers. Users experience no downtime when a single server fails.</p><p>The load balancer needs to decide which server handles each request. Common algorithms include: , , , , and .</p><p>Most teams start with Round Robin (simple, works well for most cases) and switch to Least Connections if they have requests with varying processing times.</p><p>Modern load balancers operate at different layers:</p><ul><li><p>: Routes based on IP and port. Fast, but can’t inspect HTTP headers.</p></li><li><p>: Routes based on HTTP headers, URLs, cookies. More flexible, slightly more overhead.</p></li></ul><p>For most web applications, Layer 7 load balancing is preferable because it enables:</p><ul><li><p>Path-based routing ( to API servers,  to CDN)</p></li><li><p>Header-based routing (different versions for mobile vs desktop)</p></li><li><p>SSL termination at the load balancer</p></li><li><p>Request/response inspection for security</p></li></ul><h3>Vertical vs Horizontal Scaling</h3><p>Before adding more servers, you might ask: why not just get a bigger server? This is the classic vertical vs horizontal scaling trade-off.</p><p> means moving to a larger server. It works well early on and usually requires no code changes. But you eventually run into two problems: hard hardware limits and rapidly increasing costs. </p><p>Bigger machines are priced non-linearly, so doubling CPU or memory can cost 3–4x more. And even the largest instances have a ceiling.</p><p> means adding more servers. It is harder at first because your application must be , so any server can handle any request. But it gives you effectively unlimited capacity and built-in redundancy. If one server fails, the system keeps running.</p><p>This is where horizontal scaling gets tricky. If a user logs in and their session lives in , what happens when the next request lands on ? From the app’s perspective, the session is missing, so the user looks logged out.</p><p>This is the , and it’s the biggest obstacle to horizontal scaling.</p><p>There are two common ways to handle it:</p><h4>1. Sticky Sessions (Session Affinity)</h4><p>The load balancer routes all requests from the same user to the same server, typically using a cookie or IP hash.</p><ul><li><p>Requires no application changes</p></li><li><p>Works with any session storage</p></li></ul><ul><li><p>If that server fails, the user loses their session</p></li><li><p>Uneven load distribution if some users are more active than others</p></li><li><p>Limits true horizontal scaling (can’t freely move users between servers)</p></li><li><p>New servers take time to “warm up” with sessions</p></li></ul><h4>2. External Session Store</h4><p>Move session data out of the application servers into a shared store like  or .</p><p>Now any server can handle any request because session data is centralized. This is the pattern most large-scale systems use. The added latency of a Redis lookup (sub-millisecond) is negligible compared to the flexibility it provides.</p><p>You can now handle more traffic and survive server failures. But as your user base grows, you’ll notice something: no matter how many application servers you add, they’re all hammering the same database. The database is becoming your next bottleneck.</p><p>With 10,000+ users, a new bottleneck emerges: your database. Every request hits the database, and as traffic grows, query latency increases. The database that handled 100 QPS (queries per second) fine starts struggling at 1,000 QPS. </p><p>Read-heavy applications (which most are, with read-to-write ratios of 10:1 or higher) suffer especially hard.</p><p>This stage introduces three complementary solutions: , , and . Together, they can reduce database load by 90% or more.</p><p>Most web applications follow the 80/20 rule: 80% of requests access 20% of the data. A product page viewed 10,000 times doesn’t need 10,000 database queries. The user’s profile that loads on every page view doesn’t need to be fetched fresh each time.</p><p>Caching stores frequently accessed data in memory for near-instant retrieval. While database queries take 1-100ms, cache reads take 0.1-1ms.</p><p>The most common caching pattern is  (also called lazy loading):</p><ol><li><p>Application checks the cache first</p></li><li><p>If data exists (cache hit), return it immediately</p></li><li><p>If not (cache miss), query the database</p></li><li><p>Store the result in cache for future requests (with TTL)</p></li></ol><p>Redis and Memcached are the standard choices here. Redis is more feature-rich (supports data structures like lists, sets, sorted sets; persistence; pub/sub; Lua scripting), while Memcached is simpler and slightly faster for pure key-value caching.</p><p>Most teams choose Redis because the additional features are useful (using sorted sets for leaderboards, lists for queues, etc.), and the performance difference is negligible.</p><p>Not everything should be cached. Good cache candidates include:</p><ul><li><p>Highly personalized data (different for every user, low reuse)</p></li><li><p>Frequently changing data (constant invalidation overhead)</p></li><li><p>Large blobs (consumes memory without proportional benefit)</p></li><li><p>Transactional data where staleness causes issues</p></li></ul><p>The hardest part of caching isn’t adding it, it’s keeping it accurate. When underlying data changes, cached data becomes stale. This is famously one of the “two hard problems in computer science.”</p><p><strong>Common strategies include:</strong></p><p>Most systems start with TTL-based expiration (set cache to expire after 5-60 minutes) and add explicit invalidation for data where staleness causes problems. For example:</p><pre><code>def update_user_profile(user_id, new_data):\n    # Update database\n    db.update(\"users\", user_id, new_data)\n    # Invalidate cache\n    cache.delete(f\"user:{user_id}\")</code></pre><p>The next read will miss the cache and fetch fresh data from the database.</p><p>Even with caching, some requests will still hit the database, especially  and . Read replicas help by distributing read traffic across multiple copies of the database.</p><p>The primary database handles all writes. Changes are then replicated (usually asynchronously) to one or more . Your application sends read queries to replicas and keeps the write workload on the primary, which reduces contention and improves overall throughput.</p><p>One important consideration is . Since replication is often asynchronous (for performance), replicas might be milliseconds to seconds behind the primary.</p><p>For most applications, this is acceptable. If a social media feed is a second behind, most users will not notice. But some flows require stronger consistency.</p><p>A common failure mode is <strong>read-your-writes consistency</strong>:</p><p>A user updates their profile and refreshes immediately. If that read lands on a replica that has not caught up, they see old data and assume the update failed.</p><ol><li><p><strong>Read from primary after writes</strong>: For a short window (N seconds) after a write, route that user’s reads to the primary.</p></li><li><p><strong>Session-level consistency</strong>: Track the user’s last write timestamp and only read from replicas that have caught up past that point.</p></li><li><p><strong>Explicit read-from-primary</strong>: For critical reads (viewing just-updated data), always hit the primary.</p></li></ol><p>Most frameworks have built-in support for read/write splitting. For example, Rails (ActiveRecord), Django, and Hibernate can route reads to replicas and writes to the primary automatically.</p><h3>Content Delivery Network (CDN)</h3><p>Static assets like images, CSS, JavaScript, and videos rarely change and don’t need to hit your application servers at all. They’re also the largest files you serve, which makes them expensive in both bandwidth and compute if you serve them directly.</p><p>A  solves this by caching static assets on globally distributed servers called  (or points of presence).</p><p>Here’s what happens when a user in Tokyo requests an image:</p><ul><li><p>The request is routed to the  (low latency, say ~50 ms round trip).</p></li><li><p>If the file is already cached (), the CDN serves it immediately.</p></li><li><p>If it’s not cached (), the CDN fetches it from your  (maybe in the US, ~300 ms), stores a copy at the edge, and then returns it to the user.</p></li><li><p>The next user in Tokyo gets the cached version from the edge, again at ~50 ms.</p></li></ul><p>Popular CDNs include  (strong free tier), , , and .</p><p>With caching, read replicas, and a CDN in place, your system can handle steady growth. The next challenge is . A viral post, a marketing campaign, or even the difference between 3 AM and 3 PM can create 10x traffic variation. At that point, manually adjusting capacity stops working.</p><p>At 100K+ users, traffic patterns become less predictable. You might have:</p><ul><li><p>Daily peaks (morning in US, evening in EU)</p></li><li><p>Weekly patterns (higher on weekdays for B2B, weekends for consumer)</p></li><li><p>Marketing campaign spikes (10x traffic for hours)</p></li><li><p>Viral moments (100x traffic, unpredictable duration)</p></li></ul><p>At this point, manually adding and removing servers is no longer viable. You need infrastructure that reacts automatically.</p><p>This stage focuses on  (automatically adjusting capacity) and ensuring your application is truly  (servers can be added or removed freely without data loss or user impact).</p><p>For auto-scaling to work, your application servers must be interchangeable. Any request can go to any server. Any server can be terminated without losing data. A new server can start handling requests immediately.</p><p>When a new server joins the cluster, it typically:</p><ol><li><p>Registers with the load balancer (or gets discovered)</p></li><li><p>Connects to Redis, database, and other shared services</p></li><li><p>Immediately starts handling requests</p></li></ol><p>When a server is removed:</p><ol><li><p>Load balancer stops sending new requests</p></li><li><p>In-flight requests complete (graceful shutdown)</p></li></ol><p>No data is lost, because nothing important is stored locally.</p><p>Auto-scaling adjusts capacity based on metrics. The scaling system continuously monitors metrics and adds or removes servers based on thresholds.</p><p>Most teams start with CPU-based scaling. It’s simple, works for most workloads, and is easy to reason about. Add queue-depth scaling for background job workers.</p><p>When configuring auto-scaling, you’ll set these parameters:</p><pre><code>Minimum instances: 2       # Always running, even at zero traffic\nMaximum instances: 20      # Cost ceiling and resource limit\nScale-up threshold: 70%    # CPU percentage to trigger scale-up\nScale-down threshold: 30%  # CPU percentage to trigger scale-down\nScale-up cooldown: 3 min   # Wait time after scaling up before next action\nScale-down cooldown: 10 min # Wait time after scaling down\nInstance warmup: 2 min     # Time for new instance to become fully operational</code></pre><p><strong>Important considerations:</strong></p><ul><li><p>: Should be at least 2 for redundancy. If one fails, the other handles traffic while a replacement spins up.</p></li><li><p>: Prevent thrashing (rapidly scaling up and down). Scale-down cooldown is typically longer because removing capacity is riskier than adding it.</p></li><li><p>: New servers need time to start, load code, warm up caches, establish database connections. Don’t count them toward capacity until they’re ready.</p></li><li><p>: Scale up aggressively (react quickly to load), scale down conservatively (don’t remove capacity too soon).</p></li></ul><h3>JWT for Stateless Authentication</h3><p>At this scale, many teams move from session-based to token-based authentication using JWTs (JSON Web Tokens). With session-based auth, every request requires a session store lookup. With JWTs, authentication state is contained in the token itself.</p><pre><code>Header.Payload.Signature\n\neyJhbGciOiJIUzI1NiJ9.eyJ1c2VyX2lkIjoxMjM0NTZ9.signature_here</code></pre><p>The payload contains claims like user ID, roles, and expiration. The signature ensures the token wasn’t tampered with. Any server can verify the signature using a shared secret key without querying a database.</p><ul><li><p>: Truly stateless, no session store lookup for every request</p></li><li><p>: Works across services (microservices, mobile apps, third-party APIs)</p></li><li><p>: Can’t invalidate individual tokens before expiry (user logs out, but token remains valid)</p></li><li><p>: Token size adds to each request (500 bytes vs 32-byte session ID)</p></li></ul><p>A common pattern is <strong>short-lived access tokens</strong> (for example, 15 minutes) plus <strong>long-lived refresh tokens</strong> (for example, 7 days). That limits how long a compromised or stale token can be used.</p><p>At this point, your application tier scales elastically. Traffic spikes and more servers spin up. Traffic drops and they spin down.</p><p>But a new ceiling is coming: the database can only handle so many writes, the monolith becomes harder to change safely, and some operations are too slow to run synchronously. That’s when you bring in the heavy machinery.</p><p>With 500K+ users, you’ll hit new ceilings that the previous optimizations can’t solve:</p><ul><li><p>Writes overwhelm a single primary database, even if reads are offloaded to replicas.</p></li><li><p>The monolith becomes painful to ship. A small change to notifications forces a full redeploy of the entire application.</p></li><li><p>Previously fast operations start taking seconds because too much work is happening synchronously in the request path.</p></li><li><p>Different parts of the product need different scaling profiles. Search and feeds may need 10x the capacity of profile pages.</p></li></ul><p>This is where the heavy machinery comes in: , , and  (message queues).</p><p>Read replicas solved read scaling, but writes all still go to one primary database. At high volume, this primary becomes the bottleneck. You’re limited by what one machine can handle in terms of:</p><ul><li><p>Write throughput (inserts, updates, deletes)</p></li><li><p>Storage capacity (even big disks have limits)</p></li><li><p>Connection count (even with pooling)</p></li></ul><p>Sharding splits your data across multiple databases based on a . Each shard holds a subset of the data and handles both reads and writes for that subset.</p><blockquote><p> is a popular improvement over simple hash-based sharding. Instead of , you place keys on a ring. When you add a new shard, only keys adjacent to its position move, not all keys. This means adding a fourth shard moves ~25% of data instead of ~75%.</p></blockquote><p>Sharding is a . Once you shard:</p><ul><li><p>Cross-shard queries become expensive or impossible (joining data across shards)</p></li><li><p>Transactions spanning shards are complex (two-phase commit or give up on atomicity)</p></li><li><p>Schema changes must be applied to all shards</p></li><li><p>Operations (backups, migrations) multiply by shard count</p></li><li><p>Application code becomes more complex (shard routing logic)</p></li></ul><p>Before sharding, exhaust these options:</p><ol><li><p>: Add missing indexes, rewrite slow queries, denormalize where helpful</p></li><li><p>: Upgrade to a bigger database server (more CPU, RAM, faster SSDs)</p></li><li><p>: If read-heavy, add replicas to handle reads</p></li><li><p>: Reduce load on database by caching frequently accessed data</p></li><li><p>: Move old data to cold storage (separate database, object storage)</p></li><li><p>: Reduce connection overhead</p></li></ol><p>Only shard when you’re truly write-bound and a single node physically cannot handle your throughput, or when your dataset exceeds what fits on one machine.</p><p>As the product and team grow, a monolith becomes harder to evolve safely. Common signals you might benefit from microservices:</p><ul><li><p>A change to one area (like notifications) requires redeploying the entire app.</p></li><li><p>Teams can’t ship independently without coordinating every release.</p></li><li><p>Different parts of the app have different scaling needs (search needs 10 servers, profile viewing needs 2)</p></li><li><p>Engineers frequently conflict in the same codebase.</p></li><li><p>A bug in one subsystem takes down the whole application.</p></li></ul><p>Microservices split the application into independent services that communicate over the network.</p><ul><li><p> (a database only it writes to directly)</p></li><li><p> (ship notifications without touching checkout)</p></li><li><p> (search can scale separately from profiles)</p></li><li><p><strong>Uses fit-for-purpose tech</strong> (search might use Elasticsearch, payments might need Postgres with strong consistency)</p></li><li><p><strong>Exposes a clear API contract</strong> (other services integrate via stable endpoints)</p></li></ul><p>The trade-off is a big jump in operational complexity. The safest approach is to start with : pick the service with the cleanest boundaries and the clearest independent scaling needs. Avoid splitting into dozens of services upfront.</p><h3>Message Queues and Async Processing</h3><p>Not everything needs to happen synchronously in the request path. When a user places an order, some steps must complete immediately, while others can happen in the background.</p><ul><li><p>Return order confirmation</p></li></ul><ul><li><p>Update analytics dashboard</p></li><li><p>Notify warehouse for fulfillment</p></li><li><p>Update recommendation engine</p></li><li><p>Sync to accounting system</p></li></ul><p>Message queues like , , or  decouple producers from consumers. The order service publishes an event like , and downstream systems consume it independently.</p><p><strong>Benefits of async processing:</strong></p><ul><li><p>: If email service is down, messages queue up. Order still completes. Email sends when service recovers.</p></li><li><p>: Consumers scale independently based on queue depth. Holiday rush? Add more warehouse notification processors without touching the orders service.</p></li><li><p>: The order service doesn’t need to know who consumes the event. You can add a new consumer (fraud detection, CRM sync) without changing the producer.</p></li><li><p>: Queues absorb spikes and let downstream systems process at a sustainable rate instead of getting overloaded.</p></li><li><p>: Failed messages can be retried automatically. Dead letter queues capture messages that fail repeatedly for investigation.</p></li></ul><p>A common real-world pattern is “do the write now, do the heavy work later.” </p><p>For example, in social apps, creating a post is usually a fast write and an immediate success response. Expensive work like fan-out, indexing, notifications, and feed updates happens asynchronously, which is why you sometimes see small delays in like counts or feed propagation.</p><p>At this point, your architecture can handle massive scale within a single region. But your users aren’t all in one place, and neither should your infrastructure be. </p><p>Once you have users across continents, latency becomes noticeable, and a single datacenter becomes a single point of failure for your entire global user base.</p><p>With millions of users worldwide, new challenges emerge:</p><ul><li><p>Users in Australia experience 300ms latency hitting US servers</p></li><li><p>A datacenter outage (fire, network partition, cloud provider issue) takes down your entire service</p></li><li><p>Your database schema can’t efficiently serve both write-heavy real-time updates and read-heavy analytics dashboards</p></li><li><p>Different regions have different data residency requirements (GDPR in EU, data localization laws)</p></li></ul><p>This stage covers , , and  like CQRS.</p><h3>Multi-Region Architecture</h3><p>Deploying to multiple geographic regions achieves two main goals:</p><ol><li><p>: Users connect to nearby servers. Tokyo users hit Tokyo servers (20ms) instead of US servers (200ms).</p></li><li><p>: If one region fails, others continue serving traffic. True high availability.</p></li></ol><p>There are two main approaches:</p><h4>Active-Passive (Primary-Secondary)</h4><p>One region (primary) handles all writes. Other regions serve reads and can take over if the primary fails.</p><ul><li><p>No write conflict resolution needed</p></li><li><p>Strong consistency for writes</p></li></ul><ul><li><p>Higher write latency for users far from primary</p></li><li><p>Failover isn’t instantaneous (DNS propagation, replica promotion)</p></li><li><p>Primary region is still a single point of failure</p></li></ul><p>All regions handle both reads and writes. This requires solving the hard problem: what happens when users in US and EU update the same record simultaneously?</p><ul><li><p>Lowest possible latency for all operations</p></li><li><p>True high availability, any region failure is seamless</p></li><li><p>No single point of failure</p></li></ul><ul><li><p>Conflict resolution is complex (and can cause data issues if done wrong)</p></li><li><p>Eventually consistent, not suitable for all data types</p></li><li><p>More complex to reason about and debug</p></li></ul><p>Most companies start with active-passive. Active-active requires solving distributed consensus problems and accepting eventual consistency.</p><h3>CAP Theorem at Global Scale</h3><p>The CAP theorem becomes very real at global scale. It states that a distributed system can only provide two of three guarantees:</p><ul><li><p>: Every read receives the most recent write</p></li><li><p>: Every request receives a response (not an error)</p></li><li><p>: System continues despite network partitions</p></li></ul><p>Since network partitions between regions are inevitable (undersea cables get cut, cloud providers have outages), you’re really choosing between consistency and availability during a partition.</p><p>Most global systems choose  for most operations:</p><ul><li><p>A user’s post might take 1-2 seconds to appear for followers in other regions</p></li><li><p>A product rating might show slightly different averages in different regions briefly</p></li><li><p>User profile updates might take a moment to propagate</p></li></ul><p>Only operations where inconsistency causes real problems (payments, inventory decrements, financial transactions) require strong consistency, and those might route to a primary region.</p><p>As systems grow, read and write patterns diverge significantly:</p><ul><li><p>Writes need transactions, validation, normalized data, audit logs</p></li><li><p>Reads need denormalized data, fast aggregations, full-text search</p></li><li><p>Write volume might be 1/100th of read volume</p></li></ul><p><strong>CQRS (Command Query Responsibility Segregation)</strong> separates these concerns entirely.</p><p>The write side uses a normalized schema optimized for data integrity and transactional guarantees. The read side uses denormalized views optimized for query performance. Events synchronize the two.</p><p>Real-world example: Twitter’s timeline architecture.</p><ul><li><p>: When you tweet, it’s written to a normalized tweets table with proper indexing, constraints, and transactions.</p></li><li><p>: A “tweet created” event fires.</p></li><li><p>: A fan-out service reads the event and adds the tweet to each follower’s timeline (a denormalized, per-user data structure optimized for “show me my feed” queries).</p></li><li><p>: When you open Twitter, you read from your pre-computed timeline, not a complex query joining tweets, follows, and users.</p></li></ul><p>CQRS adds complexity but enables:</p><ul><li><p>Independent scaling of read and write paths</p></li><li><p>Optimized schemas for each access pattern</p></li><li><p>Different technology choices (PostgreSQL for writes, Elasticsearch for reads)</p></li><li><p>Better performance for both operations</p></li></ul><h3>Advanced Caching Patterns</h3><p>At global scale, caching becomes more sophisticated:</p><p>When a new cache server starts (or cache expires after maintenance), the first requests face cache misses, causing latency spikes and origin load. Cache warming pre-populates caches before traffic arrives:</p><ul><li><p>: Load popular items into cache during startup, before receiving traffic</p></li><li><p>: Before a marketing push, warm caches with products/pages likely to be accessed</p></li><li><p>: When adding a new cache node, copy state from existing nodes</p></li></ul><blockquote><p>Netflix pre-warms edge caches with popular content before peak hours. When evening viewing starts, the most-watched shows are already cached at edge locations.</p></blockquote><h4>Write-Behind (Write-Back) Caching</h4><p>For write-heavy workloads, write to cache first and asynchronously persist to database:</p><ol><li><p>Write goes to cache (immediate return to user)</p></li><li><p>Background process flushes writes to database periodically</p></li></ol><p>This reduces write latency dramatically but introduces risk: if the cache fails before flushing, writes are lost. Use only when:</p><ul><li><p>Some data loss is acceptable (analytics counters, view counts)</p></li><li><p>Cache is highly available (Redis with replication and persistence)</p></li><li><p>Durability can be sacrificed for performance</p></li></ul><p>You’ve now built a globally distributed system that handles millions of users with low latency worldwide. But the journey doesn’t end here. At truly massive scale, even the best off-the-shelf solutions start showing their limits.</p><p>At 10 million users and beyond, you enter territory where off-the-shelf solutions don’t always work. Companies at this scale often build custom infrastructure tailored to their specific access patterns. The problems become unique to your workload.</p><p>No single database handles all access patterns well. The concept of “polyglot persistence” means using different databases for different use cases:</p><p>Each database is optimized for specific access patterns. Using PostgreSQL for time-series data works but is inefficient. Using Elasticsearch for transactions is possible but dangerous.</p><h3>Custom Solutions at Scale</h3><p>At extreme scale, some companies build custom infrastructure because their requirements go beyond what general-purpose systems can deliver:</p><ul><li><p> A custom data system for the social graph, built to meet Facebook’s latency and throughput needs at massive scale when off-the-shelf options couldn’t.</p></li><li><p> A globally distributed SQL database designed to provide strong consistency across regions, combining properties that were hard to get together at the time.</p></li><li><p> A large-scale caching layer built on Memcached, with additional replication, reliability, and operational tooling to support Netflix’s traffic patterns.</p></li><li><p><strong>Discord’s storage journey:</strong> MongoDB (2015) → Cassandra (2017) → ScyllaDB (2022). Each move was driven by the limits of the previous choice, and Discord has shared detailed write-ups on the trade-offs behind those migrations.</p></li><li><p> A MySQL-based storage layer designed to keep transactional semantics while scaling beyond a single MySQL setup, with operational simplicity for teams.</p></li></ul><p>These aren’t options you’ll reach for initially, but they illustrate that scaling is an ongoing journey, not a destination. The architecture that works at 1 million users is rarely the one you’ll want at 100 million.</p><p>The next frontier is pushing computation closer to users. Instead of all logic running in centralized data centers, edge computing runs code at CDN edge locations worldwide:</p><ul><li><p>: JavaScript/WASM at 250+ edge locations</p></li><li><p>: Lambda functions at CloudFront edge</p></li><li><p>: Compute at Fastly’s edge network</p></li><li><p>: Globally distributed JavaScript runtime</p></li></ul><p>Edge computing represents a fundamental shift: instead of “request → CDN → origin → CDN → response”, many requests become “request → edge → response” with the edge having enough compute capability to handle the logic.</p><p>Now that we’ve covered the full progression from a single server to global-scale infrastructure, an important question remains: how do you know when to take each step? Scaling too early wastes resources; scaling too late causes outages.</p><p>Scaling a system from zero to millions of users follows a predictable progression. Each stage solves problems that emerge at specific thresholds:</p><h3>Key Principles to Remember</h3><ol><li><p>: Don’t optimize for problems you don’t have yet. A single server is fine until it isn’t.</p></li><li><p>: Identify the actual bottleneck before adding infrastructure. CPU-bound problems need different solutions than I/O-bound ones.</p></li><li><p><strong>Stateless servers are the prerequisite</strong>: You can’t horizontally scale or auto-scale until your servers hold no local state.</p></li><li><p>: Most data is read far more often than written. Caching gives you 10-100x performance improvement for read-heavy workloads.</p></li><li><p>: Not everything needs to happen in the request path. Email sending, analytics, notifications can all be queued.</p></li><li><p>: Database sharding is a one-way door with significant complexity. Exhaust other options first.</p></li><li><p>: Perfect consistency and availability don’t coexist during network partitions. Know which operations truly need strong consistency.</p></li><li><p>: Every component you add is a component that can fail, needs monitoring, requires expertise to operate.</p></li></ol><p>The path to scale isn’t about implementing everything at once. It’s about understanding which problems emerge at each stage and applying the right solutions at the right time.</p><p>The best architecture is the simplest one that meets your current needs, with a clear path to evolve when those needs change.</p><p>That’s it. Thank you so much for reading!</p><p>If you found this article helpful, give it a like ❤️ and share it with others.</p><p>For more System Design related content, checkout my website <a href=\"https://algomaster.io/\">algomaster.io</a></p>","contentLength":34646,"flags":null,"enclosureUrl":"https://substackcdn.com/image/fetch/$s_!2Tb9!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F329f47dd-6c66-4c66-b973-487ef28e77de_725x511.png","enclosureMime":"","commentsUrl":null},{"title":"DSA was HARD until I Learned these 20 Patterns","url":"https://blog.algomaster.io/p/20-dsa-patterns","date":1768884889,"author":"Ashish Pratap Singh","guid":441,"unread":true,"content":"<p>I’ve solved over 2,000 coding problems acrossLeetCodeand other platformsandif there’s one thing I’ve learned, it’s this:</p><blockquote><p>Getting good at DSA is  about how many problems you solve and  about how many  you truly understand.</p></blockquote><p>Patterns help you solve a wide range of problems in less time because they train you to recognize the right approach, even for questions you have never seen before.</p><p>In this article, I’ll walk you through the <strong>20 most important patterns</strong> that made learning DSA and preparing for coding interviews far less painful for me.</p><p>Even better, these are the same patterns that showed up again and again in my interviews, including at companies like  and .</p><p>For each pattern, I’ll share:</p><ul><li><p>A sample problem walkthrough</p></li><li><p>Practice links to related LeetCode problems</p></li></ul><p>This article is the essence of everything I have learned about DSA and LeetCode, distilled into one guide. I’ve also added links for each pattern so you can dive deeper when you’re ready.</p><p>If you want to explore more patterns and resources, check out: <a href=\"https://algomaster.io/\">algomaster.io</a></p><p>The Prefix Sum pattern involves  an array to create a new array where each element at index  represents the sum of all elements from the start up to . This allows for  on any subarray.</p><ul><li><p>Multiple sum queries on subarrays</p></li><li><p>Finding subarrays with a target sum</p></li><li><p>Calculating cumulative totals</p></li></ul><pre><code>// Build prefix sum array\nint[] prefix = new int[n + 1];\nfor (int i = 0; i &lt; n; i++) {\n    prefix[i + 1] = prefix[i] + nums[i];\n}\n\n// Query sum of range [left, right]\nint rangeSum = prefix[right + 1] - prefix[left];</code></pre><p>: Given an array , answer multiple queries about the sum of elements within a specific range .</p><ul><li><p><code>nums = [1, 2, 3, 4, 5, 6]</code>, , </p></li></ul><h4>Step-by-Step Walkthrough:</h4><pre><code>nums = [1, 2, 3, 4, 5, 6]\n\nStep 1: Build prefix sum array\n  prefix[0] = 0\n  prefix[1] = 0 + 1 = 1\n  prefix[2] = 1 + 2 = 3\n  prefix[3] = 3 + 3 = 6\n  prefix[4] = 6 + 4 = 10\n  prefix[5] = 10 + 5 = 15\n  prefix[6] = 15 + 6 = 21\n\n  prefix = [0, 1, 3, 6, 10, 15, 21]\n\nStep 2: Query sum for range [1, 3]\n  sum = prefix[3 + 1] - prefix[1]\n  sum = prefix[4] - prefix[1]\n  sum = 10 - 1 = 9</code></pre><p>The Two Pointers pattern uses two pointers to traverse an array or list, typically from opposite ends or both moving in the same direction. It reduces time complexity from  for many problems.</p><ul><li><p>Finding pairs in sorted arrays</p></li><li><p>Comparing elements from both ends</p></li></ul><pre><code>// Opposite direction (converging)\nint left = 0, right = n - 1;\nwhile (left &lt; right) {\n    if (condition_met) {\n        // found answer\n    } else if (need_larger_sum) {\n        left++;\n    } else {\n        right--;\n    }\n}\n\n// Same direction\nint slow = 0;\nfor (int fast = 0; fast &lt; n; fast++) {\n    if (condition) {\n        // process and move slow\n        slow++;\n    }\n}</code></pre><p>: Find two numbers in a sorted array that add up to a target value. Return their indices.</p><ul><li><p>, </p></li><li><p> (indices of 2 and 4)</p></li></ul><h4>Step-by-Step Walkthrough:</h4><pre><code>nums = [1, 2, 3, 4, 6], target = 6\n\nStep 1: left = 0, right = 4\n  sum = nums[0] + nums[4] = 1 + 6 = 7 &gt; 6\n  Move right pointer left: right = 3\n\nStep 2: left = 0, right = 3\n  sum = nums[0] + nums[3] = 1 + 4 = 5 &lt; 6\n  Move left pointer right: left = 1\n\nStep 3: left = 1, right = 3\n  sum = nums[1] + nums[3] = 2 + 4 = 6 == target\n  Found! Return [1, 3]</code></pre><p>The Sliding Window pattern maintains a window of elements and slides it across the array to find subarrays or substrings that satisfy certain conditions. It avoids recalculating overlapping parts of consecutive windows.</p><ul><li><p>Contiguous subarray/substring problems</p></li><li><p>Finding maximum/minimum in window of size k</p></li><li><p>Longest/shortest substring with certain properties</p></li><li><p>Problems involving consecutive elements</p></li></ul><pre><code>// Fixed-size window\nint windowSum = 0;\nfor (int i = 0; i &lt; n; i++) {\n    windowSum += nums[i];\n    if (i &gt;= k - 1) {\n        // process window\n        result = Math.max(result, windowSum);\n        windowSum -= nums[i - k + 1];\n    }\n}\n\n// Variable-size window\nint left = 0;\nfor (int right = 0; right &lt; n; right++) {\n    // expand window by including nums[right]\n\n    while (window_condition_violated) {\n        // shrink window from left\n        left++;\n    }\n\n    // update result\n}</code></pre><p><strong>Maximum Sum Subarray of Size K</strong>: Find the maximum sum of any contiguous subarray of size .</p><ul><li><p><code>nums = [2, 1, 5, 1, 3, 2]</code>, </p></li></ul><h4>Step-by-Step Walkthrough:</h4><pre><code>nums = [2, 1, 5, 1, 3, 2], k = 3\n\nStep 1: Build initial window [2, 1, 5]\n  windowSum = 2 + 1 + 5 = 8\n  maxSum = 8\n\nStep 2: Slide window to [1, 5, 1]\n  windowSum = 8 - 2 + 1 = 7\n  maxSum = max(8, 7) = 8\n\nStep 3: Slide window to [5, 1, 3]\n  windowSum = 7 - 1 + 3 = 9\n  maxSum = max(8, 9) = 9\n\nStep 4: Slide window to [1, 3, 2]\n  windowSum = 9 - 5 + 2 = 6\n  maxSum = max(9, 6) = 9\n\nResult: 9</code></pre><p>The Fast &amp; Slow Pointers pattern (also called Tortoise and Hare) uses two pointers moving at different speeds. When there is a cycle, the fast pointer will eventually meet the slow pointer.</p><ul><li><p>Detecting cycles in linked lists or arrays</p></li><li><p>Finding the middle of a linked list</p></li><li><p>Finding the start of a cycle</p></li></ul><pre><code><code>// Find middle\nwhile (fast != null &amp;&amp; fast.next != null) {\n    slow = slow.next;\n    fast = fast.next.next;\n}\nreturn slow; // middle node\n\n// Cycle detection\nListNode slow = head, fast = head;\nwhile (fast != null &amp;&amp; fast.next != null) {\n    slow = slow.next;\n    fast = fast.next.next;\n\n    if (slow == fast) {\n        // cycle detected\n        return true;\n    }\n}\nreturn false; // no cycle</code></code></pre><p>: Detect if a linked list has a cycle.</p><ul><li><p>, tail connects to node index 1</p></li></ul><h4>Step-by-Step Walkthrough:</h4><pre><code><code>List: 3 -&gt; 2 -&gt; 0 -&gt; -4 -&gt; (back to 2)\n\nStep 1: slow = 3, fast = 3\nStep 2: slow = 2, fast = 0\nStep 3: slow = 0, fast = 2 (wrapped around)\nStep 4: slow = -4, fast = -4\n\nslow == fast, cycle detected!</code></code></pre><p>This pattern reverses parts of a linked list without using extra space. It manipulates pointers to reverse the direction of links.</p><ul><li><p>Reversing a linked list or portion of it</p></li><li><p>Reversing nodes in groups</p></li><li><p>Checking for palindromes in linked lists</p></li></ul><pre><code>// Reverse entire list\nListNode prev = null, curr = head;\nwhile (curr != null) {\n    ListNode next = curr.next;\n    curr.next = prev;\n    prev = curr;\n    curr = next;\n}\nreturn prev; // new head</code></pre><p>: Reverse the nodes of a linked list from position  to .</p><ul><li><p>, , </p></li></ul><h4>Step-by-Step Walkthrough:</h4><pre><code>Original: 1 -&gt; 2 -&gt; 3 -&gt; 4 -&gt; 5\nReverse positions 2 to 4\n\nStep 1: Position prev before node 2\n  prev = 1, curr = 2\n\nStep 2: Move 3 after 1\n  1 -&gt; 3 -&gt; 2 -&gt; 4 -&gt; 5\n\nStep 3: Move 4 after 1\n  1 -&gt; 4 -&gt; 3 -&gt; 2 -&gt; 5\n\nResult: [1, 4, 3, 2, 5]</code></pre><p>The Frequency Counting pattern uses hash maps or arrays to count occurrences of elements. It transforms O(n^2) lookup problems into O(n) by trading space for time.</p><ul><li><p>Finding duplicates or unique elements</p></li><li><p>Checking if two collections have same elements</p></li><li><p>Finding elements that appear k times</p></li></ul><pre><code><code>// Using HashMap\nMap&lt;Integer, Integer&gt; freq = new HashMap&lt;&gt;();\nfor (int num : nums) {\n    freq.put(num, freq.getOrDefault(num, 0) + 1);\n}\n\n// Using array (when range is known)\nint[] freq = new int[26]; // for lowercase letters\nfor (char c : str.toCharArray()) {\n    freq[c - 'a']++;\n}\n\n// Finding element with specific frequency\nfor (Map.Entry&lt;Integer, Integer&gt; entry : freq.entrySet()) {\n    if (entry.getValue() == target) {\n        return entry.getKey();\n    }\n}</code></code></pre><p>: Given two strings  and , return true if  is an anagram of .</p><ul><li><p>, </p></li></ul><h4>Step-by-Step Walkthrough:</h4><pre><code><code>s = \"anagram\", t = \"nagaram\"\n\nStep 1: Count frequencies in s\n  freq = {a: 3, n: 1, g: 1, r: 1, m: 1}\n\nStep 2: Decrement frequencies using t\n  't' -&gt; n: freq[n] = 1 - 1 = 0\n  't' -&gt; a: freq[a] = 3 - 1 = 2\n  't' -&gt; g: freq[g] = 1 - 1 = 0\n  't' -&gt; a: freq[a] = 2 - 1 = 1\n  't' -&gt; r: freq[r] = 1 - 1 = 0\n  't' -&gt; a: freq[a] = 1 - 1 = 0\n  't' -&gt; m: freq[m] = 1 - 1 = 0\n\nStep 3: Check all frequencies are 0\n  freq = {a: 0, n: 0, g: 0, r: 0, m: 0}\n  All zero -&gt; Return true</code></code></pre><p>A Monotonic Stack maintains elements in either increasing or decreasing order. As you iterate, you pop elements that violate the order, which reveals relationships between elements.</p><ul><li><p>Finding the next greater/smaller element</p></li><li><p>Finding previous greater/smaller element</p></li><li><p>Problems involving spans or ranges</p></li></ul><pre><code>/// Next Greater Element (decreasing stack)\nint[] result = new int[n];\nArrays.fill(result, -1);\nStack&lt;Integer&gt; stack = new Stack&lt;&gt;(); // stores indices\n\nfor (int i = 0; i &lt; n; i++) {\n    while (!stack.isEmpty() &amp;&amp; nums[i] &gt; nums[stack.peek()]) {\n        int idx = stack.pop();\n        result[idx] = nums[i];\n    }\n    stack.push(i);\n}</code></pre><p>: For each element in an array, find the next greater element. Output -1 if none exists.</p><ul></ul><h4>Step-by-Step Walkthrough:</h4><pre><code>nums = [2, 1, 2, 4, 3]\nresult = [-1, -1, -1, -1, -1]\nstack = []\n\nStep 1: i = 0, nums[0] = 2\n  stack is empty, push 0\n  stack = [0]\n\nStep 2: i = 1, nums[1] = 1\n  1 &lt; nums[0]=2, push 1\n  stack = [0, 1]\n\nStep 3: i = 2, nums[2] = 2\n  2 &gt; nums[1]=1, pop 1, result[1] = 2\n  2 &lt;= nums[0]=2, push 2\n  stack = [0, 2], result = [-1, 2, -1, -1, -1]\n\nStep 4: i = 3, nums[3] = 4\n  4 &gt; nums[2]=2, pop 2, result[2] = 4\n  4 &gt; nums[0]=2, pop 0, result[0] = 4\n  push 3\n  stack = [3], result = [4, 2, 4, -1, -1]\n\nStep 5: i = 4, nums[4] = 3\n  3 &lt; nums[3]=4, push 4\n  stack = [3, 4]\n\nResult: [4, 2, 4, -1, -1]</code></pre><p>Bit manipulation uses binary operations (AND, OR, XOR, NOT, shifts) to solve problems efficiently. XOR is particularly useful since a ^ a = 0 and a ^ 0 = a.</p><ul><li><p>Finding unique numbers (XOR)</p></li><li><p>Generating subsets using bit masks</p></li></ul><pre><code>// Basic operations\nint and = a &amp; b;        // both bits 1\nint or = a | b;         // either bit 1\nint xor = a ^ b;        // bits different\nint not = ~a;           // flip bits\nint leftShift = a &lt;&lt; n; // multiply by 2^n\nint rightShift = a &gt;&gt; n;// divide by 2^n\n\n// Check if bit i is set\nboolean isSet = (n &amp; (1 &lt;&lt; i)) != 0;\n\n// Set bit i\nn = n | (1 &lt;&lt; i);\n\n// Clear bit i\nn = n &amp; ~(1 &lt;&lt; i);\n\n// Toggle bit i\nn = n ^ (1 &lt;&lt; i);\n\n// Check if power of 2\nboolean isPowerOf2 = (n &gt; 0) &amp;&amp; ((n &amp; (n - 1)) == 0);\n\n// Count set bits\nint count = 0;\nwhile (n &gt; 0) {\n    count += n &amp; 1;\n    n &gt;&gt;= 1;\n}\n// Or: Integer.bitCount(n);\n\n// Find single number (XOR all elements)\nint single = 0;\nfor (int num : nums) {\n    single ^= num;\n}</code></pre><p>This pattern finds the k largest or smallest elements using heaps (priority queues). A min-heap of size k keeps track of k largest elements, and a max-heap keeps k smallest.</p><ul><li><p>Finding k largest/smallest elements</p></li><li><p>Finding kth largest/smallest element</p></li><li><p>Finding k most/least frequent elements</p></li></ul><pre><code>// K largest elements using min-heap\nPriorityQueue&lt;Integer&gt; minHeap = new PriorityQueue&lt;&gt;();\n\nfor (int num : nums) {\n    minHeap.offer(num);\n    if (minHeap.size() &gt; k) {\n        minHeap.poll(); // remove smallest\n    }\n}\n// minHeap now contains k largest elements\n// minHeap.peek() is the kth largest</code></pre><p>: Find the kth largest element in an unsorted array.</p><ul><li><p><code>nums = [3, 2, 1, 5, 6, 4]</code>, </p></li></ul><h4>Step-by-Step Walkthrough:</h4><pre><code>nums = [3, 2, 1, 5, 6, 4], k = 2\nUsing min-heap of size k\n\nStep 1: num = 3\n  heap = [3]\n\nStep 2: num = 2\n  heap = [2, 3]\n\nStep 3: num = 1\n  heap = [1, 3, 2], size &gt; k\n  poll smallest: heap = [2, 3]\n\nStep 4: num = 5\n  heap = [2, 3, 5], size &gt; k\n  poll smallest: heap = [3, 5]\n\nStep 5: num = 6\n  heap = [3, 5, 6], size &gt; k\n  poll smallest: heap = [5, 6]\n\nStep 6: num = 4\n  heap = [4, 6, 5], size &gt; k\n  poll smallest: heap = [5, 6]\n\nheap.peek() = 5 (kth largest)</code></pre><p>This pattern handles problems involving intervals that may overlap. The key insight is that after sorting by start time, two intervals  and  overlap if .</p><ul><li><p>Merging overlapping intervals</p></li><li><p>Finding interval intersections</p></li><li><p>Scheduling problems (meeting rooms)</p></li><li><p>Inserting into sorted intervals</p></li></ul><pre><code>// Sort by start time\nArrays.sort(intervals, (a, b) -&gt; a[0] - b[0]);\n\n// Merge overlapping intervals\nList&lt;int[]&gt; merged = new ArrayList&lt;&gt;();\nfor (int[] interval : intervals) {\n    if (merged.isEmpty() || merged.get(merged.size() - 1)[1] &lt; interval[0]) {\n        // no overlap, add new interval\n        merged.add(interval);\n    } else {\n        // overlap, merge by extending end time\n        merged.get(merged.size() - 1)[1] =\n            Math.max(merged.get(merged.size() - 1)[1], interval[1]);\n    }\n}</code></pre><p>: Given a collection of intervals, merge all overlapping intervals.</p><ul><li><p><code>intervals = [[1,3], [2,6], [8,10], [15,18]]</code></p></li></ul><h4>Step-by-Step Walkthrough:</h4><pre><code>intervals = [[1,3], [2,6], [8,10], [15,18]]\n(Already sorted by start time)\n\nStep 1: interval = [1,3]\n  merged is empty, add [1,3]\n  merged = [[1,3]]\n\nStep 2: interval = [2,6]\n  last = [1,3], start = 2 &lt;= end = 3\n  Overlap! Merge: end = max(3, 6) = 6\n  merged = [[1,6]]\n\nStep 3: interval = [8,10]\n  last = [1,6], start = 8 &gt; end = 6\n  No overlap, add [8,10]\n  merged = [[1,6], [8,10]]\n\nStep 4: interval = [15,18]\n  last = [8,10], start = 15 &gt; end = 10\n  No overlap, add [15,18]\n  merged = [[1,6], [8,10], [15,18]]</code></pre><p>This pattern adapts binary search to handle rotated arrays, finding boundaries, or searching for conditions rather than exact values.</p><ul><li><p>Searching in rotated sorted arrays</p></li><li><p>Finding first/last occurrence of element</p></li><li><p>Finding minimum/maximum satisfying a condition</p></li></ul><pre><code>// Standard binary search\nint left = 0, right = n - 1;\nwhile (left &lt;= right) {\n    int mid = left + (right - left) / 2;\n    if (nums[mid] == target) return mid;\n    else if (nums[mid] &lt; target) left = mid + 1;\n    else right = mid - 1;\n}\n\n// Find first occurrence\nwhile (left &lt; right) {\n    int mid = left + (right - left) / 2;\n    if (condition(mid)) right = mid;\n    else left = mid + 1;\n}</code></pre><p><strong>Search in Rotated Sorted Array</strong>: Search for a target in a rotated sorted array.</p><ul><li><p><code>nums = [4, 5, 6, 7, 0, 1, 2]</code>, </p></li></ul><h4>Step-by-Step Walkthrough:</h4><pre><code>nums = [4, 5, 6, 7, 0, 1, 2], target = 0\n\nStep 1: left = 0, right = 6, mid = 3\n  nums[mid] = 7 != 0\n  nums[left] = 4 &lt;= nums[mid] = 7 (left half sorted)\n  target = 0 not in [4, 7), search right\n  left = 4\n\nStep 2: left = 4, right = 6, mid = 5\n  nums[mid] = 1 != 0\n  nums[left] = 0 &lt;= nums[mid] = 1 (left half sorted)\n  target = 0 in [0, 1), search left\n  right = 4\n\nStep 3: left = 4, right = 4, mid = 4\n  nums[mid] = 0 == target\n  Found at index 4!</code></pre><p>Binary Tree Traversal visits all nodes in a specific order. The three main orders are preorder (root-left-right), inorder (left-root-right), and postorder (left-right-root).</p><ul><li><p>Processing tree nodes in specific order</p></li><li><p>Building trees from traversals</p></li><li><p>BST operations (inorder gives sorted order)</p></li><li><p>Tree serialization/deserialization</p></li></ul><pre><code>// Preorder (Root -&gt; Left -&gt; Right)\nvoid preorder(TreeNode node) {\n    if (node == null) return;\n    process(node);        // visit root\n    preorder(node.left);  // left subtree\n    preorder(node.right); // right subtree\n}\n\n// Inorder (Left -&gt; Root -&gt; Right)\nvoid inorder(TreeNode node) {\n    if (node == null) return;\n    inorder(node.left);   // left subtree\n    process(node);        // visit root\n    inorder(node.right);  // right subtree\n}\n\n// Postorder (Left -&gt; Right -&gt; Root)\nvoid postorder(TreeNode node) {\n    if (node == null) return;\n    postorder(node.left);  // left subtree\n    postorder(node.right); // right subtree\n    process(node);         // visit root\n}</code></pre><p>: Return the inorder traversal of a binary tree.</p><ul><li><p> Tree with root = </p></li></ul><h4>Step-by-Step Walkthrough:</h4><pre><code>Tree:\n    1\n     \\\n      2\n     /\n    3\n\nInorder: Left -&gt; Root -&gt; Right\n\nStep 1: Start at 1\n  No left child, visit 1\n  result = [1]\n\nStep 2: Go to right child 2\n  Has left child 3, go left\n\nStep 3: At node 3\n  No left child, visit 3\n  result = [1, 3]\n  No right child, backtrack\n\nStep 4: Back at node 2\n  Left done, visit 2\n  result = [1, 3, 2]\n  No right child\n\nDone: [1, 3, 2]</code></pre><p>DFS explores as deep as possible along each branch before backtracking. It uses a stack (or recursion) to remember which nodes to visit next.</p><ul><li><p>Exploring all paths in a tree/graph</p></li><li><p>Finding connected components</p></li><li><p>Path finding when all paths matter</p></li></ul><pre><code>// DFS on a graph - visited tracking required\nvoid dfs(int node, boolean[] visited, List&lt;List&lt;Integer&gt;&gt; graph) {\n    visited[node] = true;\n\n    // Process current node\n    process(node);\n\n    // Explore unvisited neighbors\n    for (int neighbor : graph.get(node)) {\n        if (!visited[neighbor]) {\n            dfs(neighbor, visited, graph);\n        }\n    }\n}</code></pre><p>: Determine if the tree has a root-to-leaf path where values sum to a target.</p><ul><li><p><code>root = [5, 4, 8, 11, null, 13, 4, 7, 2]</code>, </p></li><li><p> (path: 5 -&gt; 4 -&gt; 11 -&gt; 2)</p></li></ul><h4>Step-by-Step Walkthrough:</h4><pre><code>Tree:\n        5\n       / \\\n      4   8\n     /   / \\\n    11  13  4\n   /  \\      \\\n  7    2      1\n\nTarget = 22\n\nDFS Path 1: 5 -&gt; 4 -&gt; 11 -&gt; 7\n  Sum = 5 + 4 + 11 + 7 = 27 != 22\n\nDFS Path 2: 5 -&gt; 4 -&gt; 11 -&gt; 2\n  Sum = 5 + 4 + 11 + 2 = 22 == target\n  Found! Return true</code></pre><p>BFS explores nodes level by level, visiting all neighbors before moving deeper. It uses a queue and guarantees the shortest path in unweighted graphs.</p><ul><li><p>Finding shortest path (unweighted)</p></li><li><p>Finding all nodes at distance k</p></li><li><p>Spreading problems (rotting oranges, walls and gates)</p></li></ul><pre><code>public void bfs(Node start) {\n    // Handle edge case\n    if (start == null) return;\n\n    // Initialize queue and visited set\n    Queue&lt;Node&gt; queue = new LinkedList&lt;&gt;();\n    Set&lt;Node&gt; visited = new HashSet&lt;&gt;();\n\n    // Add starting node\n    queue.offer(start);\n    visited.add(start);\n\n    // Process nodes level by level\n    while (!queue.isEmpty()) {\n        Node current = queue.poll();\n\n        // Process the current node\n        process(current);\n\n        // Add unvisited neighbors to queue\n        for (Node neighbor : current.getNeighbors()) {\n            if (!visited.contains(neighbor)) {\n                visited.add(neighbor);\n                queue.offer(neighbor);\n            }\n        }\n    }\n}</code></pre><p>: Return the level order traversal of a binary tree.</p><ul><li><p><code>root = [3, 9, 20, null, null, 15, 7]</code></p></li></ul><h4>Step-by-Step Walkthrough:</h4><pre><code>Tree:\n    3\n   / \\\n  9  20\n    /  \\\n   15   7\n\nStep 1: Level 0\n  queue = [3]\n  Process 3, add children 9, 20\n  result = [[3]]\n\nStep 2: Level 1\n  queue = [9, 20]\n  Process 9 (no children)\n  Process 20, add children 15, 7\n  result = [[3], [9, 20]]\n\nStep 3: Level 2\n  queue = [15, 7]\n  Process 15 (no children)\n  Process 7 (no children)\n  result = [[3], [9, 20], [15, 7]]</code></pre><p>Shortest path algorithms find the minimum distance between nodes. Dijkstra’s works for weighted graphs with non-negative weights, while Bellman-Ford handles negative weights.</p><ul><li><p>Finding minimum cost/distance paths</p></li><li><p>Problems with varying edge costs</p></li></ul><pre><code>// Dijkstra's Algorithm using Priority Queue\nint[] dist = new int[n];\nArrays.fill(dist, Integer.MAX_VALUE);\ndist[source] = 0;\n\n// PQ: (distance, node)\nPriorityQueue&lt;int[]&gt; pq = new PriorityQueue&lt;&gt;((a, b) -&gt; a[0] - b[0]);\npq.offer(new int[]{0, source});\n\nwhile (!pq.isEmpty()) {\n    int[] curr = pq.poll();\n    int d = curr[0], node = curr[1];\n\n    if (d &gt; dist[node]) continue; // already processed\n\n    for (int[] edge : graph.get(node)) {\n        int neighbor = edge[0], weight = edge[1];\n        int newDist = dist[node] + weight;\n\n        if (newDist &lt; dist[neighbor]) {\n            dist[neighbor] = newDist;\n            pq.offer(new int[]{newDist, neighbor});\n        }\n    }\n}</code></pre><p>: Find the time it takes for a signal to reach all nodes from a source node.</p><ul><li><p><code>times = [[2,1,1], [2,3,1], [3,4,1]]</code>, , </p></li></ul><h4>Step-by-Step Walkthrough:</h4><pre><code>Graph: 2 --(1)--&gt; 1\n       2 --(1)--&gt; 3 --(1)--&gt; 4\nSource: node 2\n\nInitial: dist = [INF, INF, 0, INF, INF] (1-indexed)\n         pq = [(0, 2)]\n\nStep 1: Process (0, 2)\n  Neighbor 1: dist[1] = 0 + 1 = 1\n  Neighbor 3: dist[3] = 0 + 1 = 1\n  pq = [(1, 1), (1, 3)]\n\nStep 2: Process (1, 1)\n  No outgoing edges\n  pq = [(1, 3)]\n\nStep 3: Process (1, 3)\n  Neighbor 4: dist[4] = 1 + 1 = 2\n  pq = [(2, 4)]\n\nStep 4: Process (2, 4)\n  No outgoing edges\n\ndist = [INF, 1, 0, 1, 2]\nMax distance = 2</code></pre><p>Matrix traversal uses DFS or BFS to explore 2D grids. The key is handling 4-directional (or 8-directional) movement and boundary checks.</p><ul><li><p>Grid-based problems (islands, regions)</p></li><li><p>Finding connected components in 2D</p></li></ul><pre><code>// Direction arrays for 4 directions\nint[] dx = {0, 0, 1, -1};\nint[] dy = {1, -1, 0, 0};\n\n// DFS on matrix\nvoid dfs(int[][] grid, int i, int j, boolean[][] visited) {\n    int m = grid.length, n = grid[0].length;\n\n    // boundary and validity check\n    if (i &lt; 0 || i &gt;= m || j &lt; 0 || j &gt;= n) return;\n    if (visited[i][j] || grid[i][j] == 0) return;\n\n    visited[i][j] = true;\n    // process cell\n\n    // explore 4 directions\n    for (int d = 0; d &lt; 4; d++) {\n        dfs(grid, i + dx[d], j + dy[d], visited);\n    }\n}\n\n// BFS on matrix\nQueue&lt;int[]&gt; queue = new LinkedList&lt;&gt;();\nqueue.offer(new int[]{startRow, startCol});\nvisited[startRow][startCol] = true;\n\nwhile (!queue.isEmpty()) {\n    int[] cell = queue.poll();\n    int i = cell[0], j = cell[1];\n\n    for (int d = 0; d &lt; 4; d++) {\n        int ni = i + dx[d], nj = j + dy[d];\n        if (ni &gt;= 0 &amp;&amp; ni &lt; m &amp;&amp; nj &gt;= 0 &amp;&amp; nj &lt; n\n            &amp;&amp; !visited[ni][nj] &amp;&amp; grid[ni][nj] == 1) {\n            visited[ni][nj] = true;\n            queue.offer(new int[]{ni, nj});\n        }\n    }\n}</code></pre><p>: Count the number of islands (connected 1s) in a 2D grid.</p><pre><code>grid = [\n  [\"1\",\"1\",\"0\",\"0\",\"0\"],\n  [\"1\",\"1\",\"0\",\"0\",\"0\"],\n  [\"0\",\"0\",\"1\",\"0\",\"0\"],\n  [\"0\",\"0\",\"0\",\"1\",\"1\"]\n]</code></pre><h4>Step-by-Step Walkthrough:</h4><pre><code>Grid:\n1 1 0 0 0\n1 1 0 0 0\n0 0 1 0 0\n0 0 0 1 1\n\nStep 1: Find '1' at (0,0)\n  DFS marks connected cells: (0,0), (0,1), (1,0), (1,1)\n  islands = 1\n\nStep 2: Find '1' at (2,2)\n  DFS marks: (2,2)\n  islands = 2\n\nStep 3: Find '1' at (3,3)\n  DFS marks connected cells: (3,3), (3,4)\n  islands = 3\n\nResult: 3 islands</code></pre><p>Backtracking explores all possible solutions by making choices, and undoes (backtracks) when a path leads to an invalid solution. It builds solutions incrementally.</p><ul><li><p>Generating all permutations/combinations/subsets</p></li><li><p>Solving constraint satisfaction problems (N-Queens, Sudoku)</p></li><li><p>Finding all paths meeting certain criteria</p></li><li><p>String partitioning problems</p></li></ul><pre><code>public void backtrack(State state, Choices choices, Results results) {\n    // Base case: Is this a complete solution?\n    if (isComplete(state)) {\n        results.add(copy(state));  // Store the solution\n        return;\n    }\n\n    // Recursive case: Try each available choice\n    for (Choice choice : getAvailableChoices(state, choices)) {\n        // 1. CHOOSE: Make the choice\n        makeChoice(state, choice);\n\n        // 2. EXPLORE: Recursively explore with this choice\n        backtrack(state, choices, results);\n\n        // 3. UNCHOOSE: Undo the choice (backtrack)\n        undoChoice(state, choice);\n    }\n}</code></pre><p>: Generate all possible subsets of an array.</p><ul><li><p> [[], [1], [1,2], [1,2,3], [1,3], [2], [2,3], [3]]</p></li></ul><h4>Step-by-Step Walkthrough:</h4><pre><code>nums = [1, 2, 3]\n\nbacktrack([], start=0)\n  add [] to result\n\n  i=0: choose 1 -&gt; [1]\n    backtrack([1], start=1)\n      add [1] to result\n\n      i=1: choose 2 -&gt; [1,2]\n        backtrack([1,2], start=2)\n          add [1,2] to result\n\n          i=2: choose 3 -&gt; [1,2,3]\n            backtrack([1,2,3], start=3)\n              add [1,2,3] to result\n            unchoose 3 -&gt; [1,2]\n        unchoose 2 -&gt; [1]\n\n      i=2: choose 3 -&gt; [1,3]\n        backtrack([1,3], start=3)\n          add [1,3] to result\n        unchoose 3 -&gt; [1]\n    unchoose 1 -&gt; []\n\n  i=1: choose 2 -&gt; [2]\n    ...similar process\n\nResult: [[], [1], [1,2], [1,2,3], [1,3], [2], [2,3], [3]]</code></pre><p>A Trie (prefix tree) stores strings character by character, allowing efficient prefix lookups. Each node represents a character, and paths from root to nodes represent prefixes.</p><ul><li><p>Autocomplete and search suggestions</p></li><li><p>IP routing (longest prefix match)</p></li><li><p>Word games (finding valid words)</p></li></ul><pre><code>class TrieNode {\n    TrieNode[] children = new TrieNode[26];\n    boolean isEndOfWord = false;\n}\n\nclass Trie {\n    TrieNode root = new TrieNode();\n\n    void insert(String word) {\n        TrieNode node = root;\n        for (char c : word.toCharArray()) {\n            int idx = c - 'a';\n            if (node.children[idx] == null) {\n                node.children[idx] = new TrieNode();\n            }\n            node = node.children[idx];\n        }\n        node.isEndOfWord = true;\n    }\n\n    boolean search(String word) {\n        TrieNode node = searchPrefix(word);\n        return node != null &amp;&amp; node.isEndOfWord;\n    }\n\n    boolean startsWith(String prefix) {\n        return searchPrefix(prefix) != null;\n    }\n\n    private TrieNode searchPrefix(String prefix) {\n        TrieNode node = root;\n        for (char c : prefix.toCharArray()) {\n            int idx = c - 'a';\n            if (node.children[idx] == null) return null;\n            node = node.children[idx];\n        }\n        return node;\n    }\n}</code></pre><p>: Implement a trie with insert, search, and startsWith methods.</p><pre><code>Trie trie = new Trie();\ntrie.insert(\"apple\");\ntrie.search(\"apple\");   // returns true\ntrie.search(\"app\");     // returns false\ntrie.startsWith(\"app\"); // returns true</code></pre><h4>Step-by-Step Walkthrough:</h4><pre><code>Insert \"apple\":\n\n  root\n   |\n   a (idx=0)\n   |\n   p (idx=15)\n   |\n   p (idx=15)\n   |\n   l (idx=11)\n   |\n   e (idx=4, isEndOfWord=true)\n\nSearch \"apple\":\n  Traverse: a -&gt; p -&gt; p -&gt; l -&gt; e\n  Node exists and isEndOfWord = true\n  Return true\n\nSearch \"app\":\n  Traverse: a -&gt; p -&gt; p\n  Node exists but isEndOfWord = false\n  Return false\n\nStartsWith \"app\":\n  Traverse: a -&gt; p -&gt; p\n  Node exists (prefix found)\n  Return true</code></pre><p>Greedy algorithms make locally optimal choices at each step, hoping to find a global optimum. They work when local optimal choices lead to global optimal solutions.</p><ul><li><p>Optimization problems with greedy choice property</p></li><li><p>When proof by exchange argument works</p></li></ul><pre><code>public Result solveGreedy(Input input) {\n    // Step 1: Sort or organize input (if needed)\n    sort(input, byGreedyCriterion);\n\n    // Step 2: Initialize result and tracking variables\n    Result result = initialResult();\n    State state = initialState();\n\n    // Step 3: Iterate and make greedy choices\n    for (Element element : input) {\n        if (canInclude(element, state)) {\n            // Make the greedy choice\n            result = update(result, element);\n            state = updateState(state, element);\n        }\n    }\n\n    return result;\n}</code></pre><p>: Determine if you can reach the last index starting from index 0.</p><ul><li><p> nums = [2, 3, 1, 1, 4]</p></li></ul><h4>Step-by-Step Walkthrough:</h4><pre><code>nums = [2, 3, 1, 1, 4]\n\nTrack maxReach (farthest index we can reach)\n\nStep 1: i = 0, nums[0] = 2\n  maxReach = max(0, 0 + 2) = 2\n\nStep 2: i = 1, nums[1] = 3\n  i &lt;= maxReach (1 &lt;= 2)\n  maxReach = max(2, 1 + 3) = 4\n\nStep 3: i = 2, nums[2] = 1\n  i &lt;= maxReach (2 &lt;= 4)\n  maxReach = max(4, 2 + 1) = 4\n\nStep 4: i = 3, nums[3] = 1\n  i &lt;= maxReach (3 &lt;= 4)\n  maxReach = max(4, 3 + 1) = 4\n\nmaxReach = 4 &gt;= last index (4)\nReturn true</code></pre><p>Dynamic Programming (DP) solves problems by breaking them into overlapping subproblems and storing results to avoid recomputation. It works when problems have optimal substructure.</p><ul><li><p>Problems with overlapping subproblems</p></li><li><p>Optimization (min/max) problems</p></li><li><p>Counting problems (number of ways)</p></li><li><p>Decision problems (can we achieve X?)</p></li></ul><ol><li><p> (1D DP with previous states)</p></li><li><p> (include or exclude each item)</p></li><li><p><strong>Longest Common Subsequence</strong> (2D DP on two sequences)</p></li><li><p><strong>Longest Increasing Subsequence</strong></p></li></ol><p>You can find more DP patterns and how to approach them in <a href=\"https://blog.algomaster.io/p/20-patterns-to-master-dynamic-programming\">this article</a>.</p><p>: Find the number of ways to climb n stairs, taking 1 or 2 steps at a time.</p><h4>Step-by-Step Walkthrough:</h4><pre><code>n = 4\n\ndp[i] = number of ways to reach step i\ndp[i] = dp[i-1] + dp[i-2]\n(we can reach step i from step i-1 or step i-2)\n\nBase cases:\n  dp[0] = 1 (one way to stay at ground)\n  dp[1] = 1 (one way: take 1 step)\n\nStep 2: dp[2] = dp[1] + dp[0] = 1 + 1 = 2\n  Ways: [1,1], [2]\n\nStep 3: dp[3] = dp[2] + dp[1] = 2 + 1 = 3\n  Ways: [1,1,1], [1,2], [2,1]\n\nStep 4: dp[4] = dp[3] + dp[2] = 3 + 2 = 5\n  Ways: [1,1,1,1], [1,1,2], [1,2,1], [2,1,1], [2,2]\n\nResult: 5 ways</code></pre><p>That’s it. Thanks for reading!</p><p>I’ve put together many more patterns and problems on my website. You can explore them here: <a href=\"https://algomaster.io/\">algomaster.io</a></p><p>If this article helped you, give it a like and share it with others.</p>","contentLength":27199,"flags":null,"enclosureUrl":"https://substackcdn.com/image/fetch/$s_!Zq5_!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fce7ede95-93fd-4e84-bca9-2500d9d5f518_783x955.png","enclosureMime":"","commentsUrl":null}],"tags":["dev"]}