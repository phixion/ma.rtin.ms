{"id":"82kPqomaPXmNomrHzpZWfbkQxiiNUBTAYKxHR5qZBEpf","title":"Hacker News: Show HN","displayTitle":"HN Show","url":"https://hnrss.org/show?points=60","feedLink":"https://news.ycombinator.com/shownew","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":16,"items":[{"title":"Show HN: Program Explorer, a container playground","url":"https://programexplorer.org/","date":1741710808,"author":"aconz2","guid":206,"unread":true,"content":"<div>\n            Alpha preview, site may be unavailable without notice\n        </div>","contentLength":75,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43334192"},{"title":"Show HN: Krep a High-Performance String Search Utility Written in C","url":"https://davidesantangelo.github.io/krep/","date":1741709563,"author":"daviducolo","guid":205,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43333946"},{"title":"Show HN: We built a Plug-in Home Battery for the 99.7% of us without Powerwalls","url":"https://pilaenergy.com/","date":1741708113,"author":"coleashman","guid":204,"unread":true,"content":"","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43333661"},{"title":"Show HN: Editable Games","url":"https://playscl.com/make","date":1741620717,"author":"sclerek","guid":201,"unread":true,"content":"<p>\n\t\t\tYou can edit/change the games in the list below. You can upload your own images, and even re-program the game play using\n\t\t\t<a href=\"https://canvaslanguage.com/showcase\" target=\"_blank\">SCL</a>.\n\t\t\t</p><p>\n\t\t\tOnce you've made your new game, you can have it activated to run on a domain of your choice by visiting the\n\t\t\t<a href=\"https://playscl.com/publish\" target=\"_blank\">Publishing</a> page. For example, if you have an account a itch.io, you\n\t\t\tcan activate your game for your itch page.\n\t\t\t</p><p>\n\t\t\tThis is a demo of how editable and customizing games work using <a href=\"https://canvaslanguage.com/\" target=\"_blank\">Canvas Language</a>. If you choose a game below, you'll be taken to a dev studio where you can edit your game\n\t\t\tand download a project file, for re-upload later.\n\t\t\t</p>","contentLength":598,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43321688"},{"title":"Show HN: In-Browser Graph RAG with Kuzu-WASM and WebLLM","url":"https://blog.kuzudb.com/post/kuzu-wasm-rag/","date":1741619577,"author":"sdht0","guid":200,"unread":true,"content":"<p>We’re excited that members of our community are already building applications with the <a href=\"https://docs.kuzudb.com/client-apis/wasm/\">WebAssembly (Wasm)</a> version of Kuzu,\nwhich was only released a few weeks ago!\nEarly adopters to integrate Kuzu-Wasm include <a href=\"https://github.com/alibaba/GraphScope\">Alibaba Graphscope</a>, see: <a href=\"https://github.com/kuzudb/kuzu/discussions/4946\">1</a>\nand <a href=\"https://gsp.vercel.app/#/explore?graph_id=\">2</a>, and <a href=\"https://www.kineviz.com/\">Kineviz</a>, whose project will be launched soon.</p><p>In this post, we’ll showcase the potential of Kuzu-Wasm by building a fully in-browser chatbot\nthat answers questions over LinkedIn data using an advanced retrieval technique: Graph\nRetrieval-Augmented Generation (Graph RAG). This is achieved using Kuzu-Wasm\nalongside <a href=\"https://github.com/mlc-ai/web-llm\">WebLLM</a>, a popular in-browser LLM inference engine that can\nrun LLMs inside the browser.</p><h2>A quick introduction to WebAssembly<a aria-hidden=\"true\" tabindex=\"-1\" href=\"https://blog.kuzudb.com/post/kuzu-wasm-rag/#a-quick-introduction-to-webassembly\"></a></h2><p>WebAssembly (Wasm) has transformed browsers into general-purpose computing platforms.\nMany fundamental software components, such as full-fledged databases, machine learning\nlibraries, data visualization tools, and encryption/decryption libraries, now have Wasm versions.\nThis enables developers to build advanced applications that run entirely in users’\nbrowsers—without requiring backend servers. There are several benefits for building fully\nin-browser applications:</p><ul><li>y: Users’ data never leaves their devices, ensuring complete privacy and confidentiality.</li><li>: An in-browser application that uses Wasm-based components\ncan run in any browser in a completely serverless manner.</li><li>: Eliminating frontend-server communication can lead to a significantly faster and\nmore interactive user experience.</li></ul><p>With this in mind, let’s now demonstrate how to develop a relatively complex AI application completely in\nthe browser! We’ll build a  chatbot that uses graph retrieval- augmented\ngeneration (Graph RAG) to answer natural language questions. We demonstrate this using\n<a href=\"https://docs.kuzudb.com/client-apis/wasm/#installation\">Kuzu-Wasm</a> and <a href=\"https://github.com/mlc-ai/web-llm\">WebLLM</a>.</p><p>The high-level architecture of the application looks as follows:\n<img src=\"https://blog.kuzudb.com/img/2025-02-27-kuzu-wasm-rag/graph-rag.png\" width=\"400\"></p><p>The term “Graph RAG” is used to refer to several techniques but in its simplest form the term\nrefers to a 3-step retrieval approach. The goal is to retrieve useful context from a graph DBMS (GDBMS)\nto help an LLM answer natural language questions.\nIn our application, the additional data is information about\na user’s LinkedIn data consisting of their contacts, messages, companies the user or their contacts worked for. Yes, you can download\n<a href=\"https://www.linkedin.com/help/linkedin/answer/a1339364/downloading-your-account-data\">your own LinkedIn data</a> (and you should, if\nfor nothing else, to see how much of your data they have!).\nThe schema of the graph database we use to model this data will be shown below momentarily. First, let’s go over the 3 steps of\nGraph RAG:</p><ol><li>Q Q: A user asks a natural language question Q, such as “<em>Which of my contacts work at Google?</em>“.\nThen, using an LLM, this question is converted to a Cypher query, e.g., <code>MATCH (a:Company)&lt;-[:WorksAt]-(b:Contact) WHERE a.name = \"Google\" RETURN b</code>,\nthat aims to retrieve relevant data stored in the GDBMS to answer this question.</li><li>Q Context: Q is executed in the GBMS and a set of records is retrieved, e.g., “Karen” and “Alice”. Let’s call these retrieved records “Context”.</li><li>(Q + Context)  A: Finally, the original Q is given to the LLM along with the retrieved context and the LLM produces a natural language answer A,\ne.g., “<em>Karen and Alice work at Google.</em>”</li></ol><p>The schema for our personal LinkedIn data’s graph is shown below:</p><img src=\"https://blog.kuzudb.com/img/2025-02-27-kuzu-wasm-rag/schema.png\" width=\"400\"><ol><li>Upload CSV Files: Users drag and drop their LinkedIn CSV files, which are stored in Kuzu-Wasm’s virtual file system.</li><li>Initial Processing: Using Kuzu’s  feature, the raw CSVs are converted into JavaScript objects.</li><li>Normalization: In JavaScript, we clean and standardize the data by fixing timestamps, formatting dates, and resolving inconsistent URIs.</li><li>Data Insertion: The cleaned data is inserted back into Kuzu-Wasm as a set of nodes and relationships.</li></ol><p>Our code follows the exact 3 steps above. Specifically, we prompt WebLLM twice, once to create a Cypher query Q,\nwhich is sent to Kuzu-Wasm.\nWe adapted the prompts from our <a href=\"https://github.com/kuzudb/langchain-kuzu/\">LangChain-Kuzu integration</a>,\nwith a few modifications. Importantly, we make sure to include the schema information of the LinkedIn database from Kuzu in the prompt, which helps the LLM better understand\nthe structure and relationships (including the directionality of the relationships) in the dataset.</p><p>In this example, we represented the schema as YAML instead of raw, stringified JSON in the LLM prompt.\nIn our anecdotal experience, for Text-to-Cypher tasks that require reasoning over the schema, we find that LLMs tend do better\nwith YAML syntax than they do with stringified JSON. More experiments on such Text-to-Cypher tasks will be shown in future blog posts.</p><p>It’s indeed impressive to see such a graph-based pipeline with LLMs being done entirely in the browser! There are, however, some caveats.\nMost importantly, in the browser, resources are restricted, which limits the sizes of different components of your application.\nFor example, the size of the LLM you can use is limited. We tested our implementation on a MacBook Pro 2023 and a Chrome browser.\nWe had to choose the <code>Llama-3.1-8B-Instruct-q4f32_1-MLC</code> model (see <a href=\"https://huggingface.co/mlc-ai/Llama-3.1-8B-Instruct-q4f32_1-MLC\">here</a> for the model card),\nwhich is an instruction-tuned model in MLC format. The  format is the smallest of the Llama 3.1 models that has 8B parameters\n(the largest has 450B parameters, which is of course too large to run in the browser).\nFor simple queries, the model performed quite well. It correctly generated Cypher queries for the LinkedIn data, such as:</p><ul><li>How many companies did I follow?</li><li>Which contacts work at Kùzu, Inc?</li></ul><img src=\"https://blog.kuzudb.com/img/2025-02-27-kuzu-wasm-rag/successful-generations.png\"><p>However, we saw that for more complex queries requiring joins, filtering, and aggregation, the model struggled to return a valid Cypher query.\nIt often produced incorrect or incomplete Cypher queries for questions like: “Who endorsed me the most times?“.\nToken generation is also far slower than what you may be used to in state-of-the art interfaces,\nsuch as ChatGPT. In our experiments, we observed a speed of 15-20 tokens/sec, so generating answers took on average, around 10s.</p><p>We have deployed this demo so you can test it in your browser:</p><ul><li><a href=\"https://wasm-linkedin-example.kuzudb.com\">Live Demo</a>: Drag and drop your <a href=\"https://www.linkedin.com/help/linkedin/answer/a1339364/downloading-your-account-data\">LinkedIn data dump</a> into\nthe app and start querying your personal graph. The demo also visualizes your data in a node-link graph view using </li><li><a href=\"https://github.com/kuzudb/wasm-linkedin-example\">GitHub Repository</a>: The source code is openly available so you can experiment with it further. If you see better results with different models/prompts, we’d love to hear it!</li></ul><p>Once the data is loaded, you can see a visualization that looks something like this:</p><img src=\"https://blog.kuzudb.com/img/2025-02-27-kuzu-wasm-rag/landing-page.png\"><p>The key takeaway from this post is that such advanced pipelines that utilize graph databases and LLMs are now possible .\nWe expect that many of the performance limitations of today will improve over time, with the wider adoption of <a href=\"https://www.w3.org/TR/webgpu/\">WebGPU</a>,\n<a href=\"https://github.com/WebAssembly/memory64\">Wasm64</a>, and other <a href=\"https://github.com/WebAssembly/proposals?tab=readme-ov-file\">proposals</a>\nto improve Wasm. LLMs are also rapidly getting smaller &amp; better, and before we know it, it will be possible to use very advanced LLMs\nin the browser. The next release of Kuzu will include a native vector index (it’s already available\nin our nightly build; see <a href=\"https://github.com/kuzudb/kuzu/pull/4578\">this PR</a> for how to use it!).\nAs a result, you can also store the embeddings of documents\nalong with actual node and relationship records to enhance your graph retrievals, entirely within Kuzu.\nUsing our upcoming vector index,\nyou’ll be able to try all sorts of interesting RAG techniques, coupled with Kuzu-Wasm, all within the browser while keeping your data private.\nThe sky is the limit!</p>","contentLength":7347,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43321523"},{"title":"Show HN: Evolving Agents Framework","url":"https://github.com/matiasmolinas/evolving-agents","date":1741539119,"author":"matiasmolinas","guid":199,"unread":true,"content":"<p>I've been working on an open-source framework for creating AI agents that evolve, communicate, and collaborate to solve complex tasks. The Evolving Agents Framework allows agents to:</p><p>Reuse, evolve, or create new agents dynamically based on semantic similarity\nCommunicate and delegate tasks to other specialized agents\nContinuously improve by learning from past executions\nDefine workflows in YAML, making it easy to orchestrate agent interactions\nSearch for relevant tools and agents using OpenAI embeddings\nSupport multiple AI frameworks (BeeAI, etc.)\nCurrent Status &amp; Roadmap\n This is still a draft and a proof of concept (POC). Right now, I’m focused on validating it in real-world scenarios to refine and improve it.</p><p>Next week, I'm adding a new feature to make it useful for distributed multi-agent systems. This will allow agents to work across different environments, improving scalability and coordination.</p><p>Why?\nMost agent-based AI frameworks today require manual orchestration. This project takes a different approach by allowing agents to decide and adapt based on the task at hand. Instead of always creating new agents, it determines if existing ones can be reused or evolved.</p><p>Example Use Case:\nLet’s say you need an invoice analysis agent. Instead of manually configuring one, our framework:\n Checks if a similar agent exists (e.g., a document analyzer)\n Decides whether to reuse, evolve, or create a new agent\n Runs the best agent and returns the extracted information</p><p>Here's a simple example in Python:</p><p>import asyncio\nfrom evolving_agents.smart_library.smart_library import SmartLibrary\nfrom evolving_agents.core.llm_service import LLMService\nfrom evolving_agents.core.system_agent import SystemAgent</p><p>async def main():\n    library = SmartLibrary(\"agent_library.json\")\n    llm = LLMService(provider=\"openai\", model=\"gpt-4o\")\n    system = SystemAgent(library, llm)</p><pre><code>    result = await system.decide_and_act(\n        request=\"I need an agent that can analyze invoices and extract the total amount\",\n        domain=\"document_processing\",\n        record_type=\"AGENT\"\n    )\n\n    print(f\"Decision: {result['action']}\")  # 'reuse', 'evolve', or 'create'\n    print(f\"Agent: {result['record']['name']}\")\n</code></pre>\nif __name__ == \"__main__\":\n    asyncio.run(main())<p>Next Steps\nValidating in real-world use cases and improving agent evolution strategies\nAdding distributed multi-agent support for better scalability\nFull integration with BeeAI Agent Communication Protocol (ACP)\nBetter visualization tools for debugging\nWould love feedback from the HN community! What features would you like to see?</p>","contentLength":2587,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43310963"},{"title":"Show HN: Searchable Vim Cheat Sheet with Favorites (Open-Source)","url":"https://nvim-cheatsheet.vercel.app/","date":1741514752,"author":"lil_csom","guid":198,"unread":true,"content":"<p>Open file in new buffer, switch to it. (Creates file if does not exist)</p>","contentLength":71,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43307745"},{"title":"Show HN: I built an app to get daily wisdom from Mr. Worldwide","url":"https://daale.club/","date":1741478687,"author":"garyreckon","guid":197,"unread":true,"content":"<!DOCTYPE html>","contentLength":0,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43304785"},{"title":"Show HN: TypeLeap: LLM Powered Reactive Intent UI/UX","url":"https://www.typeleap.com/","date":1741466275,"author":"eadz","guid":196,"unread":true,"content":"<h2>Dynamic Interfaces that Anticipate Your Needs</h2><p>\n    TypeLeap UIs detect your intent as you type, not just predict words. Using LLMs, TypeLeap understands what you want to do and dynamically adapts the interface in real-time.\n    <p>\n    Instead of passive text input, TypeLeap offers proactive, intent-driven UIs. Think instant action suggestions, dynamic search results, and smarter commands, all based on understanding your typing intent.\n    </p><p>\n    Faster, more intuitive workflows. Less mode switching. TypeLeap makes UIs truly responsive to your goals as you type.\n\n  </p></p><p>\n    For years, we've been promised interfaces that are more intuitive, more… .  Autocomplete, suggestions, even Clippy (remember Clippy?) were early attempts. But with the advent of Large Language Models (LLMs), we're on the cusp of a genuinely new paradigm: .\n  </p><p>\n    Imagine typing \"weather in San…\" into a search bar, and before you even hit enter, the interface dynamically shifts. Maybe a compact weather widget pops up, or the search results page subtly re-arranges to prioritize weather forecasts.  Or consider typing \"remind me to call mom at 5pm\" – instead of waiting for you to parse menus, the interface instantly presents a streamlined reminder creation form. This isn't just smarter autocomplete; it's the UI actively\n    \n    as you type and adapting in real-time.\n  </p><p>\n    This concept, \"TypeLeap,\" uses LLMs to analyze partial input and predict what the user is .  Are they searching for information? Issuing a command? Navigating somewhere?  Based on this dynamic interpretation, the UI proactively adjusts, offering context-aware actions and streamlining workflows.  Think of it as moving beyond static input fields to interfaces that feel genuinely responsive and anticipatory.\n  </p><h3>It's Not Entirely New, But Now It's Different</h3><p>\n    The core idea of dynamic, intent-aware inputs isn't entirely novel. We've seen glimpses in existing interfaces:\n  </p><ul><li>\n      A classic example. It merges URL and search, intelligently guessing your intent as you type. \"Single word? Probably search.\" \"Looks like a URL? Let's navigate.\"  It offers suggestions for both, adapting in real-time. Early debates even considered inline web results, but the focus remained on *accelerating input* with relevant suggestions, not overwhelming the user.\n    </li><li><b>Command Palettes (Like in VS Code or Slack):</b>\n      Start with a \"/\" in Slack or Ctrl+P in VS Code, and the input field transforms into a command interface.  Rule-based, yes, but illustrating intent-based mode switching. LLMs could generalize this, understanding natural language commands like \"remind me to…\" without needing a specific prefix.\n    </li><li><b>Real-Time Query Suggestions:</b>\n      Ubiquitous now, powered by statistical models. But imagine this amplified by LLMs. Arc Browser's \"Arc Max\" uses ChatGPT in the address bar to offer AI-generated answers alongside search completions.  Platforms like Nebuly offer \"real-time prompt suggestions\" in enterprise contexts.  The trend is clear: blending typing with AI assistance to predict intent and guide users.\n    </li></ul><h3>How Does This Actually Work? (The Tech Stack)</h3><p>\n    Building TypeLeap UI/UX is a fascinating engineering challenge. The fundamental loop is: capture keystrokes -&gt; LLM intent analysis -&gt; UI update.  Crucially, this needs to be .\n  </p><p><b>Local vs. Server Processing:</b>\n    Sending every keystroke to a server for LLM analysis adds latency.  Enter . Projects like\n    <a href=\"https://webllm.mlc.ai/\">WebLLM</a>\n    are demonstrating that running moderately sized models directly in the browser (using WebGPU) is feasible.  Local analysis eliminates network latency and enhances privacy.  A hybrid approach might be best: a lightweight local model for initial intent guessing (within the first 50-100ms) to drive immediate UI hints, with heavier server-side analysis for deeper understanding or complex responses triggered concurrently.\n  </p><p><b>Optimizing for Speed is Paramount:</b>\n    Even local LLM inference needs optimization. Techniques like \n    (4/8-bit weights), \n    (smaller models trained on larger ones for intent classification), and \n    are essential.  For long queries, avoid re-analyzing prefixes – cache embeddings or intent decisions.  In client-server setups, .  The UI must\n    \n    real-time, even if heavy lifting is happening under the hood.  Think spinners, partial suggestions – visual feedback within a few hundred milliseconds.\n  </p><p><b>User Feedback and Control are Non-Negotiable:</b>\n    Dynamic UIs based on AI guesses require clear communication and user control.  Changes should be noticeable but not jarring. Subtle visual cues (highlights, ghost text) are key.  Major changes (like scheduling a meeting) should always require explicit user confirmation.  Easy dismissal or override options are crucial to prevent the UI from feeling unpredictable. Confidence scores from the LLM can gate UI changes – only trigger auto-updates when confidence is high.\n  </p><p><b>Debouncing is Your Friend (Backend Efficiency):</b>\n    Analyzing every keystroke is computationally wasteful. **Debouncing** is critical.  Wait for a short pause in typing (e.g., 300-500ms) before triggering LLM analysis.  This dramatically reduces unnecessary computations and API calls. Server-side throttling is also wise to handle rapid-fire requests.  Cancel or ignore redundant requests.  Chrome's omnibox analogy: background suggestion fetches stop when the user interacts with the dropdown.\n  </p><h3>Use Cases: Beyond Search Bars</h3><p>TypeLeap UI/UX has broad applicability:</p><p><b>Search Interfaces (Obvious, but Powerful):</b>\n    Differentiate between navigational queries (\"facebook\" -&gt; go to site), informational questions (direct AI snippet answers), and action queries (\"upload file\" -&gt; open upload dialog). Imagine typing \"weather in San…\" and a weather widget appears *as you type*. E-commerce search: \"order status 12345\" -&gt; direct tracking UI. Browser address bars: factual question answers inline, direct command suggestions (\"clear cache\").  The search bar becomes a universal interface – navigation, chatbot, command line – all dynamically.\n  </p><p><b>Knowledge Management &amp; Documentation:</b>\n    Internal wikis, note-taking apps. Differentiate between questions and keyword searches.  Natural language questions trigger FAQ mode; keywords trigger standard search.  Task-oriented intents: \"create new page about…\" -&gt; template creation UI. Note-taking tools: AI suggests related info or links as you type. API documentation search: \"how to use function X in Python\" -&gt; example snippet; \"function X\" -&gt; reference docs.\n  </p><p><b>Interactive AI Assistants:</b>\n    Chatbots move beyond text. Detect intent in chat and invoke relevant UI. Customer support bot: typing about \"refund policy\" -&gt; highlight policy article or present return form. Personal assistants: dynamic suggestions like \"set an alarm for tomorrow?\" and offer to open the Clock app. AI coding assistants: detect questions in comments (\"how to sort a list in Python?\") and proactively show answers/code snippets.  Blend chat and GUI: infer task intent and surface task-specific UI.\n  </p><p><b>LLM-Powered Software Tools (Everywhere):</b>\n    Project management: \"assign Alice to write report by next Monday\" in a text field -&gt; task assignment with deadline in UI. Calendar: \"Meeting with Bob next week about project\" -&gt; scheduling dialog with participants and date pre-filled.  CLIs: natural language commands \"git, uh, create a new branch for feature X\" -&gt; parse to `git checkout -b feature_x` and show confirmation.  Even gaming/VR: analyze typed/spoken input for intent (command, chat, emote) and dynamically adjust game UI.\n  </p><h3>This isn't magic.  Significant hurdles remain:</h3><ul><li>\n      LLMs aren't instant. Even milliseconds matter in typing interactions. Laggy suggestions are worse than no suggestions. Smaller/faster models or heavy optimization are mandatory. Computational cost is a real issue at scale. Design UIs to gracefully handle latency – placeholder hints (\"searching…\") are better than nothing.\n    </li><li><b>Accuracy of Intent Recognition:</b>\n      LLMs can misinterpret, especially with partial input. False positives (UI jumping to the wrong conclusion) are frustrating. Conservative confidence thresholds, disambiguation strategies, and fallbacks are needed.  Combine LLMs with heuristics.  Continuous learning from user corrections is essential.\n    </li><li><b>UI Stability and Predictability:</b>\n      Avoid \"jittery\" interfaces.  Users need to feel in control. Erratic UI changes erode trust.  Stable, predictable behavior is paramount.  Anchor UI changes (suggestions dropdown in the same place).  Carefully time updates. User testing is crucial to avoid unintended consequences of a \"morphing\" UI.\n    </li><li>\n      Large models for nuanced intent are slow. Smaller models might be too simplistic. LLMs aren't truly real-time streaming (batch processing). Workarounds like prompting \"what is the user likely trying to do?\" add latency.  LLMs might be overkill for simple intent detection tasks.  Hallucinations and sensitivity to phrasing are still LLM limitations.\n    </li><li>\n      Sending every keystroke to the cloud raises privacy concerns. Secure data transmission and careful handling of PII are minimum requirements. Local models mitigate this. Security is also a factor – prevent malicious input from triggering unintended commands. Sandboxing and confirmation for high-impact actions are needed.\n    </li></ul><p>\n    TypeLeap UI/UX is a compelling vision: interfaces that anticipate our needs, powered by the intelligence of LLMs.  From Chrome's omnibox to emerging AI-driven command bars, we see early examples.  For builders and tinkerers, the challenge is balancing AI power with performance, accuracy, and user expectations.  Techniques like debouncing, local inference, and confidence thresholds are crucial.\n  </p><p>\n    When done right, TypeLeap UIs can feel remarkably natural – like the interface is an attentive assistant, understanding not just your words, but your *intent*. This is a fertile ground for innovation. Expect to see more experimentation in browsers, IDEs, assistants, and beyond.  The key, as always, is to use AI to *augment* user agency, not replace it.  The coming years will be fascinating as we explore the possibilities (and navigate the pitfalls) of interfaces that truly read our minds – as we type.\n  </p><p>I am unaware of any examples of this type of UI/UX in the wild. To be clear the criteria is a search/combo text input with UI elements that dynamically change based on the user's  as they type.</p><p>Chrome's omnibox is the closest example I can think of. Any examples will be listed here.</p><p>If anyone has design concepts which they would like to share, please let me know.</p><p>I am looking for help with the following:</p><ul><li>Designers: This site/demo can use help with design of the page, and potentialy design mockups based on the concept of TypeLeap UI/UX</li><li>Developers: This site/demo can use help with development of the site/demo</li></ul>","contentLength":10855,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43303309"},{"title":"Show HN: I built didtheyghost.me, open-source tool for your next job search","url":"https://didtheyghost.me/","date":1741441514,"author":"dtgmzac","guid":195,"unread":true,"content":"<div><p>Keep track of your job applications with detailed insights for each stage of the hiring process</p><ul><li>'Applied' tab: The first stage, view response timelines: check when you can expect the first response date from a company after applying</li><li>Filter by application status: Applied, Interviewing, Rejected, Ghosted, or Offered</li><li>'Questions' tab: Engage with other applicants by asking or answering questions about the interview process.</li></ul></div>","contentLength":420,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43300204"},{"title":"Show HN: Open-Source DocumentAI with Ollama","url":"https://rlama.dev/","date":1741399933,"author":"Dontizi","guid":194,"unread":true,"content":"<p>Open-source project. All rights reserved.</p>","contentLength":41,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43296918"},{"title":"Show HN: Rust Vector and Quaternion Lib","url":"https://github.com/David-OConnor/lin-alg","date":1741293167,"author":"the__alchemist","guid":193,"unread":true,"content":"<p>I use this library I made for Vectors and Quaternions in many personal projects. I've open-sourced it, in case anyone else would get use out of it.</p><p>I use this on various projects, including quadcopter firmware, a graphics engine, a cosmology simulation, and several molecular dynamics applications. No_std compatible.</p>","contentLength":316,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43284811"},{"title":"Show HN: Shelgon: A Framework for Building Interactive REPL Shells in Rust","url":"https://github.com/NishantJoshi00/shelgon","date":1741289532,"author":"cat-whisperer","guid":192,"unread":true,"content":"<p>I've been working on Shelgon, a framework that lets you build your own custom REPL shells and interactive CLI applications in Rust.</p><p>- Create a custom shell with only a few lines of code\n- Build interactive debugging tools with persistent state between commands\n- Develop domain-specific language interpreters with shell-like interfaces\n- Add REPL capabilities to existing applications</p><p>Getting started is straightforward - implement a single trait that handles your command execution logic, and Shelgon takes care of the terminal UI, input handling, and async runtime integration.</p><p>For example, a simple echo shell takes less than 50 lines of code, including a full implementation of command history, cursor movement, and tab completion.</p>","contentLength":732,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43284227"},{"title":"Show HN: Open-source, native audio turn detection model","url":"https://github.com/pipecat-ai/smart-turn","date":1741285248,"author":"kwindla","guid":191,"unread":true,"content":"<p>Our goal with this project is to build a completely open source, state of the art turn detection model that can be used in any voice AI application.</p><p>I've been experimenting with LLM voice conversations since GPT-4 was first released. (There's a previous front page Show HN about Pipecat, the open source voice AI orchestration framework I work on. [1])</p><p>It's been almost two years, and for most of that time, I've been expecting that someone would \"solve\" turn detection. We all built initial, pretty good 80/20 versions of turn detection on top of VAD (voice activity detection) models. And then, as an ecosystem, we kind of got stuck.</p><p>A few production applications have recently started using Gemini 2.0 Flash to do context aware turn detection. [2] But because latency is ~500ms, that's a more complicated approach than using a specialized model. The team at LiveKit released an open weights model that does text-based turn detection. [3] I was really excited to see that, but I'm not super-optimistic that a text-input model will ever be good enough for this task. (A good rule of thumb in deep learning is that you should bet on end-to-end.)</p><p>So ... I spent Christmas break training several little proof of concept models, and experimenting with generating synthetic audio data. So, so, so much fun. The results were promising enough that I nerd-sniped a few friends and we started working in earnest on this.</p><p>The model now performs really well on a subset of turn detection tasks. Too well, really. We're overfitting on a not-terribly-broad initial data set of about 8,000 samples. Getting to this point was the initial bar we set for doing a public release and seeing if other people want to get involved in the project.</p><p>There are lots of ways to contribute. [4]</p><p>Medium-term goals for the project are:</p><pre><code>  - Support for a wide range of languages\n  - Inference time of &lt;50ms on GPU and &lt;500ms on CPU\n  - Much wider range of speech nuances captured in training data\n  - A completely synthetic training data pipeline. (Maybe?)\n  - Text conditioning of the model, to support \"modes\" like credit card, telephone number, and address entry.\n</code></pre>\nIf you're interested in voice AI or in audio model ML engineering, please try the model out and see what you think. I'd love to hear your thoughts and ideas.","contentLength":2287,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43283317"},{"title":"Show HN: CodeTracer – A time-traveling debugger implemented in Nim and Rust","url":"https://github.com/metacraft-labs/codetracer","date":1741271410,"author":"alehander42","guid":190,"unread":true,"content":"<p>We are presenting CodeTracer - a user-friendly time-traveling debugger designed to support a wide range of programming languages:</p><p>CodeTracer records the execution of a program into a sharable self-contained trace file. You can load the produced trace files in a GUI environment that allows you to move forward and backward through the execution and to examine the history of all memory locations. They say a picture is worth a thousand words — well, a video is even better! Watch the demo below to see CodeTracer in action:</p><p>The initial release is limited to the Noir programming language, but CodeTracer uses an open format for its trace files and we've started community-driven projects which aim to add support for Ruby and Python.</p><p>We are also developing an alternative back-end, capable of working with RR recordings, which will make CodeTracer suitable for debugging large-scale programs in a variety of system programming languages such as C/C++, Rust, Nim, D, Zig, Go, Fortran and FreePascal.</p>","contentLength":997,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43280615"},{"title":"Show HN: Leaflet.pub – a web app for creating and sharing rich documents","url":"https://news.ycombinator.com/item?id=43269928","date":1741197329,"author":"jpereira","guid":189,"unread":true,"content":"Hi HN!<p>For the last 8 months we've been working on leaflet.pub, a web app for making delightful documents. We're trying to strike a balance between Notion and Google Docs — very fast, ultralight and easy to share, but also supporting rich blocks and multiple pages.</p><p>Weirdly, none of the many notetaking/document apps that we could find hit this combination, so we made Leaflet. With it you can:</p><p>- Instantly create a doc, without an account\n- Share read and edit links\n- Sign-in with email to sync your docs to different devices\n- Add rich blocks, like canvases, subpages, rsvps, and polls</p><p>It's really useful for one-off collaborations, running events, or just when you need a blank page without having to buy into a whole organizational system.</p><p>We also spent a lot of time making sure Leaflets look good. We've found that there's a pretty blurry boundary between a document and a website, so making something that people can feel proud to publish online was key.</p><p>Here's a couple examples!</p><p>Some technical details that might be interesting:</p><p>- We do sync and all our client-side state via Replicache, which I really love!\n- Data is modeled as a set of facts about entities, a la Datomic, forming a graph. This has been flexible enough for us to quickly build new features, like canvases and nested pages, without committing to a single document structure.\n- We use ProseMirror, but not for the entire document. Instead every text block is a separate ProseMirror instance. This lets us keep the document structure in our database and our schema, without having to dive into ProseMirror's every time we want to modify things.</p><p>- Better home and document organizing features — things like search, tagging, collections etc.\n- We're really excited about ATProto and Bluesky and are working on a set of lexicons and an AppView for document publishing! This will include a lexicon for rich text documents, as well as one publications, and some concept of memberships or subscriptions.\n- More blocks! Tables, code blocks, etc.</p><p>Some things we're particularly proud of:</p><p>- Our list handling\n- Custom theming\n- Keyboard handling on iOS Safari (and generally works excellently on mobile)\n- Side-scrolling multi-page interface\n- Works as a PWA!</p><p>Some things that still need work:</p><p>- While faster than others, still a lot of work we can do on performance, both speed when working with very large documents and loading docs generally\n- Drag and drop and selection in general could be a lot nicer\n- Keyboard navigation across multiple pages\n- Multiplayer cursors, and generally real-time sync could be sped up greatly leveraging CRDTs (we already use YJS, just could move updates around faster)</p>","contentLength":2662,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":"https://news.ycombinator.com/item?id=43269928"}],"tags":["dev","hn"]}