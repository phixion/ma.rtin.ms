{"id":"MvwLKznkcWQJt9LV3qspiNNstpReRGojdXM3bsYDh","title":"Kubernetes Blog","displayTitle":"Dev - Kubernetes Blog","url":"https://kubernetes.io/feed.xml","feedLink":"https://kubernetes.io/","isQuery":false,"isEmpty":false,"isHidden":false,"itemCount":9,"items":[{"title":"Spotlight on SIG Architecture: API Governance","url":"https://kubernetes.io/blog/2026/02/12/sig-architecture-api-spotlight/","date":1770854400,"author":"","guid":475,"unread":true,"content":"<p><em>This is the fifth interview of a SIG Architecture Spotlight series that covers the different\nsubprojects, and we will be covering <a href=\"https://github.com/kubernetes/community/blob/master/sig-architecture/README.md#architecture-and-api-governance-1\">SIG Architecture: API\nGovernance</a>.</em></p><p>In this SIG Architecture spotlight we talked with <a href=\"https://github.com/liggitt\">Jordan Liggitt</a>, lead\nof the API Governance sub-project.</p><p><strong>FM: Hello Jordan, thank you for your availability. Tell us a bit about yourself, your role and how\nyou got involved in Kubernetes.</strong></p><p>: My name is Jordan Liggitt. I'm a Christian, husband, father of four, software engineer at\n<a href=\"https://about.google/\">Google</a> by day, and <a href=\"https://www.youtube.com/watch?v=UDdr-VIWQwo\">amateur musician</a> by stealth. I was born in Texas (and still\nlike to claim it as my point of origin), but I've lived in North Carolina for most of my life.</p><p>I've been working on Kubernetes since 2014. At that time, I was working on authentication and\nauthorization at Red Hat, and my very first pull request to Kubernetes attempted to <a href=\"https://github.com/kubernetes/kubernetes/pull/2328\">add an OAuth\nserver</a> to the Kubernetes API server. It never\nexited work-in-progress status. I ended up going with a different approach that layered on top of\nthe core Kubernetes API server in a different project (spoiler alert: this is foreshadowing), and I\nclosed it without merging six months later.</p><p>Undeterred by that start, I stayed involved, helped build Kubernetes authentication and\nauthorization capabilities, and got involved in the definition and evolution of the core Kubernetes\nAPIs from early beta APIs, like  to . I got tagged as an API reviewer in 2016 based on\nthose contributions, and was added as an API approver in 2017.</p><p>Today, I help lead the API Governance and code organization subprojects for SIG Architecture, and I\nam a tech lead for SIG Auth.</p><p><strong>FM: And when did you get specifically involved in the API Governance project?</strong></p><h2>Goals and scope of API Governance</h2><p><strong>FM: How would you describe the main goals and areas of intervention of the subproject?</strong></p><p>The surface area includes all the various APIs Kubernetes has, and there are APIs that people do not\nalways realize are APIs: command-line flags, configuration files, how binaries are run, how they\ntalk to back-end components like the container runtime, and how they persist data. People often\nthink of \"the API\" as only the <a href=\"https://kubernetes.io/docs/reference/using-api/\">REST API</a>... that\nis the biggest and most obvious one, and the one with the largest audience, but all of these other\nsurfaces are also APIs. Their audiences are narrower, so there is more flexibility there, but they\nstill require consideration.</p><p>The goals are to be stable while still enabling innovation. Stability is easy if you never change\nanything, but that contradicts the goal of evolution and growth. So we balance \"be stable\" with\n\"allow change\".</p><p><strong>FM: Speaking of changes, in terms of ensuring consistency and quality (which is clearly one of the\nreasons this project exists), what are the specific quality gates in the lifecycle of a Kubernetes\nchange? Does API Governance get involved during the release cycle, prior to it through guidelines,\nor somewhere in between? At what points do you ensure the intended role is fulfilled?</strong></p><p>: We have <a href=\"https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md\">guidelines and\nconventions</a>,\nboth for APIs in general and for how to change an API. These are living documents that we update as\nwe encounter new scenarios. They are long and dense, so we also support them with involvement at\neither the design stage or the implementation stage.</p><p>Sometimes, due to bandwidth constraints, teams move ahead with design work without feedback from <a href=\"https://github.com/kubernetes/community/blob/master/sig-architecture/api-review-process.md\">API Review</a>. That’s fine, but it means that when implementation begins, the API review will happen then,\nand there may be substantial feedback. So we get involved when a new API is created or an existing\nAPI is changed, either at design or implementation.</p><p><strong>FM: Is this during the Kubernetes Enhancement Proposal (KEP) process? Since KEPs are mandatory for\nenhancements, I assume part of the work intersects with API Governance?</strong></p><p>: It can. <a href=\"https://github.com/kubernetes/enhancements/blob/master/keps/README.md\">KEPs</a> vary\nin how detailed they are. Some include literal API definitions. When they do, we can perform an API\nreview at the design stage. Then implementation becomes a matter of checking fidelity to the design.</p><p>Getting involved early is ideal. But some KEPs are conceptual and leave details to the\nimplementation. That’s not wrong; it just means the implementation will be more exploratory. Then\nAPI Review gets involved later, possibly recommending structural changes.</p><p>There’s a trade-off regardless: detailed design upfront versus iterative discovery during\nimplementation. People and teams work differently, and we’re flexible and happy to consult early or\nat implementation time.</p><p><strong>FM: This reminds me of what Fred Brooks wrote in \"The Mythical Man-Month\" about conceptual\nintegrity being central to product quality... No matter how you structure the process, there must be\na point where someone looks at what is coming and ensures conceptual integrity. Kubernetes uses APIs\neverywhere -- externally and internally -- so API Governance is critical to maintaining that\nintegrity. How is this captured?</strong></p><p>: Yes, the conventions document captures patterns we’ve learned over time: what to do in\nvarious situations. We also have automated linters and checks to ensure correctness around patterns\nlike spec/status semantics. These automated tools help catch issues even when humans miss them.</p><p>As new scenarios arise -- and they do constantly -- we think through how to approach them and fold\nthe results back into our documentation and tools. Sometimes it takes a few attempts before we\nsettle on an approach that works well.</p><p><strong>FM: Exactly. Each new interaction improves the guidelines.</strong></p><p>: Right. And sometimes the first approach turns out to be wrong. It may take two or three\niterations before we land on something robust.</p><h2>The impact of Custom Resource Definitions</h2><p><strong>FM: Is there any particular change, episode, or domain that stands out as especially noteworthy,\ncomplex, or interesting in your experience?</strong></p><p>: The watershed moment was <a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/\">Custom Resources</a>.\nPrior to that, every API was handcrafted by us and fully reviewed. There were inconsistencies, but\nwe understood and controlled every type and field.</p><p>When Custom Resources arrived, anyone could define anything. The first version did not even require\na schema. That made it extremely powerful -- it enabled change immediately -- but it left us playing\ncatch-up on stability and consistency.</p><p>When Custom Resources graduated to General Availability (GA), schemas became required, but escape\nhatches still existed for backward compatibility. Since then, we’ve been working on giving CRD\nauthors validation capabilities comparable to built-ins. Built-in validation rules for CRDs have\nonly just reached GA in the last few releases.</p><p>So CRDs opened the \"anything is possible\" era. Built-in validation rules are the second major\nmilestone: bringing consistency back.</p><p>The three major themes have been defining schemas, validating data, and handling pre-existing\ninvalid data. With ratcheting validation (allowing data to improve without breaking existing\nobjects), we can now guide CRD authors toward conventions without breaking the world.</p><h2>API Governance in context</h2><p><strong>FM: How does API Governance relate to SIG Architecture and API Machinery?</strong></p><p>: <a href=\"https://github.com/kubernetes/apimachinery\">API Machinery</a> provides the actual code and\ntools that people build APIs on. They don’t review APIs for storage, networking, scheduling, etc.</p><p>SIG Architecture sets the overall system direction and works with API Machinery to ensure the system\nsupports that direction. API Governance works with other SIGs building on that foundation to define\nconventions and patterns, ensuring consistent use of what API Machinery provides.</p><p><strong>FM: Thank you. That clarifies the flow. Going back to <a href=\"https://kubernetes.io/releases/release/\">release cycles</a>: do release phases -- enhancements freeze, code\nfreeze -- change your workload? Or is API Governance mostly continuous?</strong></p><p>: We get involved in two places: design and implementation. Design involvement increases\nbefore enhancements freeze; implementation involvement increases before code freeze. However, many\nefforts span multiple releases, so there is always some design and implementation happening, even\nfor work targeting future releases. Between those intense periods, we often have time to work on\nlong-term design work.</p><p>An anti-pattern we see is teams thinking about a large feature for months and then presenting it\nthree weeks before enhancements freeze, saying, \"Here is the design, please review.\" For big changes\nwith API impact, it’s much better to involve API Governance early.</p><p>And there are good times in the cycle for this -- between freezes -- when people have bandwidth.\nThat’s when long-term review work fits best.</p><p><strong>FM: Clearly. Now, regarding team dynamics and new contributors: how can someone get involved in\nAPI Governance? What should they focus on?</strong></p><p>: It’s usually best to follow a specific change rather than trying to learn everything at\nonce. Pick a small API change, perhaps one someone else is making or one you want to make, and\nobserve the full process: design, implementation, review.</p><p>High-bandwidth review -- live discussion over video -- is often very effective. If you’re making or\nfollowing a change, ask whether there’s a time to go over the design or PR together. Observing those\ndiscussions is extremely instructive.</p><p>Start with a small change. Then move to a bigger one. Then maybe a new API. That builds\nunderstanding of conventions as they are applied in practice.</p><p><strong>FM: Excellent. Any final comments, or anything we missed?</strong></p><p>: Yes... the reason we care so much about compatibility and stability is for our users. It’s\neasy for contributors to see those requirements as painful obstacles preventing cleanup or requiring\ntedious work... but users integrated with our system, and we made a promise to them: we want them to\ntrust that we won’t break that contract. So even when it requires more work, moves slower, or\ninvolves duplication, we choose stability.</p><p>We are not trying to be obstructive; we are trying to make life good for users.</p><p>A lot of our questions focus on the future: you want to do something now... how will you evolve it\nlater without breaking it? We assume we will know more in the future, and we want the design to\nleave room for that.</p><p>We also assume we will make mistakes. The question then is: how do we leave ourselves avenues to\nimprove while keeping compatibility promises?</p><p><strong>FM: Exactly. Jordan, thank you, I think we’ve covered everything. This has been an insightful view\ninto the API Governance project and its role in the wider Kubernetes project.</strong></p>","contentLength":10358,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Introducing Node Readiness Controller","url":"https://kubernetes.io/blog/2026/02/03/introducing-node-readiness-controller/","date":1770084000,"author":"","guid":474,"unread":true,"content":"<img src=\"https://kubernetes.io/blog/2026/02/03/introducing-node-readiness-controller/node-readiness-controller-logo.svg\" alt=\"Logo for node readiness controller\"><p>In the standard Kubernetes model, a node’s suitability for workloads hinges on a single binary \"Ready\" condition. However, in modern Kubernetes environments, nodes require complex infrastructure dependencies—such as network agents, storage drivers, GPU firmware, or custom health checks—to be fully operational before they can reliably host pods.</p><p>Today, on behalf of the Kubernetes project, I am announcing the <a href=\"https://node-readiness-controller.sigs.k8s.io/\">Node Readiness Controller</a>.\nThis project introduces a declarative system for managing node taints, extending the readiness guardrails during node bootstrapping beyond standard conditions.\nBy dynamically managing taints based on custom health signals, the controller ensures that workloads are only placed on nodes that met all infrastructure-specific requirements.</p><h2>Why the Node Readiness Controller?</h2><p>Core Kubernetes Node \"Ready\" status is often insufficient for clusters with sophisticated bootstrapping requirements. Operators frequently struggle to ensure that specific DaemonSets or local services are healthy before a node enters the scheduling pool.</p><p>The Node Readiness Controller fills this gap by allowing operators to define custom scheduling gates tailored to specific node groups. This enables you to enforce\ndistinct readiness requirements across heterogeneous clusters, ensuring for example, that GPU equipped nodes only accept pods once specialized drivers are verified,\nwhile general purpose nodes follow a standard path.</p><p>It provides three primary advantages:</p><ul><li><strong>Custom Readiness Definitions</strong>: Define what  means for your specific platform.</li><li><strong>Automated Taint Management</strong>: The controller automatically applies or removes node taints based on condition status, preventing pods from landing on unready infrastructure.</li><li><strong>Declarative Node Bootstrapping</strong>: Manage multi-step node initialization reliably, with a clear observability into the bootstrapping process.</li></ul><h2>Core concepts and features</h2><p>The controller centers around the NodeReadinessRule (NRR) API, which allows you to define declarative  for your nodes.</p><h3>Flexible enforcement modes</h3><p>The controller supports two distinct operational modes:</p><dl><dd>Actively maintains the readiness guarantee throughout the node’s entire lifecycle. If a critical dependency (like a device driver) fails later, the node is immediately tainted to prevent new scheduling.</dd><dd>Specifically for one-time initialization steps, such as pre-pulling heavy images or hardware provisioning. Once conditions are met, the controller marks the bootstrap as complete and stops monitoring that specific rule for the node.</dd></dl><p>The controller reacts to Node Conditions rather than performing health checks itself. This decoupled design allows it to integrate seamlessly with other tools existing in the ecosystem as well as custom solutions:</p><ul><li><strong>Readiness Condition Reporter</strong>: A lightweight agent provided by the project that can be deployed to periodically check local HTTP endpoints and patch node conditions accordingly.</li></ul><h3>Operational safety with dry run</h3><p>Deploying new readiness rules across a fleet carries inherent risk. To mitigate this,  mode allows operators to first simulate impact on the cluster.\nIn this mode, the controller logs intended actions and updates the rule's status to show affected nodes without applying actual taints, enabling safe validation before enforcement.</p><h2>Example: CNI bootstrapping</h2><p>The following NodeReadinessRule ensures a node remains unschedulable until its CNI agent is functional. The controller monitors a custom <code>cniplugin.example.net/NetworkReady</code> condition and only removes the <code>readiness.k8s.io/acme.com/network-unavailable</code> taint once the status is True.</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>The Node Readiness Controller is just getting started, with our <a href=\"https://github.com/kubernetes-sigs/node-readiness-controller/releases/tag/v0.1.1\">initial releases</a> out, and we are seeking community feedback to refine the roadmap. Following our productive Unconference discussions at KubeCon NA 2025, we are excited to continue the conversation in person.</p><p>In the meantime, you can contribute or track our progress here:</p>","contentLength":3917,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"New Conversion from cgroup v1 CPU Shares to v2 CPU Weight","url":"https://kubernetes.io/blog/2026/01/30/new-cgroup-v1-to-v2-cpu-conversion-formula/","date":1769788800,"author":"","guid":473,"unread":true,"content":"<p>I'm excited to announce the implementation of an improved conversion formula\nfrom cgroup v1 CPU shares to cgroup v2 CPU weight. This enhancement addresses\ncritical issues with CPU priority allocation for Kubernetes workloads when\nrunning on systems with cgroup v2.</p><p>Kubernetes was originally designed with cgroup v1 in mind, where CPU shares\nwere defined simply by assigning the container's CPU requests in millicpu\nform.</p><p>For example, a container requesting 1 CPU (1024m) would get (cpu.shares = 1024).</p><p>After a while, cgroup v1 started being replaced by its successor,\ncgroup v2. In cgroup v2, the concept of CPU shares (which ranges from 2 to\n262144, or from 2¹ to 2¹⁸) was replaced with CPU weight (which ranges from\n[1, 10000], or 10⁰ to 10⁴).</p><p>With the transition to cgroup v2,\n<a href=\"https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2254-cgroup-v2\">KEP-2254</a>\nintroduced a conversion formula to map cgroup v1 CPU shares to cgroup v2 CPU\nweight. The conversion formula was defined as: <code>cpu.weight = (1 + ((cpu.shares - 2) * 9999) / 262142)</code></p><p>This formula linearly maps values from [2¹, 2¹⁸] to [10⁰, 10⁴].</p><p>While this approach is simple, the linear mapping imposes a few significant\nproblems and impacts both performance and configuration granularity.</p><p>The current conversion formula creates two major issues:</p><h3>1. Reduced priority against non-Kubernetes workloads</h3><p>In cgroup v1, the default value for CPU shares is , meaning a container\nrequesting 1 CPU has equal priority with system processes that live outside\nof Kubernetes' scope.\nHowever, in cgroup v2, the default CPU weight is , but the current\nformula converts 1 CPU (1024m) to only  weight - less than 40% of the\ndefault.</p><ul><li>Container requesting 1 CPU (1024m)</li><li>cgroup v1:  (equal to default)</li><li>cgroup v2 (current):  (much lower than default 100)</li></ul><p>This means that after moving to cgroup v2, Kubernetes (or OCI) workloads would\nde-facto reduce their CPU priority against non-Kubernetes processes. The\nproblem can be severe for setups with many system daemons that run\noutside of Kubernetes' scope and expect Kubernetes workloads to have\npriority, especially in situations of resource starvation.</p><h3>2. Unmanageable granularity</h3><p>The current formula produces very low values for small CPU requests,\nlimiting the ability to create sub-cgroups within containers for\nfine-grained resource distribution (which will possibly be much easier moving\nforward, see <a href=\"https://github.com/kubernetes/enhancements/issues/5474\">KEP #5474</a> for more info).</p><ul><li>Container requesting 100m CPU</li><li>cgroup v1: </li><li>cgroup v2 (current):  (too low for sub-cgroup\nconfiguration)</li></ul><p>With cgroup v1, requesting 100m CPU which led to 102 CPU shares was manageable\nin the sense that sub-cgroups could have been created inside the main\ncontainer, assigning fine-grained CPU priorities for different groups of\nprocesses. With cgroup v2 however, having 4 shares is very hard to\ndistribute between sub-cgroups since it's not granular enough.</p><p>The new formula is more complicated, but does a much better job mapping\nbetween cgroup v1 CPU shares and cgroup v2 CPU weight:</p><div>$$cpu.weight = \\lceil 10^{(L^{2}/612 + 125L/612 - 7/34)} \\rceil, \\text{ where: } L = \\log_2(cpu.shares)$$</div><p>The idea is that this is a quadratic function to cross the following values:</p><ul><li>(2, 1): The minimum values for both ranges.</li><li>(1024, 100): The default values for both ranges.</li><li>(262144, 10000): The maximum values for both ranges.</li></ul><p>Visually, the new function looks as follows:</p><p>And if you zoom in to the important part:</p><p>The new formula is \"close to linear\", yet it is carefully designed to\nmap the ranges in a clever way so the three important points above would\ncross.</p><h3>How it solves the problems</h3><ol><li><p><strong>Better priority alignment:</strong></p><ul><li>A container requesting 1 CPU (1024m) will now get a . This\nvalue is close to cgroup v2's default 100.\nThis restores the intended priority relationship between Kubernetes\nworkloads and system processes.</li></ul></li><li><ul><li>A container requesting 100m CPU will get , (see\n<a href=\"https://go.dev/play/p/sLlAfCg54Eg\">here</a>).\nEnables better fine-grained resource distribution within containers.</li></ul></li></ol><p>This change was implemented at the OCI layer.\nIn other words, this is not implemented in Kubernetes itself; therefore the\nadoption of the new conversion formula depends solely on the OCI runtime\nadoption.</p><ul><li>runc: The new formula is enabled from version <a href=\"https://github.com/opencontainers/runc/releases/tag/v1.3.2\">1.3.2</a>.</li><li>crun: The new formula is enabled from version <a href=\"https://github.com/containers/crun/releases/tag/1.23\">1.23</a>.</li></ul><h3>Impact on existing deployments</h3><p> Some consumers may be affected if they assume the older linear conversion formula.\nApplications or monitoring tools that directly calculate expected CPU weight values based on the\nprevious formula may need updates to account for the new quadratic conversion.\nThis is particularly relevant for:</p><ul><li>Custom resource management tools that predict CPU weight values.</li><li>Monitoring systems that validate or expect specific weight values.</li><li>Applications that programmatically set or verify CPU weight values.</li></ul><p>The Kubernetes project recommends testing the new conversion formula in non-production\nenvironments before upgrading OCI runtimes to ensure compatibility with existing tooling.</p><p>For those interested in this enhancement:</p><p>For those interested in getting involved with Kubernetes node-level\nfeatures, join the <a href=\"https://github.com/kubernetes/community/tree/master/sig-node\">Kubernetes Node Special Interest Group</a>.\nWe always welcome new contributors and diverse perspectives on resource management\nchallenges.</p>","contentLength":5123,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Ingress NGINX: Statement from the Kubernetes Steering and Security Response Committees","url":"https://kubernetes.io/blog/2026/01/29/ingress-nginx-statement/","date":1769644800,"author":"","guid":472,"unread":true,"content":"<p><strong>In March 2026, Kubernetes will retire Ingress NGINX, a piece of critical infrastructure for about half of cloud native environments.</strong> The retirement of Ingress NGINX was <a href=\"https://kubernetes.io/blog/2025/11/11/ingress-nginx-retirement/\">announced</a> for March 2026, after years of <a href=\"https://groups.google.com/a/kubernetes.io/g/dev/c/rxtrKvT_Q8E/m/6_ej0c1ZBAAJ\">public warnings</a> that the project was in dire need of contributors and maintainers. There will be no more releases for bug fixes, security patches, or any updates of any kind after the project is retired. This cannot be ignored, brushed off, or left until the last minute to address. We cannot overstate the severity of this situation or the importance of beginning migration to alternatives like <a href=\"https://gateway-api.sigs.k8s.io/guides/getting-started/\">Gateway API</a> or one of the many <a href=\"https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/\">third-party Ingress controllers</a> immediately.</p><p>To be abundantly clear: choosing to remain with Ingress NGINX after its retirement leaves you and your users vulnerable to attack. None of the available alternatives are direct drop-in replacements. This will require planning and engineering time. Half of you will be affected. You have two months left to prepare.</p><p><strong>Existing deployments will continue to work, so unless you proactively check, you may not know you are affected until you are compromised.</strong> In most cases, you can check to find out whether or not you rely on Ingress NGINX by running <code>kubectl get pods --all-namespaces --selector app.kubernetes.io/name=ingress-nginx</code> with cluster administrator permissions.</p><p>Despite its broad appeal and widespread use by companies of all sizes, and repeated calls for help from the maintainers, the Ingress NGINX project never received the contributors it so desperately needed. According to internal Datadog research, about 50% of cloud native environments currently rely on this tool, and yet for the last several years, it has been maintained solely by one or two people working in their free time. Without sufficient staffing to maintain the tool to a standard both ourselves and our users would consider secure, the responsible choice is to wind it down and refocus efforts on modern alternatives like <a href=\"https://gateway-api.sigs.k8s.io/guides/getting-started/\">Gateway API</a>.</p><p>We did not make this decision lightly; as inconvenient as it is now, doing so is necessary for the safety of all users and the ecosystem as a whole. Unfortunately, the flexibility Ingress NGINX was designed with, that was once a boon, has become a burden that cannot be resolved. With the technical debt that has piled up, and fundamental design decisions that exacerbate security flaws, it is no longer reasonable or even possible to continue maintaining the tool even if resources did materialize.</p><p>We issue this statement together to reinforce the scale of this change and the potential for serious risk to a significant percentage of Kubernetes users if this issue is ignored. It is imperative that you check your clusters now. If you are reliant on Ingress NGINX, you must begin planning for migration.</p><p>Kubernetes Steering Committee</p><p>Kubernetes Security Response Committee</p>","contentLength":2864,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Experimenting with Gateway API using kind","url":"https://kubernetes.io/blog/2026/01/28/experimenting-gateway-api-with-kind/","date":1769558400,"author":"","guid":471,"unread":true,"content":"<p>This document will guide you through setting up a local experimental environment with <a href=\"https://gateway-api.sigs.k8s.io/\">Gateway API</a> on <a href=\"https://kind.sigs.k8s.io/\">kind</a>. This setup is designed for learning and testing. It helps you understand Gateway API concepts without production complexity.</p><div role=\"alert\">This is an experimentation learning setup, and should not be used for production. The components used on this document are not suited for production usage.\nOnce you're ready to deploy Gateway API in a production environment,\nselect an <a href=\"https://gateway-api.sigs.k8s.io/implementations/\">implementation</a> that suits your needs.</div><ul><li>Set up a local Kubernetes cluster using kind (Kubernetes in Docker)</li><li>Deploy <a href=\"https://github.com/kubernetes-sigs/cloud-provider-kind\">cloud-provider-kind</a>, which provides both LoadBalancer Services and a Gateway API controller</li><li>Create a Gateway and HTTPRoute to route traffic to a demo application</li><li>Test your Gateway API configuration locally</li></ul><p>This setup is ideal for learning, development, and experimentation with Gateway API concepts.</p><p>Before you begin, ensure you have the following installed on your local machine:</p><ul><li> - Required to run kind and cloud-provider-kind</li><li> - The Kubernetes command-line tool</li><li> - Kubernetes in Docker</li><li> - Required to test the routes</li></ul><p>Create a new kind cluster by running:</p><p>This will create a single-node Kubernetes cluster running in a Docker container.</p><h3>Install cloud-provider-kind</h3><ul><li>A LoadBalancer controller that assigns addresses to LoadBalancer-type Services</li><li>A Gateway API controller that implements the Gateway API specification</li></ul><p>It also automatically installs the Gateway API Custom Resource Definitions (CRDs) in your cluster.</p><p>Run cloud-provider-kind as a Docker container on the same host where you created the kind cluster:</p><div><pre tabindex=\"0\"><code data-lang=\"shell\"></code></pre></div><p> On some systems, you may need elevated privileges to access the Docker socket.</p><p>Verify that cloud-provider-kind is running:</p><div><pre tabindex=\"0\"><code data-lang=\"shell\"></code></pre></div><p>You should see the container listed and in a running state. You can also check the logs:</p><div><pre tabindex=\"0\"><code data-lang=\"shell\"></code></pre></div><h2>Experimenting with Gateway API</h2><p>Now that your cluster is set up, you can start experimenting with Gateway API resources.</p><p>cloud-provider-kind automatically provisions a GatewayClass called . You'll use this class to create your Gateway.</p><p>It is worth noticing that while kind is not a cloud provider, the project is named as  as it provides features that simulate a cloud-enabled environment.</p><p>The following manifest will:</p><ul><li>Create a new namespace called </li><li>Deploy a Gateway that listens on port 80</li><li>Accept HTTPRoutes with hostnames matching the  pattern</li><li>Allow routes from any namespace to attach to the Gateway.\n: In real clusters, prefer Same or Selector values on the <a href=\"https://gateway-api.sigs.k8s.io/reference/spec/#fromnamespaces\"> namespace selector</a> field to limit attachments.</li></ul><p>Apply the following manifest:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>Then verify that your Gateway is properly programmed and has an address assigned:</p><div><pre tabindex=\"0\"><code data-lang=\"shell\"></code></pre></div><pre tabindex=\"0\"><code>NAME CLASS ADDRESS PROGRAMMED AGE\ngateway cloud-provider-kind 172.18.0.3 True 5m6s\n</code></pre><p>The PROGRAMMED column should show True, and the ADDRESS field should contain an IP address.</p><h3>Deploy a demo application</h3><p>Next, deploy a simple echo application that will help you test your Gateway configuration. This application:</p><ul><li>Echoes back request details including path, headers, and environment variables</li><li>Runs in a namespace called </li></ul><p>Apply the following manifest:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>Now create an HTTPRoute to route traffic from your Gateway to the echo application.\nThis HTTPRoute will:</p><ul><li>Respond to requests for the hostname <code>some.exampledomain.example</code></li><li>Route traffic to the echo application</li><li>Attach to the Gateway in the  namespace</li></ul><p>Apply the following manifest:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>The final step is to test your route using curl. You'll make a request to the Gateway's IP address with the hostname <code>some.exampledomain.example</code>. The command below is for POSIX shell only, and may need to be adjusted for your environment:</p><div><pre tabindex=\"0\"><code data-lang=\"shell\"></code></pre></div><p>You should receive a JSON response similar to this:</p><div><pre tabindex=\"0\"><code data-lang=\"json\"></code></pre></div><p>If you see this response, congratulations! Your Gateway API setup is working correctly.</p><p>If something isn't working as expected, you can troubleshoot by checking the status of your resources.</p><p>First, inspect your Gateway resource:</p><div><pre tabindex=\"0\"><code data-lang=\"shell\"></code></pre></div><p>Look at the  section for conditions. Your Gateway should have:</p><ul><li> - The Gateway was accepted by the controller</li><li> - The Gateway was successfully configured</li><li> populated with an IP address</li></ul><h3>Check the HTTPRoute status</h3><p>Next, inspect your HTTPRoute:</p><div><pre tabindex=\"0\"><code data-lang=\"shell\"></code></pre></div><p>Check the  section for conditions. Common issues include:</p><ul><li>ResolvedRefs set to False with reason ; this means that the backend Service doesn't exist or has the wrong name</li><li>Accepted set to False; this means that the route couldn't attach to the Gateway (check namespace permissions or hostname matching)</li></ul><p>Example error when a backend is not found:</p><div><pre tabindex=\"0\"><code data-lang=\"yaml\"></code></pre></div><p>If the resource statuses don't reveal the issue, check the cloud-provider-kind logs:</p><div><pre tabindex=\"0\"><code data-lang=\"shell\"></code></pre></div><p>This will show detailed logs from both the LoadBalancer and Gateway API controllers.</p><p>When you're finished with your experiments, you can clean up the resources:</p><h3>Remove Kubernetes resources</h3><p>Delete the namespaces (this will remove all resources within them):</p><div><pre tabindex=\"0\"><code data-lang=\"shell\"></code></pre></div><p>Stop and remove the cloud-provider-kind container:</p><div><pre tabindex=\"0\"><code data-lang=\"shell\"></code></pre></div><p>Because the container was started with the  flag, it will be automatically removed when stopped.</p><p>Finally, delete the kind cluster:</p><p>Now that you've experimented with Gateway API locally, you're ready to explore production-ready implementations:</p><ul><li>: Explore the <a href=\"https://gateway-api.sigs.k8s.io/\">Gateway API documentation</a> to learn about advanced features like TLS, traffic splitting, and header manipulation</li><li>: Experiment with path-based routing, header matching, request mirroring and other features following <a href=\"https://gateway-api.sigs.k8s.io/guides/getting-started/\">Gateway API user guides</a></li></ul><p>This  setup is for development and learning only.\nAlways use a production-grade Gateway API implementation for real workloads.</p>","contentLength":5400,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Cluster API v1.12: Introducing In-place Updates and Chained Upgrades","url":"https://kubernetes.io/blog/2026/01/27/cluster-api-v1-12-release/","date":1769529600,"author":"","guid":470,"unread":true,"content":"<p><a href=\"https://cluster-api.sigs.k8s.io/\">Cluster API</a> brings declarative management to Kubernetes cluster lifecycle, allowing users and platform teams to define the desired state of clusters and rely on controllers to continuously reconcile toward it.</p><p>Similar to how you can use StatefulSets or Deployments in Kubernetes to manage a group of Pods, in Cluster API you can use KubeadmControlPlane to manage a set of control plane Machines, or you can use MachineDeployments to manage a group of worker Nodes.</p><p>The <a href=\"https://github.com/kubernetes-sigs/cluster-api/releases/tag/v1.12.0\">Cluster API v1.12.0</a> release expands what is possible in Cluster API, reducing friction in common lifecycle operations by introducing in-place updates and chained upgrades.</p><h2>Emphasis on simplicity and usability</h2><p>With v1.12.0, the Cluster API project demonstrates once again that this community is capable of delivering a great amount of innovation, while at the same time minimizing impact for Cluster API users.</p><p>What does this mean in practice?</p><p>Users simply have to change the <a href=\"https://cluster-api.sigs.k8s.io/user/concepts#cluster\">Cluster</a> or the <a href=\"https://cluster-api.sigs.k8s.io/user/concepts#machine\">Machine</a> spec (just as with previous Cluster API releases), and Cluster API will automatically trigger in-place updates or chained upgrades when possible and advisable.</p><p>Like Kubernetes does for Pods in Deployments, when the <a href=\"https://cluster-api.sigs.k8s.io/user/concepts#machine\">Machine</a> spec changes also Cluster API performs rollouts by creating a new Machine and deleting the old one.</p><p>This approach, inspired by the principle of immutable infrastructure, has a set of considerable advantages:</p><ul><li>It is simple to explain, predictable, consistent and easy to reason about with users and engineers.</li><li>It is simple to implement, because it relies only on two core primitives, create and delete.</li><li>Implementation does not depend on Machine-specific choices, like OS, bootstrap mechanism etc.</li></ul><p>As a result, Machine rollouts drastically reduce the number of variables to be considered when managing the lifecycle of a host server that is hosting Nodes.</p><p>However, while advantages of immutability are not under discussion, both Kubernetes and Cluster API are undergoing a similar journey, introducing changes that allow users to minimize workload disruption whenever possible.</p><p>Over time, also Cluster API has introduced several improvements to immutable rollouts, including:</p><p>The new <a href=\"https://github.com/kubernetes-sigs/cluster-api/blob/main/docs/proposals/20240807-in-place-updates.md\">in-place update</a> feature in Cluster API is the next step in this journey.</p><p>With the v1.12.0 release, Cluster API introduces support for <a href=\"https://cluster-api.sigs.k8s.io/tasks/experimental-features/runtime-sdk/implement-in-place-update-hooks\">update extensions</a> allowing users to make changes on existing machines in-place, without deleting and re-creating the Machines.</p><p>Both KubeadmControlPlane and MachineDeployments support in-place updates based on the new update extension, and this means that the boundary of what is possible in Cluster API is now changed in a significant way.</p><p>How do in-place updates work?</p><p>The simplest way to explain it is that once the user triggers an update by changing the desired state of Machines, then Cluster API chooses the best tool to achieve the desired state.</p><p>The news is that now Cluster API can choose between immutable rollouts and in-place update extensions to perform required changes.</p><p>Importantly, this is not immutable rollouts vs in-place updates; Cluster API considers both valid options and selects the most appropriate mechanism for a given change.</p><p>From the perspective of the Cluster API maintainers, in-place updates are most useful for making changes that don't otherwise require a node drain or pod restart; for example: changing user credentials for the Machine. On the other hand, when the workload will be disrupted anyway, just do a rollout.</p><p>Nevertheless, Cluster API remains true to its extensible nature, and everyone can create their own update extension and decide when and how to use in-place updates by trading in some of the benefits of immutable rollouts.</p><p><a href=\"https://cluster-api.sigs.k8s.io/tasks/experimental-features/cluster-class/\">ClusterClass</a> and managed topologies in Cluster API jointly provided a powerful and effective framework that acts as a building block for many platforms offering Kubernetes-as-a-Service.</p><p>Now with v1.12.0 this feature is making another important step forward, by allowing users to upgrade by more than one Kubernetes minor version in a single operation, commonly referred to as a <a href=\"https://github.com/kubernetes-sigs/cluster-api/blob/main/docs/proposals/20250513-chained-and-efficient-upgrades-for-clusters-with-managed-topologies.md\">chained upgrade</a>.</p><p>This allows users to declare a target Kubernetes version and let Cluster API safely orchestrate the required intermediate steps, rather than manually managing each minor upgrade.</p><p>The simplest way to explain how chained upgrades work, is that once the user triggers an update by changing the desired version for a Cluster, Cluster API computes an upgrade plan, and then starts executing it. Rather than (for example) update the Cluster to v1.33.0 and then v1.34.0 and then v1.35.0, checking on progress at each step, a chained upgrade lets you go directly to v1.35.0.</p><p>Executing an upgrade plan means upgrading control plane and worker machines in a strictly controlled order, repeating this process as many times as needed to reach the desired state. The Cluster API is now capable of managing this for you.</p><p>Cluster API takes care of optimizing and minimizing the upgrade steps for worker machines, and in fact worker machines will skip upgrades to intermediate Kubernetes minor releases whenever allowed by the Kubernetes version skew policies.</p><p>Also in this case extensibility is at the core of this feature, and <a href=\"https://cluster-api.sigs.k8s.io/tasks/experimental-features/runtime-sdk/implement-upgrade-plan-hooks\">upgrade plan runtime extensions</a> can be used to influence how the upgrade plan is computed; similarly, <a href=\"https://cluster-api.sigs.k8s.io/tasks/experimental-features/runtime-sdk/implement-lifecycle-hooks\">lifecycle hooks</a> can be used to automate other tasks that must be performed during an upgrade, e.g. upgrading an addon after the control plane update completed.</p><p>From our perspective, chained upgrades are most useful for users that struggle to keep up with Kubernetes minor releases, and e.g. they want to upgrade only once per year and then upgrade by three versions (n-3 → n). But be warned: the fact that you can now easily upgrade by more than one minor version is not an excuse to not patch your cluster frequently!</p><p>I would like to thank all the contributors, the maintainers, and all the engineers that volunteered for the release team.</p><p>The reliability and predictability of Cluster API releases, which is one of the most appreciated features from our users, is only possible with the support, commitment, and hard work of its community.</p><p>Kudos to the entire Cluster API community for the v1.12.0 release and all the great releases delivered in 2025!\n​​\nIf you are interested in getting involved, learn about\n<a href=\"https://cluster-api.sigs.k8s.io/contributing\">Cluster API contributing guidelines</a>.</p><p>If you read the <a href=\"https://cluster-api.sigs.k8s.io/user/manifesto\">Cluster API manifesto</a>, you can see how the Cluster API subproject claims the right to remain unfinished, recognizing the need to continuously evolve, improve, and adapt to the changing needs of Cluster API’s users and the broader Cloud Native ecosystem.</p><p>As Kubernetes itself continues to evolve, the Cluster API subproject will keep advancing alongside it, focusing on safer upgrades, reduced disruption, and stronger building blocks for platforms managing Kubernetes at scale.</p><p>Innovation remains at the heart of Cluster API, stay tuned for an exciting 2026!</p>","contentLength":6886,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Headlamp in 2025: Project Highlights","url":"https://kubernetes.io/blog/2026/01/22/headlamp-in-2025-project-highlights/","date":1769047200,"author":"","guid":469,"unread":true,"content":"<p><em>This announcement is a recap from a post originally <a href=\"https://headlamp.dev/blog/2025/11/13/headlamp-in-2025\">published</a> on the Headlamp blog.</em></p><p><a href=\"https://headlamp.dev/\">Headlamp</a> has come a long way in 2025. The project has continued to grow – reaching more teams across platforms, powering new workflows and integrations through plugins, and seeing increased collaboration from the broader community.</p><p>We wanted to take a moment to share a few updates and highlight how Headlamp has evolved over the past year.</p><h3>Joining Kubernetes SIG UI</h3><p>This year marked a big milestone for the project: Headlamp is now officially part of Kubernetes <a href=\"https://github.com/kubernetes/community/blob/master/sig-ui/README.md\">SIG UI</a>. This move brings roadmap and design discussions even closer to the core Kubernetes community and reinforces Headlamp’s role as a modern, extensible UI for the project.</p><h3>Linux Foundation mentorship</h3><p>This year, we were excited to work with several students through the Linux Foundation’s Mentorship program, and our mentees have already left a visible mark on Headlamp:</p><ul><li><a href=\"https://github.com/adwait-godbole\"></a> built the KEDA plugin, adding a UI in Headlamp to view and manage KEDA resources like ScaledObjects and ScaledJobs.</li><li><a href=\"https://github.com/DhairyaMajmudar\"></a> set up an OpenTelemetry-based observability stack for Headlamp, wiring up metrics, logs, and traces so the project is easier to monitor and debug.</li><li><a href=\"https://www.linkedin.com/in/aishwarya-ghatole-506745231/\"></a> led a UX audit of Headlamp plugins, identifying usability issues and proposing design improvements and personas for plugin users.</li><li><a href=\"https://github.com/SinghaAnirban005\"></a> developed the Karpenter plugin, giving Headlamp a focused view into Karpenter autoscaling resources and decisions.</li><li><a href=\"https://github.com/useradityaa\"></a> improved Gateway API support, so you can see networking relationships on the resource map, as well as improved support for many of the new Gateway API resources.</li><li><a href=\"https://github.com/upsaurav12\"></a> worked on backend caching for Kubernetes API calls, reducing load on the API server and improving performance in Headlamp.</li></ul><p>Managing multiple clusters is challenging: teams often switch between tools and lose context when trying to see what runs where. Headlamp solves this by giving you a single view to compare clusters side-by-side. This makes it easier to understand workloads across environments and reduces the time spent hunting for resources.</p><figure><img src=\"https://kubernetes.io/blog/2026/01/22/headlamp-in-2025-project-highlights/multi-cluster-view.png\" alt=\"Multi-cluster view\"><figcaption><p>View of multi-cluster workloads</p></figcaption></figure><p>Kubernetes apps often span multiple namespaces and resource types, which makes troubleshooting feel like piecing together a puzzle. We’ve added  to give you an application-centric view that groups related resources across multiple namespaces – and even clusters. This allows you to reduce sprawl, troubleshoot faster, and collaborate without digging through YAML or cluster-wide lists.</p><figure><img src=\"https://kubernetes.io/blog/2026/01/22/headlamp-in-2025-project-highlights/projects-feature.png\" alt=\"Projects feature\"><figcaption><p>View of the new Projects feature</p></figcaption></figure><ul><li>New “Projects” feature for grouping namespaces into app- or team-centric projects</li><li>Extensible Projects details view that plugins can customize with their own tabs and actions</li></ul><h3>Navigation and Activities</h3><p>Day-to-day ops in Kubernetes often means juggling logs, terminals, YAML, and dashboards across clusters. We redesigned Headlamp’s navigation to treat these as first-class “activities” you can keep open and come back to, instead of one-off views you lose as soon as you click away.</p><figure><img src=\"https://kubernetes.io/blog/2026/01/22/headlamp-in-2025-project-highlights/new-task-bar.png\" alt=\"New task bar\"></figure><ul><li>A new task bar/activities model lets you pin logs, exec sessions, and details as ongoing activities</li><li>An activity overview with a “Close all” action and cluster information</li><li>Multi-select and global filters in tables</li></ul><p>When something breaks in production, the first two questions are usually “where is it?” and “what is it connected to?” We’ve upgraded both search and the map view so you can get from a high-level symptom to the right set of objects much faster.</p><figure><img src=\"https://kubernetes.io/blog/2026/01/22/headlamp-in-2025-project-highlights/advanced-search.png\" alt=\"Advanced search\"><figcaption><p>View of the new Advanced Search feature</p></figcaption></figure><ul><li>An Advanced search view that supports rich, expression-based queries over Kubernetes objects</li><li>Improved global search that understands labels and multiple search items, and can even update your current namespace based on what you find</li><li>EndpointSlice support in the Network section</li><li>A richer map view that now includes Custom Resources and Gateway API objects</li></ul><p>We’ve put real work into making OIDC setup clearer and more resilient, especially for in-cluster deployments.</p><figure><img src=\"https://kubernetes.io/blog/2026/01/22/headlamp-in-2025-project-highlights/user-info.png\" alt=\"User info\"><figcaption><p>View of user information for OIDC clusters</p></figcaption></figure><ul><li>User information displayed in the top bar for OIDC-authenticated users</li><li>PKCE support for more secure authentication flows, as well as hardened token refresh handling</li><li>Documentation for using the access token using <code>-oidc-use-access-token=true</code></li><li>Improved support for public OIDC clients like AKS and EKS</li></ul><p>We’ve broadened how you deploy and source apps via Headlamp, specifically supporting vanilla Helm repos.</p><ul><li>A more capable Helm chart with optional backend TLS termination, PodDisruptionBudgets, custom pod labels, and more</li><li>Improved formatting and added missing access token arg in the Helm chart</li><li>New in-cluster Helm support with an  flag and a service proxy</li></ul><p>Finally, we’ve spent a lot of time on the things you notice every day but don’t always make headlines: startup time, list views, log viewers, accessibility, and small network UX details. A continuous accessibility self-audit has also helped us identify key issues and make Headlamp easier for everyone to use.</p><figure><img src=\"https://kubernetes.io/blog/2026/01/22/headlamp-in-2025-project-highlights/learn-section.png\" alt=\"Learn section\"><figcaption><p>View of the Learn section in docs</p></figcaption></figure><ul><li>Significant desktop improvements, with up to 60% faster app loads and much quicker dev-mode reloads for contributors</li><li>Numerous table and log viewer refinements: persistent sort order, consistent row actions, copy-name buttons, better tooltips, and more forgiving log inputs</li><li>Accessibility and localization improvements, including fixes for zoom-related layout issues, better color contrast, improved screen reader support, and expanded language coverage</li><li>More control over resources, with live pod CPU/memory metrics, richer pod details, and inline editing for secrets and CRD fields</li><li>A refreshed documentation and plugin onboarding experience, including a “Learn” section and plugin showcase</li><li>A more complete NetworkPolicy UI and network-related polish</li><li>Nightly builds available for early testing</li></ul><h2>Plugins and extensibility</h2><p>Discovering plugins is simpler now – no more hopping between Artifact Hub and assorted GitHub repos. Browse our dedicated <a href=\"https://headlamp.dev/plugins\">Plugins page</a> for a curated catalog of Headlamp-endorsed plugins, along with a showcase of featured plugins.</p><figure><img src=\"https://kubernetes.io/blog/2026/01/22/headlamp-in-2025-project-highlights/plugins-page.png\" alt=\"Plugins page\"><figcaption><p>View of the Plugins showcase</p></figcaption></figure><p>Managing Kubernetes often means memorizing commands and juggling tools. Headlamp’s new AI Assistant changes this by adding a natural-language interface built into the UI. Now, instead of typing  or digging through YAML you can ask, “Is my app healthy?” or “Show logs for this deployment,” and get answers in context, speeding up troubleshooting and smoothing onboarding for new users. Learn more about it <a href=\"https://headlamp.dev/blog/2025/08/07/introducing-the-headlamp-ai-assistant/\">here</a>.</p><p>Alongside the new AI Assistant, we’ve been growing Headlamp’s plugin ecosystem so you can bring more of your workflows into a single UI, with integrations like Minikube, Karpenter, and more.</p><p>Highlights from the latest plugin releases:</p><ul><li>Minikube plugin, providing a locally stored single node Minikube cluster</li><li>Karpenter plugin, with support for Azure Node Auto-Provisioning (NAP)</li><li>KEDA plugin, which you can learn more about <a href=\"https://headlamp.dev/blog/2025/07/25/enabling-event-driven-autoscaling-with-the-new-keda-plugin-for-headlamp/\">here</a></li></ul><p>Alongside new additions, we’ve also spent time refining plugins that many of you already use, focusing on smoother workflows and better integration with the core UI.</p><figure><img src=\"https://kubernetes.io/blog/2026/01/22/headlamp-in-2025-project-highlights/backstage-plugin.png\" alt=\"Backstage plugin\"><figcaption><p>View of the Backstage plugin</p></figcaption></figure><ul><li>: Updated for Flux v2.7, with support for newer CRDs, navigation fixes so it works smoothly on recent clusters</li><li>: Now supports Helm repos in addition to Artifact Hub, can run in-cluster via /serviceproxy, and shows both current and latest app versions</li><li>: Improved card layout and accessibility, plus dependency and Storybook test updates</li><li>: Dependency and build updates, more info <a href=\"https://headlamp.dev/blog/2025/11/05/strengthening-backstage-and-headlamp-integration/\">here</a></li></ul><p>We’ve focused on making it faster and clearer to build, test, and ship Headlamp plugins, backed by improved documentation and lighter tooling.</p><figure><img src=\"https://kubernetes.io/blog/2026/01/22/headlamp-in-2025-project-highlights/plugin-development.png\" alt=\"Plugin development\"><figcaption><p>View of the Plugin Development guide</p></figcaption></figure><ul><li>Improved type checking for Headlamp APIs, restored Storybook support for component testing, and reduced dependencies for faster installs and fewer updates</li><li>Documented plugin install locations, UI signifiers in Plugin Settings, and labels that differentiated shipped, UI-installed, and dev-mode plugins</li></ul><p>We've also been investing in keeping Headlamp secure – both by tightening how authentication works and by staying on top of upstream vulnerabilities and tooling.</p><ul><li>We've been keeping up with security updates, regularly updating dependencies and addressing upstream security issues.</li><li>We tightened the Helm chart's default security context and fixed a regression that broke the plugin manager.</li><li>We've improved OIDC security with PKCE support, helping unblock more secure and standards-compliant OIDC setups when deploying Headlamp in-cluster.</li></ul><p>Thank you to everyone who has contributed to Headlamp this year – whether through pull requests, plugins, or simply sharing how you're using the project. Seeing the different ways teams are adopting and extending the project is a big part of what keeps us moving forward. If your organization uses Headlamp, consider adding it to our <a href=\"https://github.com/kubernetes-sigs/headlamp/blob/main/ADOPTERS.md\">adopters list</a>.</p><p>If you haven't tried Headlamp recently, all these updates are available today. Check out the latest Headlamp release, explore the new views, plugins, and docs, and share your feedback with us on Slack or GitHub – your feedback helps shape where Headlamp goes next.</p>","contentLength":9105,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Announcing the Checkpoint/Restore Working Group","url":"https://kubernetes.io/blog/2026/01/21/introducing-checkpoint-restore-wg/","date":1769018400,"author":"","guid":468,"unread":true,"content":"<p>The community around Kubernetes includes a number of Special Interest Groups (SIGs) and Working Groups (WGs) facilitating discussions on important topics between interested contributors. Today we would like to announce the new <a href=\"https://github.com/kubernetes/community/tree/master/wg-checkpoint-restore\">Kubernetes Checkpoint Restore WG</a> focusing on the integration of Checkpoint/Restore functionality into Kubernetes.</p><p>There are several high-level scenarios discussed in the working group:</p><ul><li>Optimizing resource utilization for interactive workloads, such as Jupyter notebooks and AI chatbots</li><li>Accelerating startup of applications with long initialization times, including Java applications and <a href=\"https://doi.org/10.1145/3731599.3767354\">LLM inference services</a></li><li>Using periodic checkpointing to enable fault-tolerance for long-running workloads, such as distributed model training</li><li>Providing <a href=\"https://doi.org/10.1007/978-3-032-10507-3_3\">interruption-aware scheduling</a> with transparent checkpoint/restore, allowing lower-priority Pods to be preempted while preserving the runtime state of applications</li><li>Facilitating Pod migration across nodes for load balancing and maintenance, without disrupting workloads.</li><li>Enabling forensic checkpointing to investigate and analyze security incidents such as cyberattacks, data breaches, and unauthorized access.</li></ul><p>Across these scenarios, the goal is to help facilitate discussions of ideas between the Kubernetes community and the growing Checkpoint/Restore in Userspace (CRIU) ecosystem. The CRIU community includes several projects that support these use cases, including:</p><ul><li><a href=\"https://github.com/checkpoint-restore/criu\">CRIU</a> - A tool for checkpointing and restoring running applications and containers</li><li><a href=\"https://github.com/checkpoint-restore/checkpointctl\">checkpointctl</a> - A tool for in-depth analysis of container checkpoints</li><li><a href=\"https://github.com/checkpoint-restore/criu-coordinator\">criu-coordinator</a> - A tool for coordinated checkpoint/restore of distributed applications with CRIU</li></ul><p>More information about the checkpoint/restore integration with Kubernetes is also available <a href=\"https://criu.org/Kubernetes\">here</a>.</p><p>If you are interested in contributing to Kubernetes or CRIU, there are several ways to participate:</p>","contentLength":1870,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null},{"title":"Uniform API server access using clientcmd","url":"https://kubernetes.io/blog/2026/01/19/clientcmd-apiserver-access/","date":1768845600,"author":"","guid":467,"unread":true,"content":"<p>If you've ever wanted to develop a command line client for a Kubernetes API,\nespecially if you've considered making your client usable as a  plugin,\nyou might have wondered how to make your client feel familiar to users of .\nA quick glance at the output of  might put a damper on that:\n\"Am I really supposed to implement all those options?\"</p><p>Fear not, others have done a lot of the work involved for you.\nIn fact, the Kubernetes project provides two libraries to help you handle\n-style command line arguments in Go programs:\n<a href=\"https://pkg.go.dev/k8s.io/client-go/tools/clientcmd\"></a> and\n<a href=\"https://pkg.go.dev/k8s.io/cli-runtime\"></a>\n(which uses ).\nThis article will show how to use the former.</p><p>As might be expected since it's part of ,\n's ultimate purpose is to provide an instance of\n<a href=\"https://pkg.go.dev/k8s.io/client-go/rest#Config\"></a>\nthat can issue requests to an API server.</p><p>It follows  semantics:</p><ul><li>defaults are taken from  or equivalent;</li><li>files can be specified using the  environment variable;</li><li>all of the above settings can be further overridden using command line arguments.</li></ul><p>It doesn't set up a  command line argument,\nwhich you might want to do to align with ;\nyou'll see how to do this\nin the <a href=\"https://kubernetes.io/blog/2026/01/19/clientcmd-apiserver-access/#bind-the-flags\">\"Bind the flags\"</a> section.</p><p> allows programs to handle</p><ul><li> selection (using );</li><li>client certificates and private keys;</li><li>HTTP Basic authentication support (username/password).</li></ul><p>In various scenarios,  supports  configuration settings:\n can specify multiple files whose contents are combined.\nThis can be confusing, because settings are merged in different directions\ndepending on how they are implemented.\nIf a setting is defined in a map, the first definition wins,\nsubsequent definitions are ignored.\nIf a setting is not defined in a map, the last definition wins.</p><p>When settings are retrieved using ,\nmissing files result in warnings only.\nIf the user explicitly specifies a path (in  style),\nthere must be a corresponding file.</p><p>If  isn't defined,\nthe default configuration file, , is used instead,\nif present.</p><p>The general usage pattern is succinctly expressed in\nthe <a href=\"https://pkg.go.dev/k8s.io/client-go/tools/clientcmd\"></a> package documentation:</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><p>In the context of this article, there are six steps:</p><h3>Configure the loading rules</h3><p><code>clientcmd.NewDefaultClientConfigLoadingRules()</code> builds loading rules which will use either the contents of the  environment variable,\nor the default configuration file name ().\nIn addition, if the default configuration file is used,\nit is able to migrate settings from the (very) old default configuration file\n().</p><p>You can build your own ,\nbut in most cases the defaults are fine.</p><p><code>clientcmd.ConfigOverrides</code> is a  storing overrides which will be applied over the settings loaded from the configuration derived using the loading rules.\nIn the context of this article,\nits primary purpose is to store values obtained from command line arguments.\nThese are handled using the <a href=\"https://github.com/spf13/pflag\">pflag</a> library,\nwhich is a drop-in replacement for Go's <a href=\"https://pkg.go.dev/flag\"></a> package,\nadding support for double-hyphen arguments with long names.</p><p>In most cases there's nothing to set in the overrides;\nI will only bind them to flags.</p><p>In this context, a flag is a representation of a command line argument,\nspecifying its long name (such as ),\nits short name if any (such as ),\nits default value,\nand a description shown in the usage information.\nFlags are stored in instances of\nthe <a href=\"https://pkg.go.dev/k8s.io/client-go/tools/clientcmd#FlagInfo\"></a> struct.</p><p>Three sets of flags are available,\nrepresenting the following command line arguments:</p><ul><li>authentication arguments (certificates, tokens, impersonations, username/password);</li><li>cluster arguments (API server, certificate authority, TLS configuration, proxy, compression)</li><li>context arguments (cluster name,  user name, namespace)</li></ul><p>The recommended selection includes all three with a named context selection argument and a timeout argument.</p><p>These are all available using the  functions.\nThe functions take a prefix, which is prepended to all the argument long names.</p><p>So calling\n<a href=\"https://pkg.go.dev/k8s.io/client-go/tools/clientcmd#RecommendedConfigOverrideFlags\"><code>clientcmd.RecommendedConfigOverrideFlags(\"\")</code></a>\nresults in command line arguments such as , , and so on.\nThe  argument is given a default value of 0,\nand the  argument has a corresponding short variant, .\nAdding a prefix, such as , results in command line arguments such as\n, , etc.\nThis might not seem particularly useful on commands involving a single API server,\nbut they come in handy when multiple API servers are involved,\nsuch as in multi-cluster scenarios.</p><p>There's a potential gotcha here: prefixes don't modify the short name,\nso  needs some care if multiple prefixes are used:\nonly one of the prefixes can be associated with the  short name.\nYou'll have to clear the short names associated with the other prefixes'\n , or perhaps all prefixes if there's no sensible\n association.\nShort names can be cleared as follows:</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><p>In a similar fashion, flags can be disabled entirely by clearing their long name:</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><p>Once a set of flags has been defined,\nit can be used to bind command line arguments to overrides using\n<a href=\"https://pkg.go.dev/k8s.io/client-go/tools/clientcmd#BindOverrideFlags\"><code>clientcmd.BindOverrideFlags</code></a>.\nThis requires a\n<a href=\"https://pkg.go.dev/github.com/spf13/pflag\"></a>\nrather than one from Go's  package.</p><p>If you also want to bind , you should do so now,\nby binding  in the loading rules:</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><h3>Build the merged configuration</h3><p>Two functions are available to build a merged configuration:</p><p>As the names suggest, the difference between the two is that the first\ncan ask for authentication information interactively,\nusing a provided reader,\nwhereas the second only operates on the information given to it by the caller.</p><p>The \"deferred\" mention in these function names refers to the fact that\nthe final configuration will be determined as late as possible.\nThis means that these functions can be called before the command line arguments are parsed,\nand the resulting configuration will use whatever values have been parsed\nby the time it's actually constructed.</p><p>The merged configuration is returned as a\n<a href=\"https://pkg.go.dev/k8s.io/client-go/tools/clientcmd#ClientConfig\"></a> instance.\nAn API client can be obtained from that by calling the  method.</p><p>If no configuration is given\n( is empty or points to non-existent files,\n doesn't exist,\nand no configuration is given using command line arguments),\nthe default setup will return an obscure error referring to .\nThis is legacy behaviour;\nseveral attempts have been made to get rid of it,\nbut it is preserved for the  and  command line arguments in .\nYou should check for \"empty configuration\" errors by calling <code>clientcmd.IsEmptyConfig()</code>\nand provide a more explicit error message.</p><p>The  method is also useful:\nit returns the namespace that should be used.\nIt also indicates whether the namespace was overridden by the user\n(using ).</p><p>Here's a complete example.</p><div><pre tabindex=\"0\"><code data-lang=\"go\"></code></pre></div><p>Happy coding, and thank you for your interest in implementing tools with\nfamiliar usage patterns!</p>","contentLength":6381,"flags":null,"enclosureUrl":"","enclosureMime":"","commentsUrl":null}],"tags":["dev","k8s"]}