<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Dev News</title><link>https://konrad.website/liveboat-github-runner/</link><description></description><item><title>Station Amsterdam Bijlmer ArenA</title><link>https://www.reddit.com/r/linux/comments/1j9pgmq/station_amsterdam_bijlmer_arena/</link><author>/u/nicq88</author><category>dev</category><category>reddit</category><pubDate>Wed, 12 Mar 2025 17:24:12 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Carefully But Purposefully Oxidising Ubuntu</title><link>https://discourse.ubuntu.com/t/carefully-but-purposefully-oxidising-ubuntu/56995</link><author>/u/urzop</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Wed, 12 Mar 2025 16:31:18 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>HTTP/3 is everywhere but nowhere</title><link>https://httptoolkit.com/blog/http3-quic-open-source-support-nowhere/</link><author>/u/pimterry</author><category>dev</category><category>reddit</category><pubDate>Wed, 12 Mar 2025 16:27:01 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[We've developed a totally new version of HTTP, and we're on track to migrate more than 1/3 of web traffic to it already! This is astonishing progress.At the same time, neither QUIC nor HTTP/3 are included in the standard libraries of any major languages including Node.js, Go, Rust, Python or Ruby. Curl recently gained support but it's experimental and disabled in most distributions. There are a rare few external libraries for some languages, but all are experimental and/or independent of other core networking APIs. Despite mobile networking being a key use case for HTTP/3, Android's most popular HTTP library has no support. Popular servers like Nginx have only experimental support, disabled by default, Apache has no support or published plan for support, and Ingress-Nginx (arguably the most popular Kubernetes reverse proxy) has dropped all plans for HTTP/3 support punting everything to a totally new (as yet unreleased) successor project instead.Really it's hard to point to any popular open-source tools that fully support HTTP/3: rollout has barely even started.This seems contradictory. What's going on?Why do we need more than HTTP/1.1?Let's step back briefly. Why does this matter? Who cares about whether HTTP/3 is being rolled out successfully or not? If browser traffic and the big CDNs support HTTP/3, do we even need it in other client or server implementations?You could make much the same argument for HTTP/3: this is useful for the high-latency many-requests traffic of web browsers & CDNs, but irrelevant elsewhere.Even just considering HTTP/1.1 vs HTTP/2 though, the reality of multiplexing benefits is more complicated:Latency of responses isn't just network RTT: a slow server response because of server processing will block your TCP connection just as hard as network latency.Your load balancer is often  co-located with your backend, e.g. if you serve everything through a geographically distributed CDN, which serves most content from its cache, but falls back to a separate application server backend for dynamic content & cache misses.Long-lived TCP connections die. Networking can fail in a million ways, even within data centers, and 'keep-alive' is a desperate plea at best. Even HTTP itself will force this: there's cases like a response body failing half-way through that are unrecoverable in HTTP/1.1 without killing the connection entirely.Any spikes in traffic mean you'll end up with the wrong number of TCP connections one way or the other: either you need an enormous unused pool available at all times, or you'll need to open a flood of new connections as traffic spikes come in, and so deal with TCP slow start, RTT & extra TLS handshakes as you do so.There's a lot of traffic that's not websites  not just within in datacenters. Mobile apps definitely do have network latency issues, API servers absolutely do have slow endpoints that can block up open connections, and IoT is a world built almost exclusively of unreliable networks and performance challenges. All of these cases get value from HTTP/2 & HTTP/3.Moving beyond multiplexing, there's plenty of other HTTP/2 benefits that apply beyond just load balancers & browsers:HTTP header compression (HPACK and QPACK in HTTP/2 & HTTP/3 respectively) makes for a significant reduction in traffic in many cases,  on long-lived connections such as within internal infrastructure. This is useful on the web, but can be an even bigger boost on mobile & IoT scenarios where networks are limited and unrealiable.Bidirectional HTTP request & response streaming (only possible in HTTP/2 & HTTP/3) enables entirely different communication patterns. Most notably used in gRPC (which  HTTP/2 for most usage) this means the client and server can exchange continuous data within the same 'request' at the same time, acting similarly to a websocket but within existing HTTP semantics.Both protocols support advanced prioritization support, allowing clients to indicate priority of requests to servers, to more efficiently allocate processing & receive the data they need most urgently. This is valuable for clients, but also between the load balancer and server: a cache miss for a waiting client has a very different priority to an optional background cache revalidation.All that is just for HTTP/2. HTTP/3 improves on this yet further, with:Significantly increased resilience to unreliable networks. By moving away from TCP's strict packet ordering, HTTP/3 makes each stream within the connection fully independent, so a missed packet on one stream doesn't slow down another stream.Zero round-trip connection initialization. TLS1.3 allows zero round-trip connection setup when resuming a connection to a known server, so you don't need to wait for the TLS handshake before sending application data. For HTTP/1 & HTTP/2 though, you still need a TCP handshake first. With QUIC, you can do 0RTT TLS handshakes, meaning you can connect to a server and send an HTTP request immediately, without waiting for a single packet in response, so there's no unnecessary RTT delay whatsoever.Reductions in traffic size, connection count, and network round trips that can result in reduced battery use for clients and reduced processing, latency & bandwidth for servers.Support for connection migration allowing a client to continue the same connection even as its IP address changes, and in theory even supporting multi-path connections using multiple addresses (e.g. a single connection to a server using both WiFi & cellular at the same time for extra bandwidth/reliability) in future.Improved network congestion handling by moving away from TCP: QUIC can use Bottleneck Bandwidth and RTT (BBR) for improved congestion control via active detection of network conditions, includes timestamps in each packet to help measure RTT, has improved detection of and recovery from packet loss, has better support for explicit congestion notifications (ECN) to actively manage congestion before packet loss, and may gain forward-error correction (FEC) support in future too.Support for WebTransport, a new protocol providing bidirectional full-duplex connections (similar to WebSockets) but supporting multiplexed streams to avoid head-of-line blocking, fixing various legacy WebSocket limitations (like incompatibility with CORS), and allowing streams to be unreliable and unordered - effectively providing UDP-style guarantees and lower-latency within web-compatible stream connections.In addition to the theory, there's some concrete measureable benefits being reported already. RequestMetric ran some detailed benchmarks showing some astonishing performance improvements for example:And Fastly shared the major improvements in time-to-first-byte they're seeing in the real world:This all very clearly Good Stuff.Now that the technology is standardized, widely supported in browsers & CDNs and thoroughly battle-tested, I think it's clear that  developers should be able to get these benefits built into their languages, servers & frameworks.That's not what's happened though: despite its benefits and frequent use in network traffic, most developers can't easily start using HTTP/3 end-to-end today. In this, HTTP/3 has thrown a long-growing divide on the internet into sharp relief. Nowadays, there's two very different kinds of web traffic:Major browsers plus some very-specific mobile app traffic, where a small set of finely tuned & automatic-updating clients talk to a small set of very big servers, with as much traffic as possible handled by either an enormous CDN (Cloudflare, Akamai, Fastly, CloudFront) and/or significant in-house infra (Google, Meta, Amazon, Microsoft).Everybody else: backend API clients & servers, every other mobile app, every smaller CDN, websites without a CDN, desktop apps, IoT, bots & indexers & scrapers, niche web browsers, self-hosted homelabs, CLI tools & scripts, students learning about network protocols, you name it.Let's simplify a bit, and describe these two cases as 'hyperscale web' and 'long-tail web'. Both groups are building on the same basic standards, but they have very different focuses and needs, and increasingly different tools & platforms. This has been true for quite a while, but the reality of HTTP/3 makes it painfully clear.There's a few notable differences in these groups:The long-tail world is fragmented into different implementations, almost by definition. Most of the biggest implementations are open-source organizations with relatively little direct funding or full-time engineering power available, and much work is done by volunteers with no mandated central direction or clear focus.The hyperscale world is controlled by a relatively small number of key stakeholders on both client & server side (you can count the relevant companies without taking your socks off). This lets them agree standards to fit their needs quickly & effectively - literally putting a representative of every implementation in the same room.The hyperscale ecosystem has far more concentrated cash & motivations. It's a small number of players comprising some of the most valuable companies in the world, with business models that tie milliseconds of web performance directly to their bottom line.The long-tail is completely dependent on open-source implementations and shared code. If you want to build a new mobile app tomorrow, you obviously should not start by building an HTTP parser.The hyperscale ecosystem isn't worried about access to open-source implementations at all. They have sufficient engineering resources and complicated use cases that building their own custom implementation of a protocol from scratch can make perfect sense. Google.com is not going to be served by an Apache module with default settings, and Instagram is not sending requests with the same HTTP library as a Hello-World app.The combination of hyperscale's evergreen clients plus money & motivation plus tight links between implementers and the business using the tools, means they can move fast to quickly build, ship & iterate new approaches.Long-tail implementations are only updated relatively rarely (how many outdated Apache servers are there on the web?) and the maintainers are a tiny subset of the users, who care significantly about stability and avoiding breaking changes. Long-tail tool maintainers  just move fast and break things.You can see the picture I'm painting. These two groups exist on the same web, but in very different ways.Some of this might sound like the hyperscale gang are the nefarious baddies. That is not what I mean (fine, yes, there's an interesting conversation there more broadly, but talking strictly about network protocols here). Regarding HTTP/3 specifically, this is some  engineering work that is solving real problems on the web, through some astonishingly tidy cooperation on open standards across different massive organizations. That's great!There are many  people using services built by these companies, and their obsession with web performance is improving the quality of those services for large numbers of real people every day. This is very cool.However, this would be much cooler if it was accessible to every other server, client & developer too. Most notably, this means the next generation of web technology is being defined & built by one minority group, and the larger majority have effectively no way to access this technology right now (despite years of standardization and development) other than paying money to the CDNs of that first minority group to help. Not cool.I think the hyperscaler/long-tail divide is the fundamental cause here, but that's created quite a few more concrete issues downstream, the most notable of which is OpenSSL's approach to QUIC.BoringSSL shipped a usable API for QUIC implementations back in 2018.OpenSSL did not, so various forks like QuicTLS appeared, providing OpenSSL plus BoringSSL's QUIC API.An ecosystem of QUIC & HTTP/3 implementations (most notably Quiche, msh3/msquic, and nghttp3/ngtcp2) were built on top of BoringSSL and these forks over the many subsequent years.OpenSSL has since slowly implemented an incompatible approach that this ecosystem can't directly use, with client support released in OpenSSL 3.2 (2023), and server support landing imminently in OpenSSL 3.5 (2025).Some would argue this is a major mistake by OpenSSL, while I think OpenSSL would argue that BoringSSL's design is flawed and/or unsuitable for OpenSSL, and it was worth taking the time to do it right.Regardless of who's actually 'right', this has created a significant schism in the entire ecosystem. Curl has a good overview of the state of play  OpenSSL:OpenSSL's approach doesn't work easily in the TLS section for any of the existing QUIC & HTTP/3 implementations. In effect, they've started another column, but with no compatible implementations currently available in the HTTP/3 & QUIC spots.This is a notable issue because for most major projects it would be an enormous & problematic undertaking to drop support for OpenSSL, which effectively means they still cannot ship built-in QUIC support today. Node.js recently briefly discussed even dropping OpenSSL entirely because of this, in favour of BoringSSL or similar, but it's clear that it's not practical: it would be an enormous breaking change, no alternative offers the same levels of long-term support guarantees, and Node and other languages are often shipped in enviroments like Linux distributions where it uses the system's shared OpenSSL implementation, so this would create big headaches downstream too.This is one example of the difference in fundamental pressures of the two tiers of organizations on the web here: open-source tools can't break things like this, and the libraries available to the long-tail are fragmented and uncoordinated. Meanwhile hyperscalers can make decisions quickly and near-unilaterally to set up implementations that work for their environments today, allowing them to get the benefits of new technologies without worrying too much about the state of the open-source common ecosystem for everybody else.I hope this is makes it clear there's a big problem here: underlying organizational differences are turning into a fundamental split in technologies on the Internet. There's an argument that despite the benefits, the long-tail web doesn't  HTTP/3, so they can just ignore it, or use a CDN with built-in support if they really care, and there is no real obligation as such for the hyperscalers to provide convenient implementations to the rest of us just because they want to use some neat new tech between themselves.The problem here though is that there are real concrete benefits to these technologies. QUIC is a significant improvement on alternatives, especially on slow or unreliable mobile internet (e.g. everywhere outside the well-connected offices of the developed world) and when roaming between connections. There are technologies built on top of it, like WebTransport which provides additional significant new features & performance to replace WebSockets. There will be more features that depend on HTTP/3 in future (see how gRPC built on HTTP/2 for example). Again: the technology here is great! But it's a challenge if those benefits are not evenly distributed, and only accessible to a small set of organizations and their customers.Continuing down this road has some plausible serious consequences:In the short term, the long-tail web gains a concerete disadvantage vs the hyperscale web, as HTTP/3 and QUIC makes hyperscale sites faster & more reliable (especially on slow & mobile internet).Other web tools and components (React et al) used by developers either working for hyperscale organizations or building on top of their tools & infra will increase in complexity to match, taking HTTP/3's benefits for granted and moving forwards on that basis, making them less and less relevant to other use cases.If we're not careful, the split between the long-tail & hyperscale cases will widen. New features & tools for each use case will emerge, and won't be implemented by the other, and tooling will increasingly stratify.If hyperscale-only tech is widespread but implementations are not, it becomes increasingly difficult to build tools to integrate with these. Building a Postman-like client for WebTransport is a whole lot harder if you're implementing the protocol from scratch instead of just a UI.You'll start to see lack of HTTP/3 support used as a signal to trigger captchas & CDN blocks, like as TLS fingerprinting is already today. HTTP/3 support could very quickly & easily become a way to detect many non-browser clients, cutting long-tail clients off from the modern web entirely.As all this escalates and self-reinforces, it becomes less & less sensible for the hyperscale side to worry about the long-tail's needs at all, and the ecosystem could stratify completelyAll of that is a way away, and quite hypothetical! I suspect  of this will happen to some degree, but there's a wide spectrum of possibility. It's notable though that this doesn't just apply to HTTP/3: the centralization and coordination of a few CDN & web clients like this could easily play out similarly in many other kinds of technological improvements too.For HTTP/3 at least, I'm hopeful that there will be a happy resolution here to improve on this split in time, although I don't know if it will come soon enough to avoid notable consequences. Many of the external libraries and experimental implementations of QUIC & HTTP/3 will mature with time, and I think eventually (I really really hope) the OpenSSL QUIC API schism will get resolved to open the door to QUIC support in the  OpenSSL-based environments, either with adapters to support both approaches or via a new HTTP/3 & QUIC stack that supports OpenSSL model directly. If you're interested in working on either, and there's anything I can do to help directly or to help fund that work, please get in touch.None of that will happen today though, so unfortunately if you want to use HTTP/3 end-to-end in your application, you may in for a hard time for a while yet. Watch this space.Want to debug HTTP/1 and HTTP/2 in the meantime? Test out  now. Open-source one-click HTTP(S) interception & debugging for web, Android, terminals, Docker & more.]]></content:encoded></item><item><title>IBM CEO Doesn&apos;t Think AI Will Replace Programmers Anytime Soon</title><link>https://developers.slashdot.org/story/25/03/12/1448242/ibm-ceo-doesnt-think-ai-will-replace-programmers-anytime-soon?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>dev</category><category>slashdot</category><pubDate>Wed, 12 Mar 2025 16:00:00 +0000</pubDate><source url="https://developers.slashdot.org/">Dev - Slashdot - Dev</source><content:encoded><![CDATA[IBM CEO Arvind Krishna has publicly disagreed with Anthropic CEO Dario Amodei's prediction that AI will write 90% of code within 3-6 months, estimating instead that only "20-30% of code could get written by AI." 

"Are there some really simple use cases? Yes, but there's an equally complicated number of ones where it's going to be zero," Krishna said during an onstage interview at SXSW. He argued AI will boost programmer productivity rather than eliminate jobs. "If you can do 30% more code with the same number of people, are you going to get more code written or less?" he asked. "History has shown that the most productive company gains market share, and then you can produce more products."]]></content:encoded></item><item><title>Greek is Greek</title><link>https://www.reddit.com/r/kubernetes/comments/1j9n9mr/greek_is_greek/</link><author>/u/amarao_san</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Wed, 12 Mar 2025 15:55:06 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I'm looking at the Agones, and, given that I learn Greek language, I can't see it just as 'random nice-sound name'.Αγώνες is plural of Αγώνας, which is 'fight' or 'battle'. And it also prononced with stress on 'o' ah-gO-nes (ah-gO-nas), and it have soft 'g' sound, which is different from English g (and closer to Ukrainian 'г').Imagine someone call the software 'fights', and every one outside of English speaking world pronounce it as 'fee-g-h-t-s'.]]></content:encoded></item><item><title>Best Practices for Multi‐Cluster OIDC Auth? (Keycloak)</title><link>https://www.reddit.com/r/kubernetes/comments/1j9ms78/best_practices_for_multicluster_oidc_auth_keycloak/</link><author>/u/8bitjohnny</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Wed, 12 Mar 2025 15:34:15 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I am trying to figure out the “industry standard” way of handling OIDC auth across multiple Kubernetes clusters with Keycloak, and could use some community support.Background: I’ve got around 10 Kubernetes clusters and about 50 users, and I need to use Keycloak for OIDC to manage access. Right now I'm still in POC stage, but I’m running one Keycloak client per cluster, each client has two roles (admin and read-only), and users can be admin in some clusters and read-only in others. I am having trouble reconciling the roleBindings and their subjects in a way that feels functionally minimal. The way I see it I end up with either crazy roleBindings, crazy keycloak clients, or an unwieldly number of groups/roles, with some funky mappers thrown in.My questions for you all:How do you handle multi-cluster RBAC when using Keycloak? How do you keep it manageable?Would you stick to the one-client-per-cluster approach, or switch to one client with a bunch of group mappings?If I have to expect it to be messy somewhere, where is better? Keycloak side or k8s side?Would love to hear your setups and any pitfalls you’ve run into! Thanks in advance.]]></content:encoded></item><item><title>The 2005 Sony Bravia ad</title><link>https://www.sfgate.com/sf-culture/article/san-francisco-sony-bouncy-ball-ad-20204385.php</link><author>coloneltcb</author><category>dev</category><category>hn</category><pubDate>Wed, 12 Mar 2025 15:12:44 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Gemini Robotics brings AI into the physical world</title><link>https://deepmind.google/discover/blog/gemini-robotics-brings-ai-into-the-physical-world/</link><author>meetpateltech</author><category>dev</category><category>hn</category><pubDate>Wed, 12 Mar 2025 15:09:09 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Introducing Gemini Robotics, our Gemini 2.0-based model designed for roboticsAt Google DeepMind, we've been making progress in how our Gemini models solve complex problems through multimodal reasoning across text, images, audio and video. So far however, those abilities have been largely confined to the digital realm. In order for AI to be useful and helpful to people in the physical realm, they have to demonstrate “embodied” reasoning — the humanlike ability to comprehend and react to the world around us— as well as safely take action to get things done.Today, we are introducing two new AI models, based on Gemini 2.0, which lay the foundation for a new generation of helpful robots.The first is Gemini Robotics, an advanced vision-language-action (VLA) model that was built on Gemini 2.0 with the addition of physical actions as a new output modality for the purpose of directly controlling robots. The second is Gemini Robotics-ER, a Gemini model with advanced spatial understanding, enabling roboticists to run their own programs using Gemini’s embodied reasoning (ER) abilities.Both of these models enable a variety of robots to perform a wider range of real-world tasks than ever before. As part of our efforts, we’re partnering with Apptronik to build the next generation of humanoid robots with Gemini 2.0. We’re also working with a selected number of trusted testers to guide the future of Gemini Robotics-ER.We look forward to exploring our models’ capabilities and continuing to develop them on the path to real-world applications.Gemini Robotics: Our most advanced vision-language-action modelTo be useful and helpful to people, AI models for robotics need three principal qualities: they have to be general, meaning they’re able to adapt to different situations; they have to be interactive, meaning they can understand and respond quickly to instructions or changes in their environment; and they have to be dexterous, meaning they can do the kinds of things people generally can do with their hands and fingers, like carefully manipulate objects.While our previous work demonstrated progress in these areas, Gemini Robotics represents a substantial step in performance on all three axes, getting us closer to truly general purpose robots.Gemini Robotics leverages Gemini's world understanding to generalize to novel situations and solve a wide variety of tasks out of the box, including tasks it has never seen before in training. Gemini Robotics is also adept at dealing with new objects, diverse instructions, and new environments. In our tech report, we show that on average, Gemini Robotics more than doubles performance on a comprehensive generalization benchmark compared to other state-of-the-art vision-language-action models.To operate in our dynamic, physical world, robots must be able to seamlessly interact with people and their surrounding environment, and adapt to changes on the fly.Because it’s built on a foundation of Gemini 2.0, Gemini Robotics is intuitively interactive. It taps into Gemini’s advanced language understanding capabilities and can understand and respond to commands phrased in everyday, conversational language and in different languages.It can understand and respond to a much broader set of natural language instructions than our previous models, adapting its behavior to your input. It also continuously monitors its surroundings, detects changes to its environment or instructions, and adjusts its actions accordingly. This kind of control, or “steerability,” can better help people collaborate with robot assistants in a range of settings, from home to the workplace.The third key pillar for building a helpful robot is acting with dexterity. Many everyday tasks that humans perform effortlessly require surprisingly fine motor skills and are still too difficult for robots. By contrast, Gemini Robotics can tackle extremely complex, multi-step tasks that require precise manipulation such as origami folding or packing a snack into a Ziploc bag.Finally, because robots come in all shapes and sizes, Gemini Robotics was also designed to easily adapt to different robot types. We trained the model primarily on data from the bi-arm robotic platform, ALOHA 2, but we also demonstrated that it could control a bi-arm platform, based on the Franka arms used in many academic labs. Gemini Robotics can even be specialized for more complex embodiments, such as the humanoid Apollo robot developed by Apptronik, with the goal of completing real world tasks.Enhancing Gemini’s world understandingAlongside Gemini Robotics, we’re introducing an advanced vision-language model called Gemini Robotics-ER (short for ‘“embodied reasoning”). This model enhances Gemini’s understanding of the world in ways necessary for robotics, focusing especially on spatial reasoning, and allows roboticists to connect it with their existing low level controllers.Gemini Robotics-ER improves Gemini 2.0’s existing abilities like pointing and 3D detection by a large margin. Combining spatial reasoning and Gemini’s coding abilities, Gemini Robotics-ER can instantiate entirely new capabilities on the fly. For example, when shown a coffee mug, the model can intuit an appropriate two-finger grasp for picking it up by the handle and a safe trajectory for approaching it.Gemini Robotics-ER can perform all the steps necessary to control a robot right out of the box, including perception, state estimation, spatial understanding, planning and code generation. In such an end-to-end setting the model achieves a 2x-3x success rate compared to Gemini 2.0. And where code generation is not sufficient, Gemini Robotics-ER can even tap into the power of in-context learning, following the patterns of a handful of human demonstrations to provide a solution.Responsibly advancing AI and roboticsAs we explore the continuing potential of AI and robotics, we’re taking a layered, holistic approach to addressing safety in our research, from low-level motor control to high-level semantic understanding.The physical safety of robots and the people around them is a longstanding, foundational concern in the science of robotics. That's why roboticists have classic safety measures such as avoiding collisions, limiting the magnitude of contact forces, and ensuring the dynamic stability of mobile robots. Gemini Robotics-ER can be interfaced with these ‘low-level’ safety-critical controllers, specific to each particular embodiment. Building on Gemini’s core safety features, we enable Gemini Robotics-ER models to understand whether or not a potential action is safe to perform in a given context, and to generate appropriate responses.To advance robotics safety research across academia and industry, we are also releasing a new dataset to evaluate and improve semantic safety in embodied AI and robotics. In previous work, we showed how a Robot Constitution inspired by Isaac Asimov’s Three Laws of Robotics could help prompt an LLM to select safer tasks for robots. We have since developed a framework to automatically generate data-driven constitutions - rules expressed directly in natural language – to steer a robot’s behavior. This framework would allow people to create, modify and apply constitutions to develop robots that are safer and more aligned with human values. Finally, the new ASIMOV dataset will help researchers to rigorously measure the safety implications of robotic actions in real-world scenarios.To further assess the societal implications of our work, we collaborate with experts in our Responsible Development and Innovation team and as well as our Responsibility and Safety Council, an internal review group committed to ensure we develop AI applications responsibly. We also consult with external specialists on particular challenges and opportunities presented by embodied AI in robotics applications.In addition to our partnership with Apptronik, our Gemini Robotics-ER model is also available to trusted testers including Agile Robots, Agility Robots, Boston Dynamics, and Enchanted Tools. We look forward to exploring our models’ capabilities and continuing to develop AI for the next generation of more helpful robots.This work was developed by the Gemini Robotics team. For a full list of authors and acknowledgements please view our technical report.]]></content:encoded></item><item><title>How to Setup Preview Environments with FluxCD in Kubernetes</title><link>https://www.reddit.com/r/kubernetes/comments/1j9m657/how_to_setup_preview_environments_with_fluxcd_in/</link><author>/u/meysam81</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Wed, 12 Mar 2025 15:08:02 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I just wrote a detailed guide on setting up GitOps-driven preview environments for your PRs using FluxCD in Kubernetes.If you're tired of PaaS limitations or want to leverage your existing K8s infrastructure for preview deployments, this might be useful.Creating PR-based preview environments that deploy automatically when PRs are createdSetting up unique internet-accessible URLs for each preview environmentAutomatically commenting those URLs on your GitHub pull requestsUsing FluxCD's ResourceSet and ResourceSetInputProvider to orchestrate everythingThe implementation uses a simple Go app as an example, but the same approach works for any containerized application.Let me know if you have any questions or if you've implemented something similar with different tools. Always curious to hear about alternative approaches!]]></content:encoded></item><item><title>Kubernetes as a foundation for XaaS</title><link>https://www.reddit.com/r/kubernetes/comments/1j9kygu/kubernetes_as_a_foundation_for_xaas/</link><author>/u/dariotranchitella</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Wed, 12 Mar 2025 14:10:10 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[If you're not familiar with the term,  stands for "Everything as a Service". By discussing with several software companies, Kubernetes has emerged as the ideal platform to embrace this paradigm: while it solves many problems, it also introduces significant challenges which I'll try to elaborate a bit more throughout the thread.We all know Kubernetes works (sic) on any infrastructure and (again, sic) hardware by abstracting the underlying environment and leveraging application-centric primitives. This flexibility has enabled a wide range of innovative services, such as:, provided by companies like Kong., exemplified by solutions from EDB., with platforms like OpenShift Virtualization.These services are fundamentally powered by Kubernetes, where an Operator handles the service's lifecycle, and end users consume the resulting outputs by interacting with APIs or Custom Resource Definitions (CRDs).This model works well in multi-tenant Kubernetes clusters, where a large infrastructure is efficiently partitioned to serve multiple customers: think of Amazon RDS, or MongoDB Atlas. However, complexity arises when deploying such XaaS solutions on tenants' own environments—be it their public cloud accounts or on-premises infrastructure.This brings us to the concept of : each tenant may require a dedicated Kubernetes cluster for security, compliance, or regulatory reasons (e.g., SOC 2, GDPR, if you're European you should be familiar with it). The result is , where each customer potentially requires multiple clusters. This raises a critical question: who is responsible for the lifecycle, maintenance, and overall management of these clusters?Managed Kubernetes services like AKS, EKS, and GKE can ease some of this burden by handling the Control Plane. However, the true complexity of delivering XaaS with Kubernetes lies in managing multiple clusters effectively.For those already facing the complexities of multi-cluster management (the proverbial  dilemma),  offers a promising solution. By creating an additional abstraction layer for cluster lifecycle management, Cluster API simplifies some aspects of scaling infrastructure. However, while Cluster API addresses certain challenges, it doesn't eliminate the complexities of deploying, orchestrating, and maintaining the "X" in XaaS — the unique business logic or service architecture that must run across multiple clusters.Beyond cluster lifecycle management, additional challenges remain — such as handling diverse storage and networking environments. Even if these issues are addressed, organizations must still find effective ways to:Distribute software reliably to multiple clusters.Perform rolling upgrades efficiently.Gain visibility into logs and metrics for proactive support.Enforce usage limits (especially for licensed software).Simplify technical support for end users.At this stage, I'm not looking for clients but rather seeking a  interested in collaborating to build a new solution from the ground up, as well as engaging with the community members who are exploring or already explored XaaS models backed by Kubernetes and the  (Bring Your Own Cloud) approach. My goal is to develop a comprehensive suite for software vendors to deploy their services seamlessly across multiple cloud infrastructures — even on-premises — without relying exclusively on managed Kubernetes services.I'm aware that companies like Replicated already provide similar solutions, but I'd love to hear about unresolved challenges, pain points, and ideas from those navigating this complex landscape.]]></content:encoded></item><item><title>Rust Newbies: What mistakes should I avoid as a beginner? Also, what IDE/setup do you swear by? 🦀</title><link>https://www.reddit.com/r/rust/comments/1j9kv5w/rust_newbies_what_mistakes_should_i_avoid_as_a/</link><author>/u/tusharg19</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Wed, 12 Mar 2025 14:06:00 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I’m just starting my Rust journey and could use your wisdom: 1. What mistakes did you make early on that I should avoid? (Borrow checker traps? Overcomplicating lifetimes? Let me learn from your pain!) 2. What IDE/tools do you recommend for a smooth coding experience? (VS Code? RustRover? Terminal plugins? Config tips?)Drop your advice below—I’ll take all the help I can get! 🙏]]></content:encoded></item><item><title>What&apos;s up with Rust? • Tim McNamara</title><link>https://youtu.be/pppU--kHLP0</link><author>/u/goto-con</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Wed, 12 Mar 2025 13:40:33 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Implementing Cross-Site Request Forgery (CSRF) Protection in Go Web Applications</title><link>https://themsaid.com/csrf-protection-go-web-applications</link><author>/u/themsaid</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Wed, 12 Mar 2025 12:59:10 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[In a previous article, we discussed web sessions and explained how a session is created when a visitor first browses our web application. This session is then stored in a browser cookie and sent on every subsequent request, allowing the server to identify the visitor.Now, imagine you are logged into GitHub and there's a session cookie in your browser that's sent with every request to GitHub. What would happen if you visit a website that has this hidden form?="" =""="" ="" =""="" ="" =""The JavaScript code here submits a form that sends a POST request to the  endpoint with two hidden fields:  and . When the form is submitted, the browser detects an existing cookie for the  domain and includes it in the request. This triggers a transfer operation, moving one of your repositories to the attacker's account.This type of attack is known as Cross-Site Request Forgery (CSRF or XSRF for short), and it can take many forms beyond the example described above. In general, a CSRF attack occurs when an attacker tricks a user into making an unintended request to a trusted website where they are already authenticated. Because the user's session cookie is automatically included in the request, the website processes it as if it were a legitimate action initiated by the user.Secure Cookies Alone Can't Stop CSRF AttacksA secure cookie is one that has the following attributes:http.{
	: ,
	:   ,
	: .,
}Setting  prevents JavaScript from accessing the cookie,  ensures it is only sent over HTTPS, and SameSite: http.SameSiteLaxMode restricts cross-site cookie transmission.You might assume that setting the  cookie attribute is sufficient to prevent cross-site requests. However, this is not always the case. Older browser versions do not consistently enforce this attribute, leaving them vulnerable to Cross-Site Request Forgery attacks. Additionally, clickjacking presents another risk. In this type of attack, an attacker embeds your application inside an invisible iframe and tricks users into clicking buttons or performing actions they didn’t intend. Since these interactions originate from the same domain, they bypass SameSite restrictions, allowing the attacker to execute unauthorized actions on behalf of the user.Lax enforcement provides reasonable defense in depth against CSRF attacks that rely on unsafe HTTP methods (like POST), but does not offer a robust defense against CSRF as a general category of attack. When possible, developers should use a session management mechanism to mitigate the risk of CSRF more completely.This emphasizes that while  provides some protection against CSRF attacks, it is not a complete solution. To fully mitigate the risk of CSRF, we should implement additional session management techniques, with the most common being the use of CSRF tokens.The concept behind CSRF tokens is to establish a shared secret token between the client and the server. When a user first starts a session, the server generates a unique CSRF token and stores it alongside the session data. The server then sends this token to the client in the response. Each time the client makes a POST, PUT, DELETE, or PATCH request, it must include the CSRF token, typically within the body of the request or in a custom header, which the server verifies by comparing it to the one stored in the token.Now, consider the CSRF attack example we shared earlier:="" =""="" ="" =""="" ="" =""When this form is submitted, the server will detect the missing CSRF token in the body and block the request. If the attacker includes an invalid token, the server will still reject the request upon recognizing that the token doesn't match the session's expected token.In our application HTML template, we may share the token in the response in the form of a hidden field:="" =""
	/** CSRF token field **/
	="" ="" =""="" =""="" =""="" When this form is submitted, the server will validate the CSRF token and allow the request.Alternatively, if we use a JavaScript single page application (SPA), the server may share the CSRF token in a  tag:The client reads it and includes it as a header when submitting a request:: : Finally, some popular JavaScript libraries (like AngularJS and Axios) read the CSRF token from a cookie named . If we include this cookie in the response, the library will read it and send its value back on every request in the form of a  header.Adding CSRF Protection to the Session ManagerBuilding on our previous implementation of a secure session manager in Go, we will now add CSRF protection to the session manager. This involves generating CSRF tokens, storing them in the session, and ensuring they are included with every state-changing request (POST, PUT, DELETE, and PATCH).To generate a CSRF token, we will add a function that creates a 42-character base64 string with 256 bits of randomness:()  {
	([], )

	, .(., )
	 {
		()
	}

	..()
}Then, we'll ensure that a fresh CSRF token is created with every new session:()  {
	{
		:             (),
		:           []{: ()},
		:      .(),
		: .(),
	}
}Validating the CSRF TokenWe will add a method to the session manager that extracts the CSRF token from a given session and validates it against the  form value or the  header: () (http., )  {
	, .().()
	 {
		
	}

	.()

	 {
		..()
	}

	
}It's important to note that the form field name () and header name () are not fixed and can be customized. Here, I'm using common naming conventions for convenience. However, in your application, you may choose unique, less predictable names to add an extra layer of security.Performing the ValidationIn our session manager's middleware, we will call the  method in the beginning of state-changing requests (POST, PUT, PATCH & DELETE) and fail the request if the token doesn't match what's in the session:........ {

	.(, ) {
		.(, , .)
		
	}

}

.(, )With this setup, any state-changing request will be blocked before reaching the next handler in the chain, preventing an attacker from executing critical business logic.The full code of the  method now looks like this: () ( http.) http. {
	.(( http., http.) {
		, .()

		{
			: ,
			: ,
			:        ,
		}

		.().(, )
		.().(, )

		........ {
			.(, ) {
				.(, , .)
				
			}
		}

		.(, )

		.()

		()
	})
}Using CSRF in the Web ApplicationAs we mentioned earlier, there are multiple ways to pass the CSRF token in the application responses:If the frontend of our application submits typical HTML forms, we may pass the CSRF token down to the template as a template data parameter:.()

.().()

[]{
	: ,
}

.(, )Then, inside our form, we add a hidden field that includes the CSRF token:="" =""="" ="" =""="" In the same way, we may include the CSRF token in a meta tag and read the value via JavaScript if the frontend uses JavaScript to submit forms:="" ="": : Finally, if we're using a JavaScript library that supports the  cookie, we may send that cookie in the handler that presents the SPA view to the browser:.()

.().()

http.{
	:     ,
	:    ,
	:   ,
	: ,
	:     ,
	:   ,
	: .,
}

.(, )Storing the session ID in a secure cookie, which browsers only send for requests originating from the same domain, is an important security measure but is not enough to fully protect users from request forgery. To strengthen security, the token validation pattern presented in this article adds a crucial layer of protection.By incorporating CSRF tokens into session management, we ensure that each request made to the server is not only coming from a trusted origin but also includes a valid token that proves the request was intentionally generated by the user. This pattern helps prevent malicious actors from exploiting session cookies to perform unauthorized actions on behalf of the user.]]></content:encoded></item><item><title>The DuckDB Local UI</title><link>https://duckdb.org/2025/03/12/duckdb-ui.html</link><author>xnx</author><category>dev</category><category>hn</category><pubDate>Wed, 12 Mar 2025 12:56:01 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[TL;DR: The DuckDB team and MotherDuck are excited to announce the release of a local UI for DuckDB shipped as part of the  extension.The DuckDB CLI provides advanced features like interactive multi-line editing, auto-complete, and progress indicators.
However, it can be cumbersome for working with lengthy SQL queries, and its data exploration tools are limited.
Many of the available third party UIs are great, but selecting, installing, and configuring one is not straightforward.
Using DuckDB through a UI should be as simple as using the CLI.
And now it is!Starting with DuckDB v1.2.1, a full-featured local web user interface is available out-of-the-box!
You can start it from the terminal by launching the DuckDB CLI client with the  argument:You can also run the following SQL command from a DuckDB client (e.g., CLI, Python, Java, etc.):Both of these approaches install the  extension (if it isn't installed yet),
then open the DuckDB UI in your browser:The DuckDB UI uses interactive notebooks to define SQL scripts and show the results of queries.
However, its capabilities go far beyond this.
Let’s go over its main features.The DuckDB UI runs all your queries locally: your queries and data never leave your computer.
If you would like to use MotherDuck through the UI, you have to opt-in explicitly.Your attached databases are shown on the left.
This list includes in-memory databases plus any files and URLs you’ve loaded.
You can explore tables and views by expanding databases and schemas.Click on a table or view to show a summary below.
The UI shows the number of rows, the name and type of each column, and a profile of the data in each column.Select a column to see a more detailed summary of its data.
You can use the  button near the top right to inspect the first 100 rows.
You can also find the SQL definition of the table or view here.You can organize your work into named notebooks.
Each cell of the notebook can execute one or more SQL statements.
The UI supports syntax highlighting and autocomplete to assist with writing your queries.You can run the whole cell, or just a selection,
then sort, filter, or further transform the results using the provided controls.The right panel contains the Column Explorer, which shows a summary of your results.
You can dive into each column to gain insights.If you would like to connect to MotherDuck, you can sign into MotherDuck to persist files and tables to a cloud data warehouse crafted for using DuckDB at scale and sharing data with your team.The DuckDB UI is under active development. Expect additions and improvements!Like the DuckDB CLI, the DuckDB UI creates some files in the  directory in your home directory.
The UI puts its files in a sub-directory, :Your notebooks and some other state are stored in a DuckDB database, .When you export data to the clipboard or a file (using the controls below the results), some tiny intermediate files (e.g. ) are generated.
Your data is cleared from these files after the export is completed, but some near-empty files remain, one per file type.Support for the UI is implemented in a DuckDB extension.
The extension embeds a localhost HTTP server, which serves the UI browser application, and also exposes an API for communication with DuckDB.
In this way, the UI leverages the native DuckDB instance from which it was started, enabling full access to your local memory, compute, and file system.Results are returned in an efficient binary form closely matching DuckDB’s in-memory representation (DataChunk).
Server-sent events enable prompt notification of updates such as attaching databases.
These techniques and others make for a low-latency experience that keeps you in your flow.In this blog post, we presented the new DuckDB UI, a powerful web interface for DuckDB.The DuckDB UI shares many of its design principles with the DuckDB database.
It’s simple, fast, feature-rich, and portable, and runs locally on your computer.
The DuckDB UI extension is also open source: visit the  repository if you want to dive in deeper into the extension's code.The repository does not contain the source code for the frontend, which is currently not available as open-source.
Releasing it as open-source is under consideration.]]></content:encoded></item><item><title>Peer-to-peer file transfers in the browser</title><link>https://github.com/kern/filepizza</link><author>keepamovin</author><category>dev</category><category>hn</category><pubDate>Wed, 12 Mar 2025 12:08:43 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The Future Is Niri</title><link>https://ersei.net/en/blog/niri</link><author>mattjhall</author><category>dev</category><category>hn</category><pubDate>Wed, 12 Mar 2025 11:42:16 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[The worst  I have is that I've been using tiling window managers for thirty-five percent of my life: five years with Sway and two with i3. As the realization of those numbers (and my age) dawns upon me, an irresistible urge wells up in my chest, threatening to overwhelm me. I try to tamp it down, but the urge is too strong—I must .This may be worse than finding grey hairs.I switched to Wayland before it was cool, so a lot of stuff was broken, and I got used to it being broken, much like my entire Linux-on-modern-laptop experience. I was  with Sway, since I had gotten used to the workflow over the years. After all, it was what all the  online were using, and I was too young to make good decisions. I went about my life, going through most of high school and all of college with a tiling window manager, dismissing alternatives as straying from the  set forth by anonymous forum-goers.Sway  me (emotionally) with a click-and-drag issue where selecting text and dragging the selection (a pretty bog-standard thing people do with their computers)  somehow to keep the selection happening after you released the mouse.My decades of muscle memory stopped working—I felt lost, adrift on a rough sea, the hot sun bearing down on me. Would I (the bug) ever be rescued (fixed)? Only time would tell, but I was getting desperate.At first, I thought I could handle it and someone would quickly fix the bug. Days turned into weeks, and I was losing my mind. The Sway IRC was silent to my pleads for help, and I had developed a Pavlovian response to clicking on text to highlight it—a burst of panic in my chest as I dread the mouse continuing to drag after I had let go.Naturally, instead of figuring out what library made a breaking change and spending four hours running , I decided to throw nearly a decade of muscle-memory and workflow refinements out the window. I was getting bored of Sway anyway. Let's switch to Niri!For those unaware, Niri is a scrollable-tiling window manager: each workspace is an infinitely-wide strip you can scroll side-to-side on. It's easier to show their official demo video than to try to explain it with words (you don't have to watch the whole thing):It was new, , , and most of all, really cool. I just  to try it, transporting me back to my youth distro-hopping and window-manager-hopping(?) with reckless abandon.It seems to be a recurring theme, throwing myself into a new productivity-altering technology in March when I should be doing more important things instead.It wasn't as bad this time around! Within a few hours, I had a setup that worked fine enough. Within a week, I had Niri working better than Sway, and I was greatly enjoying the changes (read: improvements) it brought.Opening a window does not alter other windows: I can keep my focus and Firefox doesn't scroll to another dimension if I open a terminal in its vicinity.Unlike Sway, Niri supports per-window screensharing, as well as "blackout window from appearing in screen sharing". I've been streaming my homework and it's much nicer without needing to worry about an email notification from my bank showing up in the top corner.Niri has increased my battery life by about two hours compared to Sway.I was so excited about Niri that I tried my hand at adding a feature to its IPC for something or the other, and I greatly enjoyed it! Unlike Sway/wlroots, Niri/Smithay are written in Rust and are surprisingly accessible to hack on.I genuinely can't see myself going back to a traditional tiling window manager, Niri just brings too many improvements to my workflow.The Death of the Traditional Tiling Window ManagerTraditional tiling window managers have a side effect of forcing you to be as efficient as possible with your window layout. There is an additional cognitive load incentivizing you to optimize for the wrong thing: minimizing window reflows. If you don't find yourself constantly swapping between fullscreen and non-fullscreen views and running out of workspaces, you don't have very many windows open. Don't even get me started on tabbed/stacked layouts with nested containers, the least ergonomic Band-Aid™ for the space issue I've ever seen.After many  years of optimizing for the wrong thing with Sway, Niri blesses me with the realization that I can have the speed of a traditional tiling window manager without the space limitations.On Sway, I often had  workspaces open. Ever since I switched to a tiling window manager, I kept running out of space and added shortcuts to workspaces 11-20. I drilled it into myself to close windows when I was done with them, often losing my flow when I come back to the projects I closed, all to save  I feel should be infinite. With Niri, I can have three large projects open, various chat apps, a YouTube video, and three classes worth of schoolwork and never use more than five workspaces. The same setup would have me spilling into workspace  on Sway, and I would quickly get confused and forget where I put my math textbook, switching between each workspace until I find the right one, often the very last one I check!Wow, I did not realize how much repressed anger I had at traditional tiling window managers until now.Given the variety of screen sizes and improved processing power I do not think that the traditional tiling window manager ought to be the power-user workflow of choice. It artificially limits space, forces content reflows, and does not work well with nonstandard monitor layouts.If you are using Sway or another Wayland traditional tiling window manager, you should try Niri. Right now. My configurations are published on Sourcehut if you want to have a Sway-like experience with my keybindings.Go on then, what are you waiting for?]]></content:encoded></item><item><title>Microservices: The Architectural Cult That’s Bankrupting Your Sanity (and Your Startup)</title><link>https://medium.com/mr-plan-publication/microservices-the-architectural-cult-thats-bankrupting-your-sanity-and-your-startup-877e33453785?sk=0d5e112b5ed7b53ea0633f83a9b2c57a</link><author>/u/TerryC_IndieGameDev</author><category>dev</category><category>reddit</category><pubDate>Wed, 12 Mar 2025 11:40:19 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[You’re debugging a payment failure at 2 AM when Slack explodes:“Why is Kubernetes pod #4281 mining Bitcoin?!”As you chug cold coffee, a horrifying realization hits: Your “modern” architecture has become a distributed game of Jenga. And like every overconfident engineer who came before you, you’re one shaky service mesh away from total collapse.Welcome to the microservices lie.Remember when software had ? When you could trace a bug from frontend click to database query without needing a PhD in distributed tracing?The early cloud pioneers sold us a beautiful dream: “Break your monolith into microservices! Scale components independently! Innovate faster!” What they didn’t mention? ]]></content:encoded></item><item><title>First Ammonia-Fueled Ship Hits a Snag</title><link>https://spectrum.ieee.org/ammonia-fuel-2671266100</link><author>pseudolus</author><category>dev</category><category>hn</category><pubDate>Wed, 12 Mar 2025 11:24:40 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[The , an oil platform supply shipundergoing a pioneering retrofit to run on ammonia fuel, is now scheduled to begin operations in 2026—two years later than initially planned. Once completed, it will be the first vessel capable of operating full-time on ammonia, marking a major milestone in efforts to reduce carbon dioxide (CO) emissions in the maritime industry.Industry experts attribute the delay to the complex infrastructure required to handle ammonia safely. “Ammonia is toxic, explosive, and corrosive. We must use special piping, storage tanks, and trucks outfitted with materials engineered to be both leakproof and resistant to ammonia’s corrosive properties,” says John Prousalidis, a professor of marine engineering at the National Technical University of Athens. A spokesperson for Wärtsilä told  that the delay has not been related to performance or safety issues, but to “ the challenges of being at the forefront of maritime decarbonization.” The spokesperson explains that the project “was Being first comes with a set of challenges that must be overcome. One of the biggest environmental concerns with ammonia-powered ships is the potential release of nitrogen oxides. “Instead of CO, which contributes to global warming, we could end up with nitrogen oxides, which are lethal to breathe,” says Prousalidis. “To avoid simply swapping one pollutant for another, ammonia propulsion systems must include emissions-control technologies to prevent harmful nitrogen oxides from entering the atmosphere,” he adds.A promising alternative to combustion engines is ammonia-powered fuel cells like the one originally slated to power the  Fuel cells generate electricity without producing nitrogen oxide emissions. By avoiding combustion entirely, these fuel cells allow ammonia’s nitrogen content to remain in its inert form, eliminating a key health risk.Ammonia Fuel Delays for Despite the challenges, experts believe ammonia could become a mainstream maritime fuel—but not overnight. “Twenty or thirty years ago, the shipping industry made a major shift to natural gas, believing it was the fuel of the future. Now, we know it wasn’t the right step,” says Prousalidis.Looking back at past fuel transitions, he noted that each shift—from steam to oil, and then to natural gas—took around 20 years to achieve full adoption. He expects a similar timeline for the adoption of ammonia or hydrogen. “We need to be patient while persisting in our efforts and not getting discouraged by early challenges.”That perspective aligns with what Equinor’s vice president of renewable and low-carbon technology, Henriette Undrum, told  in 2021 when she urged the public to be patient: “We are not just solving one small problem for one ship. It’s part of the bigger picture. It will be a starting point to build up the market for zero-carbon fuels.”“Seaports are already undergoing an energy transformation,” says Prousalidis. “It would make sense for them to operate as energy hubs—producing, storing, and trading alternative fuels for fuel cells and other power-generation devices.” By doing so, he says, ports could turn a profit while also breaking the stalemate and contributing to global decarbonization efforts.Although concerns about nitrogen oxide emissions remain, companies are reportedly designing post-combustion systems, analogous to the catalytic converters in automobile exhaust systems, to filter out harmful byproducts.  A technique called selective catalytic reduction converts nitrogen oxides into nitrogen and water. Ammonia slip catalysts capture unburned ammonia and break it down into the same two inert products. ]]></content:encoded></item><item><title>KDE Plasma 6.3.3, Bugfix Release for March</title><link>https://kde.org/announcements/plasma/6/6.3.3/</link><author>/u/gabriel_3</author><category>dev</category><category>reddit</category><pubDate>Wed, 12 Mar 2025 11:13:08 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[You can give us feedback and get updates on our social media channels:
Your feedback is greatly appreciated.KDE is a Free Software community that exists and grows only because of the help of many volunteers that donate their time and effort. KDE is always looking for new volunteers and contributions, whether it is help with coding, bug fixing or reporting, writing documentation, translations, promotion, money, etc. All contributions are gratefully appreciated and eagerly accepted. Please read through the Supporting KDE page for further information or become a KDE e.V. supporting member through our Join the Game initiative.KDE is an international technology team that creates free and open source software for desktop and portable computing. Among KDE’s products are a modern desktop system for Linux and UNIX platforms, comprehensive office productivity and groupware suites and hundreds of software titles in many categories including Internet and web applications, multimedia, entertainment, educational, graphics and software development. KDE software is translated into more than 60 languages and is built with ease of use and modern accessibility principles in mind. KDE’s full-featured applications run natively on Linux, BSD, Windows, Haiku, and macOS.]]></content:encoded></item><item><title>Metadata routing in scikit-learn</title><link>https://www.youtube.com/watch?v=lQ_-Aja-slA</link><author>probabl</author><category>dev</category><category>ml</category><enclosure url="https://www.youtube.com/v/lQ_-Aja-slA?version=3" length="" type=""/><pubDate>Wed, 12 Mar 2025 10:30:22 +0000</pubDate><source url="https://www.youtube.com/channel/UCIat2Cdg661wF5DQDWTQAmg">Dev - Probabl</source><content:encoded><![CDATA[Metadata routing is a relatively new feature that allows you to, as the name implies, route metadata to different components in your machine learning setup. It's especially useful for things like sample_weight, which require data per row of your input to be passed along. But you can imagine many elaborate usecases such as (custom) estimators but also (custom) scorers. This video explains how they work. 

Scikit-learn documentation:
https://scikit-learn.org/stable/metadata_routing.html

Website: https://probabl.ai/
LinkedIn: https://www.linkedin.com/company/probabl
Twitter: https://x.com/probabl_ai
Bluesky: https://bsky.app/profile/probabl.bsky.social
Discord: https://discord.probabl.ai

We also host a podcast called Sample Space, which you can find on your favourite podcast player. All the links can be found here:
https://rss.com/podcasts/sample-space/

#probabl]]></content:encoded></item><item><title>We launched serverless hosting option for Rust apps</title><link>https://www.reddit.com/r/rust/comments/1j9gpz9/we_launched_serverless_hosting_option_for_rust/</link><author>/u/OfficeAccomplished45</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Wed, 12 Mar 2025 10:11:43 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I’ve been deploying Rust apps (Rocket) for a while (and some parts of this service are even built with Rust), and one thing that always bothered me was the cost—especially when you have small projects that don’t get much traffic. – Many hosting providers charge you 24/7, even if your app is inactive most of the time.Multiple apps, multiple bills – Want to run a few small Rust services? You'll likely end up paying for each one individually, even with minimal traffic.I wanted to address this, so I built Leapcell. It’s a serverless platform where you can deploy Rust apps instantly, get a URL, and only pay for actual usage—no more paying for idle time.If you’ve struggled with the cost of Rust hosting, I’d love to hear your feedback!]]></content:encoded></item><item><title>Weekly: Share your EXPLOSIONS thread</title><link>https://www.reddit.com/r/kubernetes/comments/1j9gk1g/weekly_share_your_explosions_thread/</link><author>/u/gctaylor</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Wed, 12 Mar 2025 10:00:31 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Did anything explode this week (or recently)? Share the details for our mutual betterment.   submitted by    /u/gctaylor ]]></content:encoded></item><item><title>What′s new in Java 24</title><link>https://pvs-studio.com/en/blog/posts/java/1233/</link><author>/u/Xaneris47</author><category>dev</category><category>reddit</category><pubDate>Wed, 12 Mar 2025 09:17:53 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[On March 18, a new Java version is set to arrive! Let's take a peek at new features, including the long-awaited final implementation of Stream Gatherers!The order of the JEPs (JDK Enhancement Proposal) presented here is based on our assessment of their "interestingness" rather than their official numbering.JEP 485: Stream GatherersAs you know, Stream API operations are divided into  operations that generate a new  and  operations that create a result or have a side effect. However, terminal operations have , which allows us to create custom operations via the  implementation. The set of intermediate ones has only , , , , , , and . That's the case—until Java 24, which introduces .The key points of the new feature are as follows:1. New  method added to .2. New java.util.stream.Gatherer interface, which consists of four methods: creates an initial intermediate state using ; handles elements, optionally uses the intermediate state, and sends results further down the stream. It relies on the new  functional interface. merges states using ; performs on the intermediate state and sends the result further down the stream after all elements have been processed. It uses .3. New java.util.stream.Gatherers class, which provides several standard implementations of : is similar to the  operation; performs incremental accumulation; is a standard implementation of Fixed Window; is a standard implementation of Sliding Window.The Sliding Window technique creates a window of the N size over the input data and then shifts that window. A simple example with a number array and a window size of 3 illustrates this concept:The result of this operation is  containing all the subarrays highlighted by the blue frame.With Gatherers, we can easily output all subarrays as follows:public static void main(String[] args) {
    var list = List.of(
        "1", "2", "3", "4", "5", "6", "7", 
        "8", "9", "10", "11", "12", "13", "14"
    );
    int k = 3;

    list.stream()
        .gather(Gatherers.windowSliding(k))
        .forEach(sublist -> System.out.printf("%s ", sublist));
    System.out.println();
}[1, 2, 3] [2, 3, 4] [3, 4, 5] [4, 5, 6] [5, 6, 7] [6, 7, 8] [7, 8, 9] [8, 9, 10] [9, 10, 11] [10, 11, 12] [11, 12, 13] [12, 13, 14]Fixed Window has a similar implementation—except that the shift is equal to the window size.The usage remains the same:public static void main(String[] args) {
    var list = List.of(
        "1", "2", "3", "4", "5", "6", "7",
        "8", "9", "10", "11", "12", "13", "14"
    );
    int k = 3;

    list.stream()
        .gather(Gatherers.windowFixed(k))
        .forEach(sublist -> System.out.printf("%s ", sublist));
    System.out.println();
}[1, 2, 3] [4, 5, 6] [7, 8, 9] [10, 11, 12] [13, 14]In addition to creating custom classes that implement , Java provides static factory methods:Gatherer.ofSequential(integrator)Both methods have variations with different additional arguments in the form of functional interfaces—, , , and —which were mentioned earlier.A custom API to work with class files was introduced in JDK 22. Now, with JDK 24, this API has been finalized!The rapid Java development in recent years has led to frequent and regular bytecode updates with which standard tools like , , and others interact. They use libraries like ASM for this interaction. To support new bytecode versions, tools should wait for library updates—yet libraries, in turn, wait for the final implementation of new JDK versions. This dependency chain slows down the development and adoption of new class file features.While this API may not be directly useful for most developers, it's essential for various frameworks and libraries—including Spring and Hibernate—that work with bytecode and use ASM. The problem is that older ASM versions are incompatible with newer JDK releases. If we need to update the JDK version in a project, ASM must be updated as well—so, we need to update everything that depends on it... well, almost everything. Yet we just wanted to upgrade the JDK version.Let's explore a new API. After experimenting with it, we've put together a simple example that reads static constant primitive fields (a basic understanding of class-file structures is required):public class ClassFileExample {

  public static void main(String[] args) throws IOException {
    var classFile = ClassFile.of().parse(Path.of("./Main.class"));

    for (var field : classFile.fields()) {
      var flags = field.flags();
      if (flags.has(AccessFlag.STATIC) && flags.has(AccessFlag.FINAL)) {
        System.out.printf("static final field %s = ", field.fieldName());
        var value = field.attributes().stream()
          .filter(ConstantValueAttribute.class::isInstance)
          .map(ConstantValueAttribute.class::cast)
          .findFirst()
          .map(constant -> constant.constant().constantValue().toString())
          .orElse("null");
        System.out.printf("%s%n", value);
      }
    }
  }
}This can be surprisingly helpful because reflection leads to class initialization. One day, we might delve into the consequences of such  initialization.Developers familiar with ASM may notice that the authors chose not to use the  pattern because of Java's new features, particularly pattern matching.JEP 483: Ahead-of-Time Class Loading & LinkingThis new feature aims to streamline application loading time. To achieve this, Java now enables caching of loaded classes. The process of generating and using this cache consists of three steps:1. Generating the AOT configuration. Run the application with the  flag and specify the output file path via -XX:AOTConfiguration=PATH:java -XX:AOTMode=record -XX:AOTConfiguration=app.aotconf -jar app.jar2. Generating the cache with configuration. Change the  mode to  and specify the cache output path using the  flag:java -XX:AOTMode=create -XX:AOTConfiguration=app.aotconf -XX:AOTCache=app.aot -jar app.jar3. Running the application using the cache. Use only the  flag:java -XX:AOTCache=app.aot -jar app.jarAccording to JEP, the loading time for a simple program using the  decreased from 0.031 seconds to 0.018 seconds (a 42% difference). The loading time for a Spring-based project (Spring PetClinic) dropped from 4.486 seconds to 2.604 seconds.I also looked at the simple Quarkus application from the recently published book  (GitHub). The loading time decreased from 3.480 to 2.328 seconds (a 39.67% decrease).JEP 491: Synchronize Virtual Threads without PinningThis JEP resolves the issue of platform thread blocking when using virtual threads in  blocks. To understand the impact of this change, let's first take a look at Project Loom and, more specifically, virtual threads.When virtual threads were introduced in JEP 444, two scenarios were specified in which they wouldn't release the platform thread they were using when blocked:Blocking occurs in a  block;Blocking occurs in native methods—whether they're JNI or Foreign Functions.Now, the first case is invalid. Developers are now free to choose between using the  keyword and the java.util.concurrent.locks package, allowing them to focus solely on the specific requirements of their task.JEP 490: ZGC: Remove the Non-Generational ModeZ Garbage Collector (ZGC) used to support two modes:  and . Since Generational ZGC is the preferred option in most cases, developers have decided to streamline further ZGC support by disabling one of the modes, Non-Generational. The  flag is now deprecated, and the warning message will be displayed if it's used:JEP 498: Warn upon Use of Memory-Access Methods in sun.misc.UnsafeIf memory-related methods from  are called, the warning will be issued. These changes align with the transition toward modern alternatives such as the  and the Foreign Function & Memory API. Additionally, they pass Java closer to removing memory-related methods from , which have already been marked as . This update also encourages library developers to migrate to the new APIs.JEP 472: Prepare to Restrict the Use of JNIUsing  (JNI) and Foreign Function & Memory (FFM) now issues a warning:This is the first step in restricting the use of JNI and FFM. In the future, an exception should be thrown for the code. However, this doesn't mean these features will be removed—which would be ironic, since FFM was released in Java 22. This step is needed for the policy of integrity by default. It just means that developers, who enable native access, should explicitly state that they consider unsafe features of the JDK.JEP 493: Linking Run-Time Images without JMODThe ‑‑enable-linkable-runtime flag, introduced for JDK builds, allows  to create images without relying on  files from the JDK. This optimization reduces the final image size by 25%.JMOD files have been around since Project Jigsaw (Java 9) and are used in the optional linking phase when using  to create a space-optimized JRE.Unlike JAR files, JMODs can store not only  files and resources but also native libraries, licenses, and executables, which are then included in the final JRE. However, JAR files may be sufficient for developing standard applications, there is no need to use JMOD. Additionally, JMOD documentation is severely lacking. Although this change doesn't directly impact developers, it's particularly relevant for containers or when creating minimal runtime images. However, this optimization isn't enabled by default—it's up to individual JDK providers to decide whether to implement it.For example, Eclipse Temurin has already started using this flag, and GraalVM has added support for such builds as well.JEP 486: Permanently Disable the Security ManagerPreparations for disabling java.lang.SecurityManager started back in Java 17, when it was marked  due to the rare usage of this class at high maintenance costs. Now let's go to the changes.The  flag (in any form) is no longer supported and causes an error, except for -Djava.security.manager=disallow:Calling System::setSecurityManager throws an UnsupportedOperationException exception.System properties related to  are now ignored, and the conf/security/java.policy file has been removed.Other changes are documentation-related, for example, references to  and  have been removed.It should be noted that the classes and methods aren't removed but degraded to "empty"—they either return , , pass through the caller's request, or throw a  or UnsupportedOperationException.JEP 479: Remove the Windows 32-bit x86 PortSupport for Windows 32-bit x86 is finally being discontinued. This facilitates the build and test infrastructure, freeing up resources that are no longer needed to maintain the platform.One of the reasons for removing this port is the lack of support for , which fall back to classic . Additionally, support for the latest 32-bit version of Windows 10 will end in October 2025.JEP 501: Deprecate the 32-bit x86 Port for RemovalThe fate of the other 32-bit platforms is clear: they'll be removed, but not in this release.So, Linux remains the last 32-bit supported platform. Building the 32-bit version now requires adding the ‑‑enable-deprecated-ports=yes flag:bash ./configure –enable-deprecated-ports=yesHowever, the complete removal of this port is expected as early as Java 25.JEP 496: Quantum-Resistant Module-Lattice-Based Key Encapsulation MechanismThis and the following JEP focus on post-quantum cryptography.Post-quantum cryptography refers to the creation of cryptographic algorithms that will be effective even after the advent of quantum computers. According to the FIPS 203 standard, the  implementation for , ,  APIs, namely , , and  has been introduced. Now we can generate the key pairs as follows:KeyPairGenerator generator = KeyPairGenerator.getInstance("ML-KEM-1024");
KeyPair keyPair = generator.generateKeyPair();JEP 497: Quantum-Resistant Module-Lattice-Based Digital Signature AlgorithmAs a follow-up to the previous JEP, according to the FIPS 204 standard, the implementation for , ,  APIs, namely , , and  has been added. Similar to the previous point, let's look at an example of obtaining an appropriate signature:Signature signature = Signature.getInstance("ML-DSA");JEP 475: Late Barrier Expansion for G1A final JEP that doesn't directly impact Java developers involves changes to the Garbage-First (G1) garbage collector, shifting the implementation of its barriers to a later C2 JIT compilation stage. This adjustment aims to simplify barrier logic for future developers while also reducing C2 compilation time.Beyond this list of new features, some changes remain in the Preview or Experimental status.Hopefully, one of the upcoming releases will enable us to see these innovations in action.You can explore the full list of the JEP links here.Java continues to evolve at a brisk pace, and the introduction of the  reduces the number of dependencies, accelerating updates across the platform even further. That wraps up the Java 24 release for now—so once again, we turn our attention to the long-awaited Project Valhalla.]]></content:encoded></item><item><title>Deaf in Cloud Native Meet Up</title><link>https://www.youtube.com/watch?v=BucO0shTeYM</link><author>CNCF [Cloud Native Computing Foundation]</author><category>dev</category><category>k8s</category><enclosure url="https://www.youtube.com/v/BucO0shTeYM?version=3" length="" type=""/><pubDate>Wed, 12 Mar 2025 08:55:50 +0000</pubDate><source url="https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA">Dev - CNCF</source><content:encoded><![CDATA[Don't miss out! Join us at our next Flagship Conference: KubeCon + CloudNativeCon Europe in London from April 1 - 4, 2025. Connect with our current graduated, incubating, and sandbox projects as the community gathers to further the education and advancement of cloud native computing. Learn more at https://kubecon.io]]></content:encoded></item><item><title>I stopped everything and started writing C again</title><link>https://www.kmx.io/blog/why-stopped-everything-and-started-writing-C-again</link><author>dvrj101</author><category>dev</category><category>hn</category><pubDate>Wed, 12 Mar 2025 07:38:35 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[I've been a good student for 5 years at a French computer school. I've been a good freelance developer for 20 years. I've used Ruby on Rails exclusively however never writing my own code always for clients.One day I learned Common Lisp. It was supposed to be a short mission I thought I could learn Common Lisp in ten days and hack a quick server management protocol. I ended up writing throw-away Common Lisp code that generated C for a fully-fledged ASN.1 parser and query system for a custom Common Lisp to C SNMP server.Years later I wrote more and more Common Lisp code and ended up writing cl-unix-cybernetics which has the most stars on Github of all my repos there, and cl-streams and cl-cffi, and finally cl-facts which is a triple store that can be used as a Common Lisp graph database. The results were astonishing : very fast, atomic transactions, nestable transactions, compatible with unwind-protect, only 3 macros to learn and you're all set. Cl-facts was presented as a lightning talk at ELS (European Lisp Symposium) in Belgium. Slides for my talk there are available here : https://git.kmx.io/facts-db/cl-facts/_tree/master/doc/facts.pdf.Writing all these Common Lisp packages took a long time and I was loosing all my clients. I did not care, Common Lisp was awesome and a tool for future generations for sure.However I also had a lot of echoes around me of people failing where Theo de Raadt and others had said they were wrong. Virtual machines still suck a lot of CPU and bandwidth for nothing but emulation. Containers in Linux with cgroups are still full of RCE (remote command execution) and priviledge escalation. New ones are discovered each year. The first report I got on those listed 10 or more RCE + PE (remote root on the machine). Remote root can also escape VMs probably also. You do have backups right ? So it kept me centered on OpenBSD and avoided me the hell that most DevOps faced : Terraform, Ansible and such. So I saw people angry with VMs and containers and also people angry with their very programming language. Like Clojure : who can write a strategy game with thousands of units each having their own vision of the world, without having a garbage collector running like hell ? In fact my friend went away and tried to write the game and failed. Garbage collectors suck, and all my Common Lisp projects have very limited applications just because of the garbage collector. And we all know it's a commercial argument to the jVM : it has one of the best GCs around and it did cost a lot to write correctly. I guess it was not written in a one time scratchSo I thought OK I have a killer app but no-one will run it because it's in Common Lisp. The only rational solution for performance and portability reasons, unless another tool is developed for these specific purpose like C, is C. Linux is written in C, OpenBSD is written in C, GTK+ is object-oriented pure C, GNOME is written in C. Most of the Linux desktop apps are actually written in plain old C. So why try harder ? I know C.So I started writing my  utility library, which would become a language () with an interpreter () but could also be compiled () if we did manage to get it through at some point; and data structures emerged from UTF-8 buffers and the other way around pretty fast and all was bounds-checked at memory cost but the results were awesome. Defensive programming all the way : all bugs are reduced to zero right from the start. The system has been maintained clean of wrong bugs all of the time. There are no security implications of running KC3 code. So very fast a small interpreter was born, pumping  (an enum-tagged union of all datatypes of the language) in a REPL (read eval print loop).3 years later I just finished a 5 layered refactor and all the tests pass again and the webserver seems to not be broken again. The language was renamed from C3 to KC3 as the original name was already taken. So what do we have ?I had already ported the graph database () to C89 and though a couple of bugs remained at the time of import pretty much all the database was written during the Covid-19 lockdown in 2020. Everything was there : add a triple, remove a triple, the recursive query system, transactions, logging and persistence. Pixel perfect implementation of my original design in Common Lisp but in plain old C89 which I remembered quite well.Besides the graph data base I also wrote parsers and generators to get formal semantics for all algorithmic types I know of and could write in such a short amount of time : Structs, Linked lists, Maps, Hash tables, Time, Complex, Rationals, Tuples, Code blocks, Quotes, Unquotes, Copy on write, Skip lists, Sets, etc. I have macros like I explain in my other article. I'll do a follow up with some examples of macros. I was very much inspired by the awesome work of José Valim and Elixir. See Fly.io's blog if you want to read José Valim !I have a REPL () which parses keyboard or file input and outputs all results of a KC3 evaluation to the console (standard output). It is used for most of the second phase of unit testing KC3.I have a webserver with an MVC framework which is producing the very webpage you're reading : .I have 700 views on a Common Lisp article which is crazy I did not think Common Lisp had so many followers.I have a documentation website all written using  and a vamped Markdown to HTML C implementation.Come on Discord and join the fun.]]></content:encoded></item><item><title>What are the common uses of Rust?</title><link>https://www.reddit.com/r/rust/comments/1j9elp0/what_are_the_common_uses_of_rust/</link><author>/u/Viper2000_</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Wed, 12 Mar 2025 07:34:07 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[I have been toying around with Rust lately after hearing universal praise about the language and it's rapid growth. And I want to know, right now and at this stage, what are the most common uses of Rust and in which fields? Where does it really shine and there is demand for it?]]></content:encoded></item><item><title>ICYMI - Kubescape is now in incubation in the CNCF</title><link>https://kubescape.io/blog/2025/02/25/kubescape-incubation/</link><author>/u/oshratn</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Wed, 12 Mar 2025 07:22:19 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[We are thrilled to share that Kubescape has officially been accepted as a CNCF Incubating project. This milestone is a significant achievement for the project. Kubescape began in 2021 as a fun project to scan for compliance with NSA-CISA Kubernetes hardening guidelines. What started as a security scanner, helping Devlopers and DevOps teams implement better Kubernetes security practices, evolved into a full security platform. Still helping security Kubernetes environments 😉From the very beginning, Kubescape was built with the cloud-native community in mind. It started as a simple CLI tool designed to check cluster configurations against NSA-CISA Kubernetes Hardening Guidance. Over time, with the support of a rapidly growing community, Kubescape has evolved into one of the most complete open-source solutions for Kubernetes security. We are proud to have contributed to its development alongside contributors in the Kubescape community, and to see so many adopters leveraging Kubescape in their day-to-day workflows.The Kubescape community has been a driving force behind this success. It’s not just the maintainers and contributors that we celebrate but the many users who have adopted and integrated Kubescape into their environments. Companies like Intel, AWS, Bitnami, ARMO, and Energi Danmark are just a few of the organizations using Kubescape. Some use Kubescape to secure their Kubernetes clusters. Others leverage it for educational purposes. Other use cases that go beyond what we imagined when we made our first commit. We are grateful for the trust that these adopters, along with hundreds of others, have shown in Kubescape.As we look toward the future, the Kubescape project is poised for even greater growth. Our roadmap is not just about adding more features, but about continuing to improve usability and optimizing the performance of the platform. We are excited to welcome new contributors and users into the fold as we continue on the hamster-wheel of Kubernetes security.The Kubescape community is our foundation, and we are committed to fostering a collaborative and inclusive environment where all contributions are valued. With the incredible support of the Cloud Native Computing Foundation (CNCF) and the broader Kubernetes community, we are determined to demonstrate sustained growth, strong governance, and broad adoption on our journey toward CNCF graduation. We believe that this is just the beginning, and we are eager to see where the future takes us.Together, with the support of these vibrant communities, Kubescape will continue to evolve and grow, offering better security, deeper insights, and an ever-expanding set of features. We invite everyone - whether you are an adopter, contributor, or newcomer - to join us in shaping the future of Kubernetes security.We welcome your feedback and ideas for improvement. We hold community meetings on Zoom, on the first Tuesday of every month, at 14:00 GMT.Thanks to all our contributors! Check out our CONTRIBUTING file to learn how to join them.]]></content:encoded></item><item><title>Azure&apos;s Weakest Link? How API Connections Spill Secrets</title><link>https://binarysecurity.no/posts/2025/03/api-connections</link><author>hland</author><category>dev</category><category>hn</category><pubDate>Wed, 12 Mar 2025 06:43:07 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Binary Security found the undocumented APIs for Azure API Connections. In this post we examine the inner workings of the Connections allowing us to escalate privileges and read secrets in backend resources for services ranging from Key Vaults, Storage Blobs, Defender ATP, to Enterprise Jira and SalesForce servers.
During a client engagement, I was checking out their Azure Resources looking for common vulnerabilities. They were utilizing a Logic App to post some messages to Slack. Usually, we can find some tokens or other sensitive information in the workflow run history of these apps, as it is common to not mark input (and output) as sensitive. I could not find anything of the sort in this case, so I moved on from the investigation. However, by chance I saw an odd response from a request automatically made from the portal when going into the API Connection resource. It was something like:


HTTP/2 200 OK
Content-Length: 1893
Content-Type: application/json; charset=utf-8


{
    "kind": "V2",
    "properties": {
        "displayName": "Slack",
        "authenticatedUser": {},
        "overallStatus": "Connected",
        "statuses":[
            {
                "status":"Connected"
            }
        ],
        "connectionState": "Enabled",
        "parameterValueSet":{
            "name":"oauth",
            "values":{}
        },
        "customParameterValues": {},
        "createdTime": "2025-01-24T11:46:25.0499291Z",
        "changedTime": "2025-01-24T11:46:25.0499291Z",
        "api": {
            "name": "slack",
            "displayName": "Slack",
            "description": "Slack is a team communication tool, that brings together all of your team communications in one place, instantly searchable and available wherever you go.",
            "iconUri": "https://conn-afd-prod-endpoint-bmc9bqahasf3grgk.b01.azurefd.net/u/v-anadhar/UpdateSlackForPlugin/1.0.1715.3917/slack/icon.png",
            "brandColor": "#78D4B6",
            "category": "Standard",
            "id": "/subscriptions/8e3ce52f-d45b-4347-8705-65892507465e/providers/Microsoft.Web/locations/norwayeast/managedApis/slack",
            "type": "Microsoft.Web/locations/managedApis"
        },
        "testLinks": [
            {
                "requestUri": "https://management.azure.com:443/subscriptions/8e3ce52f-d45b-4347-8705-65892507465e/resourceGroups/Logic-app-tests/providers/Microsoft.Web/connections/slack/extensions/proxy/conversations.list?api-version=2018-07-01-preview",
                "method": "get"
            }
        ],
        "testRequests": [
            {
                "body": {
                    "request": {
                        "method": "get",
                        "path": "conversations.list"
                    }
                },
                "requestUri": "https://management.azure.com:443/subscriptions/8e3ce52f-d45b-4347-8705-65892507465e/resourceGroups/Logic-app-tests/providers/Microsoft.Web/connections/slack/dynamicInvoke?api-version=2018-07-01-preview",
                "method": "POST"
            }
        ],
        "connectionRuntimeUrl": "https://d84b73b612cf5960.16.common.logic-norwayeast.azure-apihub.net/apim/slack/4355f64966c34c0cbfc15d48ec41e0c3"
    },
    "id": "/subscriptions/8e3ce52f-d45b-4347-8705-65892507465e/resourceGroups/Logic-app-tests/providers/Microsoft.Web/connections/slack",
    "name": "slack",
    "type": "Microsoft.Web/connections",
    "location": "norwayeast"
}

Now, this might seem uninteresting at first glance, but there are two key fields in this response that really opened up a whole slew of possibilities.The Inherent Insecurity of API ConnectionsConsider the  and  fields of the above response. It seems that they provide a sort of proxy between the Azure Management API and the actual backend server, most clearly seen by the  path. We can also see that the connection perhaps is authenticated in some way, by the  value in the . Now, naively, I would think that this means that some user, probably whoever set this up, is authenticated to this connection, and we would need his token to call through the connection, or maybe do an  dance ourselves.What I would  expect is that anyone with Reader permissions on the connection is allowed to arbitrarily call any endpoint on the connection:


HTTP/2 200 OK
Content-Type: application/json
Content-Length: 18329

"ok": true,
"channels": [
    {
        "id": "C08B8RB5D39",
        "name": "social",
        "is_channel": true,
        "is_group": false,
        "is_im": false,
        "is_mpim": false,
        "is_private": false,
        "created": 1738674777,
        "is_archived": false,
        "is_general": false,
        "unlinked": 0,
        "name_normalized": "social",
        "is_shared": false,
        "is_org_shared": false,
        "is_pending_ext_shared": false,
        "pending_shared": [],
        "context_team_id": "T08BPBEC890",
        "updated": 1738674779593,
        "parent_conversation": null,
        "creator": "U08C22K3HPT",
        "is_ext_shared": false,
        "shared_team_ids": [
            "T08BPBEC890"
        ],
        "pending_connected_team_ids": [],
        "is_member": true,
<...>

The response is actually exactly the same as a direct query on the Slack API endpoint conversations.listWhile the Slack case is perhaps not the most security critical, this result begs the question: Does this work for all the other types of APIs exposed through this interface?The answer is yes. If you have created an API Connection to any backend server, this includes other Azure resources, all Readers on that subscription can call all  requests defined on the connection. Specifically, this includes Key Vaults, SQL Databases, Jira-servers, Defender ATP, etc.Azure Management (ARM) API’s Security ModelBefore I show how to exploit this properly, some background on the Azure Management API is required. While we cannot know for sure how the developers at Microsoft designed the system, it seems clear to me that initially, the security model of the management API considered that Readers should be allowed to perform  requests. You would have to be  or higher to perform any changes, i.e. using any of the , , , etc methods.This can be seen by for instance requiring a number of sensitive endpoints for  to be empty  requests, like List Host Keys.At Binary Security we have reported a number of vulnerabilities relating to the leaking of sensitive information through insecure  endpoints. The result of this is that the security model has been somewhat changed in recent times, and it is now not obvious if a Reader is allowed to call a  endpoint. This is, however, still a viable attack method, and reading the documentation is still a goldmine for exploitable bugs.Getting back to the API Connections, it should be clear that the Management’s /extensions/proxy/{action} endpoints will allow all Readers to call the defined  requests. And while this is not seen as a problem in the ARM world, there is of course no guarantee that the connected API adheres to this security model.Creating an API ConnectionAPI Connections are resources in the Azure world, just like Key Vaults, SQL Databases or VMs, but they are not required to be explicitly created. They are automatically created for you when setting up Actions in a Logic App, so even if you have never heard of them before, it is quite possible that there are a lot of them hanging out in your tenant. For instance, creating a connection to your Key Vault is as easy as going to the Logic App Designer view, finding the Key Vaults actions, setting some initial values and authenticating.
This of course requires that the person setting it up, and authenticating to the Key Vault has appropriate access to the Key Vault. After signing in, it is not required to even save the Workflow, the resource is still created, and will need to be explicitly deleted if it is not needed any more.The flows for internal Azure Resources are all similar, where you can choose between different authentication types. For external resources, the setup varies, but in all cases, some authentication information is saved within the API Connection in some way, and this is used when querying the API.This means that the authentication used on the backend API call is always the same, and does not depend on the user or principal calling the ARM API. Crucially, the backend cannot know whether the call comes from the Logic App or from the proxy endpoint, called by any Reader on the resource.The full list of API Connections (Connectors) can be seen here. The proxy endpoints are not explicitly listed, but they can either be deduced from the API of the connected service, or by querying the  endpoint for that specific Connector, which exposes a Swagger definition of the API. Here we query it for the definition of the Jira Connector:

HTTP/2 200 OK
<...>

{
    "/{connectionId}/3/issue/{issueIdOrKey}": {
        "put": {
            "description": "Edits an issue. A transition may be applied and issue properties updated as part of the edit. The edits to the issue's fields are defined using update and fields.",
            "summary": "Edit Issue",
            "tags": [
                "Issues"
            ],
            "operationId": "EditIssue",
            "deprecated": false,
            "produces": [
                "application/json"
            ],
            "consumes": [
                "application/json"
            ],
            "parameters": [
                {
                    "name": "connectionId",
                    "in": "path",
                    "required": true,
                    "type": "string",
                    "x-ms-visibility": "internal"
                },
                {
                    "name": "issueIdOrKey",
                    "in": "path",
                    "required": true,
                    "type": "string",
                    "x-ms-summary": "Issue ID or Key",
                    "description": "Provide the Issue ID or Key for the issue you wish to edit",
                    "x-ms-url-encoding": "single"
                },
<...>

The  in this case is the full path to the  endpoint, something like /subscription/[SUBSCRIPTION_ID]/resourceGroups/[RESOURCE_GROUP]/providers/Microsoft.Web/connections/[CONNECTION_NAME]/extensions/proxy/.Armed with this knowledge, we can go searching for sensitive endpoints.The Connector for Key Vaults is maybe the one with the highest impact. The Swagger definition includes these sensitive  endpoints for listing secrets/{connectionId}/secrets/{secretName}/value to retrieve the value of the secretThe SQL Connector is quite similar to the Key Vault, you are basically free to read whatever you want:/{connectionId}/databases -  List Databases - List Datasets/{connectionId}/datasets({dataset})/tables({table})/items - Get rows from a tableThere is also a hilarious error message here, when trying to do some path traversing in the dataset name. It did not seem to be exploitable in any way, but I bet you have never seen a stacktrace exposed in an HTTP status message:The Jira Connector also exposes effectively everything on your Jira instance:/{connectionId}/v2/project/search - List projects/{connectionId}/user/permission/search - List users - List issues/{connectionId}/issue/{issueKey} - Read an issueThis connector is also interesting because it, of course, must be connected to your Jira instance somewhere else on the Internet. When setting up the connection, the developer gives the connection the URL of the Jira Instance. Incredibly, this is ignored in all subsequent requests, and instead, a special  header must be included in the request. This should point to your Jira instance, but there is no verification, so an attacker is free to SSRF at will. By setting this to an attacker-controlled server, the attacker will receive the API token used by the connection. This effectively also bypasses the restriction on the requests, and allows the attacker to query any endpoint with any method.Note that this attack is only possible when using the  authentication mechanism. When using , a GUID is used to identify your Jira Instance.All API Connections must be considered insecure as long as Readers can call the backend server. In nearly all cases I have seen, the connection exposes all information on the backend service. In addition to the ones above, this includes:Google Mail, Contacts, CalendarsI think there is significant undiscovered potential in these connections. Without going into detail, I can tell you that API Connections have a significant amount of architecture hidden between the Management Server and the backend API. All calls go from ARM to a  APIM instance containing every tenant’s API Connection, utilizing a Token Store. The initial authentication setup likewise goes through a  Consent server for storing tokens. If this hidden infrastructure is compromised, there will be significant cross-tenant impact as well.Hopefully by now, you have realized the impact of a lacking security model. While these endpoints are undocumented, that only makes them harder to find, not exploit. I am confident that any security researcher who had found them would immediately have noticed the glaring security hole it puts in their tenant. Hopefully, this post will allow others to discover more insecurities in Azure, so that we can be more secure in the future.Jan 6: Report submitted to Microsoft, both a general for API Connections and one specifically for Jira.Jan 7: API Connection case is closed by Microsoft as not valid, I submit it again with more words.Jan 10: Microsoft confirms the API Connection vulnerabilityJan 12-17: Microsoft fixes the API Connection vulnerability by not allowing any requests through  except for .Jan 30: Microsoft replies on the Jira ticket, saying they cannot reproduce it, which should be obvious, since now it is fixed.Feb 12: Jira Ticket is closedFeb 13: Microsoft replies to the API Connection case, saying it has been fixed.Feb 20: The case is closed as a duplicate.]]></content:encoded></item><item><title>Gemma 3 Technical Report [pdf]</title><link>https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf</link><author>meetpateltech</author><category>dev</category><category>hn</category><pubDate>Wed, 12 Mar 2025 06:39:17 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>The Insecurity of Telecom Stacks in the Wake of Salt Typhoon</title><link>https://soatok.blog/2025/03/12/on-the-insecurity-of-telecom-stacks-in-the-wake-of-salt-typhoon/</link><author>zdw</author><category>dev</category><category>hn</category><pubDate>Wed, 12 Mar 2025 05:21:33 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[This isn’t really a blog post about that incident, but it was the catalyst that inspired a bit of curiosity within me.I can’t (legally) access most mobile phone companies’ networks to see what vulnerabilities I can find, but there are plenty of open source software projects related to telecommunications on GitHub. So when I heard about the Salt Typhoon hacks, I wondered, “Is any of this open source telecom software any good?”In a previous life, I worked with companies that used Asterisk and FreeSWITCH, but I’d never really looked into them beyond the surface-level familiarity congruent to “this uses a similar protocol as RedPhone, somewhere” (this was when Signal was still called TextSecure).I don’t know much about PBX systems, SIP, or even audio encoding. Furthermore, some of the best C programmers I’ve ever met worked in telecom. Hell, some of the longest-running hacker communities have their roots in phone phreaking from the 1980s. Not to mention all the legendary engineers that trace their roots to Bell Labs.This is all to say, I thought looking at this sort of software would be a fruitless endeavor. Surely all of the low-hanging fruit would be found already?Thus, I opened FreeSWITCH’s source code on GitHub and almost immediately found a vulnerability.Buffer Overflow in HTTP Request Handler for XMLRPCIn this excerpt of code, the HTTP request handler for the XMLRPC library bundled with FreeSWITCH writes an arbitrary-length URI to a 4096-byte stack variable called z.char z[4096];
char *p,z1[26],z2[20],z3[9],u;
const char * z4;
int16_t i;
uint32_t k;

if (text) {
    sprintf(z, "Index of %s" CRLF, uri);
I think it’s reasonable to assume that attackers are capable of sending a Request URI longer than 4096 characters.Putting these observations together, it’s pretty easy to see this is a no-auth buffer overflow in their XMLRPC library. Turning this into remote code execution is an exercise left to the reader (mostly because I’m not really up-to-date on OS-level exploit mitigation techniques, and how to bypass them).This is kind of “defensive C programming practices 101” level.Soatok Attempts Coordinated Disclosure(n.b., Please stop saying “responsible” disclosure.): I send a follow-up email to ensure they received my report.: Andrey Volk responds:Since the fixes are now public, I’m left to assume that “embargo has broken,” so to speak. That means I’m free to blog about this publicly.However, I notice they haven’t tagged a new release with this security fix for FreeSWITCH users. I reply:Oh, wonderful. Thanks for getting back to me.Do you have an ETA on when the release will be tagged? I don’t want to publish anything until people can easily install an updated version.A few hours later, Andrey responds to my email.Brace yourselves, it’s a stupid one.Thank you for your interest in FreeSWITCH.We do not have plans to make a release of FreeSWITCH Community till summer 2025.To recap: An employee of SignalWire (which develops FreeSWITCH) came right out and said they would let people who aren’t paying for FreeSWITCH Advantage stay vulnerable until their regularly scheduled release (sometime in the Summer).There are about 8,300 hits on Shodan for FreeSWITCH as I write this. I highly doubt they’re all paying for enterprise support, so we’re talking about potentially thousands of telecom stacks around the world that SignalWire has decided to keep vulnerable until the Summer, even after they published the patches on GitHub.While such a decision might be perfectly legal, it really does not inspire trust in the stewards of this software project to give a shit about the harm their careless coding practices inflict upon their users.TelecomSec: A Systemic IssueThe worst part is, when I confided in a friend that works in telecom (after SignalWire published the fixes, of course) about this carnival-quality vulnerability management from the FreeSWITCH developers, their response was:December 2024 is the last time that alarms were once again raised about known SS7 vulnerabilities that have continued to exist in telephone networks for the past 17 years.And, to be honest, that kind of took the wind out of my sails so I didn’t bother looking at Asterisk or any of the other software.I mean, why bother? I already had the answer to the question that prompted me to look in the first place: Telecom security sucks today.The reason things sucks is largely because there’s very little (if any) money to be made in securing these systems today.Things don’t  be this way, of course.Maybe there’s some opportunity for some enterprising young hacker to write a FreeSWITCH competitor in Rust that has sane vulnerability management around it.Maybe in the future, we’ll find the political will to invest in the security of America’s telecommunications infrastructure. Who knows, some of that money might even percolate towards open source software, and they can hire someone who knows how to run valgrind.Or maybe everything will continue to suck, because incentives rule anything, and there currently aren’t any to do better.It took me a long time to write this one up, despite it being a rather simple technical issue, because I’m sure there are eldritch horrors lurking beneath the surface of this relatively simple finding.The vendor’s response was pretty lame, yeah. But is this vendor the lamest in their industry? I’m not so sure about that one. At least they responded within 90 days and fixed the issue on their GitHub.But, hey, if you’re waiting on SignalWire to get around to running … maybe rebuild from source or block public HTTP access to your FreeSWITCH stack at the firewall level?]]></content:encoded></item><item><title>Why Go for TypeScript compiler?</title><link>https://github.com/microsoft/typescript-go/discussions/411</link><author>/u/RobertVandenberg</author><category>dev</category><category>reddit</category><pubDate>Wed, 12 Mar 2025 04:59:49 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Go module is just too well designed</title><link>https://www.reddit.com/r/golang/comments/1j9aii3/go_module_is_just_too_well_designed/</link><author>/u/greengoguma</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Wed, 12 Mar 2025 04:00:13 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Ability to pull directly from Git removes the need for repository manager.Requiring major version in the module name after v1 allows a project to import multiple major versions at the same time.Dependency management built into the core language removes the need to install additional toolsNo pre-compiled package imports like Jar so my IDE can go to the definition without decompiling.These, such simple design choices, made me avoid a lot of pain points I faced while working in another language. No need to install npm, yarn or even wonder what the difference between the two is. No dependencies running into each other.I simply do  and it works. Just. Amazing.]]></content:encoded></item><item><title>OpenAI Pushes AI Agent Capabilities With New Developer API</title><link>https://developers.slashdot.org/story/25/03/11/2154229/openai-pushes-ai-agent-capabilities-with-new-developer-api?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>dev</category><category>slashdot</category><pubDate>Wed, 12 Mar 2025 03:30:00 +0000</pubDate><source url="https://developers.slashdot.org/">Dev - Slashdot - Dev</source><content:encoded><![CDATA[An anonymous reader quotes a report from Ars Technica: On Tuesday, OpenAI unveiled a new "Responses API" designed to help software developers create AI agents that can perform tasks independently using the company's AI models. The Responses API will eventually replace the current Assistants API, which OpenAI plans to retire in the first half of 2026. With the new offering, users can develop custom AI agents that scan company files with a file search utility that rapidly checks company databases (with OpenAI promising not to train its models on these files) and navigate websites -- similar to functions available through OpenAI's Operator agent, whose underlying Computer-Using Agent (CUA) model developers can also access to enable automation of tasks like data entry and other operations.
 
However, OpenAI acknowledges that its CUA model is not yet reliable for automating tasks on operating systems and can make unintended mistakes. The company describes the new API as an early iteration that it will continue to improve over time. Developers using the Responses API can access the same models that power ChatGPT Search: GPT-4o search and GPT-4o mini search. These models can browse the web to answer questions and cite sources in their responses. That's notable because OpenAI says the added web search ability dramatically improves the factual accuracy of its AI models. On OpenAI's SimpleQA benchmark, which aims to measure confabulation rate, GPT-4o search scored 90 percent, while GPT-4o mini search achieved 88 percent -- both substantially outperforming the larger GPT-4.5 model without search, which scored 63 percent.
 
Despite these improvements, the technology still has significant limitations. Aside from issues with CUA properly navigating websites, the improved search capability doesn't completely solve the problem of AI confabulations, with GPT-4o search still making factual mistakes 10 percent of the time. Alongside the Responses API, OpenAI released the open source Agents SDK, providing developers free tools to integrate models with internal systems, implement safeguards, and monitor agent activities. This toolkit follows OpenAI's earlier release of Swarm, a framework for orchestrating multiple agents.]]></content:encoded></item><item><title>I&apos;m frustrated, but positive about the future - my experience with Linux</title><link>https://www.reddit.com/r/linux/comments/1j99ybp/im_frustrated_but_positive_about_the_future_my/</link><author>/u/Helios5584</author><category>dev</category><category>reddit</category><pubDate>Wed, 12 Mar 2025 03:28:13 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[I recently decided to take a deep dive into Linux and its many distro's. Due to the rapid degrading of the Windows experience; I wanted something clean, free of bloat, and most importantly, able to run my video games without hassle.I spent many minutes researching and deciding which distro to go with and landed on Nobara. It was love a first site. The interface was kinda like Windows, the default package manager was simple, and the system felt quick and snappy. I had previously tried Linux 5-8 years ago, and my experience back then was pretty negative. Some of my devices were not properly working (due to Pulse Audio) and I could not get them to work. Believe me, I really tried to get into it and fix the issues. With Nobara, everything worked right out of the gate and worked well.I was super hyped with this and was loving Linux. Then came the games.I had recently been playing Kingdom Come Deliverance 2 on Windows and that was the first game I tried installing. I grabbed the latest GE version of proton from Proton Plus, enabled the settings in Steam, and went about downloading the game. It launched great and framerates were smooth. However, upon loading into my save, I started getting firefly artifacting (tiny white boxes randomly appearing and disappearing in the game. I scoured forums, downgraded Mesa drivers, change cpupower governor's, and even went as far as flashing my BIO's. Nothing worked. According to forums, this is likely due to my AMD GPU (7900xtx) interacting with Linux (My card is not bad as it worked great in Windows).Fed up with all the troubleshooting, I decided to try other distro's thinking it might have been Nobara causing the issues. I went to Bazzite: same issue. I went to Ubuntu: same issue. I even built my own Arch install: same issue (this step took a while to build and figure out).I came to the conclusion that it must be something with the drivers. At this point, it felt like Windows was calling out to me, asking me to come back to it. The main reason for my computers existence is to play video games and play them well. If it cannot do that in Linux currently, then I feel like I am almost being forced back to Windows. This is post is not throwing shade at the driver developers for Linux or at the amount of work people put into making Linux better, massive kudo's to all of you. However, it just does not feel like an out of the box experience yet where my games just "work".I plan on trying Linux again in the future. I really enjoyed by time with both Nobara and Bazzite, and I wish to use them full time in the future if the drivers (or whatever was causing the issues) allow. I love open source and everything it stands for. Linux developers: I hope you will keep on putting the effort into making Linux a great place to be, I truly look forward to the Linux future.Thanks for coming to my Ted Talk.]]></content:encoded></item><item><title>Show HN: XPipe, a shell connection hub for SSH, Docker, K8s, VMs, and more</title><link>https://xpipe.io/</link><author>crschnick</author><category>dev</category><category>hn</category><pubDate>Wed, 12 Mar 2025 03:16:28 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[A connection hub to keep track and manage of all your remote connections in one placeA terminal launcher that can launch you into a shell session in your favorite terminal instantlyComplete SSH support, including config files, agent integrations, jump servers, tunnels, key files, smartcards, X11 forwarding, and moreIntegrations for various container runtimes like Docker, Podman, Kubernetes, LXD, incus, plus environments like WSL, Cygwin, MSYS2Support for hypervisors like Proxmox, Hyper-V, KVM, VMware workstation, and more]]></content:encoded></item><item><title>Show HN: VSC – An open source 3D Rendering Engine in C++</title><link>https://github.com/WW92030-STORAGE/VSC</link><author>NormalExisting</author><category>dev</category><category>hn</category><pubDate>Wed, 12 Mar 2025 03:08:23 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Been making this rasterizer engine in C++ for the past few months, now also adding ray-tracing functionality to the system.Simply load a model or generate a mesh, add some lights, and render.]]></content:encoded></item><item><title>Beyond Diffusion: Inductive Moment Matching</title><link>https://lumalabs.ai/news/inductive-moment-matching</link><author>outrun86</author><category>dev</category><category>hn</category><pubDate>Wed, 12 Mar 2025 03:05:47 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[Fully unlocking the potential of rich multi-modal dataThere is a growing sentiment in the AI community that generative pre-training is reaching a limit. However, we argue that these limits are not due to a lack of data itself, but rather a stagnation in algorithmic innovation. The field remains dominated by just two paradigms since around mid-2020: autoregressive models for discrete signals and diffusion models for continuous signals. This stagnation has created a bottleneck that prevents us from fully unlocking the potential of rich multi-modal data, which in turn limits the progress on multimodal intelligence.At Luma, we aim to overcome this algorithmic ceiling through the lens of efficient inference-time compute scaling. Today we are introducing a new method, Inductive Moment Matching (IMM), a pre-training technique that not only delivers superior sample quality compared to diffusion models but also offers over a tenfold increase in sampling efficiency. In contrast to consistency models (CMs), which are unstable as a pre-training technique and require special hyperparameter designs, IMM employs a single objective with enhanced stability across diverse settings.How Inductive Moment Matching WorksInference can generally be scaled along two dimensions: extending sequence length (in autoregressive models), and augmenting the number of refinement steps (in diffusion models). While adding more refinement steps significantly boosts diffusion models, simply increasing the model capacity does not yield proportional improvements. This is because diffusion models inherently require more granular steps to converge to an optimal solution, regardless of the networks’ representational power. This shows that, from an inference-time perspective, diffusion models are not optimal in utilizing the networks’ capacity.We illustrate these limitations from an inference perspective by examining the DDIM sampler for diffusion models. In each DDIM iteration, the network first generates a prediction using the current input and timestep, then linearly interpolates this prediction toward that of the next timestep. This constrains the expressive capacity of each iteration as it is linear with respect to the next timestep, ultimately capping performance regardless of the training method employed (see figure below).We design our new pre-training algorithm by first aiming to mitigate this inference limitation. Our new method, Inductive Moment Matching (IMM), introduces a subtle yet powerful modification: alongside the current timestep, the network also processes the target timestep to jump towards. This change enhances the flexibility of each inference iteration, paving the way for state-of-the-art performance and efficiency. We realize this improvement by incorporating maximum mean discrepancy — a robust moment matching technique that was developed more than 15 years ago.We test IMM on various hyperparameters and model architectures. On ImageNet 256x256, IMM achieves 1.99 Frechet Inception Distance (FID) and surpasses diffusion models (2.27 FID) and Flow Matching (2.15 FID) with 30x fewer sampling steps. It similarly achieves state-of-the-art 2-step FID of 1.98 on the standard CIFAR-10 dataset for a model trained from scratch.IMM scales with training and inference compute as well as model size. We show in the figure below FID vs. training and inference compute, and we find strong correlation between compute used and performance.Unlike consistency models, which have been shown to have unstable training dynamics, IMM is stable to train across various hyperparameters and architectures.Notably, IMM does not rely on denoising score matching or the score-based stochastic differential equations on which the foundations of diffusion models are built. The key driver of our performance gains is not only moment matching itself, but also our shift towards an inference-first perspective. This not only reveals the inherent limitations in current pre-training paradigms but also empowers us to develop innovative algorithms designed to break through the current limits of pre-training.We believe that this is just the beginning of a paradigm shift towards multi-modal foundation models that transcends current boundaries and fully unlock creative intelligence.If you are interested in the mission, join us.Linqi Zhou, Stefano Ermon, Jiaming Song. “Inductive Moment Matching”.Jiaming Song, Linqi Zhou. “Ideas in Inference-time Scaling can Benefit Generative Pre-training Algorithms”.Song et al. “Denoising Diffusion Implicit Models.” ICLR 2021.Song et al. “Consistency Models.” ICML 2023.Lipman et al. “Flow matching for generative modeling.” ICLR 2023.Gretton et al. “A Kernel Method for the Two-Sample Problem.” NeurIPS 2006.Song et al. “Score-Based Generative Modeling through Stochastic Differential Equations.” ICLR 2021.Kim et al. “Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion.” ICLR 2024.Vincent. “A Connection Between Score Matching and Denoising Autoencoders.” Neural Computation (Vol. 23).Geng et al. “Consistency Models Made Easy”. ICLR 2025.]]></content:encoded></item><item><title>Why isn’t Go used for game development, even though it performs better than C#?</title><link>https://www.reddit.com/r/golang/comments/1j99a3x/why_isnt_go_used_for_game_development_even_though/</link><author>/u/mohamed_essam_salem</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Wed, 12 Mar 2025 02:52:19 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I've been wondering why Go (Golang) isn't commonly used for game development, despite the fact that it generally has better raw performance than C#. Since Go compiles to machine code and has lightweight concurrency (goroutines), it should theoretically be a strong choice.Yet, C# (which is JIT-compiled and typically slower in general applications) dominates game development, mainly because of Unity. Is it just because of the lack of engines and libraries, or is there something deeper—like Go’s garbage collection, lack of low-level control, or weaker GPU support—that makes it unsuitable for real-time game development?Would love to hear thoughts from developers who have tried using Go for games!]]></content:encoded></item><item><title>New ARandR alternative for X11 display settings</title><link>https://github.com/bossadapt/Display-Settings-Plus</link><author>/u/bossadapt</author><category>dev</category><category>reddit</category><pubDate>Wed, 12 Mar 2025 00:52:12 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Traversal-resistant file APIs</title><link>https://go.dev/blog/osroot</link><author>Damien Neil</author><category>dev</category><category>go</category><pubDate>Wed, 12 Mar 2025 00:00:00 +0000</pubDate><source url="http://blog.golang.org/feed.atom">Dev - Golang Blog</source><content:encoded><![CDATA[A path traversal vulnerability arises when an attacker can trick a program
into opening a file other than the one it intended.
This post explains this class of vulnerability,
some existing defenses against it, and describes how the new
 API added in Go 1.24 provides
a simple and robust defense against unintentional path traversal.“Path traversal” covers a number of related attacks following a common pattern:
A program attempts to open a file in some known location, but an attacker causes
it to open a file in a different location.If the attacker controls part of the filename, they may be able to use relative
directory components ("..") to escape the intended location:f, err := os.Open(filepath.Join(trustedLocation, "../../../../etc/passwd"))
On Windows systems, some names have special meaning:// f will print to the console.
f, err := os.Create(filepath.Join(trustedLocation, "CONOUT$"))
If the attacker controls part of the local filesystem, they may be able to use
symbolic links to cause a program to access the wrong file:// Attacker links /home/user/.config to /home/otheruser/.config:
err := os.WriteFile("/home/user/.config/foo", config, 0o666)
If the program defends against symlink traversal by first verifying that the intended file
does not contain any symlinks, it may still be vulnerable to
time-of-check/time-of-use (TOCTOU) races,
where the attacker creates a symlink after the program’s check:// Validate the path before use.
cleaned, err := filepath.EvalSymlinks(unsafePath)
if err != nil {
  return err
}
if !filepath.IsLocal(cleaned) {
  return errors.New("unsafe path")
}

// Attacker replaces part of the path with a symlink.
// The Open call follows the symlink:
f, err := os.Open(cleaned)
Another variety of TOCTOU race involves moving a directory that forms part of a path
mid-traversal. For example, the attacker provides a path such as “a/b/c/../../etc/passwd”,
and renames “a/b/c” to “a/b” while the open operation is in progress.Before we tackle path traversal attacks in general, let’s start with path sanitization.
When a program’s threat model does not include attackers with access to the local file system,
it can be sufficient to validate untrusted input paths before use.Unfortunately, sanitizing paths can be surprisingly tricky,
especially for portable programs that must handle both Unix and Windows paths.
For example, on Windows  reports ,
because the path “\foo” is relative to the current drive.In Go 1.20, we added the 
function, which reports whether a path is “local”. A “local” path is one which:does not escape the directory in which it is evaluated ("../etc/passwd" is not allowed);is not an absolute path ("/etc/passwd" is not allowed);is not empty ("" is not allowed);on Windows, is not a reserved name (“COM1” is not allowed).In Go 1.23, we added the 
function, which converts a /-separated path into a local operating system path.Programs that accept and operate on potentially attacker-controlled paths should almost
always use  or  to validate or sanitize those paths.Path sanitization is not sufficient when attackers may have access to part of
the local filesystem.Multi-user systems are uncommon these days, but attacker access to the filesystem
can still occur in a variety of ways.
An unarchiving utility that extracts a tar or zip file may be induced
to extract a symbolic link and then extract a file name that traverses that link.
A container runtime may give untrusted code access to a portion of the local filesystem.Programs may defend against unintended symlink traversal by using the
path/filepath.EvalSymlinks
function to resolve links in untrusted names before validation, but as described
above this two-step process is vulnerable to TOCTOU races.Before Go 1.24, the safer option was to use a package such as
github.com/google/safeopen,
that provides path traversal-resistant functions for opening a potentially-untrusted
filename within a specific directory.In Go 1.24, we are introducing new APIs in the  package to safely open
a file in a location in a traversal-resistent fashion.The new  type represents a directory somewhere
in the local filesystem. Open a root with the 
function:root, err := os.OpenRoot("/some/root/directory")
if err != nil {
  return err
}
defer root.Close()
 provides methods to operate on files within the root.
These methods all accept filenames relative to the root,
and disallow any operations that would escape from the root either
using relative path components ("..") or symlinks.f, err := root.Open("path/to/file")
 permits relative path components and symlinks that do not escape the root.
For example,  is permitted. Filenames are resolved using the
semantics of the local platform: On Unix systems, this will follow
any symlink in “a” (so long as that link does not escape the root);
while on Windows systems this will open “b” (even if “a” does not exist). currently provides the following set of operations:func (*Root) Create(string) (*File, error)
func (*Root) Lstat(string) (fs.FileInfo, error)
func (*Root) Mkdir(string, fs.FileMode) error
func (*Root) Open(string) (*File, error)
func (*Root) OpenFile(string, int, fs.FileMode) (*File, error)
func (*Root) OpenRoot(string) (*Root, error)
func (*Root) Remove(string) error
func (*Root) Stat(string) (fs.FileInfo, error)
In addition to the  type, the new
 function
provides a simple way to open a potentially-untrusted filename within a
specific directory:f, err := os.OpenInRoot("/some/root/directory", untrustedFilename)
The  type provides a simple, safe, portable API for operating with untrusted filenames.Caveats and considerationsOn Unix systems,  is implemented using the  family of system calls.
A  contains a file descriptor referencing its root directory and will track that
directory across renames or deletion. defends against symlink traversal but does not limit traversal
of mount points. For example,  does not prevent traversal of
Linux bind mounts. Our threat model is that  defends against
filesystem constructs that may be created by ordinary users (such
as symlinks), but does not handle ones that require root privileges
to create (such as bind mounts).On Windows,  opens a handle referencing its root directory.
The open handle prevents that directory from being renamed or deleted until the  is closed. prevents access to reserved Windows device names such as  and .On WASI, the  package uses the WASI preview 1 filesystem API,
which are intended to provide traversal-resistent filesystem access.
Not all WASI implementations fully support filesystem sandboxing,
however, and ’s defense against traversal is limited to that provided
by the WASI impementation.When GOOS=js, the  package uses the Node.js file system API.
This API does not include the openat family of functions,
and so  is vulnerable to TOCTOU (time-of-check-time-of-use) races in symlink
validation on this platform.When GOOS=js, a  references a directory name rather than a file descriptor,
and does not track directories across renames.Plan 9 does not have symlinks.
On Plan 9, a  references a directory name and performs lexical sanitization of
filenames. operations on filenames containing many directory components can be much more expensive
than the equivalent non- operation. Resolving “..” components can also be expensive.
Programs that want to limit the cost of filesystem operations can use  to
remove “..” components from input filenames, and may want to limit the number of
directory components.You should use  or  if:you are opening a file in a directory; ANDthe operation should not access a file outside that directory.For example, an archive extractor writing files to an output directory should use
, because the filenames are potentially untrusted and it would be incorrect
to write a file outside the output directory.However, a command-line program that writes output to a user-specified location
should not use , because the filename is not untrusted and may
refer to anywhere on the filesystem.As a good rule of thumb, code which calls  to combine a fixed directory
and an externally-provided filename should probably use  instead.// This might open a file not located in baseDirectory.
f, err := os.Open(filepath.Join(baseDirectory, filename))

// This will only open files under baseDirectory.
f, err := os.OpenInRoot(baseDirectory, filename)
The  API is new in Go 1.24.
We expect to make additions and refinements to it in future releases.The current implementation prioritizes correctness and safety over performance.
Future versions will take advantage of platform-specific APIs, such as
Linux’s , to improve performance where possible.There are a number of filesystem operations which  does not support yet, such as
creating symbolic links and renaming files. Where possible, we will add support for these
operations. A list of additional functions in progress is in
go.dev/issue/67002.]]></content:encoded></item><item><title>Survey Surfaces High DevOps Burnout Rates Despite AI Advances - DevOps.com</title><link>https://devops.com/survey-surfaces-high-devops-burnout-rates-despite-ai-advances/</link><author>/u/Inner-Chemistry8971</author><category>dev</category><category>reddit</category><pubDate>Tue, 11 Mar 2025 23:41:06 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[A survey of 604 software developers and engineering professionals finds that while 61% work for organizations that are employing artificial intelligence (AI) to build software to some degree nearly two-thirds (65%) still experience burnout. In fact, after maintaining a high-performing team (47%), burnout (41%) is the second most often challenge, survey respondents cited.Conducted by Kickstand Research on behalf of Jellyfish, a provider of a platform for managing software engineering teams, the survey finds that among organizations that have adopted AI, a full 94% said it positively influenced their team’s productivity, with 81% reporting AI increases the quality of code.A total of 84% also said AI frees up time to focus on high-value activities, the survey finds.However, the survey makes it clear there is a disconnect between the managers (48%) and developers/engineers who participated. More than three-quarters of executives (76%) believe their team has embraced AI, while only 52% of the rank-and-file respondents agreed.Of the non-AI users, 48% said their team had not adopted the technology due to security concerns, followed by just over a third (34%) citing a lack of expertise. Just under a quarter (24%) said budget constraints had prevented them from using AI. Notably, 19% of executives who work for organizations that have not embraced AI view it as a gimmick.Jellyfish CEO Andrew Lau said that while there is a lot of hyperbole being thrown around when it comes to AI, software engineering teams should lean into it. AI agents and copilots are bringing inevitable change to software engineering that will change and impact the role of software engineering, he added.AI Models Will Become More CommonplaceLong term, there’s no doubt AI will enable software engineering teams to be more productive as additional advances are made, said Lau. For example, AI models trained for specific domains such as software engineering will become more commonplace, he noted. Many managers and businesspeople in the short term, however, are overestimating the impact AI can in the short term have on software development, he added.In fact, 43% of the developers and engineers surveyed said feel that leadership at their company is out of the loop regarding the challenges software engineering teams face. Just under a third (31%) said their team lacks sufficient visibility into project status and well over a third (37%) said efficiency, predictability and productivity have all decreased on their team in the past year.Overall, more than two-thirds said their engineering organization received a budget increase last year, with 57% of engineering leaders noting the size of their engineering team has increased over the past 12 months. More than half (56%) of all respondents expect the headcount in their department to increase over the next 12 months.However, more than a third (34%) of developers/engineers do not feel the potential for advancement in their current role, with just under a third (32%) considering a career change. Nevertheless, 80% of all respondents said the work they do is rewarding.Thanks to the rise of AI more software will probably be developed in the next few years than all of the past decade. The challenge now becomes how to manage a volume of code that is about to exponentially increase beyond what was once thought to be ever imaginable.]]></content:encoded></item><item><title>I wrote a concurrent log parser in Go to learn about concurrency</title><link>https://www.reddit.com/r/golang/comments/1j94d6e/i_wrote_a_concurrent_log_parser_in_go_to_learn/</link><author>/u/Tack1234</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Tue, 11 Mar 2025 23:02:43 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I wanted to learn about using Go for concurrent tasks (e.g. using goroutines and channels), so I built a tool to solve a real problem I had at work. I wanted to parse CLF (e.g. Apache or Nginx) logs and store them in SQLite so I would be able to perform further data analysis on them without having to resort to "heavier" tools like Grafana.It is mostly meant as a practice project but maybe someone else could also find it handy someday. It is a little rough around the edges but overall I achieved with it what I set out to do and working on it over the last few weeks has taught me a lot about Golang as a newbie. Coming from the overcomplicated world of Node.js (both on the frontend and backend), I've been loving the simplicity of Go!]]></content:encoded></item><item><title>The Startup CTO&apos;s Handbook</title><link>https://github.com/ZachGoldberg/Startup-CTO-Handbook/blob/main/StartupCTOHandbook.md</link><author>simonebrunozzi</author><category>dev</category><category>hn</category><pubDate>Tue, 11 Mar 2025 22:18:42 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>6 Years ago I went all in on Linux, Now I&apos;m just basically an AMD fanboy</title><link>https://www.reddit.com/r/linux/comments/1j91f84/6_years_ago_i_went_all_in_on_linux_now_im_just/</link><author>/u/Chaos_Blades</author><category>dev</category><category>reddit</category><pubDate>Tue, 11 Mar 2025 20:56:58 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[Lets go all the way back to my first PC. Intel P4 with an ATI X1300 (AGP Slot) Played so much Half Life 1/2 on this baby. Also Command and Conquer Generals. After this It was all Intel/Nvidia up to the GTX 1080. This is when I switched to linux because finally Proton. Quickly did I realize Nvidia GPUs on Linux were a problem. Especially once I wanted an HTPC with Holo ISO. This is when I went to the 5950X and 6900XT. Fantastic experience, has aged like fine wine. Just being part on the Linux community and looking at the Nvidia situation... Worse performance compared to Windows, tons of game specific bugs, Wayland issues, taking months to get driver issues fix, driver updates seem to break as much as they fix. So other than the HDMI 2.1 situation with AMD it has been smooth sailing. HDMI situation is more problems with HDMI Forum and TV makers not putting DisplayPort on TVs so I don't blame AMD for this at all. New GPUs just came out and I am not even considering or looking at what Nvidia is doing. Now lets talk about what is making me realize I am basically just an AMD fanboy at this point. I also have a TrueNAS server I have been running for over a decade (FreeNAS 9.2). Which other than a short period of time I was using an AMD Opteron CPU has also traditionally been Intel/Nvidia. That leads us to today. I am about to go out and upgrade a perfectly working Nvidia Quadro M2000 with an AMD Radeon Pro W6400 only because Nvidia driver (reoccurring theme) has issues with locking up SPICE remote desktop instance. Now while I was trying to find a fix for this problem I decided to do a little research for a motherboard/CPU upgrade and low and behold The best price to performance and power savings is a used 2nd gen AMD EPYC to replace my dual socket E5-2680 v3s (I need a lot of PCI-E). At this point the only Intel/Nvidia parts I have is a Quadro P4000 for Plex transcoding and an Intel Atom C3758R in my pfSense box. I also have a Framework 16 and guess what, all AMD.So TLDR, Nvidia sucks on Linux by pretty much every metric other than video encoding and decoding. Intel GPUs are not as fast as AMD for gaming and maybe one day Intel Arc Pro (A60?) will replace my Quadro P4000 but that day is not today. Intel CPUs just are not as good as AMD right now as far as I am concerned or maybe I am truly a fanboy at this point.Also if anybody is wondering by current distros of choice are... TrueNAS SCALE Bazzite (KDE, until COSMIC is stable) (Desktop, Laptop, HTPC) BIG Pop!_OS fan, just not a good distro right now. Also I have kind of fallen in love with immutable fedora.]]></content:encoded></item><item><title>gotmux - Go library for tmux</title><link>https://github.com/GianlucaP106/gotmux</link><author>/u/One_Mess_1093</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Tue, 11 Mar 2025 20:33:10 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>CLI latin/Catholic bible reader with an interactive mode.</title><link>https://gitlab.com/gee.8ruhs/writteninc/-/raw/main/latinbible.c</link><author>/u/Beautiful_Crab6670</author><category>dev</category><category>reddit</category><pubDate>Tue, 11 Mar 2025 19:40:31 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Graph RAG explained</title><link>https://diamantai.substack.com/p/graph-rag-explained?r=336pe4&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;triedRedirect=true</link><author>/u/Diamant-AI</author><category>dev</category><category>reddit</category><pubDate>Tue, 11 Mar 2025 19:24:06 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>[media] Dioxus Subsecond Rust Hotpatch Engine + Ratatui ❤️</title><link>https://www.reddit.com/r/rust/comments/1j8z3yb/media_dioxus_subsecond_rust_hotpatch_engine/</link><author>/u/jkelleyrtp</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Tue, 11 Mar 2025 19:21:56 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>I’m porting over smolagents to go, interested developers?</title><link>https://www.reddit.com/r/golang/comments/1j8z3js/im_porting_over_smolagents_to_go_interested/</link><author>/u/wait-a-minut</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Tue, 11 Mar 2025 19:21:28 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Python has been dominating the AI tooling space but not much longer. The whole agent movement is heavily reliant on networking patterns, microservices, orchestrations etc which makes Go absolutely perfect for this I’ve really liked the approach hugging face took with smolagents which is NOT bloated and overly abstracted thing like langchain. It’s minimal and manages just state, orchestration, and tools. Which is what agents are. Anyone want to help me fully port this lib over to go so we can finally let go shine in the AI agent department?]]></content:encoded></item><item><title>Show HN: Program Explorer, a container playground</title><link>https://programexplorer.org/</link><author>aconz2</author><category>dev</category><category>hn</category><pubDate>Tue, 11 Mar 2025 16:33:28 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[
            Alpha preview, site may be unavailable without notice
        ]]></content:encoded></item><item><title>Show HN: Krep a High-Performance String Search Utility Written in C</title><link>https://davidesantangelo.github.io/krep/</link><author>daviducolo</author><category>dev</category><category>hn</category><pubDate>Tue, 11 Mar 2025 16:12:43 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: We built a Plug-in Home Battery for the 99.7% of us without Powerwalls</title><link>https://pilaenergy.com/</link><author>coleashman</author><category>dev</category><category>hn</category><pubDate>Tue, 11 Mar 2025 15:48:33 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>A 10x Faster TypeScript</title><link>https://devblogs.microsoft.com/typescript/typescript-native-port/</link><author>DanRosenwasser</author><category>dev</category><category>hn</category><pubDate>Tue, 11 Mar 2025 14:32:23 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Today I’m excited to announce the next steps we’re taking to radically improve TypeScript performance.The core value proposition of TypeScript is an excellent developer experience.
As your codebase grows, so does the value of TypeScript itself, but in many cases TypeScript has not been able to scale up to the very largest codebases.
Developers working in large projects can experience long load and check times, and have to choose between reasonable editor startup time or getting a complete view of their source code.
We know developers love when they can rename variables with confidence, find all references to a particular function, easily navigate their codebase, and do all of those things without delay.
New experiences powered by AI benefit from large windows of semantic information that need to be available with tighter latency constraints.
We also want fast command-line builds to validate that your entire codebase is in good shape.To meet those goals, we’ve begun work on a native port of the TypeScript compiler and tools.
The native implementation will drastically improve editor startup, reduce most build times by 10x, and substantially reduce memory usage.
By porting the current codebase, we expect to be able to preview a native implementation of  capable of command-line typechecking by mid-2025, with a feature-complete solution for project builds and a language service by the end of the year.You can , which is offered under the same license as the existing TypeScript codebase.
Check the README for instructions on how to build and run  and the language server, and to see a summary of what’s implemented so far.
We’ll be posting regular updates as new functionality becomes available for testing.Our native implementation is already capable of loading many popular TypeScript projects, including the TypeScript compiler itself.
Here are times to run  on some popular codebases on GitHub of varying sizes:While we’re not yet feature-complete, these numbers are representative of the order of magnitude performance improvement you’ll see checking most codebases.We’re incredibly excited about the opportunities that this massive speed boost creates. Features that once seemed out of reach are now within grasp.
This native port will be able to provide instant, comprehensive error listings across an entire project, support more advanced refactorings, and enable deeper insights that were previously too expensive to compute.
This new foundation goes beyond today’s developer experience and will enable the next generation of AI tools to enhance development, powering new tools that will learn, adapt, and improve the coding experience.Most developer time is spent in editors, and it’s where performance is most important.
We want editors to load large projects quickly, and respond quickly in all situations.
Modern editors like Visual Studio and Visual Studio Code have excellent performance as long as the underlying language services are also fast.
With our native implementation, we’ll be able to provide incredibly fast editor experiences.Again using the Visual Studio Code codebase as a benchmark, the current time to load the entire project in the editor on a fast computer is about 9.6 seconds.
This drops down to about 1.2 seconds with the native language service, an 8x improvement in project load time in editor scenarios.
What this translates to is a faster working experience from the time you open your editor to your first keystroke in any TypeScript codebase.
We expect all projects to see this level of improvement in load time.Overall memory usage also appears to be roughly half of the current implementation, though we haven’t actively investigated optimizing this yet and expect to realize further improvements.
Editor responsiveness for all language service operations (including completion lists, quick info, go to definition, and find all references) will also see significant speed gains.
We’ll also be moving to the Language Server Protocol (LSP), a longstanding infrastructural work item to better align our implementation with other languages.Our most recent TypeScript release was TypeScript 5.8, with TypeScript 5.9 coming soon.
The JS-based codebase will continue development into the 6.x series, and TypeScript 6.0 will introduce some deprecations and breaking changes to align with the upcoming native codebase.When the native codebase has reached sufficient parity with the current TypeScript, we’ll be releasing it as .
This is still in development and we’ll be announcing stability and feature milestones as they occur.For the sake of clarity, we’ll refer to them simply as TypeScript 6 (JS) and TypeScript 7 (native), since this will be the nomenclature for the foreseeable future.
You may also see us refer to “Strada” (the original TypeScript codename) and “Corsa” (the codename for this effort) in internal discussions or code comments.While some projects may be able to switch to TypeScript 7 upon release, others may depend on certain API features, legacy configurations, or other constraints that necessitate using TypeScript 6.
Recognizing TypeScript’s critical role in the JS development ecosystem, we’ll still be maintaining the JS codebase in the 6.x line until TypeScript 7+ reaches sufficient maturity and adoption.Our long-term goal is to keep these versions as closely aligned as possible so that you can upgrade to TypeScript 7 as soon as it meets your requirements, or fall back to TypeScript 6 if necessary.In the coming months we’ll be sharing more about this exciting effort, including deeper looks into performance, a new compiler API, LSP, and more.
We’ve written up some FAQs on the GitHub repo to address some questions we expect you might have.
We also invite you to join us for an AMA at the TypeScript Community Discord at  on March 13th.A 10x performance improvement represents a massive leap in the TypeScript and JavaScript development experience, so we hope you are as enthusiastic as we are for this effort!]]></content:encoded></item><item><title>Happy 20th birthday, Y Combinator</title><link>https://twitter.com/garrytan/status/1899092996702048709</link><author>btilly</author><category>dev</category><category>hn</category><pubDate>Tue, 11 Mar 2025 14:14:35 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Show HN: Factorio Learning Environment – Agents Build Factories</title><link>https://jackhopkins.github.io/factorio-learning-environment/</link><author>noddybear</author><category>dev</category><category>hn</category><pubDate>Tue, 11 Mar 2025 12:02:02 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[I'm Jack, and I'm excited to share a project that has channeled my Factorio addiction recently: the Factorio Learning Environment (FLE).FLE is an open-source framework for developing and evaluating LLM agents in Factorio. It provides a controlled environment where AI models can attempt complex automation, resource management, and optimisation tasks in a grounded world with meaningful constraints.A critical advantage of Factorio as a benchmark is its unbounded nature. Unlike many evals that are quickly saturated by newer models, Factorio's geometric complexity scaling means it won't be "solved" in the next 6 months (or possibly even years). This allows us to meaningfully compare models by the order-of-magnitude of resources they can produce - creating a benchmark with longevity.The project began 18 months ago after years of playing Factorio, recognising its potential as an AI research testbed. A few months ago, our team (myself, Akbir, and Mart) came together to create a benchmark that tests agent capabilities in spatial reasoning and long-term planning.Two technical innovations drove this project forward: First, we discovered that piping Lua into the Factorio console over TCP enables running (almost) arbitrary code without directly modding the game. Second, we developed a first-class Python API that wraps these Lua programs to provide a clean, type-hinted interface for AI agents to interact with Factorio through familiar programming paradigms.Agents interact with FLE through a REPL pattern:
1. They observe the world (seeing the output of their last action)
2. Generate Python code to perform their next action
3. Receive detailed feedback (including exceptions and stdout)We provide two main evaluation settings:
- Lab-play: 24 structured tasks with fixed resources
- Open-play: An unbounded task of building the largest possible factory on a procedurally generated mapWe found that while LLMs show promising short-horizon skills, they struggle with spatial reasoning in constrained environments. They can discover basic automation strategies (like electric-powered drilling) but fail to achieve more complex automation (like electronic circuit manufacturing). Claude Sonnet 3.5 is currently the best model (by a significant margin).You'll need:
- Factorio (version 1.1.110)
- Docker
- Python 3.10+The README contains detailed installation instructions and examples of how to run evaluations with different LLM agents.We would love to hear your thoughts and see what others can do with this framework!]]></content:encoded></item><item><title>REST vs GraphQL</title><link>https://blog.algomaster.io/p/rest-vs-graphql</link><author>Ashish Pratap Singh</author><category>dev</category><enclosure url="https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc4d86506-cb5f-4c9f-a56a-257439ec46eb_1666x1210.png" length="" type=""/><pubDate>Tue, 11 Mar 2025 04:50:22 +0000</pubDate><source url="https://blog.algomaster.io/">Dev - Algomaster</source><content:encoded><![CDATA[ are the backbone of modern applications, acting as the bridge between client applications and backend servers.Among the many API design choices,  and  have emerged as two dominant approaches.Both offer powerful ways to retrieve and manipulate data, but they are built on fundamentally different philosophies.REST, a time-tested architectural style, structures APIs around fixed endpoints and HTTP methods, making it intuitive and widely adopted. On the other hand, GraphQL, a newer query language developed by Facebook, takes a more flexible and efficient approach, allowing clients to request exactly the data they need in a single request.In this article, we’ll break down REST and GraphQL, compare their differences, and help you decide which one is best suited for your use case. emerged in the early 2000s as a set of architectural principles for designing networked applications.REST is not a protocol or standard but rather a set of guiding principles that leverage the existing  to enable communication between clients and servers.At its core, REST is built around . Each resource (such as a user, order, or product) is uniquely identified by a Uniform Resource Locator, and clients interact with these resources using a fixed set of HTTP methods. → Retrieve a resource (e.g.,  to fetch user data). → Create a new resource (e.g.,  to add a new user). → Update an existing resource (e.g.,  to update user details). → Remove a resource (e.g.,  to delete a user).For example, let’s say a client needs information about a specific user with .The client makes a requestThe server responds with a JSON representation of the userREST APIs typically  and use  to communicate the outcome of the request: → Resource successfully created → Client error (e.g., missing required fields) → Requested resource does not exist500 Internal Server Error → Unexpected server issueSimplicity and Intuitive Design: The resource-based model aligns well with most business domains, making REST intuitive for developers.: Each request contains all the information needed to complete it, making REST scalable across distributed systems.: HTTP's caching mechanisms can be leveraged to improve performance.REST APIs can be easily scaled using load balancers and CDNs.: With nearly two decades of widespread use, REST enjoys robust tooling, documentation, and developer familiarity.REST endpoints often return , leading to inefficient network usage. For example, if a mobile app only needs a user’s name and email, but the API response includes additional fields like address, phone number, and metadata, it results in .: If an API doesn’t return related data, the client may need to  to retrieve all required information. For example, to get user details and their posts, a client might have to make: (fetch user) (fetch user’s posts): When APIs evolve, maintaining backward compatibility becomes difficult. REST APIs often require  (, ), adding maintenance overhead.Rigid Response Structure: The server defines how data is returned, and clients must adapt to it—even if they only need a subset of the data.For years,  was the de facto standard for building APIs. However, as applications grew more complex, REST began to show limitations—especially in scenarios where clients needed fine-grained control over the data they fetched.To address these challenges, Facebook introduced GraphQL in 2015, offering a more flexible and efficient approach to data retrieval.Unlike REST, which organizes APIs around fixed endpoints and HTTP methods, GraphQL is a  that allows clients to request exactly the data they need—nothing more, nothing less.A  () replaces multiple REST endpoints, allowing clients to structure their own queries instead of relying on predefined responses.Here, the query asks for a specific user's firstName, email, profileUrl and posts, all within a GraphQL aggregates the data from multiple services and returns precisely the requested data.It solves the problems of  (getting unnecessary data) and  (requiring multiple requests to retrieve related data).Unlike REST, where API responses are  and may vary across versions, GraphQL enforces a strict schema that defines the shape of the data.A simple GraphQL schema for the above example might look like this:type User {
  id: ID!
  firstName: String!
  lastName: String!
  email: String!
  profile: Profile!
  posts: [Post!]
}

type Profile {
  id: ID!
  url: String!
}

type Post {
  id: ID!
  title: String!
  publishedDate: String!
  content: String!
  author: User!
}

type Query {
  user(id: ID!): User
  posts: [Post!]!
}Three Core Functionalities of GraphQLGraphQL provides three core functionalities:Similar to GET requests in REST, GraphQL queries allow clients to request specific fields of data.Clients have full control over what they retrieve, avoiding unnecessary data fetching.Example: Fetching specific user and post details in a single requestquery {
  user(id: 123) {
    name
    email
    posts {
      title
      content
    }
  }
}Equivalent to POST, PUT, PATCH, or DELETE in REST. Used to create, update, or delete resources in the API.Example: Creating a new postmutation {
  createPost(title: "GraphQL vs REST", content: "GraphQL solves many of REST's limitations...", publishedDate: "2025-03-10") {
    id
    title
    content
  }
}The response will contain the newly created post with its .3.  → Real-Time UpdatesUnlike REST, which requires polling or WebSockets for real-time updates, GraphQL subscriptions enable clients to listen for changes and receive updates automatically when data is modified.Ideal for chat applications, live feeds, stock market updates, and notifications.Example: Listening for new postssubscription {
  newPost {
    title
    content
    author {
      name
    }
  }
}Whenever a , all subscribed clients will .How GraphQL Differs from RESTBoth GraphQL and REST rely on HTTP requests and responses, but they differ in how they structure and deliver data.REST centers around resources (each identified by a URL).GraphQL centers around a schema that defines the types of data available.In REST, the  decides which data is included in a response. If a client requests a blog post, the API might also return related , even if they aren’t needed.With GraphQL, the  what to fetch. This makes GraphQL more flexible but also introduces challenges in caching and performance optimization.: Clients can request only the fields they need, reducing over-fetching and under-fetching.Single Request for Multiple Resources: Related data can be retrieved in one request, solving REST’s  query problem.: GraphQL APIs use a schema to define available data, making them easier to explore and document.Real-time Data with Subscriptions: GraphQL natively supports real-time data updates through subscriptions, enabling clients to receive automatic notifications whenever data changes on the server.API Evolution Without Versioning: New fields can be added without breaking existing queries, avoiding REST-style ,  versioning issues.: Unlike REST, which can be used with basic HTTP clients (cURL, browsers), GraphQL requires a GraphQL server, schema, and resolvers.: REST APIs leverage HTTP caching (e.g., browser caching, CDNs), but GraphQL queries use POST requests, making caching trickier. Since clients can request arbitrary amounts of data, GraphQL APIs must be carefully optimized to prevent performance issues. Unoptimized queries (e.g., deeply nested requests) can lead to costly database scans, increasing the risk of denial-of-service (DoS) attacks.Performance Risks with GraphQLImagine a mobile app introduces a  that unexpectedly triggers a  on a critical database table.With REST, this scenario is less likely because API endpoints are predefined, and developers control how data is exposed.With GraphQL, the client , which could inadvertently request massive amounts of data. If a poorly designed query is executed on a high-traffic service, it could bring down the entire database.To mitigate this, GraphQL APIs require strict query rate limiting, depth restrictions, and cost analysis mechanisms—adding additional complexity to the implementation.There is no  answer.  remains a great choice for simple APIs, while  is powerful for complex applications with varying data needs.Ultimately, it’s not about which is better, but which is better for your specific needs.Your API is simple and doesn’t require flexible queries.You need caching benefits from HTTP. You need a standardized, well-established API  approach.You’re integrating with third-party services.Your team is already familiar with REST and need faster implementation.You need flexible and efficient data fetching.Your API serves multiple clients (mobile, web, IoT) with different data needs.Real-time updates are required (GraphQL subscriptions). You want to avoid API versioning issues.Your application requires deeply nested dataCan You Use Both REST and GraphQL?Absolutely! REST and GraphQL are , and many organizations implement a  to get the best of both worlds:GraphQL for client-facing applications where flexibility, performance, and dynamic querying are essential.REST for admin interfaces, third-party integrations, and internal microservices where statelessness, caching, and simplicity are beneficial.If you found it valuable, hit a like ❤️ and consider subscribing for more such content every week.This post is public so feel free to share it. If you’re enjoying this newsletter and want to get even more value, consider becoming a .I hope you have a lovely day!]]></content:encoded></item><item><title>Show HN: Seven39, a social media app that is only open for 3 hours every evening</title><link>https://www.seven39.com/</link><author>mklyons</author><category>dev</category><category>hn</category><pubDate>Tue, 11 Mar 2025 01:05:10 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Because social media is better when we're all online together.No endless scrolling. No FOMO. Just 3 hours of fun every evening.The domain was available.]]></content:encoded></item><item><title>Developer Convicted For &apos;Kill Switch&apos; Code Activated Upon His Termination</title><link>https://developers.slashdot.org/story/25/03/10/1921202/developer-convicted-for-kill-switch-code-activated-upon-his-termination?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>dev</category><category>slashdot</category><pubDate>Mon, 10 Mar 2025 19:30:00 +0000</pubDate><source url="https://developers.slashdot.org/">Dev - Slashdot - Dev</source><content:encoded><![CDATA[A 55-year-old software developer faces up to 10 years in prison after being convicted for deploying malicious code that sabotaged his former employer's network, causing hundreds of thousands of dollars in losses. 

Davis Lu was convicted by a jury for causing intentional damage to protected computers owned by power management company Eaton Corp., the US Department of Justice announced Friday. Lu, who worked at Eaton for 11 years, became disgruntled after a 2018 corporate "realignment" reduced his responsibilities. 

He created malicious code that deleted coworker profile files, prevented logins, and caused system crashes. His most destructive creation was a "kill switch" named "IsDLEnabledinAD" that automatically activated upon his termination in 2019, disrupting Eaton's global operations. Lu admitted to creating some malicious code but plans to appeal the verdict.]]></content:encoded></item></channel></rss>