<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Dev News</title><link>https://konrad.website/liveboat-github-runner/</link><description></description><item><title>Ubuntu Will Use Rust For Dozens of Core Linux Utilities</title><link>https://news.slashdot.org/story/25/11/01/079206/ubuntu-will-use-rust-for-dozens-of-core-linux-utilities?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>EditorDavid</author><category>dev</category><category>slashdot</category><pubDate>Sat, 1 Nov 2025 17:34:00 +0000</pubDate><source url="https://developers.slashdot.org/">Dev - Slashdot - Dev</source><content:encoded><![CDATA[ Ubuntu "is adopting the memory-safe Rust language," reports ZDNet, citing remarks at this year's Ubuntu Summit from Jon Seager, Canonical's VP of engineering for Ubuntu:

. Seager said the engineering team is focused on replacing key system components with Rust-based alternatives to enhance safety and resilience, starting with Ubuntu 25.10. He stressed that resilience and memory safety, not just performance, are the principal drivers: "It's the enhanced resilience and safety that is more easily achieved with Rust ports that are most attractive to me". This move is echoed in Ubuntu's adoption of sudo-rs, the Rust implementation of sudo, with fallback and opt-out mechanisms for users who want to use the old-school sudo command. 


In addition to sudo-rs, Ubuntu 26.04 will use the Rust-based uutils/coreutils for Linux's default core utilities. This setup includes ls, cp, mv, and dozens of other basic Unix command-line tools. This Rust reimplementation aims for functional parity with GNU coreutils, with improved safety and maintainability. 

On the desktop front, Ubuntu 26.04 will also bring seamless TPM-backed full disk encryption. If this approach reminds you of Windows BitLocker or MacOS FileVault, it should. That's the idea. 

In other news, Canonical CEO Mark Shuttleworth said "I'm a believer in the potential of Linux to deliver a desktop that could have wider and universal appeal." (Although he also thinks "the open-source community needs to understand that building desktops for people who aren't engineers is different. We need to understand that the 'simple and just works' is also really important.") 


Shuttleworth answered questions from Slashdot's readers in 2005 and 2012.]]></content:encoded></item><item><title>DigitalOcean is chasing me for $0.01: What it taught me about automation</title><link>https://linuxblog.io/digitalocean-1-cent-automation/</link><author>/u/modelop</author><category>dev</category><category>reddit</category><pubDate>Sat, 1 Nov 2025 16:29:33 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[There are three kinds of emails that can ruin a quiet Saturday: a security warning, an outage alert, and, apparently, a repeat reminder that you owe a cloud provider one cent, yes, $0.01. I’ve been using DigitalOcean since 2013. Personally, I don’t use it often, but I log in several times a week to support clients hosted there.A chuckle and twelve years of cloud loveOver the past twelve years I have set up and managed countless droplets, and DigitalOcean’s support and uptime have been excellent; this isn’t that kind of post.It’s a lighthearted look at what happens when automation churns out more notifications than the situation may warrant, and why even a penny‑sized bill can teach us bigger lessons about design and efficiency.On Saturday, 25 October 2025, an email with the subject “Payment required: Your pre‑payment has been used” arrived in my inbox. It informed me that my prepaid credit was insufficient to cover the month’s usage and urged me to “make another payment or add an alternate payment method.”There was just one catch: the outstanding balance was $0.01. I chuckled, and went on with my day only to receive the exact message two more times over the coming days. By the time Saturday rolled around, my inbox looked like this:The inbox search screenshot above shows the cadence: identical “Payment required” messages on October 25th, 28th, and 31st, 2025, followed by an email on November 1, 2025, titled “Your 2025‑10 invoice is available.”The invoice email (screenshot also above) contains a table that lists the usage charges for October as $0.01, notes that the payment method will only be charged if the balance exceeds $3.00, and invites me to “View Invoice.” Here’s what those other three messages look like:My immediate reaction was a bit of a chuckle, but by the fourth email, I was more curious than anything: Why does an automated billing system send four emails about a 1-cent balance? DigitalOcean’s billing documentation notes that invoices are generated monthly. In my case, the system sent several “action required” emails, maybe because I don’t have a payment method saved? But in any case, I rarely use my personal DigitalOcean account beyond just quick tests:This experience of multiple emails for 1 cent owed, prompted me to think about the hidden costs of excessive email notifications and how we can design billing and alerting in a more thoughtful way.The True Cost of an EmailEmail feels free because individuals don’t pay per message, but providers do. A 2025 breakdown of email marketing costs notes that the typical cost for a business to send emails is $1–$2 per thousand messages, translating to roughly $0.001–$0.002 per email. Amazon’s Simple Email Service charges $0.10 per 1,000 emails for outbound messages (sending or receiving) and a few cents per gigabyte for attachments.This cost is likely less for DigitalOcean, with the three “Payment required” notices and one invoice with attachment costing the company at most between a tenth and two‑tenths of a cent to send. But multiply that by hundreds of thousands of customers, and it highlights how easy it is to use resources to clutter inboxes over microbalances.The monetary cost is only part of the picture. Email has an environmental footprint because electricity powers servers, networks, and client devices. Researchers estimate that more than 306 billion emails were sent in 2021, and the total is expected to hit almost 400 billion this year, thanks to DigitalOcean. jk!!According to Mike Berners‑Lee, a short text email can produce 0.2–0.3 g of CO₂, while a longer message with attachments can produce 17 g; an email blast to 100 people may generate 26 g or more. Email‑related emissions accounted for approximately 150 million tons of CO₂e in 2019. That’s about 0.3% of the world’s carbon footprint. But more importantly, about 25% added to users’ annoyance levels – Source: Notification fatigue and design principlesIt isn’t just about costs or the environment. Usability tests consistently show that frequent alerts are one of the top user complaints. In fact, it’s been proven by Facebook and others that sending fewer notifications can be better for both engagement and retention.Good notification design also recognizes levels of severity: high‑attention alerts (e.g., security breaches or failed payments) should prompt immediate action, while low‑attention messages (informational updates) can be bundled or deferred. Services like Slack, for example, adapt notification frequency automatically when channels become very active.Looking at DigitalOcean’s billing reminders, it’s easy to see opportunities for improvement. A one‑cent balance does not warrant three emails + an invoice. The first message could have been informational (“heads up, your balance is low”), the second might wait until the balance crosses a predetermined threshold (say $1 or $3), and the third could be a month or 3 months later.Alternatively, DigitalOcean could incorporate a small balance waiver similar to the one many credit card issuers use. Banks recognize that it’s not cost‑effective to chase pennies; they round down or apply a credit adjustment on the next statement. The same logic could help cloud providers reduce overhead and user frustration.It’s not just DigitalOcean: micro‑balances happen everywhereDigitalOcean isn’t alone in sending tiny bills. Back in 2013, an Optus customer in Australia posted on Whirlpool forums that a billing error left them with a one‑cent overdue notice after receiving a reimbursement. One commenter wrote that it would cost the company “more in personnel overheads to deal with this stupid billing error, than what it’s worth”, while another explained, “It’s an automated system, mate. Just relax.”The moral of the story is that most companies rely on automated billing scripts, and without sensible thresholds, they’ll dutifully produce statements for even the most trivial amounts.In practice, if you owe 99 cents or less, many companies apply a credit adjustment and report a zero balance. The banking industry has recognized that goodwill and efficiency outweigh the pennies left on the ledger. If major financial institutions can swallow a dollar, cloud platforms with higher margins can too.What this taught me and how I’ve been guilty tooAs someone who deploys systems and manages mail servers, I can’t throw stones without acknowledging my own missteps. Earlier this year, I built dewedda.com, a storm‑watch website for the Eastern Caribbean. Part of which was to send automatic email alerts to subscribers when storms approached islands within specific distances and directions.In testing, everything looked great: the algorithm computed wind fields, adjusted for intensity, and tracked dozens of scenarios. But the first time a real storm approached, my code started hammering subscribers with unnecessary alerts. It didn’t account for storms that curved away or systems that were still unnamed, resulting in duplicates, so people kept receiving warnings even when there was no threat or duplicate emails. I had to scramble to adjust the logic.The experience taught me humility and the importance of edge cases, and it makes me more sympathetic to DigitalOcean’s engineers. Building resilient billing and notification systems is complex. Edge cases arise when accounts straddle billing cycles, use promotional credits, or move between team and personal billing. Legacy code and third‑party integrations can behave unpredictably. What matters is how we learn from these events.Conclusion (yes, I paid that one cent)I paid the 1 cent balance owed to DigitalOcean. But who covers that transaction cost?In the end, I did what any responsible business owner would do: I logged into my account and paid the one‑cent balance. Because, it would sit there for months, I only used a droplet for ~1 hour to test something. My personal DigitalOcean account goes mostly unused. So paying this invoice also means no recurring emails to pay my bill. Maybe that’s their plan? Ha!I hope this article highlights the hidden inefficiencies that creep into automation, whether it’s cloud invoices, marketing emails, or storm alerts.I still recommend DigitalOcean to friends and clients. They offer a great product at a fair price, with transparent billing. Being able to spin up a droplet in a few seconds makes life easy for Linux nerds like me. This one‑cent episode doesn’t change that; it simply underscores the value of thoughtful notification design. There are no affiliate links in this post either.In summary, as repeatedly proven, sending fewer, more relevant notifications improves user satisfaction and retention. The environmental data also shows that unnecessary emails carry hidden costs, and financial industry practices demonstrate that forgiving tiny balances can be cheaper than collecting them.A bit of humor on a Saturday morning turned into a lesson for all of us on building better systems. And yes, just in case the automated script is listening, I can confirm that as of writing this, my DigitalOcean account balance is zero.]]></content:encoded></item><item><title>GHC now runs in the browser</title><link>https://discourse.haskell.org/t/ghc-now-runs-in-your-browser/13169</link><author>kaycebasques</author><category>dev</category><category>hn</category><pubDate>Sat, 1 Nov 2025 16:29:23 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[thanks for the reports! a few seconds of freeze during start-time is expected, since it needs to download ~50M of a rootfs tarball and extract it, then link the ghc library and all its dependencies. as for safari, it’s strange since i i just landed a workaround for a webkit bug that breaks the wasm dynamic linker a few days ago, i’ll take a closer look later.]]></content:encoded></item><item><title>Convex Optimization (or Mathematical Programming) in Go</title><link>https://www.reddit.com/r/golang/comments/1ols1gn/convex_optimization_or_mathematical_programming/</link><author>/u/RobotCyclist23</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 1 Nov 2025 16:23:44 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Do you write a lot of Convex (or similar) Optimization problems and have been yearning for a way to model them in Go? MatProInterface.go can help you (and needs your input to gain more maturity)! Feel free to try it and let me know what you think!]]></content:encoded></item><item><title>Updated practice for review articles and position papers in ArXiv CS category</title><link>https://blog.arxiv.org/2025/10/31/attention-authors-updated-practice-for-review-articles-and-position-papers-in-arxiv-cs-category/</link><author>dw64</author><category>dev</category><category>hn</category><pubDate>Sat, 1 Nov 2025 14:58:05 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Cycle-accurate 6502 emulator as coroutine in Rust</title><link>https://github.com/bagnalla/6502</link><author>/u/bagnalla</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sat, 1 Nov 2025 14:44:53 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Go&apos;s Context Logger</title><link>https://github.com/pablovarg/contextlogger?tab=readme-ov-file#examples</link><author>/u/PurityHeadHunter</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 1 Nov 2025 14:24:13 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hello Gophers! A while ago, I started using contextual logging in my projects and found it made debugging significantly easier. Being able to trace request context through your entire call stack is a game-changer for understanding what's happening in your system.This project started as a collection of utility functions I copy-pasted between projects. Eventually, it grew too large to maintain that way, so I decided to turn it into a proper library and share it with the community. https://github.com/PabloVarg/contextloggerContext Logger is a library that makes it easy to propagate your logging context through Go's  and integrates seamlessly with Go's standard library, mainly  and . If this is something that you usually use or you're interested on using it for your projects, take a look at some Usage Examples.For a very simple example, here you can see how to:Embed a logger into your contextUpdate the context (this can be done many times before logging)Log everything that you have included in your context so farctx = contextlogger.EmbedLogger(ctx) contextlogger.UpdateContext(ctx, "userID", user.ID) contextlogger.LogWithContext(ctx, slog.LevelInfo, "done") ]]></content:encoded></item><item><title>Async Rust explained without Tokio or Smol</title><link>https://youtu.be/_x61dSP4ZKM?si=XPDtuH13Du-s5KTD</link><author>/u/Gisleburt</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sat, 1 Nov 2025 14:00:54 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>unsupportedConfigOverrides USAGE</title><link>https://www.reddit.com/r/kubernetes/comments/1olodfm/unsupportedconfigoverrides_usage/</link><author>/u/BigBprofessional</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Sat, 1 Nov 2025 13:52:27 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Need Advice: Bitbucket Helm Repo Structure for Multi-Service K8s Project + Shared Infra (ArgoCD, Vault, Cert-Manager, etc.)</title><link>https://www.reddit.com/r/kubernetes/comments/1olnp4b/need_advice_bitbucket_helm_repo_structure_for/</link><author>/u/Dependent_Concert446</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Sat, 1 Nov 2025 13:22:05 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I’m looking for some advice on how to organize our Helm charts and Bitbucket repos for a growing  setup.We currently have  that contains everything — about  several  (like ArgoCD, Vault, Cert-Manager, etc.).For our , we created  that’s used for microservices. We don’t have separate repos for each microservice — all are managed under the same project.Here’s a simplified view of the repo structure:app/ ├── project-argocd/ │ ├── charts/ │ └── values.yaml ├── project-vault/ │ ├── charts/ │ └── values.yaml │ ├── project-chart/ # Base chart used only for microservices │ ├── basechart/ │ │ ├── templates/ │ │ └── Chart.yaml │ ├── templates/ │ ├── Chart.yaml # Defines multiple services as dependencies using │ └── values/ │ ├── cluster1/ │ │ ├── service1/ │ │ │ └── values.yaml │ │ └── service2/ │ │ └── values.yaml │ └── values.yaml │ │ # Each values file under 'values/' is synced to clusters via ArgoCD │ # using an ApplicationSet for automated multi-cluster deployments The following  are also in the same repo right now:Project Contour (Ingress)(and other cluster-level tools like k3s, Longhorn, etc.)These are not tied to the application project — they’re might shared and deployed across multiple clusters and environments.Should I move these shared infra components into a separate “infra” Bitbucket repo (including their Helm charts, Terraform, and Ansible configs)?For GitOps with , would it make more sense to split things like this:  → all microservices + base Helm chart → cluster-level services (ArgoCD, Vault, Cert-Manager, Longhorn, etc.)How do other teams structure and manage their repositories, and what are the best practices for this in DevOps and GitOps? Used AI to help write and format this post for grammar and readability.]]></content:encoded></item><item><title>CharlotteOS – An Experimental Modern Operating System</title><link>https://github.com/charlotte-os/Catten</link><author>ementally</author><category>dev</category><category>hn</category><pubDate>Sat, 1 Nov 2025 13:12:47 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>SQLite concurrency and why you should care about it</title><link>https://jellyfin.org/posts/SQLite-locking/</link><author>HunOL</author><category>dev</category><category>hn</category><pubDate>Sat, 1 Nov 2025 12:59:03 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[SQLite is a powerful database engine, but due to its design, it has limitations that should not be overlooked.Jellyfin has used a SQLite-based database for storing most of its data for years, but it has also encountered issues on many systems. In this blog post, I will explain how we address these limitations and how developers using SQLite can apply the same solutions.This will be a technical blog post intended for developers and everyone wanting to learn about concurrency.Also Jellyfin's implementation of locking for SQLite should be fairly easy to be implemented into another EF Core application if you are facing the same issue.SQLite is a file-based database engine running within your application and allows you to store data in a relational structure.
Overall it gives your application the means of storing structured data as a single file and without having to depend on another application to do so.
Naturally this also comes at a price. If your application fully manages this file, the assumption must be made that your application is the sole owner of this file, and nobody else will tinker with it while you are writing data to it.So an application that wants to use SQLite as its database needs to be the only one accessing it.
Having established this fact, an important thought arises: if only a single write operation should be performed on a single file at a time, this rule must also apply to operations within the same application.SQLite has a feature that tries to get around this limitation: the Write-Ahead-Log (WAL).
The WAL is a separate file that acts as a journal of operations that should be applied to an SQLite file.
This allows multiple parallel writes to take place and get enqueued into the WAL.
When another part of the application wants to read data, it reads from the actual database, then scans the WAL for modifications and applies them on the fly.
This is not a foolproof solution; there are still scenarios where WAL does not prevent locking conflicts.A transaction is supposed to ensure two things.
Modifications made within a transaction can be reverted, either when something goes wrong or when the application decides it should and optionally a transaction may also block other readers from reading data that is modified within a transaction.
This is where it gets spicy and we come to the real reason why I am writing this blog post.
For some reason on some systems that run Jellyfin when a transaction takes place the SQLite engine reports the database is locked and instead of waiting for the transaction to be resolved the engine refuses to wait and just crashes.
This seems to be a not uncommon issue and there are many reports to be found on the issue.The factor that makes this issue so bad is that it does not happen reliably. So far we only have one team member where this can be (somewhat) reliably be reproduced which makes this an even worse a bug.
From the reports this issue happens across all operating systems, drive speeds and with or without virtualization.
So we do not have any deciding factor identified that even contributes to the likelihood of the issue happening.Having established the general theory on how SQLite behaves, we also have to look at the specifics of Jellyfins usage of SQLite.
During normal operations on a recommended setup (Non-Networked Storage and preferably SSD) its unusual for any problems to arise, however the way Jellyfin utilises the SQLite db up to 10.11 is very suboptimal.
In versions prior to 10.11 Jellyfin had a bug in its parallel task limit which resulted in exponential overscheduling of library scan operations which hammered the database engine with thousands of parallel write requests that an SQLite engine is simply not able to handle.
While most SQLite engine implementations have retry behavior, they also have timeouts and checks in place to prevent limitless waiting so if we stress the engine enough, it just fails with an error.
That and very long running and frankly unoptimized transactions could lead to the database just being overloaded with requests and flaking out.Since we moved the codebase over to EF Core proper, we have the tools to actually do something about this as EF Core gives us a structured abstraction level.
EF Core supports a way of hooking into every command execution or transaction by creating Interceptors.
With an interceptor we can finally do the straight forward idea of just "not" writing to the database in parallel in a transparent way to the caller.
The overall idea is to have multiple strategies of locking. Because all levels of synchronization will inevitably come at the cost of performance, we only want to do it when it is really necessary.
So, I decided on three locking strategies:As a default, the no-lock behavior does exactly what the name implies. Nothing. This is the default because my research shows that for 99% all of this is not an issue and every interaction at this level will slow down the whole application.Both the optimistic and pessimistic behaviors use two interceptors—one for transactions and one for commands—and override  in .Optimistic locking behavior​Optimistic locking means to assume the operation in question will succeed and only handle issues afterwards. In essence this can be boiled down to "Try and Retry and Retry ..." for a set number of times until either we succeed with the operation or fail entirely.
This still leaves the possibility that we will not actually be able to perform a write, but the introduced overhead is far less than the Pessimistic locking behavior.The idea behind how this works is simple: every time two operations try to write to the database, one will always win. The other will fail, wait some time, then retry a few times.Jellyfin uses the  library perform the retry behavior and will only retry operations it will find have been locked due to this exact issue.Pessimistic locking behavior​Pessimistic locking always locks when a write to SQLite should be performed. Essentially every time an transaction is started or a write operation on the database is done though EF Core, Jellyfin will wait until all other read operations are finished and then block all other operations may they be read or write until the write in question has been performed.
This however means, that Jellyfin can only ever perform a single write to the database, even if it would technically does not need to.In theory, an application should have no issue reading from table "Alice" while writing to table "Bob" however to eliminate all possible sources of concurrency related locking, Jellyfin will only ever allow a single write performed on its database in this mode.
While this will absolutely result in the most stable operation, it will undoubtedly also be the slowest.Jellyfin uses a ReaderWriterLockSlim to lock the operations, that means we allow an unlimited number of reads to happen concurrently while only one write may ever be done on the database.The future Smart locking behavior​In the future we might also consider combining both modes, to get the best of both worlds.Initial testing showed that with both modes, we had great success in handling the underlying issue. While we are not yet sure why this happens only on some systems when others work, we at least now have an option for users previously left out of using Jellyfin.When I was researching this topic, I found many reports all over the internet facing the same error but nobody was able to provide a conclusive explanation whats really happening here.
There have been similar proposals made to handle it but there wasn't a "ready to drop in" solution that handles all the different cases or only code that required massive modifications to every EF Core query.
Jellyfin's implementation of the locking behaviors should be a copy-paste solution for everyone having the same issues as its using interceptors and the caller has no idea of the actual locking behavior.]]></content:encoded></item><item><title>Do you know that there is an HTML tables API?</title><link>https://christianheilmann.com/2025/10/08/abandonware-of-the-web-do-you-know-that-there-is-an-html-tables-api/</link><author>begoon</author><category>dev</category><category>hn</category><pubDate>Sat, 1 Nov 2025 12:58:21 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[When people turn data into  tables using JavaScript, they either use the  methods (createElement() and the likes), but most of the time just append a huge string and use innerHTML, which always is a security concern. However, did you know that  tables also have an old, forgotten  ? Using this one, you can loop over tables, create bodies, rows, cells, heads, footers, captions an summaries (yes,  tables have all of those) and access the table cells. Without having to re-render the whole table on each change. Check out the Codepen to see how you can create a table from a nested array:let table 
let b  document.
let t  document.
b.t
table.rowri
  let r  t.ri
  row.li
    let c  r.i
    c. llet table = [
  ['one','two','three'],
  ['four','five','six']
];
let b = document.body;
let t = document.createElement('table');
b.appendChild(t);
table.forEach((row,ri) => {
  let r = t.insertRow(ri);
  row.forEach((l,i) => {
    let c = r.insertCell(i);
    c.innerText = l;  
  })
});You can then access each table cell with an index (with t being a reference to the table):console.t..console.log(t.rows[1].cells[1]);
// => <td>five</td>You can also delete and create cells and rows, if you want to add a row to the end of the table with a cell, all you need to do is:t.
t..
t...t.insertRow(-1);
t.rows[2].insertCell(0);
t.rows[2].cells[0].innerText = 'foo';There are a few things here that are odd – adding a -1 to add a row at the end for example – and there seems to be no way to create a TH element instead of a TD. All table cells are just cells.However, seeing how much of a pain it is to create tables, it would be fun to re-visit this  and add more functionality to it. We did add a lot of things to  forms, like formData and the change event, so why not add events and other features to tables. That way they’d finally get the status as data structures and not a hack to layout content on the web.]]></content:encoded></item><item><title>Hard Rust requirements from May onward for all Debian ports</title><link>https://lists.debian.org/debian-devel/2025/10/msg00285.html</link><author>/u/pyeri</author><category>dev</category><category>reddit</category><pubDate>Sat, 1 Nov 2025 12:32:44 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[Hi all,

I plan to introduce hard Rust dependencies and Rust code into
APT, no earlier than May 2026. This extends at first to the
Rust compiler and standard library, and the Sequoia ecosystem.

In particular, our code to parse .deb, .ar, .tar, and the
HTTP signature verification code would strongly benefit
from memory safe languages and a stronger approach to
unit testing.

If you maintain a port without a working Rust toolchain,
please ensure it has one within the next 6 months, or
sunset the port.

It's important for the project as whole to be able to
move forward and rely on modern tools and technologies
and not be held back by trying to shoehorn modern software
on retro computing devices.

Thank you for your understanding.
-- 
debian developer - deb.li/jak | jak-linux.org - free software dev
ubuntu core developer                              i speak de, en
]]></content:encoded></item><item><title>Hard Rust requirements from May onward (for Debian&apos;s package manager, APT)</title><link>https://lists.debian.org/debian-devel/2025/10/msg00285.html</link><author>/u/DeleeciousCheeps</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sat, 1 Nov 2025 12:24:42 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Hi all,

I plan to introduce hard Rust dependencies and Rust code into
APT, no earlier than May 2026. This extends at first to the
Rust compiler and standard library, and the Sequoia ecosystem.

In particular, our code to parse .deb, .ar, .tar, and the
HTTP signature verification code would strongly benefit
from memory safe languages and a stronger approach to
unit testing.

If you maintain a port without a working Rust toolchain,
please ensure it has one within the next 6 months, or
sunset the port.

It's important for the project as whole to be able to
move forward and rely on modern tools and technologies
and not be held back by trying to shoehorn modern software
on retro computing devices.

Thank you for your understanding.
-- 
debian developer - deb.li/jak | jak-linux.org - free software dev
ubuntu core developer                              i speak de, en
]]></content:encoded></item><item><title>Python refuses $1.5M grant, Unity&apos;s in trouble, AUR attacked again - Linux Weekly News</title><link>https://tilvids.com/videos/watch/02a038db-fdd0-46d4-8cb2-1f0b1b0bd04d</link><author>/u/Pure_Toe6636</author><category>dev</category><category>reddit</category><pubDate>Sat, 1 Nov 2025 12:08:29 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>I created Open Source Kubernetes tool called Forkspacer to fork entire environments + dataplane, it is like git but for kubernetes.</title><link>https://www.reddit.com/r/kubernetes/comments/1olk9we/i_created_open_source_kubernetes_tool_called/</link><author>/u/Laughing-Dawg</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Sat, 1 Nov 2025 10:15:35 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I created an open-source tool that lets you create, fork, and hibernate entire Kubernetes environments.With , you can fork your deployments while also migrating your data.. not just the manifests, but the entire data plane as well. We support different modes of forking: by default, every fork spins up a managed, dedicated virtual cluster, but you can also point the destination of your fork to a self-managed cluster. You can even set up multi-cloud environments and fork an environment from one provider (e.g., AWS) to another (e.g., GKE, AKE, or on-prem).You can clone full setups, test changes in isolation, and automatically hibernate idle workspaces to save resources all declaratively, with GitOps-style reproducibility.It’s especially useful for spinning up dev, test, pre-prod, and prod environments, and for teams where each developer needs a personal, forked environment from a shared baseline.License is Apace 2.0 and it is written in Go using Kubebuilder SDKPlease give it a try let me know, thank you]]></content:encoded></item><item><title>what exactly is open-sourced in grokipedia?</title><link>https://www.reddit.com/r/linux/comments/1olk43q/what_exactly_is_opensourced_in_grokipedia/</link><author>/u/nix-solves-that-2317</author><category>dev</category><category>reddit</category><pubDate>Sat, 1 Nov 2025 10:05:21 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Monthly: Certification help requests, vents, and brags</title><link>https://www.reddit.com/r/kubernetes/comments/1olk1no/monthly_certification_help_requests_vents_and/</link><author>/u/thockin</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Sat, 1 Nov 2025 10:01:12 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[Did you pass a cert? Congratulations, tell us about it!Did you bomb a cert exam and want help? This is the thread for you.Do you just hate the process? Complain here.(Note: other certification related posts will be removed)]]></content:encoded></item><item><title>Monthly: Who is hiring?</title><link>https://www.reddit.com/r/kubernetes/comments/1olk17i/monthly_who_is_hiring/</link><author>/u/gctaylor</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Sat, 1 Nov 2025 10:00:34 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[This monthly post can be used to share Kubernetes-related job openings within  company. Please include:Location requirements (or lack thereof)At least one of: a link to a job posting/application page or contact detailsIf you are interested in a job, please contact the poster directly. Common reasons for comment removal:Not meeting the above requirementsRecruiter post / recruiter listingsNegative, inflammatory, or abrasive tone   submitted by    /u/gctaylor ]]></content:encoded></item><item><title>Not So Fast: Analyzing the Performance of WebAssembly vs. Native Code (WASM 45% slower)</title><link>https://ar5iv.labs.arxiv.org/html/1901.09056</link><author>/u/Zomgnerfenigma</author><category>dev</category><category>reddit</category><pubDate>Sat, 1 Nov 2025 09:27:33 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[The Challenge of Benchmarking WebAssemblyThe aforementioned suite of 24 benchmarks is the PolybenchC benchmark
suite , which is designed to measure the effect of
polyhedral loop optimizations in compilers. All the benchmarks in the
suite are small scientific computing kernels rather than full
applications (e.g., matrix multiplication and LU Decomposition); each is
roughly 100 LOC. While WebAssembly is designed to accelerate scientific
kernels on the Web, it is also explicitly designed for a much richer set
of full applications.The WebAssembly documentation highlights several intended use
cases , including scientific kernels, image editing,
video editing, image recognition, scientific visualization, simulations,
programming language interpreters, virtual machines, and POSIX applications.
Therefore, WebAssembly’s strong performance on the scientific kernels in PolybenchC
do not imply that it will perform well given a different kind of application.We argue that a more comprehensive evaluation of WebAssembly should rely on an
established benchmark suite of large programs, such as the SPEC CPU benchmark
suites. In fact, the SPEC CPU 2006 and 2017 suite of
benchmarks include several applications that fall under the intended use cases of
WebAssembly: eight benchmarks are scientific applications (e.g., ,
, , , and
), two benchmarks involve image and video processing
( and ), and all of the benchmarks are POSIX
applications.Unfortunately, it is not possible to simply compile a sophisticated
native program to WebAssembly. Native programs, including the programs in
the SPEC CPU suites, require operating system services, such as a
filesystem, synchronous I/O, and processes, which WebAssembly and the
browser do not provide. The SPEC benchmarking harness itself requires
a file system, a shell, the ability to spawn processes, and other Unix
facilities. To overcome these limitations when porting native
applications to the web, many programmers painstakingly modify their
programs to avoid or mimic missing operating system
services. Modifying well-known benchmarks, such as SPEC CPU, would not
only be time consuming but would also pose a serious threat to
validity.The standard approach to running these applications today is to use
Emscripten, a toolchain for compiling C and C++ to
WebAssembly . Unfortunately, Emscripten only supports
the most trivial system calls and does not scale up to large-scale
applications. For example, to enable applications to use synchronous
I/O, the default Emscripten  filesystem loads the entire
filesystem image into memory before the program begins executing. For
SPEC, these files are too large to fit into memory.A promising alternative is to use , a framework that enables
running unmodified, full-featured Unix applications in the
browser .  implements
a Unix-compatible kernel in JavaScript, with full support for
processes, files, pipes, blocking I/O, and other Unix features.
Moreover, it includes a C/C++ compiler (based on Emscripten)
that allows programs to run in the browser
unmodified. The  case studies include complex applications,
such as , which runs entirely in the browser without any
source code modifications.Unfortunately,  is a JavaScript-only solution, since it was
built before the release of
WebAssembly. Moreover,  suffers from high performance overhead,
which would be a significant confounder while benchmarking. Using ,
it would be difficult to tease apart the poorly performing benchmarks
from performance degradation introduced by .]]></content:encoded></item><item><title>You can&apos;t refuse to be scanned by ICE&apos;s facial recognition app, DHS document say</title><link>https://www.404media.co/you-cant-refuse-to-be-scanned-by-ices-facial-recognition-app-dhs-document-says/</link><author>nh43215rgb</author><category>dev</category><category>hn</category><pubDate>Sat, 1 Nov 2025 08:58:54 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Photos captured by Mobile Fortify will be stored for 15 years, regardless of immigration or citizenship status, the document says.]]></content:encoded></item><item><title>Hard Rust requirements from May onward</title><link>https://lists.debian.org/debian-devel/2025/10/msg00285.html</link><author>rkta</author><category>dev</category><category>hn</category><pubDate>Sat, 1 Nov 2025 07:31:40 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Hi all,

I plan to introduce hard Rust dependencies and Rust code into
APT, no earlier than May 2026. This extends at first to the
Rust compiler and standard library, and the Sequoia ecosystem.

In particular, our code to parse .deb, .ar, .tar, and the
HTTP signature verification code would strongly benefit
from memory safe languages and a stronger approach to
unit testing.

If you maintain a port without a working Rust toolchain,
please ensure it has one within the next 6 months, or
sunset the port.

It's important for the project as whole to be able to
move forward and rely on modern tools and technologies
and not be held back by trying to shoehorn modern software
on retro computing devices.

Thank you for your understanding.
-- 
debian developer - deb.li/jak | jak-linux.org - free software dev
ubuntu core developer                              i speak de, en
]]></content:encoded></item><item><title>Programming Language Agnostic Naming Conventions</title><link>https://codedrivendevelopment.com/posts/programmatic-naming-conventions-guide</link><author>/u/Distinct-Panic-246</author><category>dev</category><category>reddit</category><pubDate>Sat, 1 Nov 2025 07:30:50 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[There is a famous quote when it comes to naming things in programming, which is attributed to Phil Karlton"There are only two hard things in Computer Science: cache invalidation and naming things"(Or the slight variation of "There are only two hard things in Computer Science: cache invalidation, naming things, and off by one errors")But over the last few decades there are definitely a few common conventions. Using standard names for things frees up time to work on tougher problems than naming, and means future readers of your code can probably understand the concept better.Why we spend time naming things correctlyIf you see a variable called , you can probably assume it is a boolean.  or  are not clear.Avoid Negative variable names:Negative names can lead to double negatives, which are confusing.Abbreviations can be ambiguous - not everyone will interpret it as the same meaning. Just use the full word, it is clearer.(Although  is probably an exception where it should always be used over ).Pick a language and always use thatIf you work in a modern company then it is likely you work with people originally from various countries around the world. It can be easy to end up with a codebase with a mix of words like  and .I'd recommend just picking US spelling in your code (even if the app is localised only for a UK or AU audiece)Make booleans obvious by using is/has prefixIf you name something , it is quite obvious that it is a boolean. Try to always do this, as something like  could read as if it wasn't a booleanWords like , ,  are too generic. Try to avoid these termsPick a convention for naming things, and use those everywhere.calculateAmountBad 👎:  and Good 👍:  and Also pick a style for casing, and be sure you're consistent with it. Here are some examples (there might be other typical conventions for your library/language of choice) for class names for most other variables for static constantsCommon names for specific thingsIf you're taking some data and  it to a different shape or different values then  is a common and accurate name.If you need to check if data is valid/correct, then its almost always called a validator.Used when describing the shape of some data structure. Often used with database designs.When you need to take some data (e.g. some string) and understand its own data structure. They are quite different things, parsers and transformers can  be very relatedFor code that runs 'between' different parts of your application. A typical use for middleware is in HTTP servers the incoming HTTP request can go through multiple middlewares to either transform the incoming data (before passing to next one or final endpoint handler function) or to do something with that dataWhen you have some functionality with a specific interface, and you need to convert it to another interface/shape.It is also known as a 'wrapper' (or a bridge, although that is technically a slightly different thing)When you need to make data uniform in scale, format, or structure]]></content:encoded></item><item><title>Prevent laptop&apos;s temp raises significantly during compiling</title><link>https://www.reddit.com/r/rust/comments/1olfwxy/prevent_laptops_temp_raises_significantly_during/</link><author>/u/blade_012</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Sat, 1 Nov 2025 05:22:55 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[When compiling Fyrox for the first time, my laptop temperature raised significantly from 40°C to 90°C and stays in 90°C for long time until the compilation done. Is there any way to cap the compilation activity so that it won't use up all my CPU during the process? I don't mind having the process take a bit longer as long it's safe for my poor small Dell Latitude 7290.]]></content:encoded></item><item><title>Well a old school flex i guess</title><link>https://www.reddit.com/r/linux/comments/1olftbt/well_a_old_school_flex_i_guess/</link><author>/u/Puzzleheaded-Car4883</author><category>dev</category><category>reddit</category><pubDate>Sat, 1 Nov 2025 05:16:26 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[This old Red Hat Linux 8.0 manual’s been gathering dust on my shelf. I used to read it as a kid — didn’t understand a single word back then. Fast forward to age 19, 3 years into using Linux daily... and everything suddenly makes sense.Btw this is one of those first thing that introduced me to linux ]]></content:encoded></item><item><title>Happy Halloween, nerds</title><link>https://www.reddit.com/r/linux/comments/1olf21x/happy_halloween_nerds/</link><author>/u/feelingsupersonic</author><category>dev</category><category>reddit</category><pubDate>Sat, 1 Nov 2025 04:29:17 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Java Virtual Threads VS GO routines</title><link>https://www.reddit.com/r/golang/comments/1oldyoo/java_virtual_threads_vs_go_routines/</link><author>/u/gamecrow77</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Sat, 1 Nov 2025 03:25:58 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[I recently had a argument with my tech lead about this , my push was for Go since its a new stack , new learning for the team and Go is evolving , my assumption is that we will find newer gen of devs who specialise in Go. Was i wrong here ? the argument was java with virtual threads is as efficient as go ]]></content:encoded></item><item><title>The profitable startup</title><link>https://linear.app/now/the-profitable-startup</link><author>doppp</author><category>dev</category><category>hn</category><pubDate>Sat, 1 Nov 2025 03:18:04 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[For years, startups have been taught to prioritize growth over everything else. Profitability was seen as unambitious or even wrong – something to worry about when you hit scale. Why focus on profits when money and valuations were easy to come by?But that thinking was always flawed.Profitability isn't unambitious; it's controlling your own destiny. It means you don't have to rely on investors for survival. It means you can focus on your unaltered vision and mission. And it means you as a founder decide the pace of growth. And once you experience it, it's hard to imagine doing things any other way.Paul Graham famously wrote about "ramen profitability" – the point where a founding team could survive without external funding. He argued this made startups more attractive to investors, showing they could get customers to pay, were serious about building valuable products, and were disciplined with expenses.Graham wrote his essay in 2009. I’d argue that we now live in a world where it’s not just easier to get ramen profitable, but traditionally profitable – while also growing fast.At Linear we didn't set out to be profitable but kind of stumbled into it. We believed that to win this market we really needed to build a superior tool. The best way we knew how to do that was to keep the team small and focused. And when we launched after a year in private beta, almost all of our 100 beta users converted to paid customers. To our surprise, we realized it wouldn't take that long to become profitable if we kept the costs in check. Twelve months after launch, we hit profitability, and we've stayed profitable ever since.I don't know why hiring massive teams ever became the norm. In my own experience, small teams always delivered better quality, and faster. Maybe it's fear of missing out if you don't grow the team fast. Maybe it's investors whispering that your team is "understaffed compared to benchmarks." Being understaffed compared to benchmarks almost always should be a source of pride, not a problem. People should be surprised how small your team is, not how big it is.What holds you back is rarely team size – it's the clarity of your focus, skill and ability to execute. Larger teams mean slower progress, more management overhead, more meetings, more opinions, and usually dilution of vision and standards. Yet growing the team has somehow become a symbol of success.At Linear, we hired our first employee after six months and roughly doubled the team each year. With each hire, we make sure they truly elevate the team. We don't set out to hire ten engineers – we hire the next  engineer. This intentional approach has allowed us to maintain both quality and culture.The most underrated thing about profitability is how much peace of mind it gives you. Once you're profitable, you stop worrying about survival and focus on what really matters: building something great. Building the way you want. Instead of optimizing for the next fundraising round, you optimize for value creation.While profitability might not come quickly for every startup, I believe it's achievable sooner than most think. If you're creating a new market, or truly require massive scale like a social network, or significant upfront investment like a hardware company, it might take longer. But if you're in a category where there isn't hard upfront investment, and you get some level of product-market fit with customers willing to pay, you can probably be profitable. You can decide to become profitable. And usually, it's a decision about how much and how fast you hire.Revenue per employee is one of the clearest ways to see you’re hiring appropriately. While some of the best public companies benchmark at $1-2M per employee, for startups it's not unreasonable to target the range of $500k-$1M per employee.Understand Your Risk ProfileAre you building something highly speculative where you're not sure if there's a market for it, or are you building something that already has a market but with a different take on it? In the former case profitability takes longer, but in the latter it could happen right away. Most software today, especially in the B2B space, is about building a modern version of something existing.Hire Intentionally and SlowerFor most software startups, ten people before product-market fit should be your ceiling, not your target. After PMF, every hire should address a specific, pressing need – not just fill out an org chart. At Linear, our deliberately slow headcount growth forced us to be selective, which meant making better hires. It also protected our culture, since rapid hiring often dilutes the very things that made your startup special in the first place. When you hire less, you naturally hire better.Being profitable doesn't mean you have to be anti-investors. It means you have that choice, and investors are quite interested in profitable companies that also grow fast. You can raise more, less, or nothing. You can wait for the right timing, the right partner, or fund. For most ambitious startups, it can still be a good idea to raise something even if you could get by bootstrapping. Investors can still be helpful, and the additional cash balance can help you to make larger investments, or acquisitions.The point is that you can be and are allowed to be profitable as a startup. It's not a bad thing, it's not an oxymoron or as hard as people make it out to be. The secret is that a lot of successful companies actually were quite profitable early on, they just didn't talk about it. When you're profitable, you make decisions based on what's best for your customers and your product, not what's best for impressing investors.I didn't set out to build a profitable startup. But once I got there, I realized I wouldn't want to build a company any other way.]]></content:encoded></item><item><title>IRS open-sourced the fact graph it uses for tax law</title><link>https://github.com/IRS-Public/fact-graph</link><author>/u/R2_SWE2</author><category>dev</category><category>reddit</category><pubDate>Sat, 1 Nov 2025 00:48:52 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Steinberg, creators of VST technology and the ASIO protocol, have released the SDKs for VST 3 and ASIO as Open Source.</title><link>https://www.reddit.com/r/linux/comments/1ola786/steinberg_creators_of_vst_technology_and_the_asio/</link><author>/u/fenix0000000</author><category>dev</category><category>reddit</category><pubDate>Sat, 1 Nov 2025 00:05:42 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[   submitted by    /u/fenix0000000 ]]></content:encoded></item><item><title>Show HN: Strange Attractors</title><link>https://blog.shashanktomar.com/posts/strange-attractors</link><author>shashanktomar</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 23:23:59 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[A few months back, while playing around with Three.js, I came across something that completely derailed my plans. Strange attractors - fancy math that creates beautiful patterns. At first I thought I'd just render one and move on, but then soon I realized that this is too much fun. When complexity emerges from three simple equations, when you see something chaotic emerge into beautiful, it's hard not to waste some time. I've spent countless hours, maybe more than I'd care to admit, watching these patterns form. I realized there's something deeply satisfying about seeing order emerge from randomness. Let me show you what kept me hooked.The Basics: Dynamical Systems and Chaos TheoryDynamical Systems are a mathematical way to understand how things . Imagine you have a system, which
could be anything from the movement of planets to the growth of a population. In this system, there are rules that
determine how it evolves from one moment to the next. These rules tell you what will happen next based on what is
happening now. Some examples are, a pendulum, the weather patterns, a flock of birds, the spread of a virus in a
population (we are all too familiar with this one), and stock market.There are two primary things to understand about this system:: This is like a big collection of all the possible states the system can be in. Each state is like a
snapshot of the system at a specific time. This is also called the  or the .: These are the rules that takes one state of the system and moves it to the next state. It can be
represented as a function that transforms the system from now to later.For instance, when studying population growth, a phase-space (world-state) might consist of the current population size
and the rate of growth or decline at a specific time. The dynamics would then be derived from models of population
dynamics, which, considering factors like birth rates, death rates, and carrying capacity of the environment, dictate
the changes in population size over time.Another way of saying this is that the dynamical systems describe how things change over time, in a space of
possibilities, governed by a set of rules. Numerous fields such as biology, physics, economics, and applied mathematics,
study systems like these, focusing on the specific rules that dictate their evolution. These rules are grounded in
relevant theories, such as Newtonian mechanics, fluid dynamics, and mathematics of economics, among others.There are different ways of classifying dynamical systems, and one of the most interesting is the classification into
chaotic and non-chaotic systems. The change over time in non-chaotic systems is more deterministic as compared to
chaotic systems which exhibit randomness and unpredictability. is the sub branch of dynamical systems that studies chaotic systems and challenges the traditional
deterministic views of causality. Most of the natural systems we observe are chaotic in nature, like the weather, a drop
of ink dissolving in water, social and economic behaviours etc. In contrast, systems like the movement of planets,
pendulums, and simple harmonic oscillators are extremely predictable and non-chaotic.Chaos Theory deals with systems that exhibit irregular and unpredictable behavior over time, even though they follow
deterministic rules. Having a set of rules that govern the system, and yet exhibit randomness and unpredictability,
might seem a bit contradictory, but it is because the rules do not always represent the whole system. In fact, most of
the time, these rules are an approximation of the system and that is what leads to the unpredictability. In complex
systems, we do not have enough information to come up with a perfect set of rules. And by using incomplete information
to make predictions, we introduce uncertainty, which amplifies over time, leading to the chaotic behaviour.Chaotic systems generally have many non-linear interacting components, which we partially understand (or can partially
observe) and which are very sensitive to small changes. A small change in the initial conditions can lead to a
completely different outcome, a phenomenon known as the . In this post, we will try to see the
butterfly effect in action but before that, let's talk about .To understand Strange Attractors, let's first understand what an attractor is. As discussed earlier, dynamical systems
are all about . During this change, the system moves through different possible states (remember the
phase space jargon?). An attractor is a set of states towards which a system tends to settle over time, or you can say,
towards which it is . It's like a magnet that pulls the system towards it.For example, think of a pendulum. When you release it, it swings back and forth, but eventually, it comes to rest at the
bottom. The bottom is the attractor in this case. It's the state towards which the pendulum is attracted.This happens due to the system's inherent dynamics, which govern how states in the phase space change. Here are some of
the reasons why different states get attracted towards attractors:: Attractors are stable states of the system, meaning that once the system reaches them, it tends to stay
there. This stability arises from the system's dynamics, which push it towards the attractor and keep it there.: Many dynamical systems have dissipative forces, which cause the system to lose energy over time. This
loss of energy leads the system to settle into a lower-energy state, which often corresponds to an attractor. This is
what happens in the case of the pendulum.: In some regions of the phase space, the system's dynamics cause trajectories to converge. This
contraction effect means that nearby states will tend to come closer together over time, eventually being drawn
towards the attractor.Some attractors have complex governing equations that can create unpredictable trajectories or behaviours. These
nonlinear interactions can result in multiple stable states or periodic orbits, towards which the system evolves. These
complex attractors are categorised as . They are called "strange" due to their unique
characteristics.: Strange attractors often have a fractal-like structure, meaning they display intricate
patterns that repeat at different scales. This complexity sets them apart from simpler, regular attractors.Sensitive Dependence on Initial Conditions: Systems with strange attractors are highly sensitive to their initial
conditions. Small changes in the starting point can lead to vastly different long-term behaviors, a phenomenon known
as the "butterfly effect".Unpredictable Trajectories: The trajectories on a strange attractor never repeat themselves, exhibiting
non-periodic motion. The system's behavior appears random and unpredictable, even though it is governed by
deterministic rules.Emergent Order from Chaos: Despite their chaotic nature, strange attractors exhibit a form of underlying order.
Patterns and structures emerge from the seemingly random behavior, revealing the complex dynamics at play.You can observe most of these characteristics in the visualisation. The one which is most fascinating to observe is the
butterfly effect.A butterfly can flutter its wings over a flower in China and cause a hurricane in the Caribbean.One of the defining features of strange attractors is their sensitivity to initial conditions. This means that small
changes in the starting state of the system can lead to vastly different long-term behaviors, a phenomenon known as the
. In chaotic systems, tiny variations in the initial conditions can amplify over time, leading to
drastically different outcomes.In our visualisation, let's observe this behavior on Thomas Attractor. It is governed by the following equations:A small change in the parameter  can lead to vastly different particle trajectories and the overall shape of the
attractor. Change this value in the control panel and observe the butterfly effect in action.There is another way of observing the butterfly effect in this visualisation. Change the  from  to
 in the control panel and observe how the particles move differently in the two cases. The particles
eventually get attracted to the same states but have different trajectories.This visualization required rendering a large number of particles using Three.js. To achieve this efficiently, we used a
technique called . This method handles iterative updates of particle systems directly on the GPU,
minimizing data transfers between the CPU and GPU. It utilizes two frame buffer objects (FBOs) that alternate roles: One
stores the current state of particles and render them on the screen, while the other calculates the next state.Setting Up Frame Buffer Objects (FBOs): We start by creating two FBOs,  and , to hold the current and
next state of particles. These buffers store data such as particle positions in RGBA channels, making efficient use
of GPU resources.Shader Programs for Particle Dynamics: The shader programs execute on the GPU and apply attractor dynamics to
each particle. Following is the attractor function which update the particle positions based on the attractor equation.Rendering and Buffer Swapping: In each frame, the shader computes the new positions based on the attractor's
equations and stores them in the inactive buffer. After updating, the roles of the FBOs are swapped: The previously
inactive buffer becomes active, and vice versa.This combination of efficient shader calculations and the ping-pong technique allows us to render the particle system.If you have any comments, please leave them on this GitHub discussions topic. Sooner or later, I will integrate it with the blog. The  discussion can be found here.]]></content:encoded></item><item><title>S.A.R.C.A.S.M: Slightly Annoying Rubik&apos;s Cube Automatic Solving Machine</title><link>https://github.com/vindar/SARCASM</link><author>chris_overseas</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 23:03:18 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>[GoGreement] A new linter that can help enforce interface implementation and immutability</title><link>https://www.reddit.com/r/golang/comments/1ol8das/gogreement_a_new_linter_that_can_help_enforce/</link><author>/u/Green-Sympathy-2198</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 31 Oct 2025 22:38:50 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Hey guys! I wrote this linter mainly for myself, but I hope some of you find it useful.I came to golang from JVM world and I was missing some things like explicit implementation declaration and immutability.But I see gophers love their linters, so I thought I could solve this with a linter.How does it work? You just add annotations to your types like: go // @immutable type User struct { id string name string } And run the linter and it will give you an error if you try to change fields like this: I also added annotations that let you check interface implementation: go // @implements io.Reader This lets you check that a struct actually implements an interface without all this stuff: go var _ MyInterface = (*MyStruct)(nil) And many other annotations (testonly, packageonly, ...). Would love to hear what you think!]]></content:encoded></item><item><title>Borrow checker says “No”! An error that scares me every single time!</title><link>https://polymonster.co.uk/blog/borow-checker-says-no</link><author>/u/__shufb</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Fri, 31 Oct 2025 22:25:09 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[It’s Halloween and I have just been caught out by a spooky borrow checker error that caught me by surprise. It feels as though it is the single most time consuming issue to fix and always seems to catch me unaware. The issue in particular is “cannot borrow x immutably as it is already borrowed mutably” - it manifests itself in different ways under different circumstances, but I find myself hitting it often when refactoring. It happened again recently so I did some investigating and thought I would discuss it in more detail.The issue last hit me when I was refactoring some code in my graphics engine hotline, I have been creating some content on YouTube and, after a little bit of a slog to fix the issue, I recorded a video of me going through the scenario of how it occurred and some patterns to use that I have adopted in the past to get around it. You can check out the video if you are that way inclined, the rest of this post will mostly echo what is in the video, but it might be a bit easier to follow code snippets and description in text.I have a generic graphics API, which consists of traits called gfx. This is there to allow different platform backends to implement the trait; currently I have a fully implemented Direct3D12 backend and I recently began to port macOS using Metal.The gfx backend wraps underlying graphics API primitives; in this case we are mostly concerned about  which is a command buffer. Command buffers are used to submit commands to the GPU. They do things like  or , amongst other things. For the purposes of this blog post, what the command buffer does is not really that important, just that is does , which at the starting point when the code was working is a trait method that takes an immutable self and another immutable parameter ie. fn do_something(&self, param: &Param).In the rest of the code base I have a higher level rendering system called . This is graphics engine code that is not platform specific but implements shared functionality. So where  is a low level abstraction layer,  implements concepts of a  that is a view of a scene that we can render from. A  has a camera that can look at the scene and is then passed to a render function, which can build a command buffer to render the scene from that camera’s perspective. The engine is designed to be multithreaded and render functions are dispatched through  systems, so a view gets passed into a render system but it is wrapped in an .I made a small cutdown example of this code to be able to demonstrate the problem I encounter, so let’s start with the initial working version:I tried to simplify it as much as possible so these snippets should compile if you copy and paste them, they won’t run thanks to  macro (which I absolutely love using, it is so handy!) but we only care about the borrow checker anyway.All we really need to think about is that a  can  and it also gets passed in a , which is also contained as part of ‘view’. Coming from a C/C++ background I landed on my personal preference being procedural C code with context passing, so I tend to group things together into a single struct. It makes sense to me in this case and I wanted to group everything inside , and we fetch the view from elsewhere in the engine.So the code in the snippet compiles fine and I was working with this setup for some time. I began work on macOS and it turned out that the  method needed to mutate the command buffer so that I could mutate some internal state and make the Metal graphics API behave similarly to Direct3D12. This is common for graphics API plumbing.The specific example in this case was that in Direct3D we call a function  to bind an index buffer before we make a call to , but in Metal there is no equivalent to bind an index buffer. Instead you pass a pointer to your index buffer when calling the equivalent draw indexed. So to fix this, when we call  we can store some extra state in the command buffer so we can pass it in the later call to .In hindsight any method on the command buffer trait that does anything, like set anything or write into the command buffer, should take a  because it is mutating the command buffer after all. In my case since I am calling through to methods on  , which is unsafe code and does not require any mutable references.In our simplified example, in order to store, state  now needs to change and take a mutable self: do_something(&mut self, param: &Param) it should be noted that  itself was already .Borrow checker now kicks in…my heart sinks. In the real code base not only did I have to modify a single call site, but I had hundreds of places where this error was happening, I made the decision here and now to make any methods that write to the command buffer also be mutable and make the mutabilityerror[E0502]: cannot borrow `view` as immutable because it is also borrowed as mutable
  --> src/main.rs:30:28
   |
30 |     view.cmd.do_something(&view.param);
   |     ----     ------------  ^^^^ immutable borrow occurs here
   |     |        |
   |     |        mutable borrow later used by call
   |     mutable borrow occurs here

For more information about this error, try `rustc --explain E0502`.
error: could not compile due to 1 previous error
This is not the first time I have encountered this problem and I doubt it will be the last. There are a number of ways to resolve it and they aren’t too complicated. The frustrating thing is that it seems to occur always when you are doing something else and not just when you decide to refactor, so you end up having a mountain of errors to solve before you can get back to the original task. I suppose you could call it a symptom of bad design or lack of experience, but when writing code things inevitably change and bend with new requirements, and Rust throws these unexpected issues up for me more often than I find with C, and often the required refactor takes more effort as well. But that is the cost you pay, hopefully more upfront effort to get past the borrow checker means fewer nasty debugging stages later. So let’s look at some patterns to fix the issue!The one I actually went for in this case was using . We take the  out of view so we no longer need to borrow a ‘view’ to use , and then when finished return the cmd into ‘view’. It is important to note here that  needs to derive default in order for this to work, as when we take the  in  will become This approach is the simplest I could think of at the time because any existing code using  doesn’t need updating, everything stays the same and we just separate the references. In this case it was easy to derive the default for  .You need to remember to set the  back on  here, which could be a pitfall and cause unexpected behaviour if you didn’t.If you can’t easily derive default on a struct there are some other options. If the struct is clonable or you can easily derive a clone, you can clone to achieve a similar effect.Cloning might be considered a heavier operation than ‘take’ depending on the circumstances, but this method has the same benefit as the take version whereby unaffected code that is using  elsewhere doesn’t need to be changed.Another approach would be to use  this allows for interior mutability and again we do not need to worry about default or clone.We also need to update any code that ever used  and do the same. Not ideal but it allows us to get around the need for a default or clone. I have had to resort to this in other places in the code base.There are more options; quite literally  here can help. If we make  an  then this gives us the ability to use  as the default and we can use the  approach. We can also use  and swap with . Swapping works similar to ‘take’, where we take mem and swap with the default.The  approach also requires more effort as we need to now take a reference and unwrap the option and update any code that ever used  to do the same. Not ideal, but it allows us to get around the need for a default or clone, and if your type is already optional then this will fit easily.There is one final approach that could save a lot of time, and that would be to not change the  function at all in the first place. That is to keep it as do_something(&self, param: &Param). So how do we mutate the interior state without requiring the self to be mutable?This can be done with  in single threaded code or  in multithreaded code. Since we already looked at  I will do an example of .I decided to make the mutability explicit to the trait and that was based on how the command buffers are used in the engine, in other places I have taken other approaches favouring interior mutability. For this case a view can be dispatched in parallel with other views, but the engine is designed such that 1 thread per view and no work happens to a single view on multiple threads at the same time. Command buffers are submitted in a queue in order and dispatched on the GPU.Here it made sense to me to avoid locking interior mutability for each time we call a method on a  and it works with the engine’s design. We lock a view at the start of a render thread, fill it with commands and then hand it back to the graphics engineer for submission to the GPU. The usage is explicit, we just needed to appease the borrow checker!I hope you enjoyed this article, please check out my YouTube channel for more videos or more articles on my blog, let me know what you think and if you have any other strategies or approaches I would love to hear about them. I would also like to hear about compiler and borrow checker errors you find particularly time consuming or frustrating to deal with.]]></content:encoded></item><item><title>What kind of debug tools are available that are cloud native?</title><link>https://www.reddit.com/r/kubernetes/comments/1ol64df/what_kind_of_debug_tools_are_available_that_are/</link><author>/u/lickety-split1800</author><category>dev</category><category>reddit</category><category>k8s</category><pubDate>Fri, 31 Oct 2025 20:59:38 +0000</pubDate><source url="https://www.reddit.com/r/kubernetes/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Kubernetes</source><content:encoded><![CDATA[I'm an SRE and a longtime Linux & automation person, starting in the late 90s.With the advent of apps on containers, there are fewer and fewer tools to perform debugging.Taking a look at the types of debug tools one has used to diagnose issues.even basic tools such as find, grep, ls and others are used in debugging.The Linux OS used to be under the control of the system administrator, who would put the tools required to meet operational debugging requirements, increasingly since it is the developer that maintains the container image and none of these tools end up on the image, citing most of the time startup time as the main requirement.Now a container is a slice of the operating system so I argue that the container base image should still be maintained by those who maintain Linux, because it's their role to have these tools to diagnose issues. That should be DevOps/SRE teams but many organisations don't see it this way.So what tools does Kubernetes provide that fulfil the needs I've listed above?]]></content:encoded></item><item><title>My Must-Have Apps Since Switching to Linux</title><link>https://www.reddit.com/r/linux/comments/1ol5a1k/my_musthave_apps_since_switching_to_linux/</link><author>/u/Overflow_Nuts</author><category>dev</category><category>reddit</category><pubDate>Fri, 31 Oct 2025 20:24:34 +0000</pubDate><source url="https://www.reddit.com/r/linux/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Linux</source><content:encoded><![CDATA[OnlyOffice → If you’re used to MS Office, the interface feels almost identical — super easy to adapt.Brave / Zen → When I need a Chromium-based browser, I use Brave; when I need a Firefox-based one, Zen. Both are top-tier.Okular → Opens everything from PDFs to EPUBs.yt-dlp → Downloads videos and audio straight from the terminal — and not just from YouTube, it supports tons of platforms.Qbittorrent → Clean, simple, and easily the best torrent client out there.Stremio + Add-ons → The best torrent-based media player, hands down.KeepassXC → A simple yet powerful password manager with browser integration.LocalSend → Transfers files across all your devices locally, no internet needed.KDE Connect → Perfect bridge between your phone and computer.Bottles → Makes using Wine more stable and user-friendly.Espanso → Expands text shortcuts automatically — a real time-saver.Tmux → Lets you split your terminal and run multiple sessions at once.Btop / ytop / glances → Displays system resource usage right from the terminal.Fastfetch → A faster Neofetch alternative for system info.Syncthing → Syncs your files seamlessly between devices.Czkawka → Finds duplicate or junk files on your disk.Mpv + Plugins → Lightweight, scriptable video player.Input Leap → Control multiple computers with one keyboard and mouse.Zapret → Bypasses DPI-based network restrictions.Moonlight / Sunshine → Stream your games locally across your network.Heroic Games Launcher → Great alternative for Epic Games.Lutris → Customizable launcher supporting multiple game libraries.Prism Launcher → Clean, mod- and shader-friendly Minecraft launcher.Ente Auth → The best 2FA app I’ve tried — encrypted sync between devices.GDU → Visual disk usage analyzer.Newsboat → Read RSS feeds directly in the terminal.Neovim → Fast, lightweight text editor.Waypaper / Swaybg / Hyprpaper → Manage your wallpapers easily.Easy Effects → Lets you tweak and filter your system’s audio.Waybar (+ eww + rofi) → Build a fully customizable system bar.scrcpy → The simplest way to mirror your Android screen on your PC.Podman / Distrobox → Run another Linux environment inside a container.Wireshark / mitmproxy → Monitor and analyze your network traffic.Opensnitch → See which apps are making network connections.qutebrowser → A minimalist, keyboard-driven browser.fail2ban → The most satisfying way to troll persistent brute-forcers.qemu + Virt-Manager → Create and manage virtual machines easily.Waydroid → Run Android apps directly on Linux.Lf → Terminal-based file manager.These are the tools I’ve discovered and personally enjoy using on Linux. What about yours what are your must-have apps?]]></content:encoded></item><item><title>Futurelock - Subtle Risk in async Rust</title><link>https://rfd.shared.oxide.computer/rfd/0609</link><author>/u/-Y0-</author><category>dev</category><category>reddit</category><category>rust</category><pubDate>Fri, 31 Oct 2025 20:20:15 +0000</pubDate><source url="https://www.reddit.com/r/rust/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Rust</source><content:encoded><![CDATA[Bounded channels are not really the issue here.  Even in omicron#9259, the capacity=1 channel was basically behaving as documented and as one would expect.  It woke up a sender when capacity was available, and the other senders were blocked to maintain the documented FIFO property.  However, some of the patterns that we use with bounded channels are problematic on their own and, if changed, could prevent the channel from getting caught up in a futurelock.In Omicron, we commonly use bounded channels with .  The bound is intended to cap memory usage and provide backpressure, but using the blocking  creates a second  queue: the wait queue for the channel.  Instead, we could consider using a larger capacity channel plus  and propagate failure from .As an example, when we use the actor pattern, we typically observe that there’s only one actor and potentially many clients, so there’s not much point in buffering messages  the channel.  So we use  and let clients block in .  But we could instead have  and have clients use  and propagate failure if they’re unable to send the message.  The value  here is pretty arbitrary.  You want it to be large enough to account for an expected amount of client concurrency, but not larger.  If the value is too small, you’ll wind up with spurious failures when the client could have just waited a bit longer.  If the value is too large, you can wind up queueing so much work that the actor is always behind (and clients are potentially even timing out at a higher level).  One might observe:Channel limits, channel limits: always wrong!Some too short and some too long!But as with timeouts, it’s often possible to find values that work in practice.Using  is  a mitigation because this still results in the sender blocking.  It needs to be polled after the timeout expires in order to give up.  But with futurelock, it will never be polled.]]></content:encoded></item><item><title>John Carmack on mutable variables</title><link>https://twitter.com/id_aa_carmack/status/1983593511703474196</link><author>/u/iamkeyur</author><category>dev</category><category>reddit</category><pubDate>Fri, 31 Oct 2025 19:26:29 +0000</pubDate><source url="https://www.reddit.com/r/programming/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Programming</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Write PostgreSQL functions in Go Golang example</title><link>https://www.reddit.com/r/golang/comments/1ol2tqv/write_postgresql_functions_in_go_golang_example/</link><author>/u/WinProfessional4958</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 31 Oct 2025 18:45:51 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[It took me a while to figure this out. Go compiles the C files automatically.#include "postgres.h" #include "fmgr.h" PG_MODULE_MAGIC; extern int32 Adder(int32); PG_FUNCTION_INFO_V1(add_two); Datum add_two(PG_FUNCTION_ARGS) { int32 arg = PG_GETARG_INT32(0); PG_RETURN_INT32(Adder(arg)); } package main /* #cgo CFLAGS: -DWIN32 -ID:/pg18headers -ID:/pg18headers/port/win32 #cgo LDFLAGS: -LD:/pg18lib #include "postgres.h" #include "fmgr.h" // Forward declare the C function so cgo compiles add_two.c too. extern void init_add_two(); */ import "C" //export Adder func Adder(a C.int32) C.int32 { return a + 3 } func main() {} PS D:\C\myextension> go build -o add_two.dll -buildmode=c-sharedIn PostgreSQL: open the query window (adjust path to your generated dynamically loaded library and header file (.dll, .h).CREATE FUNCTION add_two(int4) RETURNS int4AS 'D:/C/myextension/add_two.dll', 'add_two']]></content:encoded></item><item><title>Go vs Kotlin: Server throughput</title><link>https://www.reddit.com/r/golang/comments/1ol1upp/go_vs_kotlin_server_throughput/</link><author>/u/iG0tB00ts</author><category>dev</category><category>reddit</category><category>go</category><pubDate>Fri, 31 Oct 2025 18:08:15 +0000</pubDate><source url="https://www.reddit.com/r/golang/top/?sort=top&amp;t=day&amp;limit=6">Reddit - Go</source><content:encoded><![CDATA[Let me start off by saying I'm a big fan of Go. Go is my side love while Kotlin is my official (work-enforced) love. I recognize benchmarks do not translate to real world performance & I also acknowledge this is the first benchmark I've made, so mistakes are possible.That being said, I was recently tasked with evaluating Kotlin vs Go for a small service we're building. This service is a wrapper around Redis providing a REST API for checking the existence of a key.With a load of 30,000 RPS in mind, I ran a benchmark using  (the workload is a list of newline separated 40chars string) and saw to my surprise Kotlin outperforming Go by ~35% RPS. Surprise because my thoughts, few online searches as well as AI prompts led me to believe Go would be the winner due to its lightweight and performant goroutines.Go + net/http + go-redis Text Thread Stats Avg Stdev Max +/- Stdev Latency 4.82ms 810.59us 38.38ms 97.05% Req/Sec 5.22k 449.62 10.29k 95.57% 105459 requests in 5.08s, 7.90MB read Non-2xx or 3xx responses: 53529 Requests/sec: 20767.19  Kotlin + ktor + lettuce  Thread Stats Avg Stdev Max +/- Stdev Latency 3.63ms 1.66ms 52.25ms 97.24% Req/Sec 7.05k 0.94k 13.07k 92.65% 143105 requests in 5.10s, 5.67MB read Non-2xx or 3xx responses: 72138 Requests/sec: 28057.91 I am in no way an expert with the Go ecosystem, so I was wondering if anyone had an explanation for the results or suggestions on improving my Go code. ```Go package mainimport ( "context" "net/http" "runtime" "time""github.com/redis/go-redis/v9" var ( redisClient *redis.Client )func main() { redisClient = redis.NewClient(&redis.Options{ Addr: "localhost:6379", Password: "", DB: 0, PoolSize: runtime.NumCPU() * 10, MinIdleConns: runtime.NumCPU() * 2, MaxRetries: 1, PoolTimeout: 2 * time.Second, ReadTimeout: 1 * time.Second, WriteTimeout: 1 * time.Second, }) defer redisClient.Close()mux := http.NewServeMux() mux.HandleFunc("/", handleKey) server := &http.Server{ Addr: ":8080", Handler: mux, } server.ListenAndServe() // some code for quitting on exit signal // handleKey handles GET requests to /{key} func handleKey(w http.ResponseWriter, r *http.Request) { path := r.URL.Pathkey := path[1:] exists, _ := redisClient.Exists(context.Background(), key).Result() if exists == 0 { w.WriteHeader(http.StatusNotFound) return } Kotlin code for reference ```Kotlin // applicationfun main(args: Array<String>) { io.ktor.server.netty.EngineMain.main(args) }fun Application.module() { val redis = RedisClient.create("redis://localhost/"); val conn = redis.connect() configureRouting(conn) }fun Application.configureRouting(connection: StatefulRedisConnection<String, String>) { val api = connection.async()routing { get("/{key}") { val key = call.parameters["key"]!! val exists = api.exists(key).await() > 0 if (exists) { call.respond(HttpStatusCode.OK) } else { call.respond(HttpStatusCode.NotFound) } } } ]]></content:encoded></item><item><title>Addiction Markets</title><link>https://www.thebignewsletter.com/p/addiction-markets-abolish-corporate</link><author>toomuchtodo</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 17:42:55 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Use DuckDB-WASM to query TB of data in browser</title><link>https://lil.law.harvard.edu/blog/2025/10/24/rethinking-data-discovery-for-libraries-and-digital-humanities/</link><author>mlissner</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 17:37:15 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Libraries, digital humanities projects, and cultural heritage organizations have long had to perform a balancing act when sharing their collections online, negotiating between access and affordability. Providing robust features for data discovery, such as browsing, filtering, and search, has traditionally required dedicated computing infrastructure such as servers and databases. Ongoing server hosting, regular security and software updates, and consistent operational oversight are expensive and require skilled staff. Over years or decades, budget changes and staff turnover often strand these projects in an unmaintained or nonfunctioning state.The alternative, static file hosting, requires minimal maintenance and reduces expenses dramatically. For example, storing gigabytes of data on Amazon S3 may cost $1/month or less. However, static hosting often diminishes the capacity for rich data discovery. Without a dynamic computing layer between the user’s web browser and the source files, data access may be restricted to brittle pre-rendered browsing hierarchies or search functionality that is impeded by client memory limits. Under such barriers, the collection’s discoverability suffers.For years, online collection discovery has been stuck between a rock and a hard place: accept the complexity and expense required for a good user experience, or opt for simplicity and leave users to contend with the blunt limitations of a static discovery layer.Why We Explored a New ApproachWhen LIL began thinking about how to provide discovery for the Data.gov Archive, we decided that building a lightweight and easily maintained access point from the beginning would be worth our team’s effort. We wanted to provide low-effort discovery with minimal impact on our resources. We also wanted to ensure that whatever path we chose would encourage, rather than impede, long-term access.This approach builds on our recent experience when the Caselaw Access Project (CAP) hit a transition moment. At that time, we elected to switch case.law to a static site and to partner with others dedicated to open legal data to provide more feature-rich access.CAP includes some 11 TB of data; the Data.gov Archive represents nearly 18 TB, with the catalog metadata alone accounting for about 1 GB. Manually browsing the archive data in its repository, even for a user who knows what she’s looking for, is laborious and time-consuming. Thus we faced a challenge. Could we enable dynamic, scalable discovery of the Data.gov Archive while enjoying the frugality, simplicity, and maintainability of static hosting?Our Experiment: Rich Discovery, No Server RequiredRecent advancements in client-side data analysis led us to try something new. Tools like DuckDB-Wasm, sql.js-httpvfs, and Protomaps, powered by standards such as WebAssembly, web workers, and HTTP range requests, allow users to efficiently query large remote datasets in the browser. Rather than downloading a 2 GB data file into memory, these tools can incrementally retrieve only the relevant parts of the file and process query results locally.We developed Data.gov Archive Search on the same model. Here’s how it works: We store Data.gov Archive catalog metadata as sorted, compressed Parquet files on Source.coop, taking advantage of performant static file hosting. Our client-side web application loads DuckDB-Wasm, a fully functional database engine running inside the user’s browser. When a user navigates to a resource or submits a search, our DuckDB-Wasm client executes a targeted retrieval of the data needed to fulfill the request. No dedicated server is required; queries run entirely in the browser.This experiment has not been without obstacles. Getting good performance out of this model demands careful data engineering, and the large DuckDB-Wasm binary imposes a considerable latency cost. As of this writing, we’re continuing to explore speedy alternatives like hyparquet and Arquero to further improve performance.Still, we’re pleased with the result: an inexpensive, low-maintenance static discovery platform that allows users to browse, search, and filter Data.gov Archive records entirely in the browser.Why This Matters for Libraries, Digital Humanities Projects, and BeyondThis new pattern offers a compelling model for libraries, academic archives, and DH projects of all sizes: By shifting from an expensive server to lower cost static storage, projects can sustainably offer their users access to data.Reduced technical overhead: With no dedicated backend server, security risks are reduced, no patching or upgrades are needed, and crashing servers are not a concern. Projects can be set up with care, but without demanding constant attention. Organizations can be more confident that their archive and discovery interfaces remain usable and accessible, even as staffing or funding changes over time.Knowing that we are not the only group interested in approaching access in this way, we’re sharing our generalized learnings. We see a few ways forward for others in the knowledge and information world: If your organization has large, relatively static datasets, consider experimenting with a browser-based search tool using static hosting. Template applications, workflows, and lessons learned can help this new pattern gain adoption and maturity across the community.This project is still evolving, and we invite others—particularly those in libraries and digital cultural heritage—to explore these possibilities with us. We’re committed to open sharing as we refine our tools, and we welcome collaboration or feedback at lil@law.harvard.edu.]]></content:encoded></item><item><title>Just use a button</title><link>https://gomakethings.com/just-use-a-button/</link><author>moebrowne</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 16:59:22 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[One of the weirdest “debates” I seem to perpetually have with framework-enthusiastic developers is whether or not a  is “just as good” as a . it’s not. Let’s dig in.Among the React crowd, and also among people who seem to enjoy HTMX, I see a lot this…
	Open Modal
This element does not announce itself as an interactive element to screen reader users.You can’t focus on a  with a keyboard.The event only fires on , not when the  or  keys are pressed (again, keyboard users).I’ve had arguments with a very prominent React thought leader whose name starts with R who insisted that using a  was “more accessible” than using a , and that Twitter made the right decision in using this pattern in their app.It’s wrong. It’s all wrong.Many HTML elements have  that tell assistive tech like screen readers what they do.The  element is one of them. It has an implicit  of , which tells screen reader users it can be interacted with and will trigger some type of behavior in the app.The HTML  attribute can be used to add or modify the role of an element. And so, folks like React Ry–thought-leader-guy will say stuff like (I’m paraphrasing)…That attribute exists for a reason. You can add  to a  to give it the correct semantics.OK, that addresses one issue.That role doesn’t affect focusability (or lack thereof) or keyboard behavior. Visually impaired users and people who navigate with a keyboard still can’t use it.“No worries!” they say. “We can fix that, too!”You can make the element focusable with the  attribute.
	Open Modal
You , though! Seriously, just don’t fuck with focus order.It’s way too easy to go down this path and then fuck it up and have folks jumping all over the page instead of navigating through in the normal and expected order.And again, still no keyboard interactivity.But don’t fear! You can add that, too. You just need to listen for all  events, and then filter them out by  so that you only run your code if the  or  keys were pressed (the latter means checking for a literal space: ).That can’t run on the element, either. You’ve got to attach that even to the  and figure out which element has focus.So um… ok, I guess it is technically a fix, but…You’ve just recreated all of the functionality a  gives you for freeSeriously, WTF would you do that?!?All of these hoops to write this HTML…
	Open Modal
When you could write this HTML instead…
	Open Modal
Has the correct  implicitly.Is automatically focusable.Fires a  event in response to  and  presses when it has focus.Look, I’m a lazy developer.And I suspect, if you’re someone who loves tools like React, you probably are, too. It’s cool, I get it! The best code is the code you didn’t write and all that.Use the correct element for the job, and avoid writing a bunch of extra code!]]></content:encoded></item><item><title>Futurelock: A subtle risk in async Rust</title><link>https://rfd.shared.oxide.computer/rfd/0609</link><author>bcantrill</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 16:49:26 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Bounded channels are not really the issue here.  Even in omicron#9259, the capacity=1 channel was basically behaving as documented and as one would expect.  It woke up a sender when capacity was available, and the other senders were blocked to maintain the documented FIFO property.  However, some of the patterns that we use with bounded channels are problematic on their own and, if changed, could prevent the channel from getting caught up in a futurelock.In Omicron, we commonly use bounded channels with .  The bound is intended to cap memory usage and provide backpressure, but using the blocking  creates a second  queue: the wait queue for the channel.  Instead, we could consider using a larger capacity channel plus  and propagate failure from .As an example, when we use the actor pattern, we typically observe that there’s only one actor and potentially many clients, so there’s not much point in buffering messages  the channel.  So we use  and let clients block in .  But we could instead have  and have clients use  and propagate failure if they’re unable to send the message.  The value  here is pretty arbitrary.  You want it to be large enough to account for an expected amount of client concurrency, but not larger.  If the value is too small, you’ll wind up with spurious failures when the client could have just waited a bit longer.  If the value is too large, you can wind up queueing so much work that the actor is always behind (and clients are potentially even timing out at a higher level).  One might observe:Channel limits, channel limits: always wrong!Some too short and some too long!But as with timeouts, it’s often possible to find values that work in practice.Using  is  a mitigation because this still results in the sender blocking.  It needs to be polled after the timeout expires in order to give up.  But with futurelock, it will never be polled.]]></content:encoded></item><item><title>Another European agency shifts off US Tech as digital sovereignty gains steam</title><link>https://www.zdnet.com/article/another-european-agency-ditches-big-tech-as-digital-sovereignty-movement-gains-steam/</link><author>CrankyBear</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 16:39:22 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Austria's Ministry of Economy has migrated to a Nextcloud platform.It's the latest move in a European trend to shift away from Big Tech.European governments and agencies want to control sensitive data.This shift away from proprietary, foreign-owned cloud services, such as Microsoft 365, to an open-source, European-based cloud service aligns with a growing trend among European governments and agencies. They want control over sensitive data and to declare their independence from US-based tech providers. European companies are encouraging this trend. Many of them have joined forces in the newly created non-profit foundation, the EuroStack Initiative. This foundation's goal is " to organize action, not just talk, around the pillars of the initiative: Buy European, Sell European, Fund European." What's the motive behind these moves away from proprietary tech? Well, in Austria's case, Florian Zinnagl, CISO of the Ministry of Economy, Energy, and Tourism (BMWET), explained, "We carry responsibility for a large amount of sensitive data -- from employees, companies, and citizens. As a public institution, we take this responsibility very seriously. That's why we view it critically to rely on cloud solutions from non-European corporations for processing this information."All of these organizations aim to keep data storage and processing within national or European borders to enhance security, comply with privacy laws such as the EU's General Data Protection Regulation (GDPR), and mitigate risks from potential commercial and foreign government surveillance. Open-source software is seen as combining the virtues of faster development and better security, while providing companies and governments with more control, as general manager Thierry Carrez of the OpenInfra Foundation recently suggested: "Open infrastructure allows nations and organizations to maintain control over their applications, their data, and their destiny while benefiting from global collaboration."  While the US may not like it, with NextCloud's help, BMWET completed its migration in just four months. Although BMWET had already begun adopting Microsoft 365 and Teams before the project's start, the shift was still considered a success. That's because instead of reversing its path, the ministry implemented a hybrid architecture: Nextcloud handles internal collaboration and secure data management, while Teams remains available for external meetings.The project emphasized integration with existing workflows, including seamless integration with Outlook email and calendar via Sendent's Outlook app. This approach minimized disruption and ensured user acceptance. However, not all migrations progress so well. For example, in Austria, the Ministry of Justice decided to replace Office with LibreOffice. Yet the transition has run into trouble. It appears that the move of 20,000 desktops, which was prompted by a desire to reduce spending on Microsoft licenses, has been, as one person reported, an "unprofessional, rushed operation." Some offices are still on Office, others on LibreOffice, and they're running into incompatible document format problems and misfires in e-mail systems. The moral of the story is that any switch from one software suite to another requires careful handling by the IT department and helpdesk staff. Otherwise, you end up with unhappy users.That said, BMWET's bold shift to Nextcloud appears to have gone well. This initiative demonstrates that adopting sovereign cloud solutions can be practical, user-friendly, and rapid in the public sector. However, as Austria's Justice Ministry experience has shown, simply shifting to an open-source approach without careful planning can get in the way of getting work done. ]]></content:encoded></item><item><title>AI scrapers request commented scripts</title><link>https://cryptography.dog/blog/AI-scrapers-request-commented-scripts/</link><author>ColinWright</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 15:44:19 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Ask HN: Who uses open LLMs and coding assistants locally? Share setup and laptop</title><link>https://news.ycombinator.com/item?id=45771870</link><author>threeturn</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 13:39:55 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Dear Hackers,
I’m interested in your real-world workflows for using open-source LLMs and open-source coding assistants on your laptop (not just cloud/enterprise SaaS). Specifically:Which model(s) are you running (e.g., Ollama, LM Studio, or others) and which open-source coding assistant/integration (for example, a VS Code plugin) you’re using?What laptop hardware do you have (CPU, GPU/NPU, memory, whether discrete GPU or integrated, OS) and how it performs for your workflow?What kinds of tasks you use it for (code completion, refactoring, debugging, code review) and how reliable it is (what works well / where it falls short).I'm conducting my own investigation, which I will be happy to share as well when over.]]></content:encoded></item><item><title>Attention lapses due to sleep deprivation due to flushing fluid from brain</title><link>https://news.mit.edu/2025/your-brain-without-sleep-1029</link><author>gmays</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 13:14:23 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Nearly everyone has experienced it: After a night of poor sleep, you don’t feel as alert as you should. Your brain might seem foggy, and your mind drifts off when you should be paying attention.A new study from MIT reveals what happens inside the brain as these momentary failures of attention occur. The scientists found that during these lapses, a wave of cerebrospinal fluid (CSF) flows out of the brain — a process that typically occurs during sleep and helps to wash away waste products that have built up during the day. This flushing is believed to be necessary for maintaining a healthy, normally functioning brain.When a person is sleep-deprived, it appears that their body attempts to catch up on this cleansing process by initiating pulses of CSF flow. However, this comes at a cost of dramatically impaired attention.“If you don’t sleep, the CSF waves start to intrude into wakefulness where normally you wouldn’t see them. However, they come with an attentional tradeoff, where attention fails during the moments that you have this wave of fluid flow,” says Laura Lewis, the Athinoula A. Martinos Associate Professor of Electrical Engineering and Computer Science, a member of MIT’s Institute for Medical Engineering and Science and the Research Laboratory of Electronics, and an associate member of the Picower Institute for Learning and Memory.Lewis is the senior author of the study, which appears today in . MIT visiting graduate student Zinong Yang is the lead author of the paper.Although sleep is a critical biological process, it’s not known exactly why it is so important. It appears to be essential for maintaining alertness, and it has been well-documented that sleep deprivation leads to impairments of attention and other cognitive functions.During sleep, the cerebrospinal fluid that cushions the brain helps to remove waste that has built up during the day. In a 2019 study, Lewis and colleagues showed that CSF flow during sleep follows a rhythmic pattern in and out of the brain, and that these flows are linked to changes in brain waves during sleep.That finding led Lewis to wonder what might happen to CSF flow after sleep deprivation. To explore that question, she and her colleagues recruited 26 volunteers who were tested twice — once following a night of sleep deprivation in the lab, and once when they were well-rested.In the morning, the researchers monitored several different measures of brain and body function as the participants performed a task that is commonly used to evaluate the effects of sleep deprivation.During the task, each participant wore an electroencephalogram (EEG) cap that could record brain waves while they were also in a functional magnetic resonance imaging (fMRI) scanner. The researchers used a modified version of fMRI that allowed them to measure not only blood oxygenation in the brain, but also the flow of CSF in and out of the brain. They also measured each subject’s heart rate, breathing rate, and pupil diameter.The participants performed two attentional tasks while in the fMRI scanner, one visual and one auditory. For the visual task, they had to look at a screen that had a fixed cross. At random intervals, the cross would turn into a square, and the participants were told to press a button whenever they saw this happen. For the auditory task, they would hear a beep instead of seeing a visual transformation.Sleep-deprived participants performed much worse than well-rested participants on these tasks, as expected. Their response times were slower, and for some of the stimuli, the participants never registered the change at all.During these momentary lapses of attention, the researchers identified several physiological changes that occurred at the same time. Most significantly, they found a flux of CSF out of the brain just as those lapses occurred. After each lapse, CSF flowed back into the brain.“The results are suggesting that at the moment that attention fails, this fluid is actually being expelled outward away from the brain. And when attention recovers, it’s drawn back in,” Lewis says.The researchers hypothesize that when the brain is sleep-deprived, it begins to compensate for the loss of the cleansing that normally occurs during sleep, even though these pulses of CSF flow come with the cost of attention loss.“One way to think about those events is because your brain is so in need of sleep, it tries its best to enter into a sleep-like state to restore some cognitive functions,” Yang says. “Your brain’s fluid system is trying to restore function by pushing the brain to iterate between high-attention and high-flow states.”The researchers also found several other physiological events linked to attentional lapses, including decreases in breathing and heart rate, along with constriction of the pupils. They found that pupil constriction began about 12 seconds before CSF flowed out of the brain, and pupils dilated again after the attentional lapse.“What’s interesting is it seems like this isn’t just a phenomenon in the brain, it’s also a body-wide event. It suggests that there’s a tight coordination of these systems, where when your attention fails, you might feel it perceptually and psychologically, but it’s also reflecting an event that’s happening throughout the brain and body,” Lewis says.This close linkage between disparate events may indicate that there is a single circuit that controls both attention and bodily functions such as fluid flow, heart rate, and arousal, according to the researchers.“These results suggest to us that there’s a unified circuit that’s governing both what we think of as very high-level functions of the brain — our attention, our ability to perceive and respond to the world — and then also really basic fundamental physiological processes like fluid dynamics of the brain, brain-wide blood flow, and blood vessel constriction,” Lewis says.In this study, the researchers did not explore what circuit might be controlling this switching, but one good candidate, they say, is the noradrenergic system. Recent research has shown that this system, which regulates many cognitive and bodily functions through the neurotransmitter norepinephrine, oscillates during normal sleep.The research was funded by the National Institutes of Health, a National Defense Science and Engineering Graduate Research Fellowship, a NAWA Fellowship, a McKnight Scholar Award, a Sloan Fellowship, a Pew Biomedical Scholar Award, a One Mind Rising Star Award, and the Simons Collaboration on Plasticity in the Aging Brain.]]></content:encoded></item><item><title>How OpenAI uses complex and circular deals to fuel its multibillion-dollar rise</title><link>https://www.nytimes.com/interactive/2025/10/31/technology/openai-fundraising-deals.html</link><author>reaperducer</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 13:03:46 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Sam Altman, the chief executive of OpenAI, says that technological revolutions are driven by more than just technology. They are also driven, he argues, by new ways of paying for them.“There is always a lot of focus on technological innovation. What really drives a lot of progress is when people also figure out how to innovate on the financial model,” he recently said at the site of a data center that OpenAI is building in Abilene, Texas.Over the last several years, Mr. Altman’s company has found unusual and creative ways of paying for the computing power needed to fuel its ambitions.Many of the deals OpenAI has struck — with chipmakers, cloud computing companies and others — are strangely circular. OpenAI receives billions from tech companies before sending those billions back to the same companies to pay for computing power and other services.Industry experts and financial analysts have welcomed the start-up’s creativity. But these unorthodox arrangements have also fueled concerns that OpenAI is helping to inflate a potential financial bubble as it builds what is still a highly speculative technology.Here are unusual financial agreements helping to drive the ambitions of OpenAI, the poster child of the artificial intelligence revolution.From 2019 through 2023, Microsoft was OpenAI’s primary investor. The tech giant pumped more than  into the start-up. Then OpenAI funneled most of those billions back into Microsoft, buying  needed to fuel the development of new A.I. technologies.(The New York Times has sued OpenAI and Microsoft, claiming copyright infringement of news content related to A.I. systems. The two companies have denied the suit’s claims.)By the summer of last year, OpenAI could not get all the computing power it wanted from Microsoft. So it started signing cloud computing contracts with other companies, including Oracle and little-known start-ups with names like CoreWeave.Across three different deals signed this year, OpenAI agreed to pay CoreWeave, a company that builds A.I. data centers, more than  for computing power. As part of these agreements, OpenAI received  in CoreWeave stock, which could ultimately help pay for this computing power.OpenAI also struggled to get the additional investment dollars it wanted from Microsoft. So, it turned to other investors. Earlier this year, the Japanese conglomerate SoftBank led a  investment in OpenAI.At the same time, OpenAI has been working with various companies to build its own computing data centers, rather than rely on cloud computing deals. This also includes SoftBank, which is known for highly speculative technological bets that don’t always pay off. The company is raising  to help OpenAI build data centers in Texas and Ohio.Similarly, Oracle, a software and cloud computing giant, has agreed to spend  building new data centers for OpenAI in Texas, New Mexico, Michigan and Wisconsin. OpenAI will then pay Oracle roughly the same amount to use these  over the next several years.The United Arab Emirates was part of an OpenAI’s fund-raising round in October 2024. Now, G42, a firm with close ties to the Emirati government, is building a roughly  for OpenAI in the Emirates.Last month, Nvidia announced that it intended to invest  in OpenAI over the next several years. This could help OpenAI pay for its new data centers. As OpenAI buys or leases specialized chips from Nvidia, Nvidia will pump billions back into OpenAI.Two weeks later, OpenAI signed an agreement with AMD that allows OpenAI to buy up to  in the chipmaker at a penny per share. That translates to roughly a 10 percent stake in the company. This stock could supply OpenAI with additional capital as it works to build new data centers.OpenAI pulls in billions of dollars in revenue each year from customers who pay for ChatGPT, computer programming tools and other technologies. But it still loses more money than it makes, according to a person familiar with the company’s finances.If the company can use its new data centers to significantly improve A.I. technologies and expand its revenue over the next several years, it can become a viable business, as Mr. Altman believes it will. If technology progress stalls, OpenAI – and its many partners – could lose enormous amounts of money. Smaller companies like CoreWeave, which are taking on enormous amounts of debt to build new data centers, could go bankrupt.In some cases, companies are hedging their bets. Nvidia and AMD, for instance, have the option of reducing the cash and stock they send to OpenAI if the A.I. market does not expand as quickly as expected. But others would be left with enormous debt, which could send ripples across the larger economy.]]></content:encoded></item><item><title>The cryptography behind electronic passports</title><link>https://blog.trailofbits.com/2025/10/31/the-cryptography-behind-electronic-passports/</link><author>tatersolid</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 11:33:41 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Did you know that most modern passports are actually embedded devices containing an entire filesystem, access controls, and support for several cryptographic protocols? Such passports display a small symbol indicating an electronic machine-readable travel document (eMRTD), which digitally stores the same personal data printed in traditional passport booklets in its embedded filesystem. Beyond allowing travelers in some countries to skip a chat at border control, these documents use cryptography to prevent unauthorized reading, eavesdropping, forgery, and copying.This blog post describes how electronic passports work, the threats within their threat model, and how they protect against those threats using cryptography. It also discusses the implications of using electronic passports for novel applications, such as zero-knowledge identity proofs. Like many widely used electronic devices with long lifetimes, electronic passports and the systems interacting with them support insecure, legacy protocols that put passport holders at risk for both standard and novel use cases.Electronic passport basicsA passport serves as official identity documentation, primarily for international travel. The International Civil Aviation Organization (ICAO) defines the standards for electronic passports, which (as suggested by the “Chip Inside” symbol) contain a contactless integrated circuit (IC) storing digital information. Essentially, the chip contains a filesystem with some access control to protect unauthorized reading of data. The full technical details of electronic passports are specified in ICAO Doc 9303; this blog post will mostly focus on part 10, which specifies the logical data structure (LDS), and part 11, which specifies the security mechanisms.The filesystem architecture is straightforward, comprising three file types: master files (MFs) serving as the root directory; dedicated files (DFs) functioning as subdirectories or applications; and elementary files (EFs) containing actual binary data. As shown in the above figure, some files are mandatory, whereas others are optional. This blog post will focus on the eMRTD application. The other applications are part of LDS 2.0, which would allow the digital storage of travel records (digital stamps!), electronic visas, and additional biometrics (so you can just update your picture instead of getting a whole new passport!).How the eMRTD application worksThe following figure shows the types of files the eMRTD contains:There are generic files containing common or security-related data; all other files are so-called data groups (DGs), which primarily contain personal information (most of which is also printed on your passport) and some additional security data that will become important later. All electronic passports must contain DGs 1 and 2, whereas the rest is optional.Comparing the contents of DG1 and DG2 to the main passport page shows that most of the written data is stored in DG1 and the photo is stored in DG2. Additionally, there are two lines of characters at the bottom of the page called the machine readable zone (MRZ), which contains another copy of the DG1 data with some check digits, as shown in the following picture.Digging into the threat modelElectronic passports operate under a straightforward threat model that categorizes attackers based on physical access: those who hold a passport versus those who don’t. If you are near a passport but you do not hold it in your possession, you should not be able to do any of the following:Read any personal information from that passportEavesdrop on communication that the passport has with legitimate terminalsFigure out whether it is a specific passport so you can trace its movementsEven if you do hold one or more passports, you should not be able to do the following:Forge a new passport with inauthentic dataMake a digital copy of the passportRead the fingerprint (DG3) or iris (DG4) informationElectronic passports use short-range RFID for communication (ISO 14443). You can communicate with a passport within a distance of 10–15 centimeters, but eavesdropping is possible at distances of several meters. Because electronic passports are embedded devices, they need to be able to withstand attacks where the attacker has physical access to the device, such as elaborate side-channel and fault injection attacks. As a result, they are often certified (e.g., under Common Criteria).We focus here on the threats against the electronic components of the passport. Passports have many physical countermeasures, such as visual effects that become visible under certain types of light. Even if someone can break the electronic security that prevents copying passports, they would still have to defeat these physical measures to make a full copy of the passport. That said, some systems (such as online systems) only interact digitally with the passport, so they do not perform any physical checks at all.The earliest electronic passports lacked most cryptographic mechanisms. Malaysia issued the first electronic passport in 1998, which predates the first ICAO eMRTD specifications from 2003. Belgium subsequently issued the first ICAO-compliant eMRTD in 2004, which in turn predates the first cryptographic mechanism for confidentiality specified in 2005.While we could focus solely on the most advanced cryptographic implementations, electronic passports remain in circulation for extended periods (typically 5–10 years), meaning legacy systems continue operating alongside modern solutions. This means that there are typically many old passports floating around that do not support the latest and greatest access control mechanisms. Similarly, not all inspection systems/terminals support all of the protocols, which means passports potentially need to support multiple protocols. All protocols discussed in the following are described in more detail in ICAO Doc 9303 Part 11.Legacy protection mechanisms for electronic passports provide better security than what they were replacing (nothing), even though they have key shortcomings regarding confidentiality and (to a lesser extent) copying.Legacy confidentiality protections: How basic access control failsIn order to prevent eavesdropping, you need to set up a secure channel. Typically, this is done by deriving a shared symmetric key, either from some shared knowledge, or through a key exchange. However, the passport cannot have its own static public key and send it over the communication channel, because this would enable tracing of specific passports.Additionally, it should only be possible to set up this secure channel if you have the passport in your possession. So, what sets holders apart from others? Holders can read the physical passport page that contains the MRZ!This brings us to the original solution to set up a secure channel with electronic passports: basic access control (BAC). When you place your passport with the photo page face down into an inspection system at the airport, it scans the page and reads the MRZ. Now, both sides derive encryption and message authentication code (MAC) keys from parts of the MRZ data using SHA-1 as a KDF. Then, they exchange freshly generated challenges and encrypt-then-MAC these challenges together with some fresh keying material to prove that both sides know the key. Finally, they derive session keys from the keying material and use them to set up the secure channel.However, BAC fails to achieve any of its security objectives. The static MRZ is just some personal data and does not have very high entropy, which makes it guessable. Even worse, if you capture one valid exchange between passport and terminal, you can brute-force the MRZ offline by computing a bunch of unhardened hashes. Moreover, passive listeners who know the MRZ can decrypt all communications with the passport. Finally, the fact that the passport has to check both the MAC and the challenge has opened up the potential for oracle attacks that allow tracing by replaying valid terminal responses.Forgery prevention: Got it right the first timePreventing forgery is relatively simple. The passport contains a file called the Document Security Object (EF.SOD), which contains a list of hashes of all the Data Groups, and a signature over all these hashes. This signature comes from a key pair that has a certificate chain back to the Country Signing Certificate Authority (CSCA). The private key associated with the CSCA certificate is one of the most valuable assets in this system, because anyone in possession of this private key can issue legitimate passports containing arbitrary data.The process of reading the passport, comparing all contents to the SOD, and verifying the signature and certificate chain is called passive authentication (PA). This will prove that the data in the passport was signed by the issuing country. However, it does nothing to prevent the copying of existing passports: anyone who can read a passport can copy its data into a new chip and it will pass PA. While this mechanism is listed among the legacy ones, it meets all of its objectives and is therefore still used without changes.Legacy copying protections: They work, but some issues remainPreventing copying requires having something in the passport that cannot be read or extracted, like the private key of a key pair. But how does a terminal know that a key pair belongs to a genuine passport? Since countries are already signing the contents of the passport for PA, they can just put the public key in one of the data groups (DG15), and use the private key to sign challenges that the terminal sends. This is called active authentication (AA). After performing both PA and AA, the terminal knows that the data in the passport (including the AA public key) was signed by the government and that the passport contains the corresponding private key.This solution has two issues: the AA signature is not tied to the secure channel, so you can relay a signature and pretend that the passport is somewhere it’s not. Additionally, the passport signs an arbitrary challenge without knowing the semantics of this message, which is generally considered a dangerous practice in cryptography.Extended Access Control (EAC) fixes some of the issues related to BAC and AA. It comprises chip authentication (CA), which is a better AA, and terminal authentication (TA), which authenticates the terminal to the passport in order to protect access to the sensitive information stored in DG3 (fingerprint) and DG4 (iris). Finally, password authenticated connection establishment (PACE, described below) replaces BAC altogether, eliminating its weaknesses.Chip Authentication: Upgrading the secure channelCA is very similar to AA in the sense that it requires countries to simply store a public key in one of the DGs (DG14), which is then authenticated using PA. However, instead of signing a challenge, the passport uses the key pair to perform a static-ephemeral Diffie-Hellman key exchange with the terminal, and uses the resulting keys to upgrade the secure channel from BAC. This means that passive listeners that know the MRZ cannot eavesdrop after doing CA, because they were not part of the key exchange.Terminal Authentication: Protecting sensitive data in DG3 and DG4Similar to the CSCA for signing things, each country has a Country Verification Certificate Authority (CVCA), which creates a root certificate for a PKI that authorizes terminals to read DG3 and DG4 in the passports of that country. Terminals provide a certificate chain for their public key and sign a challenge provided by the passport using their private key. The CVCA can authorize document verifiers (DVs) to read one or both of DG3 and DG4, which is encoded in the certificate. The DV then issues certificates to individual terminals. Without such a certificate, it is not possible to access the sensitive data in DG3 and DG4.Password Authenticated Connection Establishment: Fixing the basic problemsThe main idea behind PACE is that the MRZ, much like a password, does not have sufficient entropy to protect the data it contains. Therefore, it should not be used directly to derive keys, because this would enable offline brute-force attacks. PACE can work with various mappings, but we describe only the simplest one in the following, which is the generic mapping. Likewise, PACE can work with other passwords besides the MRZ (such as a PIN), but this blog post focuses on the MRZ.First, both sides use the MRZ data (the password) to derive a password key. Next, the passport encrypts a nonce using the password key and sends it to the terminal, which can decrypt it if it knows the password. The terminal and passport also perform an ephemeral Diffie-Hellman key exchange. Now, both terminal and passport derive a new generator of the elliptic curve by applying the nonce as an additive tweak to the (EC)DH shared secret. Using this new generator, the terminal and passport perform another (EC)DH to get a second shared secret. Finally, they use this second shared secret to derive session keys, which are used to authenticate the (EC)DH public keys that they used earlier on in the protocol, and to set up the secure channel. Figure 6 shows a simplified protocol diagram.Anyone who does not know the password cannot follow the protocol to the end, which will become apparent in the final step when they need to authenticate the data with the session keys. Before authenticating the terminal, the passport does not share any data that enables brute-forcing the password key. Non-participants who do know the password cannot derive the session keys because they do not know the ECDH private keys.Gaps in the threat model: Why you shouldn’t give your passport to just anyoneWhen considering potential solutions to maintaining passports’ confidentiality and authenticity, it’s important to account for what the inspection system  with your passport, and not just the fancy cryptography the passport supports. If an inspection system performs only BAC/PACE and PA, anyone who has seen your passport could make an electronic copy and pretend to be you when interacting with this system. This is true even if your passport supports AA or CA.Another important factor is tracing: the specifications aim to ensure that someone who does not know a passport’s PACE password (MRZ data in most cases) cannot trace that passport’s movements by interacting with it or eavesdropping on communications it has with legitimate terminals. They attempt to achieve this by ensuring that passports always provide random identifiers (e.g., as part of Type A or Type B ISO 14443 contactless communication protocols) and that the contents of publicly accessible files (e.g., those containing information necessary for performing PACE) are the same for every citizen of a particular country.However, all of these protections go out of the window when the attacker knows the password. If you are entering another country and border control scans your passport, they can provide your passport contents to others, enabling them to track the movements of your passport. If you visit a hotel in Italy and they store a scan of your passport and get hacked, anyone with access to this information can track your passport. This method can be a bit onerous, as it requires contacting various nearby contactless communication devices and trying to authenticate to them as if they were your passport. However, some may still choose to include it in their threat models.Some countries state in their issued passports that the holder should give it to someone else only if there is a statutory need. At Italian hotels, for example, it is sufficient to provide a prepared copy of the passport’s photo page with most data redacted (such as your photo, signature, and any personal identification numbers). In practice, not many people do this.Even without the passport, the threat model says nothing about tracking particular groups of people. Countries typically buy large quantities of the same electronic passports, which comprise a combination of an IC and the embedded software implementing the passport specifications. This means that people from the same country likely have the same model of passport, with a unique fingerprint comprising characteristics like communication time, execution time, supported protocols (ISO 14443 Type A vs Type B), etc. Furthermore, each country may use different parameters for PACE (supported curves or mappings, etc.), which may aid an attacker in fingerprinting different types of passports, as these parameters are stored in publicly readable files.Security and privacy implications of zero-knowledge identity proofsAn emerging approach in both academic research and industry applications involves using zero-knowledge (ZK) proofs with identity documents, enabling verification of specific identity attributes without revealing complete document contents. This is a nice idea in theory, because this will allow proper use of passports where there is no statutory need to hand over your passport. However, there are security implications.First of all, passports cannot generate ZK proofs by themselves, so this necessarily involves exposing your passport to a prover. Letting anyone or anything read your passport means that you downgrade your threat model with respect to that entity. So when you provide your passport to an app or website for the purposes of creating a ZK proof, you need to consider what they will do with the information in your passport. Will it be processed locally on your device, or will it be sent to a server? If the data leaves your device, will it be encrypted and only handled inside a trusted execution environment (TEE)? If so, has this whole stack been audited, including against malicious TEE operators?Second, if the ZK proving service relies on PA for its proofs, then anyone who has ever seen your passport can pretend to be you on this service. Full security requires AA or CA. As long as there exists any service that relies only on PA, anyone whose passport data is exposed is vulnerable to impersonation. Even if the ZK proving service does not incorporate AA or CA in their proofs, they should still perform one of these procedures with the passport to ensure that only legitimate passports sign up for this service.Finally, the system needs to consider what happens when people share their ZK proof with others. The nice thing about a passport is that you cannot easily make copies (if AA or CA is used), but if I can allow others to use my ZK proof, then the value of the identification decreases.It is important that such systems are audited for security, both from the point of view of the user and the service provider. If you’re implementing ZK proofs of identity documents, contact us to evaluate your design and implementation.]]></content:encoded></item><item><title>My Impressions of the MacBook Pro M4</title><link>https://michael.stapelberg.ch/posts/2025-10-31-macbook-pro-m4-impressions/</link><author>secure</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 10:13:40 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[I have been using a MacBook Pro M4 as my portable computer for the last half a
year and wanted to share a few short impressions. As always, I am not a
professional laptop reviewer, so in this article you won’t find benchmarks, just
subjective thoughts!Back in 2021, I wrote about the MacBook Air
M1, which was the first computer I used that
contained Apple’s own ARM-based CPU. Having a silent laptop with long battery
life was a game-changer, so I wanted to keep those properties.When the US government announced tariffs, I figured I would replace my 4-year
old MacBook Air M1 with a more recent model that should last a few more
years. Ultimately, Apple’s prices remained stable, so, in retrospect, I could
have stayed with the M1 for a few more years. Oh well.The nano-textured displayI went to the Apple Store to compare the different options in
person. Specifically, I was curious about the display and whether the increased
weight and form factor of the MacBook Pro (compared to a MacBook Air) would be
acceptable. Another downside of the Pro model is that it comes with a fan, and I
really like absolutely quiet computers. Online, I read from other MacBook Pro
owners that the fan mostly stays off.In general, I would have preferred to go with a MacBook Air because it has
enough compute power for my needs and I like the case better (no ventilation
slots), but unfortunately only the MacBook Pro line has the better displays.Why aren’t all displays nano-textured? The employee at the Apple Store presented
the trade-off as follows: The nano texture display is great at reducing
reflections, at the expense of also making the picture slightly less vibrant.I could immediately see the difference when placing two laptops side by side:
The bright Apple Store lights showed up very prominently on the normal display
(left), and were almost not visible at all on the nano texture display (right):Personally, I did not perceive a big difference in “vibrancy”, so my choice was
clear: I’ll pick the MacBook Pro over the MacBook Air (despite the weight) for
the nano texture display!After using the laptop in a number of situations, I am very happy with this
choice. In normal scenarios, I notice no reflections at all (where my previous
laptop did show reflections!). This includes using the laptop on a train (next
to the window), or using the laptop outside in daylight.(When I chose the new laptop, Apple’s M4 chips were current. By now, they have
released the first devices with M5 chips.)I decided to go with the MacBook Pro with M4 chip instead of the M4  chip
because I don’t need the extra compute, and the M4 needs less cooling — the M4
Pro apparently runs hotter. This increases the chance of the fan staying off.Here are the specs I ended up with:14" Liquid Retina XDR Display with nano textureApple M4 Chip (10 core CPU, 10 core GPU)32 GB RAM (this is the maximum!), 2 TB SSD (enough for this computer)One thing I noticed is that the MacBook Pro M4 sometimes gets warm, even when it
is connected to power, but is suspended to RAM (and has been fully charged for
hours). I’m not sure why.Luckily, the fan indeed stays silent. I think I might have heard it spin up once
in half a year or so?The battery life is amazing! The previous MacBook Air M1 had amazing all-day
battery life already, and this MacBook Pro M4 lasts even longer. For example,
watching videos on a train ride (with VLC) for 3 hours consumed only 10% of
battery life. I generally never even carry the charger.Because of that, Apple’s re-introduction of MagSafe, a magnetic power connector
(so you don’t damage the laptop when you trip over it), is nice-to-have but
doesn’t really make much of a difference anymore. In fact, it might be better to
pack a USB-C cable when traveling, as that makes you more flexible in how you
use the charger.I was curious whether the 120 Hz display would make a difference in practice. I
mostly notice the increased refresh rate when there are animations, but not,
for example, when scrolling.One surprising discovery (but obvious in retrospect) is that even non-animations
can become faster. For example, when running a Go web server on , I
noticed that navigating between pages by clicking links felt faster on the 120
Hz display!The following illustration shows why that is, using a page load that takes 6ms
of processing time. There are three cases (the illustration shows an average
case and the worst case):Best case: Page load finishes  the next frame is displayed: no delay.Worst case: Page load finishes  a frame is displayed: one frame of delay.Most page loads are somewhere in between. We’ll have 0.x to 1.0 frames of delayAs you can see, the waiting time becomes shorter when going from 60 Hz (one
frame every 16.6ms) to 120 Hz (one frame every 8.3ms). So if you’re working with
a system that has <8ms response times, you might observe actions completing (up
to) twice as fast!I don’t notice going back to 60 Hz displays on computers. However, on phones,
where a lot more animations are a key part of the user experience, I think 120
Hz displays are more interesting.My ideal MacBook would probably be a MacBook Air, but with the nano-texture display! :)I still don’t like macOS and would prefer to run Linux on this laptop. But
Asahi Linux still needs some work before it’s usable
for me (I need external display output, and M4 support). This doesn’t bother me
too much, though, as I don’t use this computer for serious work.]]></content:encoded></item><item><title>Reasoning models reason well, until they don&apos;t</title><link>https://arxiv.org/abs/2510.22371</link><author>optimalsolver</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 09:23:41 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>AMD could enter ARM market with Sound Wave APU built on TSMC 3nm process</title><link>https://www.guru3d.com/story/amd-enters-arm-market-with-sound-wave-apu-built-on-tsmc-3nm-process/</link><author>walterbell</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 03:07:48 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[AMD is expanding its processor portfolio beyond the x86 architecture with its first ARM-based APU, internally known as “Sound Wave.” The chip’s existence was uncovered through customs import records, confirming several details about its design and purpose. Built with a BGA-1074 package measuring 32 mm × 27 mm, the processor fits within standard mobile SoC dimensions, making it suitable for thin and light computing platforms. It employs a 0.8 mm pitch and FF5 interface, replacing the FF3 socket previously used in Valve’s Steam handheld devices, further hinting at a new generation of compact AMD-powered hardware.
                                    According to leaks from industry insiders such as @Moore’s Law Is Dead and @KeplerL2, “Sound Wave” is manufactured on  and aims for a  range, positioning it directly against Qualcomm’s Snapdragon X Elite. The chip is expected to power future  products scheduled for release in 2026. four RDNA 3.5 compute unitsmachine learning accelerationMemory support is another highlight: the chip integrates a 128-bit LPDDR5X-9600 controller and will reportedly include , aligning with current trends in unified memory designs used in ARM SoCs. Additionally, the APU carries AMD’s fourth-generation AI engine, enabling on-device inference tasks and enhanced efficiency for workloads such as speech recognition, image analysis, and real-time translation.While AMD experimented with ARM over a decade ago through the abandoned “Project Skybridge,” this new effort represents a more mature and strategic approach. With industry interest in efficient, ARM-based computing accelerating, “Sound Wave” could help AMD diversify its portfolio while leveraging its strengths in graphics and AI acceleration. If reports are accurate, the processor will enter production in late 2025, with commercial devices expected the following year.]]></content:encoded></item><item><title>John Carmack on mutable variables</title><link>https://twitter.com/id_aa_carmack/status/1983593511703474196</link><author>azhenley</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 02:34:36 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Ground stop at JFK due to staffing</title><link>https://www.fly.faa.gov/adv/adv_otherdis?advn=13&amp;adv_date=10312025&amp;facId=JFK&amp;title=ATCSCC%20ADVZY%20013%20JFK/ZNY%2010/31/2025%20CDM%20GROUND%20STOP&amp;titleDate=10/31/2025</link><author>akersten</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 01:48:39 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>ICE and the Smartphone Panopticon</title><link>https://www.newyorker.com/culture/infinite-scroll/ice-and-the-smartphone-panopticon</link><author>fortran77</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 01:13:56 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Last week, as  raids ramped up in New York, city residents set about resisting in the ways they had available: confronting agents directly on sidewalks, haranguing them as they processed down blocks, and recording them on phone cameras held aloft. Relentless documentation has proved something of an effective tool against President Donald Trump’s empowerment of ; agents have taken to wearing masks in fear of exposure, and the proliferation of imagery showing armed police and mobilized National Guard troops in otherwise calm cities has underlined the cruel absurdity of their activities. Activist memes have been minted on social media: a woman on New York’s Canal Street, dressed in a polka-dotted office-casual dress, flipping  agents off; a man in Washington, D.C., throwing a Subway sandwich at a federal agent in August. The recent “No Kings” marches were filled with protesters in inflatable frog costumes, inspired by a similarly outfitted man who got pepper-sprayed protesting outside the U.S. Immigration and Customs Enforcement Building in Portland, Oregon. Some might write the memes off as resistance porn, but digital content is at least serving as a lively defense mechanism in the absence of functional politics.At the same time, social media has served as a reinvigorated source of transparency in recent weeks, harking back to the days when Twitter became an organizing tool during the Arab Spring, in the early twenty-tens, or when Facebook and Instagram helped fuel the Black Lives Matter marches of 2020. The grassroots optimism of that earlier social-media era is long gone, though, replaced by a sense of posting as a last resort. After Trump authorized the deployment of the National Guard in Chicago earlier this month, the governor of Illinois, J. B. Pritzker, told residents to “record and narrate what you see—put it on social media.” But, if the anti- opposition is taking advantage of the internet,  and the Trump Administration are, too. Right-wing creators have been using the same channels to identify and publicize targets for raids. According to reporting in Semafor, the Trump-friendly YouTuber Nick Shirley’s videos of African migrant vendors on Canal Street seemed to help drive recent  sweeps of the area.  itself is also working to monitor social media. The investigative outlet  found documents revealing that the agency has enlisted an A.I.-driven surveillance product called Zignal Labs that creates “curated detection feeds” to aid in criminal investigations. According to reporting in ,  also has plans to build out a team of dozens of analysts to monitor social media and identify targets. Recent videos, identified by 404 Media and other publications, have purportedly shown  agents using technology developed by the data-analytics firm Palantir, founded by Peter Thiel and others, to scan social-media accounts, government records, and biometrics data of those they detain. Social media has become a political panopticon in which your posts are a conduit for your politics, and what you post can increasingly be used against you.Meanwhile, a new wave of digital tools has emerged to help surveil the surveillants. The apps ICEBlock, Red Dot, and DEICER all allow users to pinpoint where  agents are active, forming an online version of a whisper network to alert potential targets. Eyes Up provides a way for users to record and upload footage of abusive law-enforcement activity, building an archive of potential evidence. Its creator is a software developer named Mark (who uses only his first name to separate the project from his professional work); he was inspired to create Eyes Up earlier this year, when he began seeing clips of  abductions and harassment circulating on social media and worried about their shelf life. As he put it to me, “They could disappear at any given moment, whether the platforms decide to moderate, whether the individual deletes their account or the post.”Ultimately, the app itself was also vulnerable to sudden disappearance. After launching, on September 1st, Eyes Up accumulated thousands of downloads and thousands of minutes of uploaded footage. Then, on October 3rd, Mark received a notice that Apple was removing the app from its store on the grounds that it may “harm a targeted individual or group.” Eyes Up is not alone. ICEBlock and Red Dot have been blocked from both Apple and Google’s app stores, the two largest marketplaces; DEICER, like Eyes Up, was removed by Apple. Pressure on the tech platforms seemed to come from the Trump Administration; after a deadly shooting at an  field office in Dallas in late September, the Attorney General, Pam Bondi, said in a statement to Fox News Digital that ICEBlock “put ICE agents at risk just for doing their jobs.” Mark is contesting Apple’s decision about Eyes Up through its official channels, and the creator of ICEBlock, Joshua Aaron, has argued that his app should be treated no differently than services, such as Google’s Waze, that allow users to warn one another of highway speed traps. But for now they must try to make do with a limited reach.]]></content:encoded></item><item><title>Myths Programmers Believe about CPU Caches (2018)</title><link>https://software.rajivprab.com/2018/04/29/myths-programmers-believe-about-cpu-caches/</link><author>whack</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 00:46:56 +0000</pubDate><source url="https://news.ycombinator.com/">HN Front</source><content:encoded><![CDATA[distributed-system-architecturedatabase-isolation-levelsstrong-vs-eventual consistencyvolatilesread/written all the way to main memoryeven single-core systems are at risk of concurrency bugsgreat wealth of nuancethis tutorialMESI protocolThe state of the cache-line is set to M, since it is now modifiedmulti-processor systemimmediately trigger cache reads/writes instead]]></content:encoded></item><item><title>Show HN: Quibbler – A critic for your coding agent that learns what you want</title><link>https://github.com/fulcrumresearch/quibbler</link><author>etherio</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 00:43:57 +0000</pubDate><source url="https://news.ycombinator.com/shownew">HN Show</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Kimi Linear: An Expressive, Efficient Attention Architecture</title><link>https://github.com/MoonshotAI/Kimi-Linear</link><author>blackcat201</author><category>dev</category><category>hn</category><pubDate>Fri, 31 Oct 2025 00:07:36 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Leaker reveals which Pixels are vulnerable to Cellebrite phone hacking</title><link>https://arstechnica.com/gadgets/2025/10/leaker-reveals-which-pixels-are-vulnerable-to-cellebrite-phone-hacking/</link><author>akyuu</author><category>dev</category><category>hn</category><pubDate>Thu, 30 Oct 2025 23:12:10 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>A change of address led to our Wise accounts being shut down</title><link>https://shaun.nz/why-were-never-using-wise-again-a-cautionary-tale-from-a-business-burned/</link><author>jemmyw</author><category>dev</category><category>hn</category><pubDate>Thu, 30 Oct 2025 22:41:50 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[For years, one of my businesses has been a regular user of  (formerly TransferWise). Wise is a financial service that lets you send and receive money across currencies, often at a better rate and lower fee than traditional banks. Sounds great, right?This is our story – a sobering, frustrating, and frankly  experience that ended with our business and personal accounts being shut down, without any meaningful reason, support, or recourse.And all we did? We .🏢 A Routine Change Turned NightmareLike many businesses, we recently moved into a new office. Alongside the usual updates to suppliers and records, we updated our  with Wise. Not long after, we received an email requesting us to  the new address.Fair enough – we had no problem with that.Wise provided a dropdown list of acceptable documents: a lease agreement, rates notice, tax document, utilities bill, or telecommunications bill. Due to our company structure, most of those documents are in the name of our parent company or show our PO Box (which NZ Post requires, since they won’t deliver to our street address). But we had a  that ticked every box:Correct physical street address ✅Even detailed our fibre connection at the new premises ✅So we uploaded it – and assumed that would be the end of it.📞 The Call That Made No SenseDays later, we received an email: our document was . No clear reason. So, I called Wise and explained the situation to the customer service representative.Her response left me stunned.“The document was rejected because it was a , not a .”I paused, trying to process this. I politely explained that in New Zealand, a “tax invoice” is a legal form of a bill – even down to the name “tax invoice” being a legal requirement by IRD, and that’s how telecommunications companies issue invoices here. But she refused to accept it.“It needs to say  at the top,” she insisted.“A tax invoice isn’t acceptable.”This is simply , and completely out of touch with New Zealand’s business documentation standards. The rep wouldn’t budge.🧠 The “Solution” That Was Beyond BeliefStill trying to find a solution, I asked: what do you recommend I do then?“You should find a local shared workspace, lease a desk under your company name, change your registered office to that address, and use that lease document to verify your address with us.”Yes, you read that right.Wise’s advice was to artificially lease a desk we didn’t need, change our registered address, and use that document – just to verify an address we actually operate from.I asked to speak to a manager. That request was . She told me, flatly:“I  providing you with the correct information.”A bit more back and forth… then the call .📞 A Glimmer of Hope – Then The Hammer FallsLater that day, I received a call back from Wise – not from a manager (because apparently, Wise doesn’t have managers), but from a more “senior” representative.This rep was  and agreed the document should have been acceptable. She escalated the issue, resubmitted the document herself, and said she’d personally follow up if it was rejected again.🚫 “We’ve Restricted Your Account”I woke to an email with a stunning subject line:“We’ve restricted your account”Just like that, our  was locked. No warning. No reason. No discussion.We could no longer send or receive money, use our Wise cards, or even contact support. The email stated:“Due to our current risk policies, your account will be closed in a few months. You will not be able to use support channels.”Even worse? My  was locked too. The same personal account which did have its address fully verified, by a rates invoice for my personal address.🧾 An “Appeal” That Wasn’t an AppealThe email offered an option to . Naturally, I did.The appeal process asked for our articles of incorporation and . No problem.Then it asked us to provide our preferred currency, and bank account details to refund the balances.Wait… I thought this was an appeal? A chance to discuss and resolve the issue?That was the end. There was no opportunity to explain anything, no communication, no questions asked. The decision was made, and we were , permanently.To summarise the absurdity of this:We moved office, and updated our address with WiseWe provided a legal, NZ-compliant  showing our entity and addressIt was rejected because it was labelled a “Tax Invoice”A rep told us to lease a coworking desk elsewhere just to get a different documentA senior rep agreed we were right, and escalated itThen our accounts were shut down – with no explanation or recourseEven trying to call support now gets an automated message: “Because your account is restricted, we cannot connect you.”⚠️ Our Final Word: Be Very, Very CarefulWe had used Wise for . Regular monthly supplier payments. International stock orders. Five-figure transactions. Never a problem – until this. A minor change triggered a totally flawed process that , with no transparency or logical path to resolution.We’re not alone – a quick search shows many others facing similar horror stories with Wise.So this is my word of warning:💡 Don’t put all your eggs in the Wise basket.If you’re a business, don’t rely on them as your sole means of transferring funds. For us, it’s back to traditional banks – slower, yes, but at least they have humans you can talk to, and actual escalation paths.🧾 28th October update on our Wise debacle – it gets worse.Following the so-called “appeal” (which gave us no option to provide any information), we received the unsurprising outcome: Wise has decided to  as we had breached their acceptable use policy. 🤨What was surprising, however, was the  they gave after I queried what was breached in Wise’s Acceptable Use Policy:I was told my  was being closed for allegedly breaching their Acceptable Use Policy — specifically, section 1.4.e, which states “you may not use your personal Wise account to receive business payments.”I’ve  used my personal account for business transactions — in fact, over 99% of transfers were to overseas family members. When I asked for clarification or examples, I got none. Just a vague statement and the very strange line:“Just because we can’t offer you our services going forward doesn’t mean that we think your business activities are illegal or illegitimate — it just means that we don’t support those types of activities.”What activities?! Again, To make matters worse — our business account’s refund transfer . Why? Because it requires documentation — the  Wise previously rejected for address verification, claiming a telecommunications tax invoice isn’t a bill.After a few days, the transfer was then cancelled as of course, Wise was unable to “verify” us.So now our , their support ticket is marked “final response,” and our attempts to get clarity have gone nowhere. We’ve escalated the issue to Financial Services Complaints Ltd, Wise’s dispute resolution provider in New Zealand.Funds stuck. No clear reason. No accountability. Wise still gets a 0/10 from us.This isn’t just poor service — it’s unacceptable.Think twice before trusting Wise with your money.]]></content:encoded></item><item><title>Phone numbers for use in TV shows, films and creative works</title><link>https://www.acma.gov.au/phone-numbers-use-tv-shows-films-and-creative-works</link><author>nomilk</author><category>dev</category><category>hn</category><pubDate>Thu, 30 Oct 2025 21:49:11 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>Denmark reportedly withdraws Chat Control proposal following controversy</title><link>https://therecord.media/demark-reportedly-withdraws-chat-control-proposal</link><author>layer8</author><category>dev</category><category>hn</category><pubDate>Thu, 30 Oct 2025 21:35:42 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[ Denmark’s justice minister on Thursday said he will no longer push for an EU law requiring the mandatory scanning of electronic messages, including on end-to-end encrypted platforms.  Earlier in its European Council presidency, Denmark had brought back a draft law which would have required the scanning, sparking an intense backlash. Known as Chat Control, the measure was intended to crack down on the trafficking of child sex abuse materials (CSAM).  After days of silence, the German government on October 8 announced it would not support the proposal, tanking the Danish effort.  Danish Justice Minister Peter Hummelgaard told reporters on Thursday that his office will support voluntary CSAM detections.  "This will mean that the search warrant will not be part of the EU presidency's new compromise proposal, and that it will continue to be voluntary for the tech giants to search for child sexual abuse material," Hummelgaard said, according to local news reports.  The current model allowing for voluntary scanning expires in April, Hummelgaard said.  "Right now we are in a situation where we risk completely losing a central tool in the fight against sexual abuse of children,” he said. "That's why we have to act no matter what. We owe it to all the children who are subjected to monstrous abuse."   Meredith Whittaker, the president of the Signal Foundation, lobbied hard against the original measure, saying the organization would leave the European market if the provision was adopted.  “What they propose is in effect a mass surveillance free-for-all, opening up everyone’s intimate and confidential communications, whether government officials, military, investigative journalists, or activists,” she said at the time. ]]></content:encoded></item><item><title>Minecraft HDL, an HDL for Redstone</title><link>https://github.com/itsfrank/MinecraftHDL</link><author>sleepingreset</author><category>dev</category><category>hn</category><pubDate>Thu, 30 Oct 2025 18:59:02 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[]]></content:encoded></item><item><title>How We Found 7 TiB of Memory Just Sitting Around</title><link>https://render.com/blog/how-we-found-7-tib-of-memory-just-sitting-around</link><author>anurag</author><category>dev</category><category>hn</category><pubDate>Thu, 30 Oct 2025 18:25:05 +0000</pubDate><source url="https://news.ycombinator.com/best">HN</source><content:encoded><![CDATA[Debugging infrastructure at scale is rarely about one big aha moment. It’s often the result of many small questions, small changes, and small wins stacked up until something clicks.Plenty of teams run Kubernetes clusters bigger than ours. , more pods, more ingresses, you name it. In most dimensions, someone out there has us beat.There's one dimension where I suspect we might be near the very top: namespaces. I say that because we keep running into odd behavior in any process that has to keep track of them. In particular, anything that listwatches them ends up using a surprising amount of memory and puts real pressure on the apiserver. This has become one of those scaling quirks you only really notice once you hit a certain threshold. As this memory overhead adds up, efficiency decreases: each byte we have to use for management is a byte we can't put towards user services.The problem gets significantly worse when a daemonset needs to listwatch namespaces or network policies (netpols, which we define per namespace). Since daemonsets run a pod on every node, each of those pods independently performs a listwatch on the same resources. As a result, memory usage increases with the number of nodes.Even worse, these listwatch calls can put significant load on the apiserver. If many daemonset pods restart at once, such as during a rollout, they can overwhelm the server with requests and cause real disruption.A few months ago, if you looked at our nodes, the largest memory consumers were often daemonsets. In particular, Calico and Vector which handle configuring networking and log collection respectively.We had already done some work to reduce Calico’s memory usage,  with the project’s maintainers to make it scale more efficiently. That optimization effort was a big win for us, and it gave us useful insight into how memory behaves when namespaces scale up.To support that work, we set up a staging cluster with several hundred thousand namespaces. We knew that per-namespace network policies (netpols) were the scaling factor that stressed Calico, so we reproduced those conditions to validate our changes.While running those tests, we noticed something strange. Vector, another daemonset, also started consuming large amounts of memory.The pattern looked familiar, and we knew we had another problem to dig into. Vector obviously wasn’t looking at netpols but after poking around a bit we found it was listwatching namespaces from every node in order to allow referencing namespace labels per-pod in the .That gave us an idea: what if Vector didn’t need to use namespaces at all? Was that even possible?As it turns out, yes, they were in use in our configuration, but only to check whether a pod belonged to a user namespace.Conveniently, we realized we could hackily describe that condition in another way, and the memory savings were absolutely worth it.At that point, we were feeling a bit too lucky. We reached out to the Vector maintainers to ask whether disabling this behavior would actually work, and whether they would be open to accepting a contribution if we made it happen.From there, all that was left was to try it. The code change was straightforward. We added a new config option and threaded it through the relevant parts of the codebase.After a few hours of flailing at rustc, a Docker image finally built and we were ready to test the theory. The container ran cleanly with no errors in the logs, which seemed promising.But then we hit a snag. Nothing was being emitted. No logs at all. I couldn’t figure out why.Thankfully, our pal Claude came to the rescue: I rebuilt it (which took like 73 hours because Rust), generated a new image, updating staging, and watched nervously. This time, logs were flowing like normal and…The change saved 50 percent of memory. A huge win. We were ready to wrap it up and ship to production.But then Hieu, one of our teammates, asked a very good question.He was right, something didn’t add up.A few hours later, after repeatedly running my head into a wall, I still hadn’t found anything. There was still a full gibibyte of memory unaccounted for. My whole theory about how this worked was starting to fall apart.I even dropped into the channel to see if anyone had Valgrind experience: anybody got a background in valgrind? seems pretty straightforward to get working so far but it won’t end up interfacing with pyroscope. we’ll have to exec in and gdb manually.In a last-ditch effort to profile it again, I finally saw the answer. It had been staring me in the face the whole time.We actually had  kubernetes_logs sources on user nodes. I had only set the flag on one of them. Once I applied it to both, memory usage dropped to the level we had seen in staging before the extra namespaces were added.Around the same time, our colleague Mark happened to be on-call. He did his usual magic — pulled everything together, tested the rollout in staging, and got it shipped to production.I’ll let the results speak for themselves.  Our largest cluster saw a 1 TiB memory drop, with savings across our other clusters adding up to a total of just over 7 TiB.Debugging infrastructure at scale is rarely about one big “aha” moment. It’s often the result of many small questions, small changes, and small wins stacked up until something clicks.In this case, it started with a memory chart that didn’t look quite right, a teammate asking the right question at the right time, and a bit of persistence. When applied to our whole infrastructure, that simple fix freed up , reduced risk during rollouts, and made the system easier to reason about.Huge thanks to Hieu for pushing the investigation forward, Mark for shipping it smoothly, and the Vector maintainers for being responsive and open to the change.If you’re running daemonsets at scale and seeing unexplained memory pressure, it might be worth asking:Do you really need those namespace labels?]]></content:encoded></item></channel></rss>