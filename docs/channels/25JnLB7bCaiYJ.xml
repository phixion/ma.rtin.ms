<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Tech News</title><link>https://konrad.website/liveboat-github-runner/</link><description></description><item><title>UK competition probe of mobile browsers finds Apple-Google duopoly is â€˜anti-innovationâ€™</title><link>https://techcrunch.com/2025/03/12/uk-competition-probe-of-mobile-browsers-finds-apple-google-duopoly-is-anti-innovation/</link><author>Natasha Lomas</author><category>tech</category><pubDate>Wed, 12 Mar 2025 17:59:41 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[A U.K. competition authority investigation of Apple and Googleâ€™s mobile browsers has concluded that the mobile duopolyâ€™s policies are â€œholding back innovationâ€ and could also be limiting economic growth. â€œMobile browsers are apps which provide the primary gateway for consumers to access the web on their mobile devices, and hence for businesses to reach them [â€¦]Â© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Donald Trump Jr. has been boosting MAGA-related startups</title><link>https://techcrunch.com/2025/03/12/donald-trump-jr-has-been-boosting-maga-related-startups/</link><author>Dominic-Madori Davis</author><category>tech</category><pubDate>Wed, 12 Mar 2025 17:37:41 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Since Donald Trump Jr. joined VC firm 1789 Capital after his father won the election, heâ€™s been busy monetizing the Make America Great Again (MAGA) ecosystem. Heâ€™s been making bets in media, pharmaceutical, guns, and crypto while pushing against environmental, social, and governance (ESG) and diversity, equity, and inclusion (DEI), the Financial Times reported. The [â€¦]Â© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>FTC Asks To Delay Amazon Prime Deceptive Practices Case, Citing Staffing Shortfalls</title><link>https://tech.slashdot.org/story/25/03/12/176223/ftc-asks-to-delay-amazon-prime-deceptive-practices-case-citing-staffing-shortfalls?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 12 Mar 2025 17:30:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[The Federal Trade Commission asked a judge in Seattle to delay the start of its trial accusing Amazon of duping consumers into signing up for its Prime program, citing resource constraints. CNBC: Attorneys for the FTC made the request during a status hearing on Wednesday before Judge John Chun in the U.S. District Court for the Western District of Washington. Chun had set a Sept. 22 start date for the trial. Jonathan Cohen, an attorney for the FTC, asked Chun for a two-month continuance on the case due to staffing and budgetary shortfalls. 

The FTC's request to delay due to staffing constraints comes amid a push by the Trump administration's Department of Government Efficiency to reduce spending. DOGE, which is led by tech baron Elon Musk, has slashed the federal government's workforce by more than 62,000 workers in February alone. "We have lost employees in the agency, in our division and on our case team," Cohen said.]]></content:encoded></item><item><title>OpenStack comes to the Linux Foundation</title><link>https://techcrunch.com/2025/03/12/openstack-comes-to-the-linux-foundation/</link><author>Frederic Lardinois</author><category>tech</category><pubDate>Wed, 12 Mar 2025 17:13:09 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Back in 2010, Rackspace and NASA launched a project called OpenStack, which was meant to become an open-source option for running an AWS-style cloud inside of private data centers. The two companies then moved OpenStack to the OpenStack Foundation, which has steadfastly shepherded the project through its many ups and downs. Right now, with the [â€¦]Â© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>US Schools Deploy AI Surveillance Amid Security Lapses, Privacy Concerns</title><link>https://news.slashdot.org/story/25/03/12/1654217/us-schools-deploy-ai-surveillance-amid-security-lapses-privacy-concerns?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 12 Mar 2025 16:54:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Schools across the United States are increasingly using artificial intelligence to monitor students' online activities, raising significant privacy concerns after Vancouver Public Schools inadvertently released nearly 3,500 unredacted, sensitive student documents to reporters. 

The surveillance software, developed by companies like Gaggle Safety Management, scans school-issued devices 24/7 for signs of bullying, self-harm, or violence, alerting staff when potential issues are detected. Approximately 1,500 school districts nationwide use Gaggle's technology to track six million students, with Vancouver schools paying $328,036 for three years of service. 

While school officials maintain the technology has helped counselors intervene with at-risk students, documents revealed LGBTQ+ students were potentially outed to administrators through the monitoring.]]></content:encoded></item><item><title>Daprâ€™s microservices runtime now supports AI agents</title><link>https://techcrunch.com/2025/03/12/daprs-microservices-runtime-now-supports-ai-agents/</link><author>Frederic Lardinois</author><category>tech</category><pubDate>Wed, 12 Mar 2025 16:26:29 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Back in 2019, Microsoft open-sourced Dapr, a new runtime for making building distributed microservice-based applications easier. At the time, nobody was talking about AI agents yet, but as it turns out, Dapr had some of the fundamental building blocks for supporting AI agents built-in from the outset. Thatâ€™s because one of Daprâ€™s core features is [â€¦]Â© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>How I Vibe Coded the Pixel Icon Library Website Without Learning to Code (Thanks, Cursor AI!)</title><link>https://hackernoon.com/how-i-vibe-coded-the-pixel-icon-library-website-without-learning-to-code-thanks-cursor-ai?source=rss</link><author>Devansh Chaudhary</author><category>tech</category><pubDate>Wed, 12 Mar 2025 16:00:03 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[As a designer, I've always been fascinated by the intersection of design and tech. The gap between designer and developer has always intrigued meâ€”I just never expected to cross it this soon.The Pixel Icon Library by HackerNoon started as a fun design project to create pixelated icons that captured the nostalgic essence of HackerNoonâ€™s design languageâ€”but transforming a Figma Design file into a fully functional website myself? That was not on my card for 2025 until I discovered Cursor AI. Having the library open-sourced via GitHub, xFigma & NPM, we always wanted to make these icons more accessible to the community. The idea was simple: letâ€™s build a website where designers and developers could browse, search, and download these icons for their projects.\
The challenge? I had literally zero experience with coding.ðŸ’€\
Enter Cursor AI, an AI-powered code editor that quickly became my mentor and guide throughout this journey. Here's how I pushed past my limits from design to full-stack with AI assistance and some serious vibe coding.Starting from Ground ZeroThe first step was to establish what I wanted to build and take stock of what I already had:A library of pixel icons in SVG format.A Figma design for the website.A list of features I wanted on the website.Absolutely no idea how to bring it to life.Setting Up the Project with Cursor AI\
After installing Cursor, the next thing was to check for: - for the NPM package\
With the essentials installed, it was time to get my hands dirty. I cloned the repo to my system, opened the project folder in Cursor AI, and initiated a conversation with the built-in AI assistant.Having had a solid experience with Claude, I opted for Claude 3.7 Sonnet in Agent Mode to guide me through the process.\
With the first prompt, I briefly explained what I wanted to build and listed the required features along with a basic structure of the project, asking for suggestions on which framework(s) to prioritize efficiency and speed. Cursor helped with the necessary folder structure and suggested I stick to HTML & Tailwind for the project. It even created the files required for starting the project. Hereâ€™s how the folder structure looked after this:The next step was setting up Tailwind & starting with the UI.Setting Up Tailwind CSS & Building The UITo my surprise, Cursor messed up the Tailwind installation and mixed up commands from Tailwind v3.4 & v4.0. So, it was time for me to step up! I went over to Tailwindcss Installation Docs and followed these steps:To install Tailwind, open the terminal & run:npm install tailwindcss @tailwindcss/cliImport Tailwind in src/style.css file:To set up the Build Process, run:npx @tailwindcss/cli -i ./src/style.css -o ./src/output.css --watchStart using Tailwind in the HTML:<link href="/src/output.css" rel="stylesheet">\
Once Tailwind was installed and the classes were ready to be used, I started defining colors, fonts, and other atoms for the AI agent before building out more complex molecules, organisms, and pages as per my Figma Design.\
Then, I installed the Pixel Icon Library NPM Package to use the icons in the project. Here are the steps for installing the NPM Package:npm i @hackernoon/pixel-icon-libraryImport the CSS in your HTML(I moved all the necessary icon font files to the /fonts folder for ease of access.)<link href="/fonts/iconfont.css" rel="stylesheet"><i class="hn hn-icon-name"></i>\
For the UI, I took an element-by-element approach to keep things organized and make it easy to revert to the previous iteration if needed.\
I made sure to define paddings, margins, border radius, colors, and dimensions for all these elements while adding hover & click states as well. In addition to CSS properties, I included responsive behavior & interactions in the prompts as well.:::tip
To generate a preview for all the code I was approving, I used the Live Server extension. This extension launches a local development server with a live reload feature for static & dynamic pages in one click!Icon Data Challenge & Implementing Search FunctionalityWith the UI Elements in place, it was time for the real challenge: displaying all icons with their details - Icon Name, Icon Type Tag, and SVG Code while ensuring smooth search functionality. To overcome this, Cursor suggested a structured approach:Creating a JSON file with icon metadata and SVG codeLoading the data from the JSON to display it efficiently.Implement search based on the icon name.Add search filters based on the icon type tags - solid, regular, brands/social-media-icons, purrcats\
To further automate this process, I asked Cursor to create a script to add all the icon data to the /data/icons.json file.\
Now, with the search working, search filters in place, and icon modal working as intended, all that was left to do was add remaining interactions to the UI, thorough testing, and deployment!Once I was happy with the UI and thoroughly tested every functionality, it was time for the next challenge - Deployment!!!\
Since the project was a static site, I needed a hosting solution that was fast, free, and easy to maintain. GitHub Pages was a no-brainer! It offered:Seamless integration with the GitHub repo, making deployment effortless.Itâ€™s free to use & best suited for static projects like this.Updates are super easy - all you need is a commit!\
But before I could push all my code to the repository and deploy via GitHub Pages, I needed to clean up the code and check for any production issues. After a quick back and forth with Cursor, and a few tweaks, it was all ready for launch!\
Hereâ€™s all you need to do to deploy your project via GitHub Pages:Push all your code to the GitHub repo & make sure itâ€™s publicEnabling GitHub pages for the repoSet the  to â€œâ€ (The branch where your code is. In my case, it was in website branch)Add your custom domain (Like I used: pixeliconlibrary.com)Configure DNS (Thanks to Richard for helping me out with this)Wait for a few minutes & your website will be LIVE!From designing pixel-art icons to vibe-coding my way into a fully functional site, this project pushed me beyond my comfort zone in the best way possible. Looking back, it was more than just building a website - it was about widening my creative horizons & realizing that with AI, the line between design and development is blurring faster than ever. And to me, it's like an endless road has opened up in front of my eyes.:::tip
One designer to another: If I can do it, so can you. So, what are you waiting for? Letâ€™s get building!Wanna take a peek at the code behind the website? Check out ]]></content:encoded></item><item><title>Sakana claims its AI-generated paper passed peer review â€” but itâ€™s a bit more nuanced than that</title><link>https://techcrunch.com/2025/03/12/sakana-claims-its-ai-paper-passed-peer-review-but-its-a-bit-more-nuanced-than-that/</link><author>Kyle Wiggers</author><category>tech</category><pubDate>Wed, 12 Mar 2025 16:00:02 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Japanese AI startup Sakana said that its AI generated one of the first peer-reviewed scientific publications. But while the claim isnâ€™t necessarily untrue, there are caveats to note. The debate swirling around AI and its role in the scientific process grows fiercer by the day. Many researchers donâ€™t think AI is quite ready to serve [â€¦]Â© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Snap introduces AI Video Lenses powered by its in-house generative model</title><link>https://techcrunch.com/2025/03/12/snap-introduces-ai-video-lenses-powered-by-its-in-house-generative-model/</link><author>Aisha Malik</author><category>tech</category><pubDate>Wed, 12 Mar 2025 16:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Snapchat is introducing its first ever video generative AI Lenses, the company told TechCrunch exclusively. The Lenses are powered by Snapâ€™s in-house built generative video model. The three new AI Video LensesÂ are available to users on the appâ€™s premium subscription tier, Snapchat Platinum, which costs $15.99 per month. The launch comes as Snap unveiled an [â€¦]Â© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Moonvalley releases a video generator it claims was trained on licensed content</title><link>https://techcrunch.com/2025/03/12/moonvalley-releases-a-video-generator-it-claims-was-trained-on-licensed-content/</link><author>Kyle Wiggers</author><category>tech</category><pubDate>Wed, 12 Mar 2025 16:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Los Angeles-based startup Moonvalley has launched an AI video-generating model it claims is one of the few trained on openly licensed â€” not copyrighted â€” data. Named â€œMareyâ€ after cinema trailblazer Ã‰tienne-Jules Marey, the model was built in collaboration with Asteria, a newer AI animation studio. Marey was trained on â€œowned or fully licensedâ€ source [â€¦]Â© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Garantex administrator arrested in India under extradition law</title><link>https://techcrunch.com/2025/03/12/garantex-administrator-arrested-in-india-under-extradition-law/</link><author>Jagmeet Singh, Lorenzo Franceschi-Bicchierai</author><category>tech</category><pubDate>Wed, 12 Mar 2025 15:55:29 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Garantex co-founder Aleksej Besciokov was arrested in India's Kerala on Tuesday under the country's extradition law.Â© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Ubuntu 25.10 Looks To Make Use Of Rust Coreutils &amp; Other Rust System Components</title><link>https://www.phoronix.com/news/Ubuntu-25.10-Rust-Coreutils</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 12 Mar 2025 15:54:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Plans have been drafted to begin using more Rust-rewritten Linux system components within the Ubuntu 25.10 release due out later this year and ahead of next year's all important Ubuntu 26.04 LTS release. Among the Rust components being planned for use in Ubuntu 25.10 is the Rust Coreutils "uutils" software...]]></content:encoded></item><item><title>AMD&apos;s 3D V-Cache Optimizer Driver For Squeezing More Ryzen 9 9950X3D Performance</title><link>https://www.phoronix.com/review/amd-3d-vcache-optimizer-9950x3d</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 12 Mar 2025 15:30:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Merged for the Linux 6.13 kernel was the AMD 3D V-Cache Optimizer driver for being able to influence the kernel's scheduling decisions on AMD processors where only a subset of CCDs have the larger 3D V-Cache. With this new driver users can communicate their cache vs. frequency preference for influencing where new tasks are first placed if on the CCD with the larger L3 cache or with the higher frequency potential. Here is a look at the impact of using the AMD 3D V-Cache Optimizer driver with the new AMD Ryzen 9 9950X3D.]]></content:encoded></item><item><title>Roomba-maker iRobot Warns of Possible Shutdown Within 12 Months</title><link>https://hardware.slashdot.org/story/25/03/12/1437241/roomba-maker-irobot-warns-of-possible-shutdown-within-12-months?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 12 Mar 2025 15:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Roomba maker iRobot has warned it may cease operations within 12 months unless it can refinance debt or find a buyer, just one day after launching a new vacuum cleaner line. In its March 12 quarterly report, the company disclosed it had spent $3.6 million to amend terms on a $200 million Carlyle Group loan from 2023, as U.S. revenue plunged 47% in the fourth quarter. 

"Given these uncertainties and the implication they may have on the Company's financials, there is substantial doubt about the Company's ability to continue as a going concern for a period of at least 12 months from the date of the issuance of its consolidated 2024 financial statements," the company wrote. 

The robot vacuum pioneer has initiated a formal strategic review after a failed Amazon acquisition, the departure of founder Colin Angle, and layoffs affecting over half its workforce. iRobot cited mounting competition from Chinese manufacturers and expects continued losses for "the foreseeable future."]]></content:encoded></item><item><title>With Gemini Robotics, Google Aims for Smarter Robots</title><link>https://spectrum.ieee.org/gemini-robotics</link><author>Eliza Strickland</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NzAyNTM1Ni9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc3MDc5MzU0NH0.qUSCBPf2eO8a4aagmxU9QqBUARy6BdaMRq9xw3qPOQg/image.jpg?width=600" length="" type=""/><pubDate>Wed, 12 Mar 2025 15:02:24 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[DeepMind launches two new foundation models to help robots reason ]]></content:encoded></item><item><title>Why Onyx thinks its open source solution will win enterprise search</title><link>https://techcrunch.com/2025/03/12/why-onyx-thinks-its-open-source-solution-will-win-enterprise-search/</link><author>Rebecca Szkutak</author><category>tech</category><pubDate>Wed, 12 Mar 2025 15:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Enterprises have troves of internal data and information that employees need to complete their tasks or answer questions for potential customers. But that doesnâ€™t mean the right information is easy to find. Onyx wants to solve that problem through its internal enterprise search tool. There are other big names in the category, like Glean â€” [â€¦]Â© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Morgan Stanley Cuts iPhone Shipment Forecast on Siri Upgrade Delay, China Tariffs</title><link>https://apple.slashdot.org/story/25/03/12/1416206/morgan-stanley-cuts-iphone-shipment-forecast-on-siri-upgrade-delay-china-tariffs?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 12 Mar 2025 14:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Morgan Stanley has reduced its iPhone shipment forecasts after Apple confirmed the delay of a more advanced Siri personal assistant, dampening prospects for accelerating phone upgrades. The investment bank now predicts 230 million iPhone shipments in 2025 (flat year-over-year) and 243 million in 2026 (up 6%), down from previous estimates. 

An upgraded Siri was the most sought-after Apple Intelligence feature among prospective buyers, according to the bank's survey data. "Access to Advanced AI Features" appeared as a top-five driver of smartphone upgrades for the first time, with about 50% of iPhone owners who didn't upgrade to iPhone 16 citing the delayed Apple Intelligence rollout as affecting their decision. The firm also incorporated headwinds from China tariffs in its assessment, noting Apple is unlikely to fully offset these costs without broader exemptions.]]></content:encoded></item><item><title>Lithium-ion batteries are remaking Googleâ€™s data centers</title><link>https://techcrunch.com/2025/03/12/lithium-ion-batteries-are-remaking-googles-data-centers/</link><author>Tim De Chant</author><category>tech</category><pubDate>Wed, 12 Mar 2025 14:39:55 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Google is replacing lead-acid battery backup units with lithium-ion cells, freeing up space for more servers.Â© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Project Europe, a new early-stage fund, will back people under 25 to build the next tech titan</title><link>https://techcrunch.com/2025/03/12/project-europe-a-new-early-stage-fund-will-back-under-25s-with-200k-to-build-the-next-tech-titan/</link><author>Ingrid Lunden</author><category>tech</category><pubDate>Wed, 12 Mar 2025 14:36:18 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[A recurring theme in Europeâ€™s tech world is that the region needs its own Google or Microsoft. Now comes the launch of a new fund to support this initiative. Project Europe â€” a new fund for founders â€œsolving hard problems with technical solutionsâ€ â€” says it has initially pulled together $10 million from 128 different [â€¦]Â© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Battery manufacturer Northvolt nears the end as it files for bankruptcy in Sweden</title><link>https://techcrunch.com/2025/03/12/battery-manufacturer-northvolt-nears-the-end-as-it-files-for-bankruptcy-in-sweden/</link><author>Tim De Chant</author><category>tech</category><pubDate>Wed, 12 Mar 2025 14:34:07 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Though the Swedish startup has raised over $14 billion, it has been running short on cash recently.Â© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>OpenSSL 3.5 Alpha 1 Released With Server-Side QUIC</title><link>https://www.phoronix.com/news/OpenSSL-3.5-Alpha-1</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 12 Mar 2025 14:04:26 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[OpenSSL 3.5 Alpha 1 is out today as the first development milestone on the path to releasing OpenSSL 3.5.0 in April...]]></content:encoded></item><item><title>How La Fourche, an online organic supermarket, is thriving after q-commerceâ€™s bust</title><link>https://techcrunch.com/2025/03/12/how-la-fourche-an-online-organic-supermarket-is-thriving-after-q-commerces-bust/</link><author>Romain Dillet</author><category>tech</category><pubDate>Wed, 12 Mar 2025 14:01:23 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[La Fourche is just seven years old but it has been quite a rollercoaster for the French startup. During this time, the online grocery retailer has gone through a global pandemic, followed by the rise of venture-backed quick-commerce startups that promised grocery deliveries in less than 15 minutes, followed by the implosion of that vertical. [â€¦]Â© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Amazon, Google and Meta Support Tripling Nuclear Power By 2050</title><link>https://hardware.slashdot.org/story/25/03/12/1350256/amazon-google-and-meta-support-tripling-nuclear-power-by-2050?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Wed, 12 Mar 2025 14:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Amazon, Alphabet's Google and Meta Platforms on Wednesday said they support efforts to at least triple nuclear energy worldwide by 2050. From a report: The tech companies signed a pledge first adopted in December 2023 by more than 20 countries, including the U.S., at the U.N. Climate Change Conference. Financial institutions including Bank of America, Goldman Sachs and Morgan Stanley backed the pledge last year. 

The pledge is nonbinding, but highlights the growing support for expanding nuclear power among leading industries, finance and governments. Amazon, Google and Meta are increasingly important drivers of energy demand in the U.S. as they build out AI centers. The tech sector is turning to nuclear power after concluding that renewables alone won't provide enough reliable power for their energy needs. Microsoft and Apple did not sign the statement.]]></content:encoded></item><item><title>AMD Ryzen 9 9900X3D Linux Benchmarks Forthcoming</title><link>https://www.phoronix.com/news/AMD-Ryzen-9-9900X3D-Linux</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 12 Mar 2025 13:50:28 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Today marks the retail availability of the AMD Ryzen 9 9900X3D and Ryzen 9 9950X3D processors. At the top of the hour when the new AMD Zen 5 3D V-Cache processors went on sale, I found both the 9900X3D and 9950X3D in-stock and at MSRP pricing... Less than a half hour later, the 9950X3D is now out of stock while as of writing the 9900X3D remains in-stock at major Internet retailers at its $599 price point...]]></content:encoded></item><item><title>Pentera nabs $60M at a $1B+ valuation to build simulated network attacks to train security teams</title><link>https://techcrunch.com/2025/03/12/pentera-nabs-60m-at-a-1b-valuation-to-build-simulated-network-attacks-to-train-security-teams/</link><author>Ingrid Lunden</author><category>tech</category><pubDate>Wed, 12 Mar 2025 13:00:23 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Strong and smart security operations teams are at the heart of any cybersecurity strategy, and today a startup that builds tooling to help keep them on their toes is announcing some funding on the back of a lot of growth. Pentera â€” which has built a system that launches simulations of network attacks to stress [â€¦]Â© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>New Brain Tech Gives Voice to ALS Patients</title><link>https://spectrum.ieee.org/als</link><author>Greg Uyeno</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NjcwNzkxNS9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc4NTM4MjYzN30.gmAWwtt2dlg0mZD6jMC0v8N2_D0y_2xo_v7IjK1wTwY/image.jpg?width=600" length="" type=""/><pubDate>Wed, 12 Mar 2025 13:00:02 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Cognixionâ€™s headset offers a communication tool for people with locked-in syndrome]]></content:encoded></item><item><title>Allstate Insurance Sued For Delivering Personal Info In Plaintext</title><link>https://yro.slashdot.org/story/25/03/11/225252/allstate-insurance-sued-for-delivering-personal-info-in-plaintext?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 12 Mar 2025 13:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from The Register: New York State has sued Allstate Insurance for operating websites so badly designed they would deliver personal information in plain-text to anyone that went looking for it. The data was lifted from Allstate's National General business unit, which ran a website for consumers who wanted to get a quote for a policy. That task required users to input a name and address, and once that info was entered, the site searched a LexisNexis Risk Solutions database for data on anyone who lived at the address provided. The results of that search would then appear on a screen that included the driver's license number (DLN) for the given name and address, plus "names of any other drivers identified as potentially living at that consumer's address, and the entire DLNs of those other drivers."
 
Naturally, miscreants used the system to mine for people's personal information for fraud. "National General intentionally built these tools to automatically populate consumers' entire DLNs in plain text -- in other words, fully exposed on the face of the quoting websites -- during the quoting process," the court documents [PDF] state. "Not surprisingly, attackers identified this vulnerability and targeted these quoting tools as an easy way to access the DLNs of many New Yorkers," according to the lawsuit. The digital thieves then used this information to "submit fraudulent claims for pandemic and unemployment benefits," we're told. ... [B]y the time the insurer resolved the mess, crooks had built bots that harvested at least 12,000 individuals' driver's license numbers from the quote-generating site.]]></content:encoded></item><item><title>Wolf Games, backed by â€˜Law &amp; Orderâ€™ creator, uses AI to create murder mystery games</title><link>https://techcrunch.com/2025/03/12/wolf-games-backed-by-law-order-creator-uses-ai-to-create-murder-mystery-games/</link><author>Lauren Forristal</author><category>tech</category><pubDate>Wed, 12 Mar 2025 13:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Elliot Wolf, the executive producer and son of â€œLaw & Orderâ€ creator Dick Wolf, is entering a new venture aimed at engaging true crime fans.Â  He, along with co-founders Andrew Adashek (CEO) and Noah Rosenberg (CTO), are developing Wolf Games, a new startup that leverages AI to generate daily murder mystery games. The company also [â€¦]Â© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Linux 6.15 Set To Include Better Handling For Intel P Or E Core Only Mitigations</title><link>https://www.phoronix.com/news/Linux-6.15-P-E-Core-Mitigation</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 12 Mar 2025 12:46:52 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[A set of patches from Intel for utilizing the CPU type for CPU matching as part of the x86 mitigation handling is likely to be part of the upcoming Linux 6.15 kernel. These patches are intended for helping with CPU security mitigations on Intel Core hybrid processors where there are security vulnerabilities affecting only P cores or only E cores but not both sets of CPU cores present in the system...]]></content:encoded></item><item><title>Meta faces publisher copyright AI lawsuit in France</title><link>https://techcrunch.com/2025/03/12/meta-faces-publisher-copyright-ai-lawsuit-in-france/</link><author>Natasha Lomas</author><category>tech</category><pubDate>Wed, 12 Mar 2025 12:11:01 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Meta is facing an AI copyright lawsuit in France thatâ€™s been brought by authors and publishers who are accusing it of economic â€œparasitism,â€ Reuters reports. The French litigation was filed in a Paris court this week by the National Publishing Union (SNE), the National Union of Authors and Composers (SNAC), and the Society of People [â€¦]Â© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Ditto lands $82M to synchronize data from the edge to the cloud</title><link>https://techcrunch.com/2025/03/12/ditto-lands-82m-to-synchronize-data-from-the-edge-to-the-cloud/</link><author>Paul Sawers</author><category>tech</category><pubDate>Wed, 12 Mar 2025 12:02:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Ditto, a company thatâ€™s setting out to bring â€œresilientâ€ connectivity to edge devices, has raised $82 million in a Series B round of funding at a post-money valuation of $462 million, more than double its Series A valuation from 2023. â€œEdge,â€ in the context of Dittoâ€™s industry, refers to a distributed computing model that brings [â€¦]Â© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Mercedes-Benz Drives Toward Solid-State EV Batteries</title><link>https://spectrum.ieee.org/mercedes-benz</link><author>Lawrence Ulrich</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81Njc2OTUwOS9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc4ODU2NTg0NX0.kWQaB9dF0zGesDWKBhA70-XztMQk1cdzlWVpWCNBs7E/image.jpg?width=600" length="" type=""/><pubDate>Wed, 12 Mar 2025 12:00:03 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Factorial Energy's semi-solid-state cells could be in EVs in a few years]]></content:encoded></item><item><title>PokÃ©mon GO maker Niantic is selling its games division to Scopely for $3.5B</title><link>https://techcrunch.com/2025/03/12/pokemon-go-maker-niantic-is-selling-its-games-division-to-scopely-for-3-5b/</link><author>Ivan Mehta</author><category>tech</category><pubDate>Wed, 12 Mar 2025 12:00:00 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Mobile gaming giant Scopely on Wednesday said it has agreed to acquire PokÃ©mon Go maker Niantic's gaming division for $3.5 billion. Â© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>10 Questions for Every Startup Founder: Brex on Rewriting the Rules of Business Finance</title><link>https://hackernoon.com/10-questions-for-every-startup-founder-brex-on-rewriting-the-rules-of-business-finance?source=rss</link><author></author><category>tech</category><pubDate>Wed, 12 Mar 2025 11:44:53 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[:::tip
Youâ€™re reading an interview fromÂ â€™s Startup Founder Interview Series! Would you like to take a stab at answering some of these questions? The modern spend platform.We originally got our start when we saw no one in our YC cohort could get a corporate card. We realized that the payments industry needed to be shaken up - so we pivoted from our original idea and launched Brex to help founders scale in 2017. \n From there, we realized just how much opportunity existed when it came to how companies manage their spend.Â Existing tools for planning, spending, and tracking company dollars were antiquated and disconnected, leading to tedious processes for founders and finance teams.\
A fundamental shift was required. That inspired our effort to go beyond just a corporate card and to build an entire platform from the ground up thatÂ unified financial services and software, allowing money movement and spend controls to coexist.\
This need to optimize spend is even more relevant in todayâ€™s macroeconomic environment, where founders and financeÂ teams are being asked to maximize the ROI of every dollar. We help ensure that theyâ€™re equipped to spend smarter and accordingly grow faster.3. What do you love about your team, and why are you the ones to solve this problem?Weâ€™ve got a team of builders who are willing to operate with the utmost intensity to build a generational company that exceeds the expectations of our customers. We talk often at Brex about breaking trade-offs between speed and quality. We need to move fast to bring the future we imagine forward, but we never cut corners in that journey. I am insanely proud of how the team approaches this every day, keeping an eye on the big picture but always obsessing about the details. \n The other thing worth noting is that Brex exists at the intersection of financial services and software, and weâ€™ve got an incredible team of experts from both domains (capital marketsÂ <>Â engineers) charting our path forward. The result is that weâ€™ve got the most advanced global capabilities and payment rails in the market, and weâ€™re consistently raising the bar in everyÂ product releaseÂ to make our offering easy to use for smaller companies and customizable at scale for our enterprise customers.4. If you werenâ€™t building your startup, what would you be doing?I truly canâ€™t imagine not building Brex - it is so core to who I am.5. At the moment, how do you measure success? What are your metrics?We look at revenue growth driven by both new customer acquisition and net revenue retention. We also set targets around burn rate, sales efficiency, and CAC paybacks with the goal of 2025 being the last year that Brex burns money.\
Beyond the hard metrics, it is incredibly important to us that our platform can scale.Â Brex is the only player that can credibly serve companies at every stage of growth, from YC founders starting out to trillion-dollar public companies. We will continue to prioritize this as a core competency - success is serving this wide range of customers for years to come.6. In a few sentences, what do you offer to whom?Brex offers a breadth of products to help businesses spend smarter and make every dollar count including the worldâ€™s smartest corporate card with intuitive expense management, banking, bill pay, accounting automation, travel, and more.7. Whatâ€™s most exciting about your traction to date?Weâ€™ve been able to empower companies to make smarter spending decisions at every stage of growthâ€”from startups, to mid-size companies, all the way up to leading, global enterprises like Anthropic, Arm, Robinhood, ServiceTitan, Wiz, and more. Weâ€™ve seen companies that began with just two people grow into 1,000+ employee enterprises, and Brex has been there with them every step of the way. It's incredibly rewarding to see how Brex has supported them in scaling efficiently and effectively. With Brex, our customers are saving 169,000 hours per month on expense, documentation, and accounting and customers have a 94% employee compliance rate, compared to the industry standard of 70%.\
But it's not just about providing tools; it's about being a true strategic partner and helping them fuel their exponential growth. Thatâ€™s the part that truly excites us.8. Where do you think your growth will be next year?2024 was a year of sharp improvement at Brex. Revenue growth accelerated almost 3x this year, net revenue retention is up more than 15 points, while the burn rate is down almost 70%. In 2025, we will continue to compound the momentum built this year, growing faster than 2024 and being the last year in which we burn money â€” an important milestone to our future IPO. We will also continue to raise the bar on product quality and innovation, while increasing the pace of customer acquisition with healthy unit economics at scale.9. Tell us about your first paying customer and revenue expectations over the next year.Scale AI was Brexâ€™s first customer. I vividly remember Henrique, my co-founder, and I walking to their office and physically handing Alex their Brex card. Itâ€™s a moment Iâ€™ll never forget.10. Whatâ€™s your biggest threat?Focus. The central tension of building a company is obsessing over outcomes, but at the same time, not caring about them at all. Ironically, the more you let go of the outcomes and focus on putting out great work, the more the outcomes come. Itâ€™s the old adage: the score takes care of itself. All we need to do is to put great work in front of customers, and the score will take care of itself.]]></content:encoded></item><item><title>Scimplify raises $40M to help manufacturers access specialty chemicals</title><link>https://techcrunch.com/2025/03/12/scimplify-raises-40m-to-help-manufacturers-access-specialty-chemicals/</link><author>Jagmeet Singh</author><category>tech</category><pubDate>Wed, 12 Mar 2025 11:00:34 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Scimplify has raised $40 million in a new equity round backed by Accel to expand its presence in the U.S. and enter new markets.Â© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Can a New Power Link Boost the EUâ€™s Energy Independence?</title><link>https://spectrum.ieee.org/black-sea-energy-link</link><author>Amos Zeeberg</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NjY2NzY1My9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc5MDMyNTE5NH0.hL7VXlZjYYVXlowC14gTHx1atEgcdIKzlIAmDPaCrgk/image.jpg?width=600" length="" type=""/><pubDate>Wed, 12 Mar 2025 11:00:02 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Undersea cable would supply clean electricity from the Caucasus]]></content:encoded></item><item><title>KDE&apos;s KWin Wayland &amp; X11 Code Are Now Split, KWin_X11 To Be Maintained Until Plasma 7</title><link>https://www.phoronix.com/news/KWin-Wayland-X11-Split</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 12 Mar 2025 10:53:49 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Yesterday marked the milestone of KWin's kwin_x11 and kwin_wayland code being split up. The Wayland and X11 code for the KWin compositor is now separate from each other but can be co-installable for systems wanting to support both X11 and Wayland environments...]]></content:encoded></item><item><title>Users Hate Search Filtersâ€”Maybe Let AI Handle It Instead</title><link>https://hackernoon.com/users-hate-search-filtersmaybe-let-ai-handle-it-instead?source=rss</link><author>Oleksandr Rudin</author><category>tech</category><pubDate>Wed, 12 Mar 2025 10:48:35 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Usually, when you want to find a product on the internet, you face complex forms with many filters. Letâ€™s take the example of Zillow, the real-estate marketplace website. This is how their interface looks:It seems pretty simple, which is great. Things become a little complicated when we click on â€œmoreâ€œ and see extended form\
 This still doesnâ€™t look complicated. In my opinion, Zillow has a great UI/UX design.\
Many users find filling out multiple filters frustrating and time-consuming. My idea for this article is to provide an option to the user to have it much simpler using AI. Instead of all these fields, we can have only one, where the user can provide a free text, and all those parameters can be extracted from this field. The design for this can look like this:\
This is a good alternative for the complex forms. Yes, the user still needs to write all the details to provide the best match, but it will be much faster than selecting all the fields in the forms.Implementing an AI algorithm can be a challenge and can cost some time/money. But what if we keep the old structure of existing search functionality and just parse the free text into filter fields?\
I got this challenge for one of my clients, and I  am providing you with an adapted version of using OpenAI to do that job:openai.chat.completions.create({
  model: "gpt-4-turbo",
  messages: [
    {
      role: "system",
      content: `You are a helpful assistant that parses real-estate related text into a strict JSON format. JSON format should be just a simple string without any addition symbols, for example { "property_type": ["house"] }`
    },
    {
      role: "user",
      content: `Parse the following real-estate related text into strict JSON format with the following structure:
        {
          "location": "Extract the mentioned city, neighborhood, or region. Set empty string if none found.",
          "price_range": "Extract the mentioned price range or budget. Return as an object {min: number, max: number}. Set null if none found.",
          "property_type": "Extract the type of property (e.g., apartment, house, studio, condo). Set empty array if none found.",
          "bedrooms": "Extract the number of bedrooms. Set null if none found.",
          "bathrooms": "Extract the number of bathrooms. Set null if none found.",
          "amenities": "Extract key amenities mentioned (e.g., parking, balcony, pool, pet-friendly). Set empty array if none found.",
          "keywords": "Extract any additional keywords relevant to the real estate context. Set empty array if none found."
        }
        Text: "${userFreeTextInput}"`
    }
  ],
  max_tokens: 500
});
\
In this example, I am using the  gpt-4-turbo model, which will return strict JSON that I can use as filters. For an MVP, using OpenAI's API is ideal. However, for a production setting with high traffic, a fine-tuned or self-hosted model may be more cost-effective in the long run. The result of the AI request can be configured to have the same as the user can select in traditional filters UI (first image). We can also improve that by adding exceptions, for example, when the user types something like thisLooking for a 2-bedroom apartment in Texas under $2,500/month, pet-friendly, with a balcony and parking.
I am not considering Austin or Dallas.
\
To exclude specific locations, we modify the JSON structure to include an  key. The AI should extract places users donâ€™t want and store them in an array.{
  "except_location": "Extract the cities, neighborhoods, or regions the user wants to exclude. Set empty array if none found.",
}
\
And here we go with obvious benefits by using AI. The user can even ask to exclude all big cities from the list, and the AI can find all big cities in Texas and does the job.\
Instead of traditional forms, we can give users the  possibility to use a chat, the next level of communication with services. Chat can ask additional questions in case some required information for searching is not found in the free text.\
This is similar to real communication with the agent when users come to the office of the real-estate agency. Having more natural interaction improves user experience.\
Since user preferences are different, I recommend introducing this as a beta option and allowing users to choose their preferred method. In our case, we will run an A/B test comparing AI-powered search against traditional filters. Metrics like time-to-search, conversion rate, and user satisfaction scores will determine which method performs better. In my opinion, in the future, as more people will use AI Chats, they will become more familiar with those types of interfaces and will prefer this option.\
The main benefit of using this approach is that we can simply add voice recognition to the input, and this will look very similar to talking to a real agent. Search results can be easily extended by adding more details to the filters.\
By collecting anonymized user queries, we can analyze patterns and fine-tune a model to provide better recommendations over time.\
I hope this article will help your business to provide your customers with a modern idea of communication.]]></content:encoded></item><item><title>Haiku OS Wrapping Up Its New malloc &amp; Various Performance Optimizations</title><link>https://www.phoronix.com/news/Haiku-OS-New-malloc-More-Perf</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 12 Mar 2025 10:44:13 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The BeOS-inspired Haiku open-source operating system project is out with a new monthly progress report to highlight its latest development accomplishments...]]></content:encoded></item><item><title>LLVM 20&apos;s Great Fortran Language Support With Flang</title><link>https://www.phoronix.com/news/LLVM-20-Flang</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 12 Mar 2025 10:33:39 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[With the newly-released LLVM 20.1 compiler stack among the many changes throughout the massive codebase is renaming the "flang-new" compiler just to "flang". This new Flang compiler front-end has matured quite well over the years to providing robust and reliable Fortran language support within the confines of the LLVM toolchain...]]></content:encoded></item><item><title>Every Truth (And Lie) Told in Netflix&apos;s &apos;Zero Day,&apos; Ranked</title><link>https://hackernoon.com/every-truth-and-lie-told-in-netflixs-zero-day-ranked?source=rss</link><author>Moonlock (by MacPaw)</author><category>tech</category><pubDate>Wed, 12 Mar 2025 10:21:33 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[By Mykhailo Pazyniuk, Malware Research Engineer at Moonlock, the cybersecurity division of MacPaw.\
Netflixâ€™s â€˜Zero Dayâ€™ places viewers in the midst of a massive cyberattack that cripples the United States. With Robert De Niro starring as a former U.S. President investigating the attack, the series explores themes of political intrigue, digital warfare, and the fragility of modern infrastructure. But just how realistic is â€˜Zero Dayâ€™ from a cybersecurity perspective?\
As a malware research engineer, I watched the show with a critical eye. While some aspects of the cyberattack feel eerily plausible, others stray into sci-fi. Hereâ€™s my breakdown of the â€˜Zero Dayâ€™ where I'll cover three cyber threats that could realistically happen, and three that are pure fiction (at least for now).Three threats that could happenThe series shows several combined attack vectors along with consequences that follow. The main methods include a full-scale attack on critical infrastructure using weaponized malware, as well as a supply chain attack that spreads through fake versions of legitimate software.\
To assess the showâ€™s accuracy, letâ€™s compare these scenarios with real-life cyberattacks.Cyberattack on the critical infrastructure (Colonial Pipeline, 2021)Cybercriminals disrupting essential services is one of the most realistic aspects of â€˜Zero Dayâ€™. Targeted attacks on power grids, water supplies, and hospitals are not just possible â€“ they are already happening. The 2021 Colonial Pipeline ransomware attack shut down one of the largest fuel pipelines in the U.S., leading to gas shortages and widespread panic buying.Weaponized malware (Stuxnet, 2010)The show suggests that a cyber attack could be designed to cripple a nationâ€™s security by sabotaging industrial systems. In reality, malware infection of critical systems have influenced global geopolitics for years. A historical precedent is Stuxnet, a highly sophisticated cyber weapon used to damage Iranâ€™s nuclear centrifuges.\
Stuxnet closely resembles the attack shown in the series, especially since it also caused physical damage to infrastructure. This type of malware functions like a worm, crawling through networks, spreading across devices, and causing failures in software or hardware â€” remaining persistent for long periods. We can only imagine the dire consequences if such a worm used AI to adapt to its environment.Supply chain attack (NotPetya, 2017)â€˜Zero Dayâ€™ suggests that an attack could rapidly spread through interconnected systems â€” a scenario that is entirely plausible. Today, a single compromised vendor in a supply chain can infect thousands of organizations. Our team  many similar attack techniques while analyzing fake software bundled with stealer implants, tricking users into believing they were using legitimate programs.\
One of the most devastating cyberattacks in history, NotPetya, spread through a compromised update for widely used software in Ukraine, causing billions of dollars in damages worldwide.Three threats that are far from realityNetflix excels at storytelling, which is why its shows are so captivating. However, hereâ€™s how Zero Day dramatizes hacking for suspense.Instant and simultaneous system collapseIn â€˜Zero Dayâ€™, the cyberattack appears to take down everything at once â€” financial markets, emergency services, transportation. While coordinated attacks are possible, real-world cyberattacks donâ€™t usually spread with such precision. Attacks like NotPetya or SolarWinds took time to propagate, and organizations reacted at different speeds.â€œIf we're talking about a common vulnerability, it would likely be in the baseband or hardware. But with multiple vendors supplying critical infrastructure across the country, this remains unrealistic for now,â€ notes Senior Reverse Engineer at MacPawâ€™s Moonlock (who chose to remain anonymous).Total control with a few keystrokes'Zero Day' relies on a classic Hollywood trope â€” a hacker typing furiously in a dark room, instantly causing systems to crash like dominoes. In reality, cyberattacks take weeks, months, or even years to prepare. Breaching critical infrastructure requires intricate social engineering, vulnerability hunting, lateral movement, and stealth to evade detection. Itâ€™s never as simple as hitting a few keystrokes and watching the world burn.The unstoppable supervirusThe show portrays an unstoppable cyber weapon with no way to mitigate its effects. It's true that advanced malware can be highly persistent, but no cyberattack is truly unpatchable. Even the most destructive malware can be taken down with countermeasures, whether through endpoint protection, network segmentation, or manual intervention. The notion "once itâ€™s launched, itâ€™s game over" is pure fiction.â€œIf the goal is to find vulnerabilities in an infected system, fuzzers are used. They run non-stop on CI servers. But instead of brute-forcing all possible values, they rely on smart mutations. Moreover, not every crash dump leads to an exploitable vulnerability. And to even operate on an infected system in the first place, youâ€™d already need a vulnerability to execute code. So, you've got something like a time loop in â€˜Terminatorâ€™. Therefore, Iâ€™d say these scenarios arenâ€™t plausible with the current state of AI development,â€ adds Senior Reverse Engineer at MacPawâ€™s Moonlock.While 'Zero Day' takes creative liberties, some of its fictional elements could eventually become real. Advancements in AI-driven attacks, deepfake social engineering, and autonomous malware may one day bring us closer to the threats depicted in the show. AI-assisted hacking tools are already reshaping the threat landscape, making cyberattacks faster and more efficient.\
For example, Moonlock Labâ€™s team recently discovered  that uses the OpenAI API (ChatGPT) to run phishing campaigns. While the AI still requires well-structured queries to generate personalized phishing content, it significantly simplifies and accelerates the work of threat actors.\
Moreover, as governments and nation-state actors invest in cyber warfare, the line between fiction and reality continues to blur. AI-driven disinformation campaigns, automated zero-day exploits, and self-spreading malware are no longer far-fetched scenarios. However, at this stage, threat actors primarily use AI for automation and attack preparation â€” not ==â€˜using AI to adapt the code in the process of executionâ€™,== as seen in the series.Fiction as a cautionary taleThe â€˜Zero Dayâ€™ series may exaggerate some elements of cyber warfare, but it effectively highlights an important truth â€” our digital infrastructure is vulnerable. While we may not see a Hollywood-style â€˜doomsday virusâ€™ anytime soon, real-world threats like ransomware, critical infrastructure attacks, and AI-driven cybercrime demand our attention and awareness.\
As malware researchers, security professionals, and even everyday users, we should learn from both real-world incidents and fictional warnings. Cyber threats are evolving, 'Zero Day' just accelerates the timeline. \n ]]></content:encoded></item><item><title>Jonathan Riddell Stepping Down From KDE Plasma Release Management</title><link>https://www.phoronix.com/news/Riddell-Stepping-Down-Plasma-RM</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 12 Mar 2025 10:18:08 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Longtime KDE developer who has served with Plasma release management duties, KDE Neon operating system development, and former Kubuntu release manager, among other roles, announced he will be stepping down from his Plasma release management duties...]]></content:encoded></item><item><title>Salesforce to invest $1B in Singapore to boost adoption of AI</title><link>https://techcrunch.com/2025/03/12/salesforce-to-invest-1b-in-singapore-to-boost-adoption-of-ai/</link><author>Kate Park</author><category>tech</category><pubDate>Wed, 12 Mar 2025 10:12:26 +0000</pubDate><source url="https://techcrunch.com/">TechCrunch</source><content:encoded><![CDATA[Salesforce plans to invest $1 billion in Singapore over the next five years as it seeks to fuel the adoption of its AI agent development platform, Agentforce. Salesforce claimed that Agentforce can help alleviate Singaporeâ€™s ongoing labor issues and augment the countryâ€™s workforce and enterprises by creating â€œdigital workforcesâ€ that combine humans with autonomous AI [â€¦]Â© 2024 TechCrunch. All rights reserved. For personal use only.]]></content:encoded></item><item><title>Solar Adds More New Capacity To the US Grid In 2024 Than Any Energy Source In 20 Years</title><link>https://hardware.slashdot.org/story/25/03/11/2133237/solar-adds-more-new-capacity-to-the-us-grid-in-2024-than-any-energy-source-in-20-years?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 12 Mar 2025 10:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[AmiMoJo shares a report from Electrek: The U.S. installed 50 gigawatts (GW) of new solar capacity in 2024, the largest single year of new capacity added to the grid by any energy technology in over two decades. That's enough to power 8.5 million households. According to the U.S. Solar Market Insight 2024 Year in Review report (PDF) released today by the Solar Energy Industries Association (SEIA) and Wood Mackenzie, solar and storage account for 84% of all new electric generating capacity added to the grid last year.
 
In addition to historic deployment, surging U.S. solar manufacturing emerged as a landmark economic story in 2024. Domestic solar module production tripled last year, and at full capacity, U.S. factories can now produce enough to meet nearly all demand for solar panels in the U.S. Solar cell manufacturing also resumed in 2024, strengthening the U.S. energy supply chain. [...] Total US solar capacity is expected to reach 739 GW by 2035, but the report forecasts include scenarios showing how policy changes could impact the solar market. [...] The low case forecast shows a 130 GW decline in solar deployment over the next decade compared to the base case, representing nearly $250 billion of lost investment.]]></content:encoded></item><item><title>Data Transformation and Discretization: A Comprehensive Guide</title><link>https://hackernoon.com/data-transformation-and-discretization-a-comprehensive-guide?source=rss</link><author>Aleeza Adnan</author><category>tech</category><pubDate>Wed, 12 Mar 2025 09:57:04 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
Data transformation and discretization are critical steps in the data preprocessing pipeline. They prepare raw data for analysis by converting it into forms suitable for mining, improving the efficiency and accuracy of data mining algorithms. This article dives deep into the concepts, techniques, and practical applications of data transformation and discretization.Data transformation involves converting data into appropriate forms for mining. This step is essential because raw data is often noisy, inconsistent, or unsuitable for direct analysis. Common data transformation strategies include:: Remove noise from the data (e.g., using binning or clustering).: Create new attributes from existing ones (e.g., area = height Ã— width).: Summarize data (e.g., daily sales â†’ monthly sales).: Scale data to a smaller range (e.g., 0.0 to 1.0).: Replace numeric values with intervals or conceptual labels (e.g., age â†’ "youth," "adult," "senior").Concept Hierarchy Generation: Generalize data to higher-level concepts (e.g., street â†’ city â†’ country).: Removes noise, inconsistencies, and redundancies.Enhances Mining Efficiency: Reduces data volume and complexity, speeding up algorithms.Facilitates Better Insights: Transforms data into forms that are easier to analyze and interpret.Normalization scales numeric attributes to a specific range, such as [0.0, 1.0] or [-1.0, 1.0]. This is particularly useful for distance-based mining algorithms (e.g., k-nearest neighbors, clustering) to prevent attributes with larger ranges from dominating those with smaller ranges.3.1.1 Min-Max Normalizationv*â€™* : Original value of the attribute.minâ¡A â€‹: Minimum value of attributeÂ .maxâ€‹: Maximum value of attributeÂ .new_minâ€‹: Minimum value of the new range (e.g., 0.0).new_maxâ€‹: Maximum value of the new range (e.g., 1.0).Suppose the attribute "income" has a minimum value of $12,000 and a maximum value of $98,000.We want to normalize an income value of $73,600 to the range [0.0, 1.0].The normalized value is .3.1.2 Z-Score NormalizationSuppose the mean income is $54,000 and the standard deviation is $16,000.We want to normalize an income value of $73,600.The normalized value is .3.1.3 Decimal Scaling Normalizationj : Smallest integer such that ( max(|v'|) < 1 ).Suppose the attribute "price" has values ranging from -986 to 917.The maximum absolute value is 986.The smallest integer ( j ) such that ( 986 / 10^j < 1 ) is  j = 3 .The normalized value is .Discretization replaces numeric values with interval or conceptual labels. This is useful for simplifying data and making patterns easier to understand.Binning divides the range of an attribute into bins (intervals). There are two main types:Divide the range into ( k ) equal-width intervals.Example: For the attribute "age" with values [12, 15, 18, 20, 22, 25, 30, 35, 40], create 3 bins:Divide the range into ( k ) bins, each containing approximately the same number of values.Example: For the same "age" values, create 3 bins:Histograms partition the values of an attribute into disjoint ranges (buckets). The histogram analysis algorithm can be applied recursively to generate a multilevel concept hierarchy.For the attribute "price" with values [1, 1, 5, 5, 5, 5, 8, 8, 10, 10, 10, 10, 12, 14, 14, 15, 15, 15, 15, 15, 18, 18, 18, 18, 18, 18, 20, 20, 20, 20, 20, 20, 20, 21, 21, 21, 25, 25, 25, 25, 28, 28, 30, 30, 30]:Create an equal-width histogram with a bin width of $10:3.2.3 Cluster, Decision Tree, and Correlation AnalysesGroup similar values into clusters and replace raw values with cluster labels.Example: Cluster "age" values into "young," "middle-aged," and "senior."Use decision trees to split numeric attributes into intervals based on class labels.Example: Split "income" into intervals that best predict "credit risk."Use measures like chi-square to merge intervals with similar class distributions.Example: Merge adjacent intervals if they have similar distributions of "purchase behavior."3.3 Concept Hierarchy Generation for Nominal DataConcept hierarchies generalize nominal attributes to higher-level concepts (e.g., street â†’ city â†’ country). They can be generated manually or automatically based on the number of distinct values per attribute.For the attributes "street," "city," "province," and "country":Sort by the number of distinct values:Country (15) â†’ Province (365) â†’ City (3567) â†’ Street (674,339).Country â†’ Province â†’ City â†’ Street.4. Practical Applications: Normalize income and age attributes to cluster customers into segments.: Discretize purchase amounts into intervals to identify patterns.: Use concept hierarchies to generalize transaction locations (e.g., street â†’ city â†’ country).Data transformation and discretization are essential steps in data preprocessing. They improve data quality, enhance mining efficiency, and facilitate better insights. By normalizing, discretizing, and generating concept hierarchies, you can transform raw data into a form that is ready for analysis.]]></content:encoded></item><item><title>AI Wants to Fix Your Network Before It Breaksâ€”But Can You Trust It?</title><link>https://hackernoon.com/ai-wants-to-fix-your-network-before-it-breaksbut-can-you-trust-it?source=rss</link><author>Anna Naumova</author><category>tech</category><pubDate>Wed, 12 Mar 2025 09:52:23 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Not that long ago, the experience in managing networks andâ€‚data centers was one of humans doing the effort, as admins entered commands, configured routers, balanced loads, and hunted down outages. Fast forward to today, and the AI revolution is a game changer â€” automating fast decision-making, predicting failures before vibrations cause them, and optimizing infrastructure in ways that humans never could.\
But how much of this is real? And how much is simply marketing hype?\
Slicing itâ€”and slicing it againâ€”letâ€™s take a look at AI in networks and data centers today, whoâ€™s using it and where it makes a difference, as well as what it needs toâ€‚do better.Network management has traditionally been a reactive job. Something breaks? An engineer fixes it. Latency spikes? Someone troubleshoots it. However, AI-enabled networking is moving us away from the reactiveâ€‚firefighting and toward the proactive self-healing system.Artificial Intelligence Is Redefining Networking.Traffic Optimization Automatically: AI-driven SD-WANs and intent-based networking systems allow bandwidthâ€‚to be dynamically provisioned, traffic routed dynamically, and loads balanced across data centers, in real time, based on current demand. ML algorithms can be trained on historical network data that assist AI in predicting hardware failures (e.g. switches, routers, fiber links) before they happen, minimizing failures and downtime that can be expensive.Anomaly Detection & Security: AI can spot anomalous traffic patterns that might indicate a cyberattack, misconfiguration, or insider threat â€” all common occurrences that traditional rule-based systems tend to miss. Artificial intelligence (AI) â€” based network monitoring tools dynamically prioritize traffic based on real-time business needs, followed by ensuringâ€‚that data-heavy applications can get precedence over other not-so-important traffic.ðŸ¢ AI and the Data Center of the FutureSo, modern data centers are a logistical hellscape â€” youâ€™re managingâ€‚power, cooling, security, storage, compute, and network, all at the same time. AI is coming to optimize everything to efficiency, scale, and autonomy.Data Centers Are Getting Smarter Thanks to AI.Cooling Optimization & Energy Efficiency: AI reads on temperature, load distribution, and airflow to passivelyâ€‚adjust the cooling power accordingly, which reduces power consumption. (One industry example: Google cut its data center cooling costs to 40% by adoptingâ€‚DeepMind AI.)Smart Resource Allocation: This includes workload orchestration with AI, which can intelligently distribute workloads of jobs over multiple servers to decrease resourceâ€‚waste and increase efficiency. Predictably, Artificial Intelligence, on the other hand, makes possible scaling that is at a granular level as opposed to a resource allocation of fixed resources, significantly reducing waste of powerâ€‚and cost.Self-Healing Infrastructure: And AI detects early-stage hardware degradation before faults occur and initiates proactive mitigationâ€‚(e.g., moving workloads from nodes in failure). Hyperscalers are experimenting with replacingâ€‚hardware with AI-driven robotics.Automatic Service Establishment: AI coordinates what are often complex, parallel changes to the network so that if many changes are needed, then less human labor is involved and the changes get deployedâ€‚faster. Business intent-based configuration of BGP peering, VLAN tagging, or cloud interconnects instead ofâ€‚scripts by hand is done via AI for example.ðŸ™ï¸The Significance of Edge AI in Networking and Data Center EnvironmentsAs AI workloads are on the hike, putting everything on the cloud is no moreâ€‚a smart option. Enter edge computing.Why AI is Moving to the Edge. Processing AI workloads closer toâ€‚the user results in timely decisions â€” autonomous vehicles, industrial robotics, smart cities, etc. Not every application requires returning the data to a centralized data center â€” edge AI performsâ€‚this task locally. Applications like security, fraud detection, and machine vision are all AI-powered and must respond in real time, so tasks at the edge are ideal. Teslaâ€™s Autopilot doesnâ€™t stream raw video to a cloud data centerâ€”its on-device edge AI processes itâ€‚in the moment.How Networks Need to Adapt.Provisionâ€‚of high-speed low-latency connections between devices, edge nodes, and central data centers is AI at the Edge. 5G and next-gen networking is going to play a huge roleâ€‚in making AI-enabled edge computing practical.ðŸ›‘ AI is NOT the Solution: Challenges and RisksAnd even while AI is coming toâ€‚revolutionize networking and data centers, itâ€™s not a sure thing. Here are some of the most significantâ€‚roadblocks:AI Still Needs Quality Data. AI models are only as good as the data theyâ€™reâ€‚trained on. So garbage dataâ€‚= garbage predictions. AI-based Network Monitoring has too many false positives, where normal traffic is flagged as an anomaly. AI-powered automation can occasionally introduce an additional level of complexity,â€‚and so that when things go awry, itâ€™s even harder for engineers to diagnose the problem. â€œAI misconfigurationâ€ is a real threat â€” perhaps an AI auto-optimizes a network that somehow addsâ€‚congestion. Attackers are alreadyâ€‚leveraging adversarial attacks to evade security systems powered by AI. A poorly trained AI could even create new vulnerabilities insteadâ€‚of shutting them down.ðŸ”® How AI Will Change Networks and Data CentersWe arenâ€™t there just yet â€” not fully at the AI-driven, self-optimizing network/data center â€” but weâ€‚are moving in that direction. Some trends to watch: Companies likeâ€‚Juniper, Cisco, and Arista are using AI-native network controllers so you donâ€™t even have to adjust anything by hand.AI-Optimized Networking Chips: Custom AI accelerator chips will pinpoint packet routing and conduct deep packet inspection faster than ever before.AI for Carbon-Neutral Data Centers: Look for carbon-aware AIâ€‚schedulingâ€”to move workloads to the server farms with the greenest energy mix in real time.AI is helping networks and data centers to be more clever, speedy, and efficient.\
Self-optimizing fabric networks, AI-powered cooling, and predictive maintenance are just some of the ways the industry is working towardsâ€‚fully autonomous infrastructure.\
But AI is not a cure-all â€” bad data, hard-to-reduce errors, and security risksâ€‚all need human supervision.\
 AI-driven networkingâ€‚architecture, AI-first chipsets, true self-healing.What impact will thereâ€‚be from AI on networking and data centers in the future?\
Leave your comments below!]]></content:encoded></item><item><title>How to Teach a Tiny AI Model Everything a Huge One Knows</title><link>https://hackernoon.com/how-to-teach-a-tiny-ai-model-everything-a-huge-one-knows?source=rss</link><author>Raviteja Reddy Ganta</author><category>tech</category><pubDate>Wed, 12 Mar 2025 09:44:49 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Additional experiment on MNISTIn this article, I will explore the knowledge distillation process in AI â€”how it works in general, its significance, and the reasons for using it.\
How can we compress and transfer knowledge from a bigger model or ensemble of models(which were trained on very large datasets to extract structure from data) to a single small model without much dip in performance?\
But why do we want to do this? Why we need a smaller model when a bigger model or ensemble model is already giving great results on test data?\
At training time we typically train large/ensemble of models because the main goal is to extract structure from very large datasets. We could also be applying many things like dropout, data augmentation at train times to feed these large models all kinds of data.\
But at prediction time our objective is totally different. We want to get results as quickly as possible. So using a bigger/ensemble of models is very expensive and will hinder deployment to large number of users. So, now the question is how can we compress knowledge from this bigger model into a smaller model which can be easily deployed.\
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean from google through theirpaperÂ came up with a different kind of training calledÂ Â to transfer this knowledge to the smaller model. This is the same technique that hugging face used in theirÂ Distill BERTÂ implementation.\
If we can train this smaller model toÂ Â in the same way as a large model, then this smaller model trained this way will do much better than the smaller model trained on the same data but in the normal way. This is one of theÂ Â principles behind DistillationUsually, in Machine learning, a model that learns to discriminate between a large number of classes, the main training objective is to maximize the average log probability of correct answer. For example, take the example of the MNIST dataset where the goal is to classify an image as to whether itâ€™s 1 or 2 or â€¦ 9. So if the actual image is 2 then the objective of any model is to maximizeÂ Â (which can be read as probability that a particular image is 2 given the image). But the model also gives probabilities to all incorrect answers even though those probabilities are very small, some of them are much larger than others. Point is that even though these probabilities are small, relative probabilities of incorrect answers tell us a lot about how the model can generalize. To understand it, letâ€™s have a look at the below example.\
In the above figure, this version of 2 was given a probability of 10-6Â of being a 3 and 10-9Â of being a 7 whereas for another version it may be the other way around. This is valuable information that defines a rich similarity structure over the data(i. e. it says which 2â€™s look like 3â€™s and which look like 7â€™s) but it has very little influence on the cross-entropy cost function during the transfer stage because the probabilities are so close to zero.\
But before we move on to the distillation procedure, letâ€™s spend time on how the model actually produced output probabilities. This is where softmax activation comes in. Last step of model processing is softmax and this component is what gives output probabilities. Input to softmax is called logits and we design the final layer of NN in such a way that number of hidden units = number of classes we want to classify.The formula for calculating softmax is given asThe above equation gives probabilities for eachÂ Â and the sum of all probabilities overallÂ Â equals 1. During training time, loss for any single training example is calculated by comparing these softmax probabilities with hard targets(labels) and using backpropagation coefficients are updated until the loss is minimum.\
As seen above this softmax gives a high probability to a true label and low probabilities to incorrect labels. We also see that probabilities of incorrect answers even though small, have lot of information hidden in them which helps the model to generalize. We call thisÂ 3. Distillation ProcedureAccording to the paper, the best way to transfer generalization capabilities of the larger model to a small model is to use class probabilities produced by the cumbersome model asÂ Â for training the small model.\
So the process is as follows:Take the original training set which was used to train the bigger model then pass that training data through the bigger model and get softmax probabilities over different classes. As seen above, true label will get high probability and incorrect labels will get low probabilities. But we saw these low probabilities have a lot of information hiding in them. So to magnify importance of these probabilities authors of the papers used a variable called Temperature(T) to divide all logits before passing through softmax. This produces a softer probability distribution over classes. We can see below\
The output of applying softmax with temperature(T) is what we call Soft targets. This process is what authors called. Analogy with removing impurities in water by increasing temperatureMuch of information about learned function from large model resides in the ratios of very small probabilities in the soft targets.Â - output from the large model after temperature T has been applied during softmaxÂ - output from the smaller model after temperature T has been applied during softmaxÂ - output from smaller model when temperature T = 1(regular softmax)Â - actual targets from training set\
Below is a flowchart of the entire training process\
So training process for a small model has 2 loss functions. The first loss function takes both soft predictions and soft targets and is the cross-entropy loss function. This is the way generalization ability is transferred from large model to small model by trying to match soft targets. For this loss function, both softmax uses the temperature of â€˜Tâ€™.\
Authors also found that using the small model to match true targets helps. This is incorporated in the second cost function. The final cost is a weighted average of these two cost functions with hyper-parameters alpha and beta.The authors used the MNIST dataset to test this approach. They used two architectures for this which differs only in the number of hidden units in middle layers. The authors used 2 hidden layer neural network in both casesSmaller model which can be viewed as 784 -> 800 -> 800 -> 10 (where 784 is unrolled dimensions of an image, 800 is the number of hidden units with RELU activation and 10 is the number of classes we are predicting). This model gave 146 test errors with no regularization.Bigger model which can be viewed as 784 -> 1200 -> 1200 -> 10 (where 784 is unrolled dimensions of an image, 1200 is the number of hidden units with RELU activation and 10 is the number of classes we are predicting). This model is trained on MNIST using dropout, weight-constraints, and jittering input images and this net achieved 67 test errors.\
Can we transfer this improvement in the bigger model to a small model?\
Authors now used both soft targets obtained from the big net and true targets without dropout and no jittering of images i.e, the smaller net was regularized solely by adding the additional task of matching soft targets produced by the large net at a temperature of 20 and the result is.Â using 784 -> 800 -> 800 -> 10\
This shows that soft targets can transfer a great deal of knowledge to the small model, including the knowledge about how to generalize that is learned from translated training data. In other words, the benefit we got from transforming inputs transfers across to the little net even though we are not transforming inputs for the small net.\
Itâ€™s well-known fact that transforming inputs by different transformations make the model generalize much better and in our case information about how toÂ Â is showing up in Dark knowledge and this is hiding in soft targets. None of this information is in True targets. So by using information from soft targets our small net is performing much better.Big net using soft targets learned similarity metric that learned â€˜whatâ€™s like whatâ€™ and with this knowledge transfer, we are telling the little net â€˜whatâ€™s like whatâ€™\
All of above experiment on MNIST is summarized below5. Additional experiment on MNISTIn addition, authors also tried omitting examples of digit 3 when training a smaller model using distillation. So from the perspective of the small model, 3 is a mythical digit that it has never seen. Since the smaller model has never seen 3 during training, we expect it to make a lot of errors when encountering 3 in the test set. Despite this, the distilled model only made 206 test errors of which 133 are on 1010 threes in the test set. This clearly shows that generalization capabilities of the large model were transferred to the small model during distillation and this causes the small model to correctly predict 3 in most cases\
So moral of the story is.Transforming input images greatly improves generalization. Transforming the targets also has a similarly large effect and if we can get soft targets from somewhere itâ€™s much cheaper as we can get the same performance with the smaller modelHinton, Geoffrey, Oriol Vinyals, and Jeff Dean. â€œDistilling the knowledge in a neural network.â€Knowledge distillation by intellabs.]]></content:encoded></item><item><title>In-House vs. Agency PR: Which Strategy Boosts Your Brand?</title><link>https://hackernoon.com/in-house-vs-agency-pr-which-strategy-boosts-your-brand?source=rss</link><author>Valeriya Mingova</author><category>tech</category><pubDate>Wed, 12 Mar 2025 09:18:09 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[PR is a crucial component of communication between businesses and clients. Most people agree on that. With well-executed PR, a business can reach new heights, expand its horizons, build audience loyalty, and strengthen trust. But itâ€™s just as easy to mess things up if handled incorrectly.Letâ€™s start with the fact that Iâ€™m generally against this kind of opposition, even though itâ€™s very common. "We have our own team," "Weâ€™d rather hire an in-house PR person," "We do everything ourselves"â€”these are frequent comments.\
But hereâ€™s an obvious analogy. If you need to sell your product, do you hire a traveling salesman, hand him a suitcase with product samples, put him in a cart, and send him off to knock on doors? Of course not. You hire a team of salespeople, provide them with mailing services, landing pages, lead generators, analytics, and so on. Can a salesperson sell on their own? Yes. Will their efficiency increase if you create the right environment? Absolutely. The same applies to PR.\
Looking at trends in integrating PR into businesses, letâ€™s examine how Russian IT companiesâ€”technological flagships of the economyâ€”implement this. Previously, we  that 17% of companies significantly expanded their PR teams. More than 50% of companies want to strengthen their marketing units by involving PR agencies. This indicates that the sector understands that they canâ€™t survive without PR reinforcements.Sometimes, companies have both in-house specialists and external partners working together towards results. Of course, situations vary, and itâ€™s not always about budget constraints. Letâ€™s break this down neatly.Pros and Cons of Each OptionThe most obvious advantage of an in-house PR specialist: theyâ€™re all yours. Always available, and not strictly limited by a contract (just check PR job listings on HH.ruâ€”thereâ€™s a ton of responsibilities). You can assign them anything! And they will do something. All for one salary! Whether they have the time, expertise, or resources to do everything effectively is another question.\
An in-house PR person is deeply immersed in the business and knows it better than anyone else. Thatâ€™s a unique asset. But there are two nuances:When will they have time to work with the external world if theyâ€™re always learning about the product and engaging with internal business structures?Everyone knows that if you stay in one environment too long, you lose objectivity, start seeing customers through your own lens, and lose critical perspective on the product and business.\
An in-house PR person might embark on a vague quest, given ambiguous tasks like "figure something out." If management isnâ€™t clear on what they want, they hire someone, vaguely outline a task, and then oversee execution with a "seagull management" approachâ€”flying in occasionally to check progress. No news? "Make something up. Walk around the factory, talk to smart people, follow the director for two days and write down their ideasâ€”thatâ€™s what we pay you for!" Agencies donâ€™t work like that.\
And hereâ€™s the kickerâ€”lack of mandatory results is compensated by cost savings. One salary. But this advantage diminishes if thereâ€™s a whole PR team, in which case costs become comparable to those of an agency, where a team also works on your project.Should you then hire an agency? Not so fastâ€”thereâ€™s more to consider.\
Agencies have more resources. They handle multiple clients, have broader experience, stronger connections, and more data to understand target audiences and markets. However, agencies must work closely with clients since they lack in-depth product knowledge and internal access. Thereâ€™s an adjustment period to fine-tune collaboration.\
Agencies are results-driven, working with deadlines and specific metricsâ€”what, how much, and how effectively. Sounds great, but only if the client knows what results they want and is ready to cooperateâ€”approving materials, providing information, and answering questions promptly. If not, an agency can become an annoying force constantly demanding input to meet deadlines.\
Agencies are skilled but donâ€™t like doing much beyond the contractâ€™s scope. At some point, additional costs may ariseâ€”especially when a client gets excited and wants more.\
Good work costs money. Not astronomical amounts, but agencies are always pricier than a single in-house PR specialist. However, if the in-house team grows beyond one person, costs become comparable.\
Finally, working with an agency can be dauntingâ€”not because they bite, but because they make publicity a reality. Suddenly, people will read about your business, discuss it, scrutinize it. What if something doesnâ€™t work well or look good enough? This fear of public exposure is more common among business owners than youâ€™d think.A global study shows that brands outsource the following to agencies:Pitching to journalists (78% of companies)Revising or building media lists (73%)Project work for product launches (51%)Content creation, including blogs (39%)\
Additionally, businesses often hire multiple agencies for different tasks.The best approach, if circumstances allow, is a PR team strengthened by a professional agency. Alternatively, an in-house PR manager overseeing a partner agency can be effective. This mix optimally distributes resources, workload, and ensures measurable, predictable results.But if you have to choose one? Hereâ€™s how to decide.Process matters more than results. Social media is managed, texts are written, publications happen, and requests come in. Ambitious growth may be tough, but for some, thatâ€™s enough.You value internal knowledge. The in-house PR person understands your business deeply, maintains great internal relationships, and keeps communications smooth.You prefer a slow and steady approach. If there are no major growth ambitions and no clear promotional plans, an in-house PR specialist is a solid, cost-effective solution.You operate in a competitive market. Quick, well-planned PR actions are crucial, and predicting their consequences requires expertise and resources.You have ambitious growth plans. PR enhances audience interaction, speeds up decision-making, builds loyalty, and creates brand ambassadors. A well-known brand has at least a 30% higher selection probability over an unknown but identical product.You donâ€™t have (or plan to build) an in-house PR team. Many business owners prefer to delegate processes outside their expertise to professionals, making agency selection critical.\
Some might argue: "What about companies with full-scale in-house PR teams?" Yes, having an in-house team is strategically sound. But look at major playersâ€”they have plenty of partners, including agencies. Strength lies in diversification.]]></content:encoded></item><item><title>Here Are the 7 Productivity Tips Pro Software Engineers Use to Stay Ahead</title><link>https://hackernoon.com/here-are-the-7-productivity-tips-pro-software-engineers-use-to-stay-ahead?source=rss</link><author>Maksim Zhelezniakov</author><category>tech</category><pubDate>Wed, 12 Mar 2025 08:31:14 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Modern software engineering is highly competitive and ever-changing space that requires many different skills. Itâ€™s not nearly enough just to know how to code well and implement tasks as you are asked to. You better know well your companyâ€™s business side, its important money-making metrics, learn how to cut corners to achieve results faster, find crucial edge cases the product team is not aware of, communicate expectations and your ideas clearly and test the outcomes diligently. Basically, you need to wear many hats and be a bit of everything for your team and company. That usually comes with some heavy experience, try and error ways and constant learning. But you can always grow professionally and become more productive in the shorter period of time, hopefully even earning a promotion while doing it.\
In this article Iâ€™m going to share 7 productivity tips I learned the hard way over the years as a senior iOS engineer. Letâ€™s see how exactly you can be a better engineer starting today:1. Keep your internal backlogYes, Jira and other tracking systems are great, but they are designed for your team first, not just for you personally. Everyone uses it to check your progress on sprint tasks, but itâ€™s not granular and not tailored to your goals.\
You donâ€™t need another sophisticated tracking system here. It can be just a notes app on your device or a simple task manager, to-do list or any other way of structuring your tasks. Here is a step by step example of how it can work:Take the sprint tasks from your favorite Jira, look at them and prioritize. What should be your primary task? Hint: usually itâ€™s something business related, maybe a new feature your team expects to be done by the end of the sprint. What are the tasks that can be safely postponed for later and completed if you have some time left? Itâ€™s always a good idea to clarify and share your understanding with a manager or a product person. It can be done during sprint planning if you have them.Break down your tasks by days, allocate more time and efforts for your main ones and leave less space for those you deem not so important.Add team meetings and other activities to your list. For example, I find it a good practice to have a dedicated time slot for code reviews every day. This way you donâ€™t need to rush your comments, and there are more chances to contribute better. I published a separate piece on getting the most out of your PR reviews here.Add some other todos to your list that are not included in your sprint. For example, â€œask Tom about a change we did last week to see if there is something I need to do next timeâ€œCorporate responsibilities should be in the list, as well. Example: â€œupdate my goalsâ€, â€œrespond to a company survey requestâ€, etc.Finally, keep track of all these things at the beginning of the day.\
This internal todo helps you to not forget about small things you might miss otherwise.2. Be proactive at team meetingsIt might sound tempting to be present at your teamâ€™s meeting, but at the same time try to finish that annoying task or follow up with someone in Slack. But, believe me, it is not the best time to lose focus. Your team members, especially both engineering manager and product manager expect you to be vocal and present. Share your ideas and opinions, ask questions on why and when, take initiative.\
It is a hard work, and I know, there are days you want to sit through quietly with low energy levels. But these efforts usually pay off in the long term.Communication is crucial for software engineers. Try to find a balance in what exactly youâ€™re saying to other people and how.\
For example, during a cross-functional team standup where you have a product manager, a designer, a QA engineer â€” meaning, people with different backgrounds and responsibilities, â€” itâ€™s better to filter out in your speech the things they wonâ€™t understand. Letâ€™s look at the difference:Yesterday I was applying a patch to our delivery endpoint, when the Cosmos database got corrupted entities inside. I had to roll it back and change the way we insert the hash values in its tables, adding better sorting algorithms.Yesterday I was having a hard time while working on a delivery task. There was a problem during a rollout which I immediately fixed. It was a beta environment, so weâ€™re completely fine. In fact, I improved the way we work with it which we will see in our metrics next week.\
Notice how the second version leaves the tech terms out and focuses on the results that are perfectly clear to everyone? To practice this simpler type of communication try to ask yourself these questions:What do they really want to know?What I am really trying to say?\
It is a virtue to explain a complicated tech stuff in simpler terms. But the better you do it, the better is understanding of it by other people. In the end youâ€™re gonna have less miscommunication problems, less uncertainties and less concerns.\
At last, you can always dig deeper into the tech things if itâ€™s needed or asked. It is the perfect place to do it on the tech syncs with other engineers.4. Build a bridge with your managerThis one does not depend entirely on you, of course, but in general, a good manager is someone who advocates you and your stakes in the eyes of the business. They are the ones who discuss promotions for teammates with upper management and make judgements on whether you are ready to take a bigger role and responsibilities. Hence, itâ€™s in your interests to be in good relationships with your manager.\
How exactly can you plan on doing that? Well, try to put yourself into your managerâ€™s shoes. What are their goals and what does the upper management expect from them? Plan a set of goals for you to achieve in the upcoming performance cycle and commit to them. Present them to the manager, discuss together and see them through. For example, take on a good refactoring project that will improve the product performance. By the way, a good sign of experienced engineer is to be able to delegate some tasks to others, be able to control the process and get the work done.5. Focused work over constant multitaskingLetâ€™s talk about the actual coding routine. Letâ€™s say, you have a task pending to be done from our first tip. How do you approach it? Everywhere around you there are â€œnoiseâ€œ factors: meetings, constant Slack messages, etc. I find it to be more effective to shield yourself from external non-important things and let yourself to be in the context of your task preferably without unnecessary interruptions. Get rid of these non-mandatory meetings if possible and do not rush into answering on every Slack message right away. Of course, it doesnâ€™t mean you should ignore DMs all the time, but itâ€™s better to avoid disruptions of your â€œthinking cycleâ€œ. What do I mean by that?\
Hereâ€™s an example of how it can be done:You have an important bug to solve. There is a couple of theories you want to try.Youâ€™re working on the first one. You applied a fix and ready to test if it works.Then you receive a Slack message. By skimming through a notification (literally a second of your time) you know that it can wait.You finish testing the theory. It doesnâ€™t pan out, so next youâ€™re gonna try the second one.Then you take a break and answer on that message.After coming back to the bug youâ€™re ready to work on the next theory.In my experience itâ€™s much harder to come back to something that you left partially unfinished, just left in the middle of the change. The more time in between the breaks, the harder is to get back that context later, â€œload it into your RAMâ€œ if you will.\
This approach doesnâ€™t mean that itâ€™s ok to ghost your colleagues. No, but in many cases no one expects you to write back instantly. Thereâ€™s definitely some grace courtesy period, so try to not overreach it. Sometimes, if they ask me to check something, and it takes time whereas my task is more important at that moment, I would let them know that I will check that and come back in an hour or so:Hey, sure, Iâ€™m gonna check this and come back to you in an hour. Hope itâ€™s ok with you. Itâ€™s just that I have this high-priority task Iâ€™ve been dealing with the whole day, and I need to finish it first.\
This way youâ€™re giving an actual ETA to your colleague and donâ€™t leave them hanging while being polite and friendly in your communication.Iâ€™ve known very talented engineers with diverse technical backgrounds who were of tremendous help to me. It was a delight to learn from these knowledgeable people. But have they done great in their careers moving up the ladder? Maybe earning a promotion, more responsibilities and a bigger paycheck? Surprisingly, not all of them. One of the reasons it happened this way is because almost no one knew about their work except for the people in their small dev teams.\
Honestly, if you work at a decent company with no micromanaging, no one is watching your pull requests under a microscope 24/7. And that small change that may be saved a company a significant amount of money can easily go unnoticed if not told properly by you to the stakeholders involved. Quite often these people are managers, product owners or heads of departmentsâ€¦ Meaning that they are not the tech people in the slightest. So, I advise you to not just show your golden PR, but present it in a way everyone in the company outside of your â€œtech bubbleâ€œ would understand the meaning of it and more importantly, the company value. Presentation, nice graphs, real numbers â€” all of that. Which leads us back to the 3rd point about soft skills and communication.\
There is a different side to this point that I hear sometimes in complains from fellow engineers. Like that person in your company who is good at self-marketing themselves and makes everything a much bigger thing than it actually was. They might have been praised more by leadership teams just because of their showcase skills. And just because of that it can be looked down upon. But hey, it will be beneficial to pick up a marketing trick or two from them, as well. Yes, â€œsellingâ€ your work is also important.\
If your presentation skills are lacking, think about the ways you could improve it. I would make it as one of personal goals with a clear set of action points. See whether your manager can be of help here. Winking at the 4th tip over there :wink:Last but not least, do not crunch extra hours. Do not overwork. Period.\
It depends on your company culture, but in some places management can be very manipulative in creating the â€œhurry hurry hurryâ€œ type of atmosphere where NOT staying late in front of your screen is frowned upon. The bottom line is that I would avoid such companies in favor of a better work-life balance. In a nutshell, it means that the company does not value its employees and does not care about them.\
I used to work extra hours before in my early years as an engineer. One time, we were told to release a huge and complicated feature in some fixed amount of time. The deadline was not realistic in a first place. We all knew about it, but for some reason that I already donâ€™t remember of, the team had to get along with it. We crunched through multiple weekends, working at nights. Can you guess what was the result? We couldnâ€™t make it, naturally. In the end, no one actually did anything about it, but I needed a month of no-work weekends after it to get recovered. As our team leader nicely said to me: â€œThis company would take as much of your time as you allow it toâ€œ.\
After that incident and many similar ones before, I put my own personal boundaries of where and when the work that Iâ€™m being paid for starts and stops. Remember, if you work 5/7 officially, you have a number of hours you are paid for. Even if you work as a freelancer, itâ€™s basically the same thing. There is always a â€œmoney per hourâ€ relation. If you do extra hours without extra pay, in most cases you do a disservice to yourself. Trust me, all this can easily lead to a burnout at some point that will be hard to come out of.\
In some rare occasions, it can be beneficial to do extra time, but only if there is a real profit out of it on the horizon. Extra pay, promotion, extra vacation days or some other form of retribution that is actually worth to do it for. But take it as a rule of thumb: make the most out of your working hours and work hard, but at the end of the day, when your clock is up, leave it to the next day. Value your time.I hope that these tips were somewhat of help for you. Maybe some of them you already know about, but others did get you thinking on making improvements with. And that was the reason for writing down these thoughts â€” to share my experience and see if it can be useful to others. Till the next time, keep growing and be better versions of yourselves!]]></content:encoded></item><item><title>Building Enterprise Angular Apps? Youâ€™re Doing It Wrong (Unless You Use Standalone Components)</title><link>https://hackernoon.com/building-enterprise-angular-apps-youre-doing-it-wrong-unless-you-use-standalone-components?source=rss</link><author>Tom Smykowski</author><category>tech</category><pubDate>Wed, 12 Mar 2025 08:25:43 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Building scalable, enterprise Angular apps comes with some challenges, one of which is to organize project in a way, that something maintained by two-three developers will be good to be developed by several teams like this.\
So the organisation of the project is really important. Especially because an enterprise app can consist of anything between 100 and 500 components and same amount of services.\
By the way, my name is Tom Smykowski, Iâ€™m an expert in building scalable, enterprise applications and in this series Iâ€™ll be teaching you how to do it!\
Before standalone components were introduced, you had to put every component, directive and pipe into a module. Hereâ€™s an example of a module:@NgModule({
  declarations: [AppComponent, HeaderComponent],
  imports: [CommonModule, FormsModule],
  providers: [SomeService],
  bootstrap: [AppComponent],
})
export class AppModule {}
\
There were several problems with this approach. First of all, modules arenâ€™t always necessary. Other libraries like React doesnâ€™t require this construct. For a reason, it increases complexity, increases risk of circular dependencies, makes the codebase harder to understand (you have to figure out what module exports your component, especially difficult when working with 3rd party libraries), lazy loading was a bit complex, and wellâ€¦ there was really a nice pattern to use them.\
If you put too much in a module, it gets bloated and your app suffers on performance, because Angular is unable to tree-shake them properly. If you create a module for every componentâ€¦ what is the best optionâ€¦ whatâ€™s the point of modules at all?\
These issues led to creation of standalone elements in Angular. Now you import a component, service, or a pipe directly where you use it, from where itâ€™s defined:import { Component } from '@angular/core';
import { CommonModule } from '@angular/common';
import { MeasurementCardComponent } from './measurement-card.component';
import { UnitConverterPipe } from './unit-converter.pipe';
import { StatusDirective } from './status.directive';

@Component({
  selector: 'app-factory-dashboard',
  imports: [
    CommonModule,
    MeasurementCardComponent,
    UnitConverterPipe,
    StatusDirective,
  ],
  // Add other metadata properties here if needed
})
export class FactoryDashboardComponent {
  // Component logic goes here
}
\
As you can see, itâ€™s easier to figure out and find dependencies of the component using standalone entities.When you are not there yet with your codebase to use standalone entities, you have to know that in Angular 19 components are standalone by default. You donâ€™t have to add standalone: true to the decorator:@Component({
  selector: 'app-factory-dashboard',
  standalone: true,
  // Add other metadata properties here
})
\
You can also use the schematic to automatically switch to standalone:ng generate @angular/core:standalone
\
It will also change other elements like bootstrapping and routing. So a lot of things will be taken care of automatically.\
In my experience the schematics works quite good. If you use some special things in your application, you may need to do some manual adjustments. But as long as you sticked to Angular standards, it should go smoothly.\
To summarize by updating and switching to standalone components, your app complexity will drop, maintainability increase, build size will drop and initial load time drop. All of these contribute strongly to building scalable Angular app.\
In my experience of working on Angular apps serving millions of users, standalone components are mature, and serving purpose they were designed for. So, I encourage you to switch to standalone entities.\
Iâ€™ve prepared a free checklist how to make your Angular app scalable and ready for enterprise demand. If youâ€™re interested, you can get thisÂ .\
If you have any questions, ask these in the comment section!]]></content:encoded></item><item><title>Founder of Chinaâ€™s New AI Model Says His Agent is More Autonomous Than Rivals&apos;</title><link>https://hackernoon.com/founder-of-chinas-new-ai-model-says-his-agent-is-more-autonomous-than-rivals?source=rss</link><author>Akriti Galav</author><category>tech</category><pubDate>Wed, 12 Mar 2025 08:16:49 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[In the ever-changing world of artificial intelligence, there's always buzz about the "next big thing." Yet every now and then, something revolutionary appears that totally just rewrites the rules. Right now, that innovation is Manus AI, a groundbreaking creation by a Chinese startup called Monica.\
Since its official launch on March 6, 2025, Manus has quickly caught global attention, prompting intense conversations and excitement about what's next in AI technology. But why exactly is everyone talking about Manus?Introducing Manus: An AI That Actually Gets Things DoneDuring Manusâ€™s recent launch event, Peak, the co-founder and Chief Scientist of Manus, said that they have been quietly building what they believe is the next big evolution in AI. He further said,"Today, we're launching an early preview of Manus, the first general AI agent. This isn't just another chat-based workflow but a truly autonomous agent that bridges the gap between conception and execution."\
Unlike traditional chatbots, Manus doesnâ€™t merely answer questionsâ€”it independently completes entire tasks without continuous oversight.\
Picture this: Youâ€™re planning a two-week trip to Japan and simply instruct Manus, "Plan my trip and keep the budget under $2,000." Within moments, Manus independently scours travel websites, compares flights, arranges accommodations, and compiles a detailed itineraryâ€”without any additional guidance from you.\
But the magic doesn't stop at travel planning. Manus can screen job resumes, conduct complex market research, build fully functioning websites, and even analyze financial data in real-time. As Peak confidently showcased during the demonstration:â€œIn this example, we ask Manus to help screen resumes. I've just sent Manus a zip file containing 10 documents. Since each Manus session has its own virtual computer, it works just like a humanâ€”first unzipping the file, reading each rÃ©sumÃ© page-by-page, and compiling important information. It then ranks the candidates and, if you prefer, presents everything neatly in a spreadsheet.â€\
Manus is also capable of tackling complex research projects. Peak shared another example:â€œWe asked Manus to filter New York properties based on safety, school quality, and budget constraints. It independently researched safe neighborhoods, investigated middle schools, wrote a Python script to handle budget calculations, and finally delivered a detailed report along with recommended properties.â€\
This kind of autonomy, performing intricate tasks, even while users are offline, makes Manus stand out.Why Manus AI is Creating So Much BuzzWith heavyweights like OpenAI, Google, and Anthropic dominating AI headlines, Manus manages to stand apart for several reasons:Full Autonomy: Manus doesnâ€™t need constant prompts. As Peak explained, â€œWhile other AI stops at generating ideas, Manus delivers results. It's built for long-term projects, continuing its work even after you log off.â€  \n Complete Transparency: AI is often criticized as a â€œblack box,â€ where nobody knows exactly how decisions get made. Manus addresses this head-on. "We built Manus to openly show each step in its thinking process, making decisions clear and trustworthy," Peak noted during the demo.  \n Real-World Results: Manus doesn't just promiseâ€”it delivers. In recent benchmarks evaluating real-world scenarios, Manus has already surpassed OpenAIâ€™s famed DeepResearch system. Peak highlighted, â€œBeyond benchmarks, Manus has proven its capabilities on real-world platforms like Upwork, Fiverr, and Kaggle.â€How Did Manus Suddenly Capture Everyoneâ€™s Attention?The rapid rise in Manusâ€™s popularity echoes recent AI sensations like DeepSeekâ€™s R1. But Manus might be even bigger. When Manus's demo video surfaced on social media platform X, it quickly went viralâ€”within hours, it had racked up over 200,000 views. Tech enthusiasts worldwide scrambled for invitation codes. The video itself was astonishing: Manus seamlessly jumped between platforms like X and Telegram, managing over fifty screens simultaneously, effortlessly multitasking at a level previously unseen.\
While the AI's capabilities are clear, its origins are still somewhat mysterious. The entrepreneur behind Manus is known only as Peak, a Chinese tech innovator famed previously for creating mammoth mobile browsers. Beyond this, not much is publicly known, adding to the intrigue and speculation around this technology.A Glimpse into the FutureManus isn't widely available yetâ€”itâ€™s still in an exclusive preview phaseâ€”but even now, its impact is undeniable. Peak summed up its broader significance perfectly at the launch event:â€œThe name Manus comes from the motto â€˜Mens et Manusâ€™â€”Mind and Hand. It embodies our belief that knowledge must be applied to make a meaningful impact on the world. Manus aims to extend your capabilities, amplify your impact, and become the hand that brings your mindâ€™s vision into reality.â€\
As Manus continues to evolve, it prompts some fascinating questions:Will fully autonomous AIs soon manage entire areas of our lives?How might tools like Manus reshape our approach to work, creativity, or even daily decisions?Peak offered an inspiring final note:â€œThe future isn't just about what AI can do for usâ€”it's about how we'll collaborate, coexist, and build together.â€\
One thing is clear: Manus AI isnâ€™t merely another passing tech trend. Itâ€™s a pivotal moment that could redefine what we believe is possible with artificial intelligence.\
In tech, itâ€™s exactly these kinds of momentsâ€”filled with uncertainty, potential, and endless possibilitiesâ€”that keep us dreaming, exploring, and innovating. And Manus just gave us a thrilling reason to look forward.]]></content:encoded></item><item><title>OpenAI Launches $50 million AI fund</title><link>https://hackernoon.com/openai-launches-$50-million-ai-fund?source=rss</link><author>This Week in AI Engineering</author><category>tech</category><pubDate>Wed, 12 Mar 2025 08:01:16 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[\
Welcome to the ninth edition of "This Week in AI Engineering"!\
OpenAI launched a $50M funding connecting 15 research institutions, Inception Labs released Mercury with speeds 10x faster than current LLMs, Cohere For AI unveiled Aya Vision for multilingual capabilities, and Alibaba's QwQ-32B matches DeepSeek-R1 with far fewer parameters.\
With this, weâ€™ll also be talking about some must-know tools to make developing AI agents and apps easier.NextGenAI: OpenAI's $50M Consortium Connecting 15 Research InstitutionsOpenAI has launched , an alliance uniting 15 leading research institutions with $50M in funding to accelerate scientific breakthroughs and transform education through AI. The initiative provides research grants, compute resources, and API access to support academic innovation across disciplines. Direct access for model training, fine-tuning, and application development Dedicated compute resources for university-led AI model developmentCross-Institutional Collaboration: Shared resources and findings across consortium members Student access to hands-on AI model training and application development AI-powered manufacturing, energy, and healthcare advancement at Ohio State Boston Public Library digitizing public domain materials for broader access Harvard and Boston Children's Hospital accelerating rare disease diagnostics Duke University conducting metascience research to identify high-impact AI fields Texas A&M implementing Generative AI Literacy Initiative Oxford digitizing rare texts at Bodleian Library with OpenAI's API\
The initiative expands on OpenAI's educational commitment following ChatGPT Edu's launch in May 2024. NextGenAI focuses specifically on providing API-level access and research funding to drive innovations in scientific research, university operations, and educational methodologies.Mercury: Inception Labs Launches 10x Faster Diffusion LLMsInception Labs has released , the first commercial-scale diffusion large language model (dLLM) family that achieves output speeds faster than DeepSeek Coder V2 Lite, GPT-4o Mini, and Claude 3.5 Haiku. The technology demonstrates breakthrough performance with Mercury Coder running at over 1000 tokens per second on standard NVIDIA H100s. Coarse-to-fine diffusion process instead of traditional autoregressive generation Transformer-based neural network that modifies multiple tokens in parallel Compatible with existing NVIDIA GPUs without requiring specialized chips Available via API and on-premise installations with fine-tuning support 1109 tokens/second for Mercury Coder Mini vs 59 tokens/second for GPT-4o Mini 88.0% for Mercury Coder Mini, matching GPT-4o Mini's 88.0% 78.6% for Mercury Coder Mini vs 78.5% for GPT-4o Mini 82.2% for Mercury Coder Mini, significantly outperforming GPT-4o Mini's 60.9% 20x faster than some frontier models running below 50 tokens/second Mercury Coder Small scores 90.0% on HumanEval, tied with Gemini 2.0 Flash-Lite Second place in Copilot Arena, outperforming GPT-4o Mini and Gemini 1.5-Flash 5-10x reduction in inference costs while maintaining competitive code quality\
The models enable new capabilities for latency-sensitive applications that previously required compromising on model quality to meet speed requirements. Mercury's architecture allows continuous refinement of outputs to correct mistakes and hallucinations, similar to approaches used in leading image and video generation systems.Aya Vision: Cohere For AI Launches State-of-the-Art Multilingual Vision ModelCohere For AI has released , an advanced open-weight vision model that significantly expands multilingual and multimodal capabilities across 23 languages spoken by over half the world's population. The model excels in image captioning, visual question answering, and cross-modal translation tasks. Available in 8B and 32B parameter configurations Processes 23 languages with consistent performance across linguistic domains Combines synthetic annotations, translation rephrasing, and multimodal merging Unified image and text understanding with cross-modal translation capabilities Aya Vision 8B achieves up to 70% win rates against comparable models 79% win rate in multilingual vision tasks for the 8B variant Aya Vision 8B outperforms Llama-3.2 90B Vision with 63% win rates 32B model outperforms models 2x its size (Llama-3.2 90B, Molmo 72B) Performance scaling from 40.9% to 79.1% win rates through technical refinements Open-sourced Aya Vision Benchmark for multilingual multimodal assessment Optimized for researchers with limited computation resources Free access via WhatsApp integration for global usability\
The release includes open-weights for both model sizes on Kaggle and Hugging Face, continuing Cohere's expansion of multilingual AI research that began with the Aya initiative two years ago. The model builds upon Aya Expanse, supporting research collaboration across 3,000 researchers from 119 countries.Alibaba has released , a new open-source reinforcement learning (RL) enhanced language model that achieves performance comparable to DeepSeek-R1 despite using significantly fewer parameters. The model demonstrates that strategic RL applications can dramatically close the performance gap with much larger models. 32B parameters versus DeepSeek-R1's 671B (37B activated) Two-stage reinforcement learning with outcome-based rewards Math and coding task optimization using accuracy verifiers General capability enhancement with reward models and rule-based verifiers Apache 2.0 open-source availability 79.5% accuracy, matching DeepSeek-R1's 79.8% 63.4% score compared to DeepSeek-R1's 65.9% 73.1% performance versus 71.6% for DeepSeek-R1 83.9% accuracy, comparable to R1's 83.3% 65.4% score, outperforming R1's 60.3%\
Integration Capabilities: Built-in agent capabilities for environmental interaction Dynamic thought process adjustment based on feedback Available through Hugging Face, ModelScope, and Alibaba Cloud DashScope Accessible via Qwen Chat with Python integration examples\
Alibaba's team identifies this as an initial step toward developing more capable AGI systems by combining stronger foundation models with scaled RL and computational resources. is a technical platform that automates the creation of user interfaces (UIs) from JSON data. It generates UIs in HTML and Tailwind CSS, allowing users to copy and share the code via links. This tool streamlines UI development by providing a straightforward method to transform data into functional interfaces, enhancing productivity and collaboration among developers. is a specialized search engine designed to efficiently scan and index public GitHub repositories. It enables users to quickly find specific code snippets, files, or functionalities within the vast ecosystem of open-source projects. By offering targeted search capabilities, GitSearch helps developers discover and leverage existing code, contributing to increased efficiency and collaboration in software development. is an enterprise-grade AI solution designed to automate unit test generation and management for complex Java code. Unlike LLM-driven assistants, it uses reinforcement learning to produce reliable, executable, and correct test code, ensuring IP security through on-premises operation. It integrates into IntelliJ and CI pipelines, generating tests 250x faster than manual methods and increasing code coverage. Diffblue Cover aims to is an AI-powered platform designed to help developers understand and modernize complex mainframe codebases. It leverages AI to uncover code insights, generate documentation for languages like COBOL and Assembly, and extract business logic from legacy systems. Swimm provides features such as visualizing program flows, identifying dependencies, and assessing the impact of changes. It aims to reduce mainframe complexity, create missing specs, and ensure secure, compliant, and scalable operations, with options for both cloud and on-premises deployment.And that wraps up this issue of "This Week in AI Engineering." \
Thank you for tuning in! Be sure to share this with your fellow AI enthusiasts and follow for the latest weekly updates.\
Until next time, happy building!]]></content:encoded></item><item><title>Why Giving Up Is So Easy: 5 Common Reasons People Quit</title><link>https://hackernoon.com/why-giving-up-is-so-easy-5-common-reasons-people-quit?source=rss</link><author>Pawan Pratap Singh</author><category>tech</category><pubDate>Wed, 12 Mar 2025 08:00:07 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[â€œNothing comes easy. Success doesn't just drop on your lap. You have to go out and fight for it every day.â€ \n  \n â€• Dana White\
It has been proven time and time again that qualities like grit can make all the difference when it comes to achieving successâ€”no matter how you define it or the kind of success youâ€™re chasing. However, beyond grit, determination, and drive, there are countless distractions that can stand in your way.\
These distractions can crush your dreams and goals in an instant, draining every ounce of ambition and determination from within. They are the very obstacles that cause many of us to abandon our dreams too soon, regardless of how significant our aspirations may be.\
In this post, I will share with you the 5 simple reasons why people give up far too easily.1. Surrounded By People Whoâ€™ve Given Upâ€œSurround yourself with people on the same mission as you.â€\
Who better to surround yourself with than people whoâ€™ve given up on their life goals and dreamsâ€¦ Doing this is the dumbest thing you can do if you expect more out of your life. What chance does an aspiring singer have if he/she is surrounded by people whoâ€™ve quit on their own dreams and goals?\
Or worse, these are the types of people they take bullshit advice from? These types of people will tell you things like:Thereâ€™s no way youâ€™ll be successful.If I didnâ€™t succeed, you wonâ€™t either.Nobody in our family/area is successful, so what makes you any different?Itâ€™s nice to dream, but itâ€™s not realistic.Just get a basic job, start a family, and settle.Your goals are unrealistic.\
Theyâ€™ll tell you things like this because thatâ€™s their reality. And if they donâ€™t tell you directly, theyâ€™ll tell you through their behavior. Theyâ€™re not giving you advice, theyâ€™re giving themselves advice. Theyâ€™re not talking to you, theyâ€™re talking to a mirror.\
So, one way or another, youâ€™ll more than likely give up after a couple of months, or even after a short yearâ€”unless you change your circle of friends, associates, and the people you let into your life every day.2. Reading Too Many â€œ30 under 30â€ Billionaire Listsâ€œMoney is only a tool. It will take you wherever you wish, but it will not replace you as the driver.â€\
You know the ones Iâ€™m talking about. The types of lists Forbes publishes every year or so. Thereâ€™s nothing wrong with these lists, in fact, theyâ€™re inspirational, and thereâ€™s something we can learn from those who are on it.\
But when you get caught up in the thought of mimicking what theyâ€™ve done, as quickly as theyâ€™ve done it, you become delusional. The fact is almost 100% of people will never accumulate billions of pounds before theyâ€™re 30 years old. Itâ€™s not that itâ€™s impossible, but the probability is low.\
When youâ€™re looking at the world through delusional eyes, you end up disappointed, depressed, failing, and questioning whether youâ€™re good enough or not. Then your self-esteem starts taking a pounding like itâ€™s just been in a fight with Mike Tyson.\
And thatâ€™s when the thought of giving up starts creeping in before it becomes your reality. Having high expectations is NOT the same as having delusional expectations. Donâ€™t get the two definitions confused.3. Not Having A Clear Purposeâ€œA lack of clarity could put the breaks on any journey to success.â€\
Most of us have no idea why weâ€™re doing what weâ€™re doing. I know Iâ€™ve been there. Having no clear sense of purpose, or â€œwhyâ€ behind my actions is what caused me to procrastinate, and quit too fast and too soon.\
Just imagine how disastrous that would be if the pilot flying tons of people from one country to another had NO idea what he/she was doing. Well, thatâ€™s exactly how disastrous it is to your life, your goals, your aims, and your dreams.\
This is why smart goal setting is essentialâ€”it gives you direction and clarity, helping you stay focused on your purpose. Not knowing why, and not having a strong enough â€œwhyâ€ will cause YOUR plane to crash, burn, and explode until thereâ€™s nothing left of it.\
Then thatâ€™ll cause you to quit, give up, or worse, to never ever dare to try again. Write down WHY youâ€™re doing what youâ€™re doing, and WHY it matters. And If it doesnâ€™t matter, then itâ€™s your job to find something that does matter.\
Something that gives you a strong enough reason to persevere and see it through until completion. Something that you wake up thinking about and go to bed thinking about. And something that keeps you focused on whatâ€™s important instead of whatâ€™s irrelevant.4. Not Willing To Give Up Old Habits For Better Habitsâ€œDonâ€™t make a habit of choosing what feels good over whatâ€™s actually good for you.â€\
Ever since I started my personal development journey 6+ years ago, Iâ€™ve given up video games, watching too much TV, the mainstream news, and many other things. If I never gave up those habits, I wouldnâ€™t even be sat writing out this article.\
Instead, Iâ€™d be swimming in a pool of bad habits that are poisonous to my goals and my plans. And I would have shot my ambition in the head a long time ago.\
Grant Cardone is a great example of this.\
Until the age of 25, he was taking drugs, hanging around druggies, and negative people. Then he completely turned his life around by changing his drug habits, and the types of people and things he surrounded himself with.\
If youâ€™re not willing to give up old habits for habits that support your goals, then you may as well tear up the sheet of paper you wrote your goals down on. Or burn it. Because youâ€™ll only get in your way, waste time, and quit on yourself too soon and too early.â€œWithout promotion, something terrible happensâ€¦ Nothing.â€\
There was a time when I was terrified of promoting myself. It wasnâ€™t self-promotion itself that scared me; it was the opinions of other people that scared me.\
What will they think of me? How will they respond? What if they criticize me? What if they donâ€™t agree? The WHAT IF disease stood in my way and I wasnâ€™t willing to fight back.\
If you donâ€™t promote yourself, or worse, you promote yourself too little, then thereâ€™s no way youâ€™ll have the stomach to keep persevering and pursuing the things you want.\
One way or another youâ€™ll quit on yourself and make excuses as to why things arenâ€™t working out for you, or why things will NEVER work out for you. Youâ€™ll excuse yourself from the fact that you never promoted yourself enough, if at all.\
Nobody will give you what you want if youâ€™re too afraid to ask for it. And thereâ€™s no achievement without the involvement of other people. Nothing is achieved alone.]]></content:encoded></item><item><title>Pasifika Web3 Tech Hub Promises Personal Freedom, Financial Sovereignty for Pacific Islands</title><link>https://hackernoon.com/pasifika-web3-tech-hub-promises-personal-freedom-financial-sovereignty-for-pacific-islands?source=rss</link><author>Edwin Liava&apos;a</author><category>tech</category><pubDate>Wed, 12 Mar 2025 07:57:20 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Today marks a watershed moment for our Pacific Island communities. With immense pride and profound hope, I officially launch the Pasifika Web3 Tech Hub i.e. a groundbreaking initiative designed to unite our cultural heritage with blockchain innovation.\
On this day, I am giving you the key to your personal freedom and financial sovereignty. This isn't merely a technological project, it's a pathway to economic empowerment and cultural preservation for Pacific Islanders everywhere.\
The Pasifika Web3 Tech Hub is a DAO-governed, PASIFIKA token-powered marketplace built on Blockchain technology, connecting our island communities to global markets through a decentralized physical infrastructure (DePIN). This platform will enable us to share our data, digital content, traditional knowledge, handicrafts, and local produce with the world on our own terms, preserving our cultural identity while creating unprecedented economic opportunities.Moving Beyond Legacy SystemsIn recent days, many fellow Tongans have approached me asking if I would consider taking on CEO positions in public enterprises if such roles were advertised. Their queries echo similar questions about the Board of Directors positions I addressed recently.\
My response remains consistent i.e. Why would I want to inherit a mess orchestrated by other people? Let them fix their own problems. I have contributed enough already in the utilities sector, fighting uphill battles against outdated structures and resistant bureaucracies.I have moved on. I am already in Web3, and I am now offering this future to all citizens.\
Just as I outlined in my recent piece about the obsolescence of the Public Enterprises Statutory Board of Directors, our progress demands that we leave behind systems that no longer serve their purpose. The traditional corporate structures that have failed to deliver prosperity to our people cannot be the vehicles for our future success.Building Rather Than RepairingInstead of pouring energy into fixing broken systems, I've chosen to build something entirely new i.e. a system designed specifically for our needs, values, and aspirations.\
The Pasifika Web3 Tech Hub represents a fundamental shift in thinking. Rather than creating more oversight layers, redundant bureaucracies, or positions of limited influence, we're establishing direct pathways to market, transparent governance, and genuine community ownership.\
Our decentralized approach eliminates the very inefficiencies I've spent years pointing out in traditional governance structures. Where the current systems create barriers between producers and consumers, our platform creates bridges. Where existing frameworks drain resources through unnecessary administrative layers, our model channels resources directly to value creators.Your Invitation to True SovereigntyToday's launch is an invitation. An invitation to artisans to share their crafts with global markets. An invitation to farmers to receive fair compensation for their produce. An invitation to knowledge keepers to preserve and monetize our cultural wisdom on their own terms.But most importantly, it's an invitation to all Pacific Islanders to claim ownership of their digital and economic future.\
The journey continues, but now with a clear destination i.e. a Pacific that leverages technology not just to participate in the global economy, but to lead it in ways that honor our values, strengthen our communities, and preserve our unique cultural heritage.\
To those who continue asking if I'll return to traditional roles within the existing system, my answer is clear i.e. I've chosen to build the future rather than repair the past. I invite you to join me on this journey toward genuine digital sovereignty and economic empowerment.\
The key to your personal freedom and financial sovereignty is now in your hands. The Pasifika Web3 Tech Hub is more than a platform, it's our collective declaration that we will chart our own course in the digital economy, on our own terms, with our cultural values intact.\
The Pasifika Web3 Tech Hub starts today, and it begins with us. You can start by reading our Constitution here on our GitHub repository.\
Watch this space for updatesThe Pasifika Web3 Tech Hub is a DAO-governed, PASIFIKA token-powered marketplace built on Blockchain, connecting island communities to global markets through decentralized infrastructure. We unite traditional knowledge with cutting-edge AI to preserve cultural heritage while creating economic opportunities across the Pacific.]]></content:encoded></item><item><title>AI Coding Tools Are Bad News for Lazy Programmers</title><link>https://hackernoon.com/ai-coding-tools-are-bad-news-for-lazy-programmers?source=rss</link><author>Maximiliano Contieri</author><category>tech</category><pubDate>Wed, 12 Mar 2025 07:36:25 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[How AI is Redefining the Role of Programmers in the Tech IndustryTL;DR: AI-generated code outperforms lazy programmers. Software jobs are declining, but centaurs will survive.\
Today, a friend of mine shared his frustration about being asked to fill out a registration form requiring a username and password.\
The system was poorly designed and demanded a secure password without explaining what the issue was.\
This is a clear example of terrible UX and a lazy programmer unaware of writing error messages with empathy.\
I told my friend this is unacceptable in today's world since modern AIs can easily solve such problems.\
I took my phone and said, "L.M.P.T.F.Y." (Let Me Prompt That For You), the modern equivalent to LMGTFY.\
I quickly wrote a prompt:Give me the code for a secure password validation site with helpful error messages when the user enters a weak password.\
The A.I. successfully provided a working page with this code:document.getElementById('passwordForm')
  .addEventListener('submit', function(event) {
  event.preventDefault(); 
  // Prevent the form from submitting
  const password = document.getElementById('password').value; 
  // Get the password value
  const errorMessages = document.getElementById('errorMessages'); 
  // Get the error messages container
  errorMessages.innerHTML = ''; // Clear previous error messages

  let errors = []; // Array to store error messages

  // Validate minimum length
  if (password.length < 8) {
    errors.push('The password must be at least 8 characters long.');
  }

  // Validate uppercase letters
    if (!/[A-Z]/.test(password)) {
      errors.push('The password must contain one uppercase letter.');
    }

    // Validate lowercase letters
    if (!/[a-z]/.test(password)) {
      errors.push('The password must contain one lowercase letter.');
    }

    // Validate numbers
    if (!/[0-9]/.test(password)) {
      errors.push('The password must contain one number.');
    }

    // Validate special characters
    if (!/[!@#$%^&*(),.?":{}|<>]/.test(password)) {
      errors.push('The password must contain one special character.');
    }

    // Display errors or success message
    if (errors.length > 0) {
      errors.forEach(error => {
          const p = document.createElement('p'); 
          // Create a new paragraph element
          p.textContent = error; 
          // Set the error message as text
          errorMessages.appendChild(p);
          // Add the error message to the container
      });
    } else {
        alert('Valid password!'); 
        // Show success message if no errors
    }
});
\
You can also try it yourself:The AI-generated solution with helpful error messages is bad news for lazy programmers.\
Software development jobs are already declining, and this trend is expected to continue:This is something many people have been forecasting.\
I wrote an article five years ago during the last AI Winter predicting this would happen.As the great Niels Bohr once said:Prediction is very difficult, especially about the future.Now, it's clear: lazy programmers are doomed!What can we do as software engineers besides writing mediocre code?Soon, there will be a shortage of handy people such as electricians, plumbers, and painters.A.I. won't take your job. A developer mastering AI tools will.\
In these articles, you can compare the output of many AIs with and without guidance.\
For example, the above code has several problems unnoticed by AIs:Humans remain invaluable when they know how to harness AI effectively.\
Here's a video benchmarking some tools:Hopefully, my friend will soon complete the password form â€” or better yet developers will deprecate all passwords.Also, I hope you'll write solutions like these and get paid as a "Centaur"- a developer who masters AI tools to enhance their craft.]]></content:encoded></item><item><title>Anonymous Sources: Starship Needs a Major Rebuild After Two Consecutive Failures</title><link>https://science.slashdot.org/story/25/03/11/2159228/anonymous-sources-starship-needs-a-major-rebuild-after-two-consecutive-failures?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 12 Mar 2025 07:00:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Longtime Slashdot reader schwit1 shares a report from Behind The Black: According to information at this tweet from anonymous sources, parts of Starship will likely require a major redesign due to the spacecraft's break-up shortly after stage separation on its last two test flights. These are the key take-aways, most of which focus on the redesign of the first version of Starship (V1) to create the V2 that flew unsuccessfully on those flights:

- Hot separation also aggravates the situation in the compartment.
- Not related to the flames from the Super Heavy during the booster turn.
- This is a fundamental miscalculation in the design of the Starship V2 and the engine section. 
- The fuel lines, wiring for the engines and the power unit will be urgently redone.
- The fate of S35 and S36 is still unclear. Either revision or scrap. 
- For the next ships, some processes may be paused in production until a decision on the design is made. 
- The team was rushed with fixes for S34, hence the nervous start. There was no need to rush. 
- The fixes will take much longer than 4-6 weeks.
- Comprehensive ground testing with long-term fire tests is needed. [emphasis mine] 

It must be emphasized that this information comes from leaks from anonymous sources, and could be significantly incorrect. It does however fit the circumstances, and suggests that the next test flight will not occur in April but will be delayed for an unknown period beyond.]]></content:encoded></item><item><title>The TechBeat: Your Writing Has a Fingerprintâ€”And This Cutting Edge AI Model Can Identify It (3/12/2025)</title><link>https://hackernoon.com/3-12-2025-techbeat?source=rss</link><author>Techbeat</author><category>tech</category><pubDate>Wed, 12 Mar 2025 06:11:07 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[By @superlinked [ 11 Min read ] 
 In this tutorial, weâ€™ll guide you through the process of creating a movie recommendation system using vector databases.  Read More.By @contactraac [ 4 Min read ] 
 real-world assets (RWAs) will lead the DeFi renaissance and help grow TVLs in 2025 through borrowing, collateralized lending, yield generation. Read More.By @benoitmalige [ 4 Min read ] 
 Simplify your life with small changes to your space, time, and relationships for a lighter, focused you. Read More.By @redact [ 3 Min read ] 
 Musk claims a 'massive cyberattack' is behind a Twitter/X outage impacting thousands globally, but skepticism arises over the lack of evidence. Learn about the  Read More.By @kamalsamaila [ 4 Min read ] 
 Trump flips on crypto, signs order for U.S. Bitcoin reserve worth $17.5B. Visionary move or risky bet? Explore the bold policy shift Read More.By @hackernooncontests [ 4 Min read ] 
 1 month left to enter Round 1 of the Spacecoin Writing Contest! Write about #decentralized-internet, #spacetech, #blockchain-use-case to compete for 15000 USDT! Read More.By @hayday [ 6 Min read ] 
 It Takes More than Thinking: Humans Put the Vibe into Vibe Coding. An article about software engineers in a post-AI world, Vibeware and embracing ourselves Read More.By @alexpiskarev [ 9 Min read ] 
 I built and shut down a â‚¬500K neobank for immigrants in Portugal. Here's a post-mortem. Read More.By @HuseynG [ 7 Min read ] 
 MCP (Model Context Protocol) is an open standard that allows AI systems to connect seamlessly with a wide variety of data sources. Read More.By @4rkal [ 5 Min read ] 
 In this article, I will be showing you how to deploy the GoTTH stack (Go Templ htmx tailwind) to production. Read More.By @mlsprwr1337 [ 2 Min read ] 
 Procedural generation with isometric Wave Function Collapse (WFC). Generate your own little microcosmos using a browser-based editor. Read More.By @marcryan [ 5 Min read ] 
 Understand the 4 types of synthetic dataâ€”Imputation, User Creation, Insights Modeling, and Manufactured Outcomesâ€”to enhance AI, analytics, and market research Read More.By @mcsee [ 3 Min read ] 
 When you add flags like isTesting, you mix testing and production code. This creates hidden paths that are only active in tests. Read More.By @authoring [ 5 Min read ] 
 Using grammatical structures from parsed text, this study explores a new method for detecting authorship, improving accuracy in AI and fake text identification. Read More.By @hennygewichers [ 6 Min read ] 
 From pizza orders to smart home cameras, empathetic AI is now nudging our decisions in real time. Can we keep up? Read More.By @hackercm5drdgdo0000357l4uxtvhpm [ 6 Min read ] 
 Explore how Agentic AI transitions beyond chatbots into autonomous automation, empowering businesses with innovative workflows, and enhanced productivity. Read More.By @truewebber [ 9 Min read ] 
 Discover why using a shared database table is an anti-pattern and how a â€˜Contract Firstâ€™ approach fosters clear ownership and smoother integration. Read More.By @zbruceli [ 11 Min read ] 
 Part II of the series: use MCP and Solana AgentKit to build an AI Agent that can trade USD and EUR stablecoins. Read More.]]></content:encoded></item><item><title>Finding XPath Bugs in XML Document Processors: Testing XPath Functionality and Other Related Work</title><link>https://hackernoon.com/finding-xpath-bugs-in-xml-document-processors-testing-xpath-functionality-and-other-related-work?source=rss</link><author>XPath</author><category>tech</category><pubDate>Wed, 12 Mar 2025 01:53:35 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[While various related approaches to our work exist, to the best of our knowledge, we propose the first general approach to testing XML processors to find logic bugs. As discussed above, the most closely related work proposed testing the index support of SQLServer in the context of XPath and XQuery [41], which, to the best of our knowledge, is the only work that has tackled the test-oracle problem for XML processors, but is limited in scope.\
Testing XPath functionality. Various approaches to benchmarking XPath implementations or test suites for them have been proposed, the most representative being XPathMark and the W3C qt3 test suite. XPathMark [25] is a benchmark for testing XML processorsâ€™ XPath standard 1.0 functionality, containing both correctness as well as performance tests. The W3C qt3 test suite developed by the W3C XQuery and XSLT Working Groups [19] contains around 30,000 tests for XPath and XQuery targeting XPath 3.0 and later versions, which cover a broad range of functions and expressions.\
XML-related automated synthetic data generation. Previous works have proposed approaches for automatically generating XML-related data, such as XML documents, XPath, and XQuery expressions. Aboulnaga et al. proposed an XML document generator to generate synthetic, but complex, structured XML data by introducing recursion and repetition on tag name assignment and controlling the element frequency distribution [20]. RychnovskÃ½ and HolubovÃ¡ proposed an approach to generate XML documents related to given XPath queries from a specific XML schema to improve query efficiency [37], which is useful for developers to create micro-benchmarks for testing performance over certain XPath expressions. XQGen [42] is a tool for generating XPath queries that conform to a given XML schema, allowing users to specify multiple parameters, such as the percentage of empty queries desired and the percentage of queries with predicates. XPath generated by XQGen includes only direct node tests without introducing complex expressions, such as axes or function transformations. Similarly, the XQuery generator designed by Todic and Uzelac [41] includes XQuery FLWOR expressions, but the logic predicate consists only of simple operations, such as value comparisons. Neither of these works tackled the test oracle problem, and, as indicated by the results in Section 4.3, given their different focus, they cannot be effectively combined with a differential testing oracle.\
Targeted test case generation. Many testing tools guide their test case generation process to improve testing efficiency, for random approaches such as random byte mutation used in fuzzing approaches generate a large proportion of invalid queries [47]. DynSQL [27] guides the fuzzing process of DBMSs towards increased code coverage and high statement validity. APOLLO [28] is a system for detecting performance regression bugs in DBMSs. It increases the probability of including components from previously encountered performance issues. Cynthia [39] was proposed to test Object Relational Mappers (ORMs) and generates targeted databases dependent on generated abstract SQL queries, which are likely to return non-empty results. Query Plan Guidance (QPG) [22] guides testing towards exploring more unique query plans.\
 The targeted node in XPress was inspired by the pivot row in Pivoted Query Synthesis (PQS) [36], which was originally proposed to test relational DBMSs. PQSâ€™ and XPressâ€™ commonality is that they select a random element, in PQS, a row in the database, while for XPress, a node in an XML document, based on which they generate a query that is guaranteed to fetch the element. However, both the purpose and use of the targeted node and pivot row differ. In PQS, the pivot row is used both for test-case generation and to construct the test oracle, by evaluating an expression and ensuring that it evaluates to true for the pivot row so that it can be used in a query that is guaranteed to fetch the row. Doing so requires a naive reimplementation of all the DBMSsâ€™ operators that should be tested, which incurs a high implementation effort, as highlighted in follow-up work [? ]. In XPress, the targeted node is used only for test-case generation, to improve testing efficiency and to ensure non-empty intermediate results; to this end, XPress uses the XML processor to determine the result of the expression, rather than requiring the reimplementation of operators. In addition, for predicate rectification, XPress provides operator-specific rules, rather than relying on a generic one, aiming to generate more interesting test cases. The high-level idea of a pivot element also inspired other works; for example, recent work on Android testing introduced the concept of a  [40].(1) Shuxin Li, Southern University of Science and Technology China and Work done during an internship at the National University of Singapore (shuxin.li.lv@gmail.com);(2) Manuel Rigger, National University of Singapore Singapore (rigger@nus.edu.sg).]]></content:encoded></item><item><title>An Analysis of BaseX Historical Bug Reports</title><link>https://hackernoon.com/an-analysis-of-basex-historical-bug-reports?source=rss</link><author>XPath</author><category>tech</category><pubDate>Wed, 12 Mar 2025 01:39:26 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[4.4 Analysis of BaseX Historical Bug ReportsUnlike formal verification approaches, automatic testing approaches might miss bugs in the system tested. Due to the lack of ground truth, we cannot generally determine which bugs are overlooked by our approach. However, as a best-effort approach, we studied historical bug reports in order to determine whether XPress could have found them.\
 We analyzed all historical BaseX bug reports in its GitHub bug tracker. We selected BaseX, because the majority of issues are closed (1618 out of 1640). The issue tracker of BaseX is used for confirmed bug reports filtered from reports from the mailing list, and the BaseX maintainers carefully label and document them. For these reasons, it was easy to identify and classify the underlying problem of each bug report.\
. We manually analyzed all historical bug issues until 2023 Apr 17 in BaseX, which were 1597 issues, after excluding the issues we reported. To confine the study of bug reports within the scope of XPath, we selected bug reports triggered by only XPath expressions. To determine whether a bug could be theoretically found by XPress, we mainly checked three aspects of the reports. For XPress to cover the test case, both the XML document and the XPath expression in the test case should not include any unimplemented functions or language features. Second, we could construct the sections and the predicate tree structure of XPress for involved predicates to form the pattern of the bug-inducing XPath expression. Third, XML processors should disagree on the result set. Note that this is a best-effort approach, because we might both incorrectly conclude that XPress might find a bug (e.g., it might be unlikely that the test case would be generated in practice) or incorrectly conclude that a bug cannot be found even when a different test-case within the reach of XPress would trigger the same underlying bug.\
. Out of the total 78 bugs that we collected, we identified 20 bugs that could have been detected by XPress. For the other 58 bugs, we identified 4 kinds of bugs that XPress would have failed to find, namely due to (1) unimplemented functionalities (51 cases), (2) invalid inputs where the expected result would be an error (6 cases), (3) processors producing different results (2 cases), and (4) miscellaneous other issues (6 cases). Bugs belonging to more than one group are included in all involved groups. The differential testing oracle fails to detect the bugs with processors producing different results, while we consider the other categories mostly as implementation limitations in test-case generation. Therefore, out of all 78 bugs, 76 bugs (97%) could be detected through differential testing. This further demonstrates the effectiveness of employing a differential testing oracle for XPath-related testing.\
Unimplemented functionalities. Most uncovered bug reports are due to unimplemented functionalities. Unsupported functions include constructors defined by the XML or XPath language standards, array and map functions, and also constructors of derived datatypes [2], such as xs:NMtokens. Given enough time, it would be straightforward to implement them in XPress. For/while loops, variable declaration, if-else conditional expressions, and self-defined functions are also unimplemented. These could be supported based on approaches that have been proposed in the context of compiler testing [32, 43]. Neither the XML documents nor XPath expressions that XPress constructs involve namespaces, which allow distinguishing items with the same tag name. They could be integrated into the XPress test-case generator. By implementing all these features, an additional 38 bugs (48%) could have been found.\
 Bug reports grouped into expected is error refers to invalid test cases, which are successfully executed instead of throwing an error. XPress constructs both syntactically and semantically valid expressions and therefore could not detect bugs within this category. However, the differential testing oracle could detect these bugs by comparing the errors of the different XML processors.\
 The different result category contains queries for which different processors intentionally produce different results, which shows the limitation of the differential testing oracle. One example is the function id, which selects nodes with xml:id attributes. BaseX takes attributes named as id as xml:id attributes, while Saxon and eXist-DB require an explicit declaration.(1) Shuxin Li, Southern University of Science and Technology China and Work done during an internship at the National University of Singapore (shuxin.li.lv@gmail.com);(2) Manuel Rigger, National University of Singapore Singapore (rigger@nus.edu.sg).]]></content:encoded></item><item><title>Index Support in BaseX, eXist-DB, Saxon, and libxml2: Explained</title><link>https://hackernoon.com/index-support-in-basex-exist-db-saxon-and-libxml2-explained?source=rss</link><author>XPath</author><category>tech</category><pubDate>Wed, 12 Mar 2025 01:34:36 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[We are aware of only one automated testing approach that has been proposed to test XML processors [41]. It tackled the test oracle problem by using differential testing by comparing the results of Microsoftâ€™s SQLServer with and without using indexes. Their approach was specifically designed to test SQLServerâ€™s index support and is not publicly available. Due to the narrow testing scope, and since the tool is not publicly available, we could not conduct experiments to directly compare the approaches. However, we further extended our tool to support differential testing with index configurations. Both approaches are complementary, as XPress could not only use differential testing among various XML processors, but also create or omit indexes to find additional bugs.\
Index support in BaseX, eXist-DB, Saxon, and libxml2. Database indexes are data structures built to speed up data retrieval [31] and are DBMS-specific. Not all XML processors are DBMSsâ€”as in-memory processors, Saxon and libxml2 lack support for indexes. BaseX and eXist-DB both enable structural indexes, such as storing all distinct paths of nodes by default. For value indexes to optimize querying on content values, BaseX creates text index and attribute index automatically. Users can further define additional indexes. Additionally, BaseX provides token indexes, which apply to specific functions, such as contains-token. eXist supports range indexes, which could be defined for specific nodes or attributes to speed up related comparison searches on their contents.\
. We tested eXistâ€™s range index and BaseXâ€™s token index using the XPath expression generation approach as described in Section 3.2. Due to the found unfixed bugs in eXist, we conducted differential testing within eXist by checking the results with and\
without range index definition. For BaseX, we defined a token index and compared its results directly with the results of Saxon.\
. Throughout the testing method, we detected one additional bug for BaseX[10] and found no additional bugs in eXist. We reported the found bug shown in Figure 9 to the BaseX developers, who quickly fixed it. The query selects all nodes with tag name M in the document which holds attribute v that contains token "a". BaseX returned node M without token index, as expected, while unexpectedly returning an empty result set when not using an index. Overall, while the results suggest that using or removing indexes might find additional bugs, doing so had low effectiveness. A potential explanation could be that our test-case generation approach does not consider when indexes could be applied, which might result in low testing efficiency.[10] https://github.com/BaseXdb/basex/issues/2222(1) Shuxin Li, Southern University of Science and Technology China and Work done during an internship at the National University of Singapore (shuxin.li.lv@gmail.com);(2) Manuel Rigger, National University of Singapore Singapore (rigger@nus.edu.sg).]]></content:encoded></item><item><title>Geothermal Could Power Nearly All New Data Centers Through 2030</title><link>https://news.slashdot.org/story/25/03/11/2149222/geothermal-could-power-nearly-all-new-data-centers-through-2030?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 12 Mar 2025 01:25:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[An anonymous reader quotes a report from TechCrunch: There's a power crunch looming as AI and cloud providers ramp up data center construction. But a new report suggests that a solution lies beneath their foundations. Advanced geothermal power could supply nearly two-thirds of new data center demand by 2030, according to an analysis by the Rhodium Group. The additions would quadruple the amount of geothermal power capacity in the U.S. -- from 4 gigawatts to about 16 gigawatts -- while costing the same or less than what data center operators pay today. In the western U.S., where geothermal resources are more plentiful, the technology could provide 100% of new data center demand. Phoenix, for example, could add 3.8 gigawatts of data center capacity without building a single new conventional power plant.
 
Geothermal resources have enormous potential to provide consistent power. Historically, geothermal power plants have been limited to places where Earth's heat seeps close to the surface. But advanced geothermal techniques could unlock 90 gigawatts of clean power in the U.S. alone, according to the U.S. Department of Energy. [...] Because geothermal power has very low running costs, its price is competitive with data centers' energy costs today, the Rhodium report said. When data centers are sited similarly to how they are today, a process that typically takes into account proximity to fiber optics and major metro areas, geothermal power costs just over $75 per megawatt hour. But when developers account for geothermal potential in their siting, the costs drop significantly, down to around $50 per megawatt hour.
 
The report assumes that new generating capacity would be "behind the meter," which is what experts call power plants that are hooked up directly to a customer, bypassing the grid. Wait times for new power plants to connect to the grid can stretch on for years. As a result, behind the meter arrangements have become more appealing for data center operators who are scrambling to build new capacity.]]></content:encoded></item><item><title>Finding XPath Bugs in XML Document Processors: Existing-Generator Baselines and More</title><link>https://hackernoon.com/finding-xpath-bugs-in-xml-document-processors-existing-generator-baselines-and-more?source=rss</link><author>XPath</author><category>tech</category><pubDate>Wed, 12 Mar 2025 00:54:13 +0000</pubDate><source url="https://hackernoon.com/">Tech - HackerNoon</source><content:encoded><![CDATA[Existing-generator baselines. We considered the only twoâ€”to the best of our knowledgeâ€”approaches to generate XPath expressions. Neither of them was specifically designed to be combined with a XPath test oracle. XQgen [42] generates XPath queries for micro benchmarking. Its generated predicates only check for sub-element existence. The XQuery generator designed by Todic and Uzelac [41] generates XPath queries for automatically testing index support in DBMSs. Given that indexes apply only to sargable queries (i.e., simple comparisons), the expressions it generates are simple. Both approaches generate XPath expressions based on an XML schema, while XPress generates XPath expressions based on the actual XML document. Based on this, we expect both of them to have low applicability for our differential-testing approach. Given that neither implementations are publicly available, we re-implemented them based on the description in the papers.\
Self-constructed baselines. We also constructed our own baselines to investigate the efficiency of the separate components of XPress. XPress has two main components, namely (1) the targeted predicate generation by using the targeted node to refer to existing nodes and attributes and (2) the predicate rectification to avoid empty result sets. To evaluate the effect of the components individually, we enabled them individually to test whether they improve XPressâ€™s bug detection efficiency.\
. We considered four configurations for our selfconstructed baselines. Apart from our proposed approach introduced in Section 3.2 as (1) Targeted, we derive configuration (2) Targeted without Rectification, (3) Untargeted with Rectification, and (4) Untargeted without Rectification. In (2) Targeted without Rectification, we disable the rectification process, which would otherwise ensure targeted node selection. Since selecting a targeted node for predicate generation guidance always requires at least one node in the result set, we stop generating new sections after an empty result set is produced. In (3) Untargeted with Rectification, we generate predicates without using targeted node information to supply parameters that reference existent context and trigger corner cases for function nodes, while keeping the rectification to ensure that at least one node from the candidate set is included in the result set. In (4) Untargeted without rectification, we remove both components to generate predicates randomly, while omitting rectification.\
. We set each baseline to run for 24 hours [30]. We repeated each experiment 10 times to account for potential performance deviations, and report the arithmetic mean for all metrics. As our testing target, we selected BaseX 10.4, which is the BaseX version that we first started testing. The reason for selecting BaseX as a representative is that we found most bugs in BaseX and all bugs were fixed, allowing us to determine the number of unique bugs we found in a testing campaign by deduplicating bug-inducing test cases automatically. Specifically, given two bug-inducing test cases, we could determine whether they trigger the same underlying bug by identifying their fix commits; only if their associated fix commit are different, do we consider the bugs unique. This is a best-effort technique, as, for example, one fix commit might address multiple bugs. We disabled the generation of the has-children functions as well as using relative XPath expressions in predicates, as they consistently lead to crashes, triggering known bugs.\
Results of existing generators. Neither XQGen nor the Combined XML/XQuery generator found bugs in our experiment. This is expected, as previously proposed approaches were not designed for\
automated testing. As mentioned above, XQGen generates predicates that only check for element existence. The XQuery generator designed by Todic and Uzelac generates simple predicates that include at most one comparison operator.\
Results of different configurations. As Figure 8 shows, our proposed approach, Targeted outperforms the other configurations. Within 24 hours, it found the most number of unique bugs (namely 12.5). Both configurations with targeted generation clearly outperformed the untargeted approaches, while rectification shows a similar performance in the speed of bug detection. As shown in Table 3, both targeted generation and rectification reduce the testing throughput, as they obtain intermediate results using the XML processor under test. Despite generating only 50% of the number of test cases as compared to (4) Untargeted without Rectification, (1) Targeted detected 20Ã— more bug-inducing test cases and 2Ã— more unique bugs. The results show that selecting a target node to guide the XPath generation process improves testing efficiency significantly. As observed above when discussing the small-scope hypothesis, most of the bugs that we found can be reproduced using a single section, explaining the limited effectiveness of rectification. However, we still believe that rectification is an important component, since without it, bugs requiring multiple sections with non-empty results could hardly be found.\
 We collected code coverage for three processorsâ€™ core modules for XPress for 24 hours [30] of execution. The result is shown in Table 4. To put the numbers in relation, we collected coverage also for the projectsâ€™ test suites; Saxon has no publicly available test suites and is therefore excluded. For the three XML processors, the line coverage ranged from 15% to 20%, and the\
branch coverage ranged from 10% to 16%. The coverage percentages are low, which is expected. The main reason for low code coverage is that XML processors typically also have other components than XPath processing. Taking BaseX as an example, around 21% of uncovered code was GUI-related, 10% was due to lack of full-text functionality support, and 5% were database commands. In Saxon, as another example, XSLT modules have not been covered. A further 18% uncovered code in BaseX involved unimplemented functions; it would be straightforward to implement many additional ones, such as math functions, but the many functions available would make this a tedious task. In Section 4.4, we detail unsupported XPath features, implementing which might allow us to find more bugs. XPressâ€™s test-case generation process primarily aims at generating semantically valid expressions, which results in low error-checking branch coverage, quantifying which is difficult, as the relevant code is spread throughout the code base.(1) Shuxin Li, Southern University of Science and Technology China and Work done during an internship at the National University of Singapore (shuxin.li.lv@gmail.com);(2) Manuel Rigger, National University of Singapore Singapore (rigger@nus.edu.sg).]]></content:encoded></item><item><title>Team Behind Las Vegas Sphere Plans 5,000-Capacity &apos;Mini-Spheres&apos;</title><link>https://entertainment.slashdot.org/story/25/03/11/2143232/team-behind-las-vegas-sphere-plans-5000-capacity-mini-spheres?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 12 Mar 2025 00:45:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Sphere Entertainment Co, the company behind the Las Vegas Sphere, said they are considering opening scaled-down versions of the immersive venue in other cities. AV Magazine reports: While this has been been feasible for its high-profile residencies such as U2, the Eagles, Dead & Company and Anyma, smaller venues could attract a broader range of artists who might not have the budget or demand to fill the flagship Las Vegas location. By scaling down the size while retaining the signature technology, Sphere Entertainment Co can offer a similar spectacle at a more sustainable cost for artists and spectators.
 
The possibility of mini-Spheres follows news that a full-scale venue will open in the UAE as a result of a partnership between Sphere Entertainment Co and the Department of Culture and Tourism -- Abu Dhabi. Beyond concerts, the Las Vegas Sphere has proven successful with immersive films such as V-U2: An Immersive Concert Film and the Sphere Expeience featuring Darren Aronofsky's Postcard from Earth, which In January passed 1,000 screenings. "As we enter a new fiscal year, we see significant opportunities to drive our Sphere business forward in Las Vegas and beyond," said Dolan. "We believe we are on a path toward realizing our vision for this next-generation medium and generating long-term shareholder value."]]></content:encoded></item><item><title>GStreamer 1.26 Released With Vulkan Improvements, H.266/VVC + LCEVC + JPEG-XS Support</title><link>https://www.phoronix.com/news/GStreamer-1.26-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Wed, 12 Mar 2025 00:19:50 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[GStreamer 1.26 is out today as the newest major feature release for this widely-used open-source multimedia framework...]]></content:encoded></item><item><title>AMD Ryzen 9 9950X3D With 3D V-Cache Impresses In Launch Day Testing</title><link>https://hardware.slashdot.org/story/25/03/11/2126204/amd-ryzen-9-9950x3d-with-3d-v-cache-impresses-in-launch-day-testing?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>BeauHD</author><category>tech</category><pubDate>Wed, 12 Mar 2025 00:02:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[MojoKid writes: AMD just launched its latest flagship desktop processors, the Ryzen 9 9950X3D. Ryzen 9 9950X3D is a 16-core/32-thread, dual-CCD part with a base clock of 4.3GHz and a max boost clock of 5.7GHz. There's also 96MB of second-gen 3D V-Cache on board. Standard Ryzen 9000 series processors feature 32MB of L3 cache per compute die, but with the Ryzen 9 9950X3D, one compute die is outfitted with an additional 96MB of 3D V-Cache, bringing the total L3 up to 128MB (144MB total cache). The CCD outfitted with 3D V-Cache operates at more conservative voltages and frequencies, but the bare compute die is unencumbered.
 
The Ryzen 9 9950X3D turns out to be a high-performance, no-compromise desktop processor. Its complement of 3D V-Cache provides tangible benefits in gaming, and AMD's continued work on the platform's firmware and driver software ensures that even with the Ryzen 9 9950X3D's asymmetrical CCD configuration, performance is strong across the board. At $699, it's not cheap but its a great CPU for gaming and content creation, and one of the most powerful standard desktop CPUs money can buy currently.]]></content:encoded></item><item><title>Microsoft is Replacing Remote Desktop With Its New Windows App</title><link>https://it.slashdot.org/story/25/03/11/2037221/microsoft-is-replacing-remote-desktop-with-its-new-windows-app?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 11 Mar 2025 23:20:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[Microsoft is ending support of its Remote Desktop app for Windows on May 27th. From a report: If you use the Remote Desktop app to connect to Windows 365, Azure Virtual Desktop, or Microsoft Dev Box machines then you'll have to transition to the Windows app instead. 

The new Windows app, which launched in September, includes multimonitor support, dynamic display resolutions, and easy access to cloud PCs and virtual desktops. Microsoft says "connections to Windows 365, Azure Virtual Desktop, and Microsoft Dev Box via the Remote Desktop app from the Microsoft Store will be blocked after May 27th, 2025."]]></content:encoded></item><item><title>Preprint Sites bioRxiv and medRxiv Launch New Era of Independence</title><link>https://science.slashdot.org/story/25/03/11/2031217/preprint-sites-biorxiv-and-medrxiv-launch-new-era-of-independence?utm_source=rss1.0mainlinkanon&amp;utm_medium=feed</link><author>msmash</author><category>tech</category><pubDate>Tue, 11 Mar 2025 22:40:00 +0000</pubDate><source url="https://slashdot.org/">Slashdot</source><content:encoded><![CDATA[A new chapter has begun for two of the world's most popular preprint platforms, bioRxiv and medRxiv, with the launch of a non-profit organization that will manage them, their co-founders announced today. From a report: The servers allow researchers to share manuscripts for free before peer review and have become an integral part of publishing biology and medical research. Until now, they had been managed by Cold Spring Harbor Laboratory (CSHL) in New York. The new organization, named openRxiv, will have a board of directors and a scientific and medical advisory board. It is supported by a fresh US$16-million grant from the Chan Zuckerberg Initiative (CZI), the projects' main financial backer. 

"It's just exciting to see this key piece of infrastructure really get the attention that it deserves as a dedicated initiative," says Katie Corker, executive director of ASAPbio, a scientist-driven non-profit organization, which is based in San Francisco, California. Preprints are "the backbone of the scientific publishing ecosystem, maybe especially at the current moment, when there's a lot of worries about who has control of information." 

The launch of openRxiv "reflects a maturation of the projects," which started as an experiment at CSHL, says Richard Sever, a co-founder of both servers and chief science and strategy officer at openRxiv. It has "become so important that they should have their own organization running them, which is focused on the long-term sustainability of the servers, as opposed to being a side project within a big research institution," says Sever.]]></content:encoded></item><item><title>OpenAI pushes AI agent capabilities with new developer API</title><link>https://arstechnica.com/ai/2025/03/openai-pushes-ai-agent-capabilities-with-new-developer-api/</link><author>Benj Edwards</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2025/03/agent_test-1152x648.jpg" length="" type=""/><pubDate>Tue, 11 Mar 2025 20:42:17 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT â€“ Ars Technica</source><content:encoded><![CDATA[The AI industry is doing its best to will "agents"â€”pieces of AI-driven software that can perform multistep actions on your behalfâ€”into reality. Several tech companies, including Google, have emphasized agentic features recently, and in January, OpenAI CEO Sam Altman wrote that 2025 would be the year AI agents "join the workforce."OpenAI is working to make that promise happen. On Tuesday, OpenAI unveiled a new "Responses API" designed to help software developers create AI agents that can perform tasks independently using the company's AI models. The Responses API will eventually replace the current Assistants API, which OpenAI plans to retire in the first half of 2026.With the new offering, users can develop custom AI agents that scan company files with a file search utility that rapidly checks company databases (with OpenAI promising not to train its models on these files) and navigates websitesâ€”similar to functions available through OpenAI's Operator agent, whose underlying Computer-Using Agent (CUA) model developers can also access to enable automation of tasks like data entry and other operations.]]></content:encoded></item><item><title>Apple patches 0-day exploited in â€œextremely sophisticated attackâ€</title><link>https://arstechnica.com/security/2025/03/apple-patches-0-day-exploited-in-extremely-sophisticated-attack/</link><author>Dan Goodin</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2025/03/iPhone-16e-notch-1152x648.jpg" length="" type=""/><pubDate>Tue, 11 Mar 2025 20:26:11 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT â€“ Ars Technica</source><content:encoded><![CDATA[Apple on Tuesday patched a critical zero-day vulnerability in virtually all iPhones and iPad models it supports and said it may have been exploited in â€œan extremely sophisticated attack against specific targeted individualsâ€ using older versions of iOS.The vulnerability, tracked as CVE-2025-24201, resides in Webkit, the browser engine driving Safari and all other browsers developed for iPhones and iPads. Devices affected include the iPhone XS and later, iPad Pro 13-inch, iPad Pro 12.9-inch 3rd generation and later, iPad Pro 11-inch 1st generation and later, iPad Air 3rd generation and later, iPad 7th generation and later, and iPad mini 5th generation and later. The vulnerability stems from a bug that wrote to out-of-bounds memory locations.â€œImpact: Maliciously crafted web content may be able to break out of Web Content sandbox,â€ Apple wrote in a bare-bones advisory. â€œThis is a supplementary fix for an attack that was blocked in iOS 17.2. (Apple is aware of a report that this issue may have been exploited in an extremely sophisticated attack against specific targeted individuals on versions of iOS before iOS 17.2.)â€]]></content:encoded></item><item><title>Intel Overclocking Watchdog Driver Posted For The Linux Kernel</title><link>https://www.phoronix.com/news/Intel-Overclocking-Watchdog</link><author>Michael Larabel</author><category>tech</category><pubDate>Tue, 11 Mar 2025 18:10:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[An unexpected patch on the Linux kernel mailing list today by a Siemens engineer is implementing a driver for the Intel Over-Clocking Watchdog...]]></content:encoded></item><item><title>Nouveau On NVIDIA Turing GPUs &amp; Newer Will Now Prefer NVK+Zink For OpenGL</title><link>https://www.phoronix.com/news/Nouveau-Turing-Zink-NVK-OpenGL</link><author>Michael Larabel</author><category>tech</category><pubDate>Tue, 11 Mar 2025 16:40:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[As a sign of the times for both the NVK open-source NVIDIA Vulkan driver within Mesa and the generic Zink OpenGL-on-Vulkan code, with next quarter's Mesa 25.1 release when using a NVIDIA Turing GPU or newer with the Nouveau driver stack it will now default to using Zink atop NVK for OpenGL rather than the existing NVC0 Gallium3D driver...]]></content:encoded></item><item><title>CrossOver 25.0 Announced - Built Atop Wine 10.0 For Linux &amp; macOS</title><link>https://www.phoronix.com/news/CrossOver-25.0-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Tue, 11 Mar 2025 15:19:39 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[CodeWeavers that continues to be the largest patron to the development of the open-source Wine software announced today CrossOver 25.0 as the newest version of their commercial downstream...]]></content:encoded></item><item><title>Google&apos;s Pixel Watch Knows When Your Heart Stops Beating</title><link>https://spectrum.ieee.org/heart-attack-smartwatch</link><author>Elissa Welle</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NjcwNTY3Mi9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc3Mjg3MTEyOH0.GzE5L4Fb8ueiISIYEvX3bBMzEfsD14la9xuOopm36Mk/image.jpg?width=600" length="" type=""/><pubDate>Tue, 11 Mar 2025 14:30:03 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[The feature set to roll out in the US this month will call 911 if you canâ€™t]]></content:encoded></item><item><title>AMD Ryzen 9 9950X3D Delivers Excellent Performance For Linux Developers, Creators &amp; Technical Computing</title><link>https://www.phoronix.com/review/amd-ryzen-9-9950x3d-linux</link><author>Michael Larabel</author><category>tech</category><pubDate>Tue, 11 Mar 2025 13:00:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Ahead of tomorrow's availability of the Ryzen 9 9900X3D and Ryzen 9 9950X3D CPUs in retail channels, today the embargo lifts on being able to deliver Ryzen 9 9950X3D reviews and performance benchmarks. Simply put, for Linux creators, developers, enthusiasts, and others running technical computing workloads and other similar tasks on their desktop, the Ryzen 9 9950X3D with its 16 cores / 32 threads and 144MB total cache makes for an excellent desktop CPU. In this review are around 400 Linux benchmarks looking at the captivating performance and competitive power efficiency of the AMD Ryzen 9 9950X3D.]]></content:encoded></item><item><title>GNOME Dash To Panel Extension Development Being &quot;Passed On&quot;</title><link>https://www.phoronix.com/news/GNOME-Dash-To-Panel-Pause</link><author>Michael Larabel</author><category>tech</category><pubDate>Tue, 11 Mar 2025 12:52:34 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The GNOME Dash To Panel extension that allows moving the dash into the GNOME main panel has proven popular with GNOME desktop users for an integrated icon taskbar and status panel on GNOME Shell. Unfortunately though one of the main developers to Charles Gagnon is "passing on" development of the extension moving forward...]]></content:encoded></item><item><title>Microsoftâ€™s Muse AI Edits Video Games on the Fly</title><link>https://spectrum.ieee.org/ai-video-games</link><author>Matthew S. Smith</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81Njc1NjE4Ny9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc2OTg5MjY5MH0.b8svwZw9hhFzoCD1y3INt5Zdwo9jCK1sB1mO7oKUG_E/image.jpg?width=600" length="" type=""/><pubDate>Tue, 11 Mar 2025 12:00:03 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Muse is a proof of concept for more consistent AI gameplay]]></content:encoded></item><item><title>Why extracting data from PDFs is still a nightmare for data experts</title><link>https://arstechnica.com/ai/2025/03/why-extracting-data-from-pdfs-is-still-a-nightmare-for-data-experts/</link><author>Benj Edwards</author><category>tech</category><enclosure url="https://cdn.arstechnica.net/wp-content/uploads/2025/03/digitizing_a_book_header_3-1152x648.jpg" length="" type=""/><pubDate>Tue, 11 Mar 2025 11:15:36 +0000</pubDate><source url="https://arstechnica.com/">Biz &amp; IT â€“ Ars Technica</source><content:encoded><![CDATA[For years, businesses, governments, and researchers have struggled with a persistent problem: How to extract usable data from Portable Document Format (PDF) files. These digital documents serve as containers for everything from scientific research to government records, but their rigid formats often trap the data inside, making it difficult for machines to read and analyze."Part of the problem is that PDFs are a creature of a time when print layout was a big influence on publishing software, and PDFs are more of a 'print' product than a digital one," Derek Willis, a lecturer in Data and Computational Journalism at the University of Maryland, wrote in an email to Ars Technica. "The main issue is that many PDFs are simply pictures of information, which means you need Optical Character Recognition software to turn those pictures into data, especially when the original is old or includes handwriting."Computational journalism is a field where traditional reporting techniques merge with data analysis, coding, and algorithmic thinking to uncover stories that might otherwise remain hidden in large datasets, which makes unlocking that data a particular interest for Willis.]]></content:encoded></item><item><title>UK Greenlights Amazon Kuiper, Starlink Faces New Rival</title><link>https://spectrum.ieee.org/starlink-internet-kuiper-competition</link><author>Margo Anderson</author><category>tech</category><enclosure url="https://spectrum.ieee.org/media-library/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpbWFnZSI6Imh0dHBzOi8vYXNzZXRzLnJibC5tcy81NjYzMTg2Ni9vcmlnaW4uanBnIiwiZXhwaXJlc19hdCI6MTc3MDc2MjA5Nn0.bmrElVf6-ehF4bT2-fpNTfzZKS4uhqwZHxx_YwvZ1bM/image.jpg?width=600" length="" type=""/><pubDate>Tue, 11 Mar 2025 11:00:03 +0000</pubDate><source url="https://spectrum.ieee.org/">IEEE Spectrum</source><content:encoded><![CDATA[Satellite broadband tech squares off, while also trying to avoid collisions]]></content:encoded></item><item><title>COBOL Language Frontend Merged For GCC 15 Compiler</title><link>https://www.phoronix.com/news/GCC-15-Merges-COBOL</link><author>Michael Larabel</author><category>tech</category><pubDate>Tue, 11 Mar 2025 10:22:00 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[A big albeit late feature landed today for the upcoming GCC 15 compiler... The COBOL programming language front-end has been merged!..]]></content:encoded></item><item><title>Linux Kernel Patches Posted For The ESWIN EIC7700 SoC + SiFive HiFive Premier P550</title><link>https://www.phoronix.com/news/Linux-Patches-EIC7700-HiFive</link><author>Michael Larabel</author><category>tech</category><pubDate>Tue, 11 Mar 2025 10:10:51 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Patches were posted to the Linux Kernel Mailing List this morning for wiring up the ESWIN EIC7700 RISC-V SoC support and the most notable board using this SoC so far, the SiFive HiFive Premier P550...]]></content:encoded></item><item><title>Servo Makes Improvements To Its Demo Browser &amp; Embedding API</title><link>https://www.phoronix.com/news/Servo-February-2025</link><author>Michael Larabel</author><category>tech</category><pubDate>Tue, 11 Mar 2025 09:55:39 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[The Servo open-source web engine is out with its February 2025 status update to highlight work on the engine itself as well as its demo browser and embed API capabilities for using Servo by other applications...]]></content:encoded></item><item><title>AMD Announces The EPYC Embedded 9005 Series</title><link>https://www.phoronix.com/news/AMD-EPYC-Embedded-9005-Turin</link><author>Michael Larabel</author><category>tech</category><pubDate>Tue, 11 Mar 2025 09:11:30 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Since last year we have continued to be impressed by the AMD EPYC 9005 "Turin" server processors while today they are announcing the EPYC Embedded 9005 line-up. The AMD EPYC Embedded 9005 Series processors are much like the EPYC 9005 series processors but with a few differences...]]></content:encoded></item><item><title>FreeBSD 13.5 Released With Device Driver Updates &amp; Fixes</title><link>https://www.phoronix.com/news/FreeBSD-13.5-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Tue, 11 Mar 2025 00:53:25 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[FreeBSD 13.5 is out today as the final update to the FreeBSD 13 series. Users should begin making plans for upgrading to the current FreeBSD 14 stable series or eyeing the future FreeBSD 15.0 release...]]></content:encoded></item><item><title>OpenZFS 2.3.1 Released With Linux 6.13 Compatibility, Many Fixes</title><link>https://www.phoronix.com/news/OpenZFS-2.3.1-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Mon, 10 Mar 2025 20:53:01 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Building off the big OpenZFS 2.3 feature release from January, OpenZFS 2.3.1 is out today with Linux 6.13 kernel compatibility as well as various bug fixes...]]></content:encoded></item><item><title>Mir 2.20 Brings Focus Stealing Prevention, Workaround/Quirk Fixes</title><link>https://www.phoronix.com/news/Mir-2.20-Released</link><author>Michael Larabel</author><category>tech</category><pubDate>Mon, 10 Mar 2025 19:02:28 +0000</pubDate><source url="https://www.phoronix.com/">Phoronix</source><content:encoded><![CDATA[Mir 2.20 is out today as the newest version of this Canonical-developed Wayland compositor and set of libraries for developing Wayland-based shells...]]></content:encoded></item></channel></rss>